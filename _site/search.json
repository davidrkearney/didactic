[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science",
    "section": "",
    "text": "news\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\nDRK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2022\n\n\nDRK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 19, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 4, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 29, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 7, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "",
    "text": "Spark uses Java Virtual Machine (JVM) objects Resilient Distributed Datasets (RDD) which are calculated and stored in memory.\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructField,StringType,IntegerType,StructType, DoubleType, FloatType\nfrom pyspark.sql.functions import *\n\n\ndata_schema = [\nStructField(\"_c0\", IntegerType(), True)\n,StructField(\"province\", StringType(), True)\n,StructField(\"specific\", DoubleType(), True)\n,StructField(\"general\", DoubleType(), True)\n,StructField(\"year\", IntegerType(), True)\n,StructField(\"gdp\", FloatType(), True)\n,StructField(\"fdi\", FloatType(), True)\n,StructField(\"rnr\", DoubleType(), True)\n,StructField(\"rr\", FloatType(), True)\n,StructField(\"i\", FloatType(), True)\n,StructField(\"fr\", IntegerType(), True)\n,StructField(\"reg\", StringType(), True)\n,StructField(\"it\", IntegerType(), True)\n]\n\nfinal_struc = StructType(fields=data_schema)\n\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").schema(final_struc).option(\"header\", True).load(file_location)\n\n#df.printSchema()\n\ndf.show()\n\n\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|     fdi| rnr|       rr|        i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661.0| 0.0|      0.0|      0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443.0| 0.0|      0.0|      0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673.0| 0.0|      0.0|      0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131.0|null|     null|     null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0|      0.0|      0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672.0| 0.0|      0.0|      0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0|      0.0|      0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0|      0.0|      0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669.0| 0.0|      0.0|      0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000.0| 0.0|      0.0|0.3243243|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354.0| 0.0|      0.0|0.3243243|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892.0| 0.0|      0.0|0.3243243|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290.0|null|     null|     null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286.0| 0.0|      0.0|      0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800.0| 0.0|      0.0|     0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525.0| 0.0|      0.0|     0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368.0| 0.0|      0.0|     0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818.0| 0.0|      0.0|     0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464.0| 0.0|      0.0|     0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126.0| 0.0|0.7948718|      0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#using-topandas-to-look-at-the-data",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#using-topandas-to-look-at-the-data",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Using toPandas to look at the data",
    "text": "Using toPandas to look at the data\n\ndf.limit(10).toPandas()\n\n\n\n\n\n  \n    \n      \n      _c0\n      province\n      specific\n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      reg\n      it\n    \n  \n  \n    \n      0\n      0\n      Anhui\n      147002.0\n      NaN\n      1996\n      2093.300049\n      50661.0\n      0.0\n      0.0\n      0.000000\n      1128873.0\n      East China\n      631930\n    \n    \n      1\n      1\n      Anhui\n      151981.0\n      NaN\n      1997\n      2347.320068\n      43443.0\n      0.0\n      0.0\n      0.000000\n      1356287.0\n      East China\n      657860\n    \n    \n      2\n      2\n      Anhui\n      174930.0\n      NaN\n      1998\n      2542.959961\n      27673.0\n      0.0\n      0.0\n      0.000000\n      1518236.0\n      East China\n      889463\n    \n    \n      3\n      3\n      Anhui\n      285324.0\n      NaN\n      1999\n      2712.340088\n      26131.0\n      NaN\n      NaN\n      NaN\n      1646891.0\n      East China\n      1227364\n    \n    \n      4\n      4\n      Anhui\n      195580.0\n      32100.0\n      2000\n      2902.090088\n      31847.0\n      0.0\n      0.0\n      0.000000\n      1601508.0\n      East China\n      1499110\n    \n    \n      5\n      5\n      Anhui\n      250898.0\n      NaN\n      2001\n      3246.709961\n      33672.0\n      0.0\n      0.0\n      0.000000\n      1672445.0\n      East China\n      2165189\n    \n    \n      6\n      6\n      Anhui\n      434149.0\n      66529.0\n      2002\n      3519.719971\n      38375.0\n      0.0\n      0.0\n      0.000000\n      1677840.0\n      East China\n      2404936\n    \n    \n      7\n      7\n      Anhui\n      619201.0\n      52108.0\n      2003\n      3923.110107\n      36720.0\n      0.0\n      0.0\n      0.000000\n      1896479.0\n      East China\n      2815820\n    \n    \n      8\n      8\n      Anhui\n      898441.0\n      349699.0\n      2004\n      4759.299805\n      54669.0\n      0.0\n      0.0\n      0.000000\n      NaN\n      East China\n      3422176\n    \n    \n      9\n      9\n      Anhui\n      898441.0\n      NaN\n      2005\n      5350.169922\n      69000.0\n      0.0\n      0.0\n      0.324324\n      NaN\n      East China\n      3874846"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#renaming-columns",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#renaming-columns",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Renaming Columns",
    "text": "Renaming Columns\n\ndf = df.withColumnRenamed(\"reg\",\"region\")\n\n\n\n\n\n\ndf.limit(10).toPandas()\n\n\n\n\n\n  \n    \n      \n      _c0\n      province\n      specific\n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      region\n      it\n    \n  \n  \n    \n      0\n      0\n      Anhui\n      147002.0\n      NaN\n      1996\n      2093.300049\n      50661.0\n      0.0\n      0.0\n      0.000000\n      1128873.0\n      East China\n      631930\n    \n    \n      1\n      1\n      Anhui\n      151981.0\n      NaN\n      1997\n      2347.320068\n      43443.0\n      0.0\n      0.0\n      0.000000\n      1356287.0\n      East China\n      657860\n    \n    \n      2\n      2\n      Anhui\n      174930.0\n      NaN\n      1998\n      2542.959961\n      27673.0\n      0.0\n      0.0\n      0.000000\n      1518236.0\n      East China\n      889463\n    \n    \n      3\n      3\n      Anhui\n      285324.0\n      NaN\n      1999\n      2712.340088\n      26131.0\n      NaN\n      NaN\n      NaN\n      1646891.0\n      East China\n      1227364\n    \n    \n      4\n      4\n      Anhui\n      195580.0\n      32100.0\n      2000\n      2902.090088\n      31847.0\n      0.0\n      0.0\n      0.000000\n      1601508.0\n      East China\n      1499110\n    \n    \n      5\n      5\n      Anhui\n      250898.0\n      NaN\n      2001\n      3246.709961\n      33672.0\n      0.0\n      0.0\n      0.000000\n      1672445.0\n      East China\n      2165189\n    \n    \n      6\n      6\n      Anhui\n      434149.0\n      66529.0\n      2002\n      3519.719971\n      38375.0\n      0.0\n      0.0\n      0.000000\n      1677840.0\n      East China\n      2404936\n    \n    \n      7\n      7\n      Anhui\n      619201.0\n      52108.0\n      2003\n      3923.110107\n      36720.0\n      0.0\n      0.0\n      0.000000\n      1896479.0\n      East China\n      2815820\n    \n    \n      8\n      8\n      Anhui\n      898441.0\n      349699.0\n      2004\n      4759.299805\n      54669.0\n      0.0\n      0.0\n      0.000000\n      NaN\n      East China\n      3422176\n    \n    \n      9\n      9\n      Anhui\n      898441.0\n      NaN\n      2005\n      5350.169922\n      69000.0\n      0.0\n      0.0\n      0.324324\n      NaN\n      East China\n      3874846\n    \n  \n\n\n\n\n\n# df = df.toDF(*['year', 'region', 'province', 'gdp', 'fdi', 'specific', 'general', 'it', 'fr', 'rnr', 'rr', 'i', '_c0', 'specific_classification', 'provinceIndex', 'regionIndex'])"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#selecting-columns-of-interest",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#selecting-columns-of-interest",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Selecting Columns of Interest",
    "text": "Selecting Columns of Interest\n\ndf = df.select('year','region','province','gdp', 'fdi')\n\n\n\n\n\n\ndf.sort(\"gdp\").show()\n\n\n+----+---------------+--------+------+-------+\nyear|         region|province|   gdp|    fdi|\n+----+---------------+--------+------+-------+\n1996|Southwest China|   Tibet| 64.98|  679.0|\n1997|Southwest China|   Tibet| 77.24|   63.0|\n1998|Southwest China|   Tibet|  91.5|  481.0|\n1999|Southwest China|   Tibet|105.98|  196.0|\n2000|Southwest China|   Tibet| 117.8|    2.0|\n2001|Southwest China|   Tibet|139.16|  106.0|\n2002|Southwest China|   Tibet|162.04|  293.0|\n1996|Northwest China| Qinghai|184.17|  576.0|\n2003|Southwest China|   Tibet|185.09|  467.0|\n1997|Northwest China| Qinghai|202.79|  247.0|\n1996|Northwest China| Ningxia| 202.9| 2826.0|\n2004|Southwest China|   Tibet|220.34| 2699.0|\n1998|Northwest China| Qinghai|220.92| 1010.0|\n1997|Northwest China| Ningxia|224.59|  671.0|\n1999|Northwest China| Qinghai|239.38|  459.0|\n1998|Northwest China| Ningxia|245.44| 1856.0|\n2005|Southwest China|   Tibet| 248.8| 1151.0|\n2000|Northwest China| Qinghai|263.68|11020.0|\n1999|Northwest China| Ningxia|264.58| 5134.0|\n2006|Southwest China|   Tibet|290.76| 1522.0|\n+----+---------------+--------+------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#sorting-rdds-by-columns",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#sorting-rdds-by-columns",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Sorting RDDs by Columns",
    "text": "Sorting RDDs by Columns\n\nfrom pyspark.sql import functions as F\ndf.sort(F.desc(\"gdp\")).show()\n\n\n+----+-------------------+---------+--------+---------+\nyear|             region| province|     gdp|      fdi|\n+----+-------------------+---------+--------+---------+\n2007|South Central China|Guangdong|31777.01|1712603.0|\n2006|South Central China|Guangdong|26587.76|1451065.0|\n2007|         East China| Shandong|25776.91|1101159.0|\n2005|South Central China|Guangdong|22557.37|1236400.0|\n2006|         East China| Shandong|21900.19|1000069.0|\n2007|         East China|  Jiangsu|21742.05|1743140.0|\n2004|South Central China|Guangdong|18864.62|1001158.0|\n2007|         East China| Zhejiang|18753.73|1036576.0|\n2006|         East China|  Jiangsu|18598.69|1318339.0|\n2005|         East China| Shandong|18366.87| 897000.0|\n2003|South Central China|Guangdong|15844.64| 782294.0|\n2006|         East China| Zhejiang|15718.47| 888935.0|\n2004|         East China| Shandong|15021.84| 870064.0|\n2007|South Central China|    Henan|15012.46| 306162.0|\n2005|         East China|  Jiangsu| 15003.6|1213800.0|\n2007|        North China|    Hebei|13607.32| 241621.0|\n2002|South Central China|Guangdong|13502.42|1133400.0|\n2005|         East China| Zhejiang|13417.68| 772000.0|\n2007|         East China| Shanghai|12494.01| 792000.0|\n2004|         East China|  Jiangsu|12442.87|1056365.0|\n+----+-------------------+---------+--------+---------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#casting-data-types",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#casting-data-types",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Casting Data Types",
    "text": "Casting Data Types\n\nfrom pyspark.sql.types import IntegerType, StringType, DoubleType\ndf = df.withColumn('gdp', F.col('gdp').cast(DoubleType()))\n\n\n\n\n\n\ndf = df.withColumn('province', F.col('province').cast(StringType()))\n\n\n\n\n\n\ndf.filter((df.gdp>10000) & (df.region=='East China')).show()\n\n\n+----+----------+--------+----------------+---------+\nyear|    region|province|             gdp|      fdi|\n+----+----------+--------+----------------+---------+\n2003|East China| Jiangsu| 10606.849609375|1018960.0|\n2004|East China| Jiangsu|12442.8701171875|1056365.0|\n2005|East China| Jiangsu| 15003.599609375|1213800.0|\n2006|East China| Jiangsu| 18598.689453125|1318339.0|\n2007|East China| Jiangsu|  21742.05078125|1743140.0|\n2002|East China|Shandong|         10275.5| 473404.0|\n2003|East China|Shandong| 12078.150390625| 601617.0|\n2004|East China|Shandong|  15021.83984375| 870064.0|\n2005|East China|Shandong| 18366.869140625| 897000.0|\n2006|East China|Shandong| 21900.189453125|1000069.0|\n2007|East China|Shandong|  25776.91015625|1101159.0|\n2006|East China|Shanghai| 10572.240234375| 710700.0|\n2007|East China|Shanghai| 12494.009765625| 792000.0|\n2004|East China|Zhejiang|11648.7001953125| 668128.0|\n2005|East China|Zhejiang|   13417.6796875| 772000.0|\n2006|East China|Zhejiang|15718.4697265625| 888935.0|\n2007|East China|Zhejiang|  18753.73046875|1036576.0|\n+----+----------+--------+----------------+---------+"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#aggregating-using-groupby-.agg-and-summax",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#aggregating-using-groupby-.agg-and-summax",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Aggregating using groupBy, .agg and sum/max",
    "text": "Aggregating using groupBy, .agg and sum/max\n\nfrom pyspark.sql import functions as F\n\ndf.groupBy([\"region\",\"province\"]).agg(F.sum(\"gdp\") ,F.max(\"gdp\")).show()\n\n\n+-------------------+---------+------------------+------------------+\n             region| province|          sum(gdp)|          max(gdp)|\n+-------------------+---------+------------------+------------------+\nSouth Central China|    Hunan| 57190.69970703125|    9439.599609375|\n        North China|  Tianjin|30343.979858398438|    5252.759765625|\n    Northwest China| Xinjiang| 21946.75994873047| 3523.159912109375|\n        North China|  Beijing|56081.439208984375|   9846.8095703125|\nSouth Central China|Guangdong|  184305.376953125|   31777.009765625|\nSouth Central China|    Henan| 86507.60034179688|  15012.4599609375|\n         East China|  Jiangsu|129142.15966796875|    21742.05078125|\n    Northwest China|    Gansu| 16773.99005126953|  2703.97998046875|\n    Southwest China|  Guizhou|17064.130249023438| 2884.110107421875|\n    Southwest China|  Sichuan|64533.479736328125|  10562.3896484375|\nSouth Central China|   Hainan| 8240.570068359375|1254.1700439453125|\n         East China| Shandong| 147888.0283203125|    25776.91015625|\n    Southwest China|Chongqing|29732.549926757812|   4676.1298828125|\n    Northwest China|  Shaanxi|31896.409790039062|   5757.2900390625|\n         East China| Shanghai|  77189.4501953125|   12494.009765625|\n    Southwest China|    Tibet| 2045.120002746582|341.42999267578125|\n        North China|    Hebei|  83241.8994140625|     13607.3203125|\n    Northeast China|    Jilin|27298.250366210938|   4275.1201171875|\n         East China| Zhejiang|109657.81884765625|    18753.73046875|\n        North China|   Shanxi| 33806.52990722656|   6024.4501953125|\n+-------------------+---------+------------------+------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.groupBy([\"region\",\"province\"]).agg(F.sum(\"gdp\").alias(\"SumGDP\"),F.max(\"gdp\").alias(\"MaxGDP\")).show()\n\n\n+-------------------+---------+------------------+------------------+\n             region| province|               GDP|            MaxGDP|\n+-------------------+---------+------------------+------------------+\nSouth Central China|    Hunan| 57190.69970703125|    9439.599609375|\n        North China|  Tianjin|30343.979858398438|    5252.759765625|\n    Northwest China| Xinjiang| 21946.75994873047| 3523.159912109375|\n        North China|  Beijing|56081.439208984375|   9846.8095703125|\nSouth Central China|Guangdong|  184305.376953125|   31777.009765625|\nSouth Central China|    Henan| 86507.60034179688|  15012.4599609375|\n         East China|  Jiangsu|129142.15966796875|    21742.05078125|\n    Northwest China|    Gansu| 16773.99005126953|  2703.97998046875|\n    Southwest China|  Guizhou|17064.130249023438| 2884.110107421875|\n    Southwest China|  Sichuan|64533.479736328125|  10562.3896484375|\nSouth Central China|   Hainan| 8240.570068359375|1254.1700439453125|\n         East China| Shandong| 147888.0283203125|    25776.91015625|\n    Southwest China|Chongqing|29732.549926757812|   4676.1298828125|\n    Northwest China|  Shaanxi|31896.409790039062|   5757.2900390625|\n         East China| Shanghai|  77189.4501953125|   12494.009765625|\n    Southwest China|    Tibet| 2045.120002746582|341.42999267578125|\n        North China|    Hebei|  83241.8994140625|     13607.3203125|\n    Northeast China|    Jilin|27298.250366210938|   4275.1201171875|\n         East China| Zhejiang|109657.81884765625|    18753.73046875|\n        North China|   Shanxi| 33806.52990722656|   6024.4501953125|\n+-------------------+---------+------------------+------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.groupBy([\"region\",\"province\"]).agg(\n    F.sum(\"gdp\").alias(\"SumGDP\"),\\\n    F.max(\"gdp\").alias(\"MaxGDP\")\\\n    ).show()\n\n\n+-------------------+---------+------------------+------------------+\n             region| province|            SumGDP|            MaxGDP|\n+-------------------+---------+------------------+------------------+\nSouth Central China|    Hunan| 57190.69970703125|    9439.599609375|\n        North China|  Tianjin|30343.979858398438|    5252.759765625|\n    Northwest China| Xinjiang| 21946.75994873047| 3523.159912109375|\n        North China|  Beijing|56081.439208984375|   9846.8095703125|\nSouth Central China|Guangdong|  184305.376953125|   31777.009765625|\nSouth Central China|    Henan| 86507.60034179688|  15012.4599609375|\n         East China|  Jiangsu|129142.15966796875|    21742.05078125|\n    Northwest China|    Gansu| 16773.99005126953|  2703.97998046875|\n    Southwest China|  Guizhou|17064.130249023438| 2884.110107421875|\n    Southwest China|  Sichuan|64533.479736328125|  10562.3896484375|\nSouth Central China|   Hainan| 8240.570068359375|1254.1700439453125|\n         East China| Shandong| 147888.0283203125|    25776.91015625|\n    Southwest China|Chongqing|29732.549926757812|   4676.1298828125|\n    Northwest China|  Shaanxi|31896.409790039062|   5757.2900390625|\n         East China| Shanghai|  77189.4501953125|   12494.009765625|\n    Southwest China|    Tibet| 2045.120002746582|341.42999267578125|\n        North China|    Hebei|  83241.8994140625|     13607.3203125|\n    Northeast China|    Jilin|27298.250366210938|   4275.1201171875|\n         East China| Zhejiang|109657.81884765625|    18753.73046875|\n        North China|   Shanxi| 33806.52990722656|   6024.4501953125|\n+-------------------+---------+------------------+------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.limit(10).toPandas()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.300049\n      50661.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.320068\n      43443.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.959961\n      27673.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.340088\n      26131.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.090088\n      31847.0\n    \n    \n      5\n      2001\n      East China\n      Anhui\n      3246.709961\n      33672.0\n    \n    \n      6\n      2002\n      East China\n      Anhui\n      3519.719971\n      38375.0\n    \n    \n      7\n      2003\n      East China\n      Anhui\n      3923.110107\n      36720.0\n    \n    \n      8\n      2004\n      East China\n      Anhui\n      4759.299805\n      54669.0\n    \n    \n      9\n      2005\n      East China\n      Anhui\n      5350.169922\n      69000.0"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#exponentials-using-exp",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#exponentials-using-exp",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Exponentials using exp",
    "text": "Exponentials using exp\n\ndf = df.withColumn(\"Exp_GDP\", F.exp(\"gdp\"))\ndf.show()\n\n\n+----+-----------+--------+-----------------+--------+--------+\nyear|     region|province|              gdp|     fdi| Exp_GDP|\n+----+-----------+--------+-----------------+--------+--------+\n1996| East China|   Anhui|2093.300048828125| 50661.0|Infinity|\n1997| East China|   Anhui|2347.320068359375| 43443.0|Infinity|\n1998| East China|   Anhui|  2542.9599609375| 27673.0|Infinity|\n1999| East China|   Anhui|2712.340087890625| 26131.0|Infinity|\n2000| East China|   Anhui|2902.090087890625| 31847.0|Infinity|\n2001| East China|   Anhui|  3246.7099609375| 33672.0|Infinity|\n2002| East China|   Anhui|3519.719970703125| 38375.0|Infinity|\n2003| East China|   Anhui|3923.110107421875| 36720.0|Infinity|\n2004| East China|   Anhui|  4759.2998046875| 54669.0|Infinity|\n2005| East China|   Anhui|   5350.169921875| 69000.0|Infinity|\n2006| East China|   Anhui|           6112.5|139354.0|Infinity|\n2007| East China|   Anhui|   7360.919921875|299892.0|Infinity|\n1996|North China| Beijing|1789.199951171875|155290.0|Infinity|\n1997|North China| Beijing|2077.090087890625|159286.0|Infinity|\n1998|North China| Beijing|2377.179931640625|216800.0|Infinity|\n1999|North China| Beijing|2678.820068359375|197525.0|Infinity|\n2000|North China| Beijing|3161.659912109375|168368.0|Infinity|\n2001|North China| Beijing|  3707.9599609375|176818.0|Infinity|\n2002|North China| Beijing|           4315.0|172464.0|Infinity|\n2003|North China| Beijing|  5007.2099609375|219126.0|Infinity|\n+----+-----------+--------+-----------------+--------+--------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#window-functions",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#window-functions",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Window functions",
    "text": "Window functions\n\n\n\n\n\n\nNote\n\n\n\nWindow functions\n\n\n\n# Window functions\n\nfrom pyspark.sql.window import Window\nwindowSpec = Window().partitionBy(['province']).orderBy(F.desc('gdp'))\ndf.withColumn(\"rank\",F.rank().over(windowSpec)).show()\n\n\n+----+-------------------+---------+-----------------+---------+--------+----+\nyear|             region| province|              gdp|      fdi| Exp_GDP|rank|\n+----+-------------------+---------+-----------------+---------+--------+----+\n2007|South Central China|Guangdong|  31777.009765625|1712603.0|Infinity|   1|\n2006|South Central China|Guangdong|  26587.759765625|1451065.0|Infinity|   2|\n2005|South Central China|Guangdong|  22557.369140625|1236400.0|Infinity|   3|\n2004|South Central China|Guangdong|  18864.619140625|1001158.0|Infinity|   4|\n2003|South Central China|Guangdong| 15844.6396484375| 782294.0|Infinity|   5|\n2002|South Central China|Guangdong|  13502.419921875|1133400.0|Infinity|   6|\n2001|South Central China|Guangdong|         12039.25|1193203.0|Infinity|   7|\n2000|South Central China|Guangdong|         10741.25|1128091.0|Infinity|   8|\n1999|South Central China|Guangdong|     9250.6796875|1165750.0|Infinity|   9|\n1998|South Central China|Guangdong|  8530.8798828125|1201994.0|Infinity|  10|\n1997|South Central China|Guangdong| 7774.52978515625|1171083.0|Infinity|  11|\n1996|South Central China|Guangdong| 6834.97021484375|1162362.0|Infinity|  12|\n2007|South Central China|    Hunan|   9439.599609375| 327051.0|Infinity|   1|\n2006|South Central China|    Hunan|   7688.669921875| 259335.0|Infinity|   2|\n2005|South Central China|    Hunan| 6596.10009765625| 207200.0|Infinity|   3|\n2004|South Central China|    Hunan| 5641.93994140625| 141800.0|Infinity|   4|\n2003|South Central China|    Hunan|   4659.990234375| 101835.0|Infinity|   5|\n2002|South Central China|    Hunan|  4151.5400390625|  90022.0|Infinity|   6|\n2001|South Central China|    Hunan| 3831.89990234375|  81011.0|Infinity|   7|\n2000|South Central China|    Hunan|3551.489990234375|  67833.0|Infinity|   8|\n+----+-------------------+---------+-----------------+---------+--------+----+\nonly showing top 20 rows\n\n\n\n\n\nfrom pyspark.sql.window import Window\nwindowSpec = Window().partitionBy(['province']).orderBy('year')"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#lagging-variables",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#lagging-variables",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Lagging Variables",
    "text": "Lagging Variables\n\ndfWithLag = df.withColumn(\"lag_7\",F.lag(\"gdp\", 7).over(windowSpec))\n\n\n\n\n\n\ndf.filter(df.year>'2000').show()\n\n\n+----+---------------+---------+------------------+--------+--------+\nyear|         region| province|               gdp|     fdi| Exp_GDP|\n+----+---------------+---------+------------------+--------+--------+\n2001|     East China|    Anhui|   3246.7099609375| 33672.0|Infinity|\n2002|     East China|    Anhui| 3519.719970703125| 38375.0|Infinity|\n2003|     East China|    Anhui| 3923.110107421875| 36720.0|Infinity|\n2004|     East China|    Anhui|   4759.2998046875| 54669.0|Infinity|\n2005|     East China|    Anhui|    5350.169921875| 69000.0|Infinity|\n2006|     East China|    Anhui|            6112.5|139354.0|Infinity|\n2007|     East China|    Anhui|    7360.919921875|299892.0|Infinity|\n2001|    North China|  Beijing|   3707.9599609375|176818.0|Infinity|\n2002|    North China|  Beijing|            4315.0|172464.0|Infinity|\n2003|    North China|  Beijing|   5007.2099609375|219126.0|Infinity|\n2004|    North China|  Beijing|   6033.2099609375|308354.0|Infinity|\n2005|    North China|  Beijing|  6969.52001953125|352638.0|Infinity|\n2006|    North China|  Beijing|  8117.77978515625|455191.0|Infinity|\n2007|    North China|  Beijing|   9846.8095703125|506572.0|Infinity|\n2001|Southwest China|Chongqing|1976.8599853515625| 25649.0|Infinity|\n2002|Southwest China|Chongqing| 2232.860107421875| 19576.0|Infinity|\n2003|Southwest China|Chongqing| 2555.719970703125| 26083.0|Infinity|\n2004|Southwest China|Chongqing|    3034.580078125| 40508.0|Infinity|\n2005|Southwest China|Chongqing| 3467.719970703125| 51600.0|Infinity|\n2006|Southwest China|Chongqing|  3907.22998046875| 69595.0|Infinity|\n+----+---------------+---------+------------------+--------+--------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#looking-at-windows-within-the-data",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#looking-at-windows-within-the-data",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Looking at windows within the data",
    "text": "Looking at windows within the data\n\nfrom pyspark.sql.window import Window\n\nwindowSpec = Window().partitionBy(['province']).orderBy('year').rowsBetween(-6,0)\n\n\n\n\n\n\ndfWithRoll = df.withColumn(\"roll_7_confirmed\",F.mean(\"gdp\").over(windowSpec))\n\n\n\n\n\n\ndfWithRoll.filter(dfWithLag.year>'2001').show()\n\n\n+----+-------------------+---------+------------------+---------+--------------------+------------------+\nyear|             region| province|               gdp|      fdi|             Exp_GDP|  roll_7_confirmed|\n+----+-------------------+---------+------------------+---------+--------------------+------------------+\n2002|South Central China|Guangdong|   13502.419921875|1133400.0|            Infinity|  9810.56849888393|\n2003|South Central China|Guangdong|  15844.6396484375| 782294.0|            Infinity|11097.664132254464|\n2004|South Central China|Guangdong|   18864.619140625|1001158.0|            Infinity|12681.962611607143|\n2005|South Central China|Guangdong|   22557.369140625|1236400.0|            Infinity|14685.746791294643|\n2006|South Central China|Guangdong|   26587.759765625|1451065.0|            Infinity|17162.472516741072|\n2007|South Central China|Guangdong|   31777.009765625|1712603.0|            Infinity|  20167.5810546875|\n2002|South Central China|    Hunan|   4151.5400390625|  90022.0|            Infinity|3309.1999860491073|\n2003|South Central China|    Hunan|    4659.990234375| 101835.0|            Infinity| 3612.037179129464|\n2004|South Central China|    Hunan|  5641.93994140625| 141800.0|            Infinity|4010.9900251116073|\n2005|South Central China|    Hunan|  6596.10009765625| 207200.0|            Infinity|  4521.07146344866|\n2006|South Central China|    Hunan|    7688.669921875| 259335.0|            Infinity| 5160.232875279018|\n2007|South Central China|    Hunan|    9439.599609375| 327051.0|            Infinity| 6001.391392299107|\n2002|        North China|   Shanxi| 2324.800048828125|  21164.0|            Infinity|1749.4771379743304|\n2003|        North China|   Shanxi|  2855.22998046875|  21361.0|            Infinity| 1972.779994419643|\n2004|        North China|   Shanxi|   3571.3701171875|  62184.0|            Infinity| 2272.118582589286|\n2005|        North China|   Shanxi|  4230.52978515625|  27516.0|            Infinity| 2646.325701032366|\n2006|        North China|   Shanxi|  4878.60986328125|  47199.0|            Infinity|3105.1128278459823|\n2007|        North China|   Shanxi|   6024.4501953125| 134283.0|            Infinity| 3702.074288504464|\n2002|    Southwest China|    Tibet| 162.0399932861328|    293.0|2.360885537826244E70|108.38571493966239|\n2003|    Southwest China|    Tibet|185.08999633789062|    467.0|2.418600091901801E80|125.54428536551339|\n+----+-------------------+---------+------------------+---------+--------------------+------------------+\nonly showing top 20 rows\n\n\n\n\n\nfrom pyspark.sql.window import Window\nwindowSpec = Window().partitionBy(['province']).orderBy('year').rowsBetween(Window.unboundedPreceding,Window.currentRow)\n\n\n\n\n\n\ndfWithRoll = df.withColumn(\"cumulative_gdp\",F.sum(\"gdp\").over(windowSpec))\n\n\n\n\n\n\ndfWithRoll.filter(dfWithLag.year>'1999').show()\n\n\n+----+-------------------+---------+-----------------+---------+--------+------------------+\nyear|             region| province|              gdp|      fdi| Exp_GDP|    cumulative_gdp|\n+----+-------------------+---------+-----------------+---------+--------+------------------+\n2000|South Central China|Guangdong|         10741.25|1128091.0|Infinity|  43132.3095703125|\n2001|South Central China|Guangdong|         12039.25|1193203.0|Infinity|  55171.5595703125|\n2002|South Central China|Guangdong|  13502.419921875|1133400.0|Infinity|  68673.9794921875|\n2003|South Central China|Guangdong| 15844.6396484375| 782294.0|Infinity|   84518.619140625|\n2004|South Central China|Guangdong|  18864.619140625|1001158.0|Infinity|   103383.23828125|\n2005|South Central China|Guangdong|  22557.369140625|1236400.0|Infinity|  125940.607421875|\n2006|South Central China|Guangdong|  26587.759765625|1451065.0|Infinity|    152528.3671875|\n2007|South Central China|Guangdong|  31777.009765625|1712603.0|Infinity|  184305.376953125|\n2000|South Central China|    Hunan|3551.489990234375|  67833.0|Infinity|  15180.9599609375|\n2001|South Central China|    Hunan| 3831.89990234375|  81011.0|Infinity| 19012.85986328125|\n2002|South Central China|    Hunan|  4151.5400390625|  90022.0|Infinity| 23164.39990234375|\n2003|South Central China|    Hunan|   4659.990234375| 101835.0|Infinity| 27824.39013671875|\n2004|South Central China|    Hunan| 5641.93994140625| 141800.0|Infinity|   33466.330078125|\n2005|South Central China|    Hunan| 6596.10009765625| 207200.0|Infinity| 40062.43017578125|\n2006|South Central China|    Hunan|   7688.669921875| 259335.0|Infinity| 47751.10009765625|\n2007|South Central China|    Hunan|   9439.599609375| 327051.0|Infinity| 57190.69970703125|\n2000|        North China|   Shanxi|1845.719970703125|  22472.0|Infinity|7892.0098876953125|\n2001|        North China|   Shanxi|2029.530029296875|  23393.0|Infinity| 9921.539916992188|\n2002|        North China|   Shanxi|2324.800048828125|  21164.0|Infinity|12246.339965820312|\n2003|        North China|   Shanxi| 2855.22998046875|  21361.0|Infinity|15101.569946289062|\n+----+-------------------+---------+-----------------+---------+--------+------------------+\nonly showing top 20 rows\n\n\n\n\n\nPivot Dataframes\n\n\n\n\n\n\nNote\n\n\n\nPivot Dataframes\n\n\n\npivoted_df = df.groupBy('year').pivot('province') \\\n                      .agg(F.sum('gdp').alias('gdp') , F.sum('fdi').alias('fdi'))\npivoted_df.limit(10).toPandas()\n\n\n\n\n\n  \n    \n      \n      year\n      Anhui_gdp\n      Anhui_fdi\n      Beijing_gdp\n      Beijing_fdi\n      Chongqing_gdp\n      Chongqing_fdi\n      Fujian_gdp\n      Fujian_fdi\n      Gansu_gdp\n      Gansu_fdi\n      Guangdong_gdp\n      Guangdong_fdi\n      Guangxi_gdp\n      Guangxi_fdi\n      Guizhou_gdp\n      Guizhou_fdi\n      Hainan_gdp\n      Hainan_fdi\n      Hebei_gdp\n      Hebei_fdi\n      Heilongjiang_gdp\n      Heilongjiang_fdi\n      Henan_gdp\n      Henan_fdi\n      Hubei_gdp\n      Hubei_fdi\n      Hunan_gdp\n      Hunan_fdi\n      Jiangsu_gdp\n      Jiangsu_fdi\n      Jiangxi_gdp\n      Jiangxi_fdi\n      Jilin_gdp\n      Jilin_fdi\n      Liaoning_gdp\n      Liaoning_fdi\n      Ningxia_gdp\n      Ningxia_fdi\n      Qinghai_gdp\n      Qinghai_fdi\n      Shaanxi_gdp\n      Shaanxi_fdi\n      Shandong_gdp\n      Shandong_fdi\n      Shanghai_gdp\n      Shanghai_fdi\n      Shanxi_gdp\n      Shanxi_fdi\n      Sichuan_gdp\n      Sichuan_fdi\n      Tianjin_gdp\n      Tianjin_fdi\n      Tibet_gdp\n      Tibet_fdi\n      Xinjiang_gdp\n      Xinjiang_fdi\n      Yunnan_gdp\n      Yunnan_fdi\n      Zhejiang_gdp\n      Zhejiang_fdi\n    \n  \n  \n    \n      0\n      2003\n      3923.110107\n      36720.0\n      5007.209961\n      219126.0\n      2555.719971\n      26083.0\n      4983.669922\n      259903.0\n      1399.829956\n      2342.0\n      15844.639648\n      782294.0\n      2821.110107\n      41856.0\n      1426.339966\n      4521.0\n      713.960022\n      42125.0\n      6921.290039\n      96405.0\n      4057.399902\n      32180.0\n      6867.700195\n      53903.0\n      4757.450195\n      156886.0\n      4659.990234\n      101835.0\n      10606.849609\n      1018960.0\n      2450.479980\n      108197.0\n      2348.540039\n      24468.0\n      5458.220215\n      341168.0\n      445.359985\n      1743.0\n      390.200012\n      2522.0\n      2587.719971\n      33190.0\n      12078.150391\n      601617.0\n      6694.229980\n      546849.0\n      2855.229980\n      21361.0\n      5333.089844\n      41231.0\n      2578.030029\n      153473.0\n      185.089996\n      467.0\n      1886.349976\n      1534.0\n      2556.020020\n      8384.0\n      9705.019531\n      498055.0\n    \n    \n      1\n      2007\n      7360.919922\n      299892.0\n      9846.809570\n      506572.0\n      4676.129883\n      108534.0\n      9248.530273\n      406058.0\n      2703.979980\n      11802.0\n      31777.009766\n      1712603.0\n      5823.410156\n      68396.0\n      2884.110107\n      12651.0\n      1254.170044\n      112001.0\n      13607.320312\n      241621.0\n      7104.000000\n      208508.0\n      15012.459961\n      306162.0\n      9333.400391\n      276622.0\n      9439.599609\n      327051.0\n      21742.050781\n      1743140.0\n      4820.529785\n      280657.0\n      4275.120117\n      76064.0\n      9304.519531\n      598554.0\n      919.109985\n      5047.0\n      797.349976\n      31000.0\n      5757.290039\n      119516.0\n      25776.910156\n      1101159.0\n      12494.009766\n      792000.0\n      6024.450195\n      134283.0\n      10562.389648\n      149322.0\n      5252.759766\n      527776.0\n      341.429993\n      2418.0\n      3523.159912\n      12484.0\n      4772.520020\n      39453.0\n      18753.730469\n      1036576.0\n    \n    \n      2\n      2006\n      6112.500000\n      139354.0\n      8117.779785\n      455191.0\n      3907.229980\n      69595.0\n      7583.850098\n      322047.0\n      2277.350098\n      2954.0\n      26587.759766\n      1451065.0\n      4746.160156\n      44740.0\n      2338.979980\n      9384.0\n      1065.670044\n      74878.0\n      11467.599609\n      201434.0\n      6211.799805\n      170801.0\n      12362.790039\n      184526.0\n      7617.470215\n      244853.0\n      7688.669922\n      259335.0\n      18598.689453\n      1318339.0\n      4056.760010\n      242000.0\n      3620.270020\n      66100.0\n      8047.259766\n      359000.0\n      725.900024\n      3718.0\n      648.500000\n      27500.0\n      4743.609863\n      92489.0\n      21900.189453\n      1000069.0\n      10572.240234\n      710700.0\n      4878.609863\n      47199.0\n      8690.240234\n      120819.0\n      4462.740234\n      413077.0\n      290.760010\n      1522.0\n      3045.260010\n      10366.0\n      3988.139893\n      30234.0\n      15718.469727\n      888935.0\n    \n    \n      3\n      1997\n      2347.320068\n      43443.0\n      2077.090088\n      159286.0\n      1509.750000\n      38675.0\n      2870.899902\n      419666.0\n      793.570007\n      4144.0\n      7774.529785\n      1171083.0\n      1817.250000\n      87986.0\n      805.789978\n      4977.0\n      411.160004\n      70554.0\n      3953.780029\n      110064.0\n      2667.500000\n      73485.0\n      4041.090088\n      69204.0\n      2856.469971\n      79019.0\n      2849.270020\n      91702.0\n      6004.209961\n      507208.0\n      1409.739990\n      30068.0\n      1346.790039\n      45155.0\n      3157.689941\n      167142.0\n      224.589996\n      671.0\n      202.789993\n      247.0\n      1363.599976\n      62816.0\n      6537.069824\n      249294.0\n      3438.790039\n      422536.0\n      1476.000000\n      26592.0\n      3241.469971\n      24846.0\n      1264.630005\n      251135.0\n      77.239998\n      63.0\n      1039.849976\n      2472.0\n      1676.170044\n      16566.0\n      4686.109863\n      150345.0\n    \n    \n      4\n      2004\n      4759.299805\n      54669.0\n      6033.209961\n      308354.0\n      3034.580078\n      40508.0\n      5763.350098\n      474801.0\n      1688.489990\n      3539.0\n      18864.619141\n      1001158.0\n      3433.500000\n      29579.0\n      1677.800049\n      6533.0\n      819.659973\n      64343.0\n      8477.629883\n      162341.0\n      4750.600098\n      123639.0\n      8553.790039\n      87367.0\n      5633.240234\n      207126.0\n      5641.939941\n      141800.0\n      12442.870117\n      1056365.0\n      2807.409912\n      161202.0\n      2662.080078\n      19059.0\n      6002.540039\n      282410.0\n      537.109985\n      6689.0\n      466.100006\n      22500.0\n      3175.580078\n      52664.0\n      15021.839844\n      870064.0\n      8072.830078\n      654100.0\n      3571.370117\n      62184.0\n      6379.629883\n      70129.0\n      3110.969971\n      247243.0\n      220.339996\n      2699.0\n      2209.090088\n      4586.0\n      3081.909912\n      14200.0\n      11648.700195\n      668128.0\n    \n    \n      5\n      1996\n      2093.300049\n      50661.0\n      1789.199951\n      155290.0\n      1315.119995\n      21878.0\n      2484.250000\n      407876.0\n      722.520020\n      9002.0\n      6834.970215\n      1162362.0\n      1697.900024\n      66618.0\n      723.179993\n      3138.0\n      389.679993\n      78960.0\n      3452.969971\n      123652.0\n      2370.500000\n      54841.0\n      3634.689941\n      52566.0\n      2499.770020\n      68878.0\n      2540.129883\n      70344.0\n      5155.250000\n      478058.0\n      1169.729980\n      28818.0\n      1137.229980\n      39876.0\n      2793.370117\n      140405.0\n      202.899994\n      2826.0\n      184.169998\n      576.0\n      1215.839966\n      33008.0\n      5883.799805\n      259041.0\n      2957.550049\n      471578.0\n      1292.109985\n      13802.0\n      2871.649902\n      22519.0\n      1121.930054\n      200587.0\n      64.980003\n      679.0\n      900.929993\n      6639.0\n      1517.689941\n      18000.0\n      4188.529785\n      152021.0\n    \n    \n      6\n      1998\n      2542.959961\n      27673.0\n      2377.179932\n      216800.0\n      1602.380005\n      43107.0\n      3159.909912\n      421211.0\n      887.669983\n      3864.0\n      8530.879883\n      1201994.0\n      1911.300049\n      88613.0\n      858.390015\n      4535.0\n      442.130005\n      71715.0\n      4256.009766\n      142868.0\n      2774.399902\n      52639.0\n      4308.240234\n      61654.0\n      3114.020020\n      97294.0\n      3025.530029\n      81816.0\n      6680.339844\n      543511.0\n      1605.770020\n      47768.0\n      1464.339966\n      40227.0\n      3582.459961\n      220470.0\n      245.440002\n      1856.0\n      220.919998\n      1010.0\n      1458.400024\n      30010.0\n      7021.350098\n      220274.0\n      3801.090088\n      360150.0\n      1611.079956\n      24451.0\n      3474.090088\n      37248.0\n      1374.599976\n      211361.0\n      91.500000\n      481.0\n      1106.949951\n      2167.0\n      1831.329956\n      14568.0\n      5052.620117\n      131802.0\n    \n    \n      7\n      2001\n      3246.709961\n      33672.0\n      3707.959961\n      176818.0\n      1976.859985\n      25649.0\n      4072.850098\n      391804.0\n      1125.369995\n      7439.0\n      12039.250000\n      1193203.0\n      2279.340088\n      38416.0\n      1133.270020\n      2829.0\n      579.169983\n      46691.0\n      5516.759766\n      66989.0\n      3390.100098\n      34114.0\n      5533.009766\n      45729.0\n      3880.530029\n      118860.0\n      3831.899902\n      81011.0\n      8553.690430\n      642550.0\n      2003.069946\n      22724.0\n      1951.510010\n      33701.0\n      4669.060059\n      204446.0\n      337.440002\n      1680.0\n      300.130005\n      3649.0\n      2010.619995\n      35174.0\n      9195.040039\n      352093.0\n      5210.120117\n      429159.0\n      2029.530029\n      23393.0\n      4293.490234\n      58188.0\n      1919.089966\n      213348.0\n      139.160004\n      106.0\n      1491.599976\n      2035.0\n      2138.310059\n      6457.0\n      6898.339844\n      221162.0\n    \n    \n      8\n      2005\n      5350.169922\n      69000.0\n      6969.520020\n      352638.0\n      3467.719971\n      51600.0\n      6554.689941\n      260800.0\n      1933.979980\n      2000.0\n      22557.369141\n      1236400.0\n      3984.100098\n      37866.0\n      2005.420044\n      10768.0\n      918.750000\n      68400.0\n      10012.110352\n      191000.0\n      5513.700195\n      145000.0\n      10587.419922\n      123000.0\n      6590.189941\n      218500.0\n      6596.100098\n      207200.0\n      15003.599609\n      1213800.0\n      3456.699951\n      205238.0\n      3122.010010\n      45266.0\n      6672.000000\n      540679.0\n      612.609985\n      14100.0\n      543.320007\n      26600.0\n      3933.719971\n      62800.0\n      18366.869141\n      897000.0\n      9247.660156\n      685000.0\n      4230.529785\n      27516.0\n      7385.100098\n      88686.0\n      3905.639893\n      332885.0\n      248.800003\n      1151.0\n      2604.189941\n      4700.0\n      3462.729980\n      17352.0\n      13417.679688\n      772000.0\n    \n    \n      9\n      2000\n      2902.090088\n      31847.0\n      3161.659912\n      168368.0\n      1791.000000\n      24436.0\n      3764.540039\n      343191.0\n      1052.880005\n      6235.0\n      10741.250000\n      1128091.0\n      2080.040039\n      52466.0\n      1029.920044\n      2501.0\n      526.820007\n      43080.0\n      5043.959961\n      67923.0\n      3151.399902\n      30086.0\n      5052.990234\n      56403.0\n      3545.389893\n      94368.0\n      3551.489990\n      67833.0\n      7697.819824\n      607756.0\n      1853.650024\n      32080.0\n      1672.959961\n      30120.0\n      4171.689941\n      106173.0\n      295.019989\n      1741.0\n      263.679993\n      11020.0\n      1804.000000\n      28842.0\n      8337.469727\n      297119.0\n      4771.169922\n      316014.0\n      1845.719971\n      22472.0\n      3928.199951\n      43694.0\n      1701.880005\n      116601.0\n      117.800003\n      2.0\n      1363.560059\n      1911.0\n      2011.189941\n      12812.0\n      6141.029785\n      161266.0\n    \n  \n\n\n\n\n\npivoted_df.columns\n\n\nOut[55]: ['year',\n 'Anhui_gdp',\n 'Anhui_fdi',\n 'Beijing_gdp',\n 'Beijing_fdi',\n 'Chongqing_gdp',\n 'Chongqing_fdi',\n 'Fujian_gdp',\n 'Fujian_fdi',\n 'Gansu_gdp',\n 'Gansu_fdi',\n 'Guangdong_gdp',\n 'Guangdong_fdi',\n 'Guangxi_gdp',\n 'Guangxi_fdi',\n 'Guizhou_gdp',\n 'Guizhou_fdi',\n 'Hainan_gdp',\n 'Hainan_fdi',\n 'Hebei_gdp',\n 'Hebei_fdi',\n 'Heilongjiang_gdp',\n 'Heilongjiang_fdi',\n 'Henan_gdp',\n 'Henan_fdi',\n 'Hubei_gdp',\n 'Hubei_fdi',\n 'Hunan_gdp',\n 'Hunan_fdi',\n 'Jiangsu_gdp',\n 'Jiangsu_fdi',\n 'Jiangxi_gdp',\n 'Jiangxi_fdi',\n 'Jilin_gdp',\n 'Jilin_fdi',\n 'Liaoning_gdp',\n 'Liaoning_fdi',\n 'Ningxia_gdp',\n 'Ningxia_fdi',\n 'Qinghai_gdp',\n 'Qinghai_fdi',\n 'Shaanxi_gdp',\n 'Shaanxi_fdi',\n 'Shandong_gdp',\n 'Shandong_fdi',\n 'Shanghai_gdp',\n 'Shanghai_fdi',\n 'Shanxi_gdp',\n 'Shanxi_fdi',\n 'Sichuan_gdp',\n 'Sichuan_fdi',\n 'Tianjin_gdp',\n 'Tianjin_fdi',\n 'Tibet_gdp',\n 'Tibet_fdi',\n 'Xinjiang_gdp',\n 'Xinjiang_fdi',\n 'Yunnan_gdp',\n 'Yunnan_fdi',\n 'Zhejiang_gdp',\n 'Zhejiang_fdi']\n\n\n\nnewColnames = [x.replace(\"-\",\"_\") for x in pivoted_df.columns]\n\n\n\n\n\n\npivoted_df = pivoted_df.toDF(*newColnames)\n\n\n\n\n\n\nexpression = \"\"\ncnt=0\nfor column in pivoted_df.columns:\n    if column!='year':\n        cnt +=1\n        expression += f\"'{column}' , {column},\"\n        \nexpression = f\"stack({cnt}, {expression[:-1]}) as (Type,Value)\"\n\n\n\n\n\n\n\nUnpivoting RDDs\n\nunpivoted_df = pivoted_df.select('year',F.expr(expression))\nunpivoted_df.show()\n\n\n+----+-------------+------------------+\nyear|         Type|             Value|\n+----+-------------+------------------+\n2003|    Anhui_gdp| 3923.110107421875|\n2003|    Anhui_fdi|           36720.0|\n2003|  Beijing_gdp|   5007.2099609375|\n2003|  Beijing_fdi|          219126.0|\n2003|Chongqing_gdp| 2555.719970703125|\n2003|Chongqing_fdi|           26083.0|\n2003|   Fujian_gdp|    4983.669921875|\n2003|   Fujian_fdi|          259903.0|\n2003|    Gansu_gdp|1399.8299560546875|\n2003|    Gansu_fdi|            2342.0|\n2003|Guangdong_gdp|  15844.6396484375|\n2003|Guangdong_fdi|          782294.0|\n2003|  Guangxi_gdp| 2821.110107421875|\n2003|  Guangxi_fdi|           41856.0|\n2003|  Guizhou_gdp|1426.3399658203125|\n2003|  Guizhou_fdi|            4521.0|\n2003|   Hainan_gdp| 713.9600219726562|\n2003|   Hainan_fdi|           42125.0|\n2003|    Hebei_gdp|   6921.2900390625|\n2003|    Hebei_fdi|           96405.0|\n+----+-------------+------------------+\nonly showing top 20 rows\n\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-10-10-dask_fiscal-db-to-dask-dataframe.html",
    "href": "posts/2020-10-10-dask_fiscal-db-to-dask-dataframe.html",
    "title": "Working with dask data frames. Reading Fiscal Data from a sqlite db to a dask dataframe. Computing, visualizing and groupby with dask dataframes. Using dask.distributed locally.",
    "section": "",
    "text": "import numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///fiscal_data.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nengine.execute(\"SELECT * FROM fiscal_data LIMIT 1\").fetchall()\n\n[(1996, 'East China', 'Anhui', '2093.3', 50661, 631930, 147002)]\n\n\n\nsql = \"\"\"\nSELECT year\n, region\n, province\n, gdp\n, fdi\n, it\n, specific\nFROM fiscal_data\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.3\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.7\n      668128\n      3162299\n      656175.0\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      18753.73\n      1036576\n      2939778\n      844647.0\n    \n  \n\n360 rows  7 columns\n\n\n\n\ndf.columns\n\nIndex(['year', 'region', 'province', 'gdp', 'fdi', 'it', 'specific'], dtype='object')\n\n\n\nfrom dask import dataframe as dd \n\n\nddf = dd.from_pandas(df, npartitions=5)\n\n\nprint(ddf)\n\nDask DataFrame Structure:\n                year  region province     gdp    fdi     it specific\nnpartitions=5                                                       \n0              int64  object   object  object  int64  int64  float64\n72               ...     ...      ...     ...    ...    ...      ...\n...              ...     ...      ...     ...    ...    ...      ...\n288              ...     ...      ...     ...    ...    ...      ...\n359              ...     ...      ...     ...    ...    ...      ...\nDask Name: from_pandas, 5 tasks\n\n\n\nddf.npartitions\n\n5\n\n\n\nddf.npartitions\n\n5\n\n\n\nlen(ddf)\n\n360\n\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n\n\n\n\nClient\n\n  Scheduler: inproc://192.168.1.71/12451/1\n  Dashboard: http://localhost:8787/status\n\n\n\nCluster\n\n  Workers: 3\n  Cores: 6\n  Memory: 12.00 GB\n\n\n\n\n\n\n\nddf.describe().compute()\n\n\n\n\n\n  \n    \n      \n      year\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      count\n      360.000000\n      3.600000e+02\n      3.600000e+02\n      3.560000e+02\n    \n    \n      mean\n      2001.500000\n      1.961394e+05\n      2.165819e+06\n      5.834707e+05\n    \n    \n      std\n      3.456857\n      3.030440e+05\n      1.769294e+06\n      6.540553e+05\n    \n    \n      min\n      1996.000000\n      2.000000e+00\n      1.478970e+05\n      8.964000e+03\n    \n    \n      25%\n      1998.750000\n      3.309900e+04\n      1.077466e+06\n      2.237530e+05\n    \n    \n      50%\n      2001.500000\n      1.411025e+05\n      2.020634e+06\n      4.243700e+05\n    \n    \n      75%\n      2004.250000\n      4.065125e+05\n      3.375492e+06\n      1.011846e+06\n    \n    \n      max\n      2007.000000\n      1.743140e+06\n      1.053331e+07\n      3.937966e+06\n    \n  \n\n\n\n\n\nddf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.3\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n  \n\n\n\n\n\ngroupby_yr = ddf.groupby('year').count()\n\n\ngroupby_yr.compute()\n\n\n\n\n\n  \n    \n      \n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1996\n      30\n      30\n      30\n      30\n      30\n      29\n    \n    \n      1997\n      30\n      30\n      30\n      30\n      30\n      28\n    \n    \n      1998\n      30\n      30\n      30\n      30\n      30\n      30\n    \n    \n      1999\n      30\n      30\n      30\n      30\n      30\n      30\n    \n    \n      2000\n      30\n      30\n      30\n      30\n      30\n      29\n    \n    \n      2001\n      30\n      30\n      30\n      30\n      30\n      30\n    \n    \n      2002\n      30\n      30\n      30\n      30\n      30\n      30\n    \n    \n      2003\n      30\n      30\n      30\n      30\n      30\n      30\n    \n    \n      2004\n      30\n      30\n      30\n      30\n      30\n      30\n    \n    \n      2005\n      30\n      30\n      30\n      30\n      30\n      30\n    \n    \n      2006\n      30\n      30\n      30\n      30\n      30\n      30\n    \n    \n      2007\n      30\n      30\n      30\n      30\n      30\n      30\n    \n  \n\n\n\n\n\ngroup_region = ddf.groupby('region')['gdp'].sum()\n\n\ngroup_region.compute()\n\nregion\nEast China             2093.32347.322542.962712.342902.093246.713519....\nNorth China            1789.22077.092377.182678.823161.663707.964315....\nNorthwest China        722.52793.57887.67956.321052.881125.371232.031...\nSouth Central China    6834.977774.538530.889250.6810741.2512039.2513...\nSouthwest China        1315.121509.751602.381663.21791.01976.862232.8...\nNortheast China        2370.52667.52774.42866.33151.43390.13637.24057...\nName: gdp, dtype: object\n\n\n\nddf.nlargest(5, 'fdi').compute()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      179\n      2007\n      East China\n      Jiangsu\n      21742.05\n      1743140\n      3557071\n      1188989.0\n    \n    \n      71\n      2007\n      South Central China\n      Guangdong\n      31777.01\n      1712603\n      4947824\n      859482.0\n    \n    \n      70\n      2006\n      South Central China\n      Guangdong\n      26587.76\n      1451065\n      4559252\n      1897575.0\n    \n    \n      178\n      2006\n      East China\n      Jiangsu\n      18598.69\n      1318339\n      2926542\n      1388043.0\n    \n    \n      69\n      2005\n      South Central China\n      Guangdong\n      22557.37\n      1236400\n      4327217\n      1491588.0\n    \n  \n\n\n\n\n\nddf.sum().visualize()\n\n\n\n\n\nddf.sum().visualize(rankdir=\"LR\")\n\n\n\n\n\n(ddf).visualize(rankdir=\"LR\")\n\n\n\n\n\nddf.visualize(rankdir=\"LR\")\n\n\n\n\n\nclient.close()"
  },
  {
    "objectID": "posts/2020-09-19-stockmarketportfolioanaylsis.html",
    "href": "posts/2020-09-19-stockmarketportfolioanaylsis.html",
    "title": "Stock Market and Portfolio Anaylsis with pandas_datareader and quandl",
    "section": "",
    "text": "import pandas as pd\nfrom pandas_datareader import data, wb\nimport datetime\n\n\nstart = pd.to_datetime('2020-02-04')\nend = pd.to_datetime('today')\n\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nSNOW_stock = web.DataReader('SNOW', 'yahoo', start, end)\nSNOW_stock.head()\n\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Open')\n\nMSFT_stock['Open'].plot(label='Microsoft')\nZOOM_stock['Open'].plot(label='Zoom')\nSNOW_stock['Open'].plot(label='Snowflake')\nplt.legend()\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Volume')\n\nMSFT_stock['Volume'].plot(label='Microsoft')\nZOOM_stock['Volume'].plot(label='Zoom')\nSNOW_stock['Volume'].plot(label='Snowflake')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f557129fa90>\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport quandl\n\n\nstart = pd.to_datetime('2010-01-01')\nend = pd.to_datetime('today')\n\n\n# Grabbing a bunch of tech stocks for our portfolio\nCOST = quandl.get('WIKI/COST.11',\n                  start_date = start,\n                  end_date = end)\nNLSN = quandl.get('WIKI/NLSN.11',\n                   start_date = start,\n                   end_date = end)\nNKE = quandl.get('WIKI/NKE.11',\n                 start_date = start,\n                 end_date = end)\nDIS = quandl.get('WIKI/DIS.11',\n                  start_date = start,\n                  end_date = end)\n\n\n\n# Example\nCOST.iloc[0]['Adj. Close']\n\nfor stock_df in (COST, NLSN, NKE, DIS):\n    stock_df['Normed Return'] = stock_df['Adj. Close'] / stock_df.iloc[0]['Adj. Close']\n\nCOST.head()\n\nCOST.tail()\n\n## Allocations\n\nfor stock_df,allo in zip([COST,NLSN,NKE,DIS],[.2, .1, .4, .3]):\n    stock_df['Allocation'] = stock_df['Normed Return'] * allo\n\nCOST.head()\n\n## Investment\n\nfor stock_df in [COST,NLSN,NKE,DIS]:\n    stock_df['Position Values'] = stock_df['Allocation'] * 1000000\n\n## Total Portfolio Value\n\nportfolio_val = pd.concat([COST['Position Values'],\n                           NLSN['Position Values'],\n                           NKE['Position Values'],\n                           DIS['Position Values']],\n                          axis = 1)\n\nportfolio_val.head()\n\nportfolio_val.columns = ['COST Pos', 'NLSN Pos', 'NKE Pos', 'DIS Pos']\n\nportfolio_val.head()\n\nportfolio_val['Total Pos'] = portfolio_val.sum(axis = 1)\n\nportfolio_val.head()\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nportfolio_val['Total Pos'].plot(figsize = (12, 8))\nplt.title('Total Portfolio Value')\n\nportfolio_val.drop('Total Pos',\n                   axis = 1).plot(kind = 'line', figsize = (12, 8))\n\nportfolio_val.tail()\n\n\n\n\n\n  \n    \n      \n      COST Pos\n      NLSN Pos\n      NKE Pos\n      DIS Pos\n      Total Pos\n    \n    \n      Date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2018-03-21\n      758153.014553\n      144489.827125\n      1.799185e+06\n      1.054741e+06\n      3.756569e+06\n    \n    \n      2018-03-22\n      744177.280475\n      141728.307617\n      1.746850e+06\n      1.042104e+06\n      3.674859e+06\n    \n    \n      2018-03-23\n      736843.076002\n      140347.547864\n      1.752545e+06\n      1.020764e+06\n      3.650500e+06\n    \n    \n      2018-03-26\n      762838.756299\n      142663.660999\n      1.786983e+06\n      1.042622e+06\n      3.735107e+06\n    \n    \n      2018-03-27\n      746255.305075\n      142930.904822\n      1.794304e+06\n      1.029259e+06\n      3.712749e+06"
  },
  {
    "objectID": "posts/2021-05-31-pandas-interoperability.html",
    "href": "posts/2021-05-31-pandas-interoperability.html",
    "title": "Pandas Interoperability",
    "section": "",
    "text": "#code adapted from https://github.com/thomasjpfan/ml-workshop-intermediate-1-of-2"
  },
  {
    "objectID": "posts/2021-05-31-pandas-interoperability.html#categorical-data",
    "href": "posts/2021-05-31-pandas-interoperability.html#categorical-data",
    "title": "Pandas Interoperability",
    "section": "Categorical Data",
    "text": "Categorical Data\n\nOridinalEncoder\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\nord_encoder = OrdinalEncoder()\nord_encoder.fit_transform(df)\n\narray([[  0.,   0.,  17., ...,  24.,   0.,  28.],\n       [  1.,   0.,  48., ...,  29.,   0.,  60.],\n       [  2.,   0.,  64., ...,  40.,   0.,  67.],\n       ...,\n       [115.,  27.,  44., ...,  97.,   0.,  53.],\n       [116.,  27.,  84., ...,   4.,   0.,  64.],\n       [117.,  27.,  78., ...,  25.,   0.,  71.]])\n\n\n\nord_encoder.categories_\n\n[array([  4,   6,   7,  10,  11,  16,  18,  19,  22,  23,  34,  35,  40,\n         42,  43,  46,  47,  52,  54,  58,  64,  66,  67,  70,  71,  76,\n         78,  79,  82,  83,  88,  90,  91,  94,  95, 107, 112, 119, 124,\n        126, 127, 130, 131, 136, 138, 139, 142, 143, 148, 150, 151, 154,\n        155, 160, 162, 163, 166, 167, 172, 174, 175, 178, 179, 184, 186,\n        187, 190, 191, 196, 198, 199, 202, 203, 220, 222, 223, 226, 227,\n        232, 234, 235, 239, 244, 246, 247, 250, 251, 258, 259, 262, 263,\n        280, 282, 283, 292, 294, 295, 298, 310, 316, 318, 319, 322, 323,\n        328, 330, 331, 334, 335, 340, 342, 343, 346, 347, 354, 355, 358,\n        359]),\n array(['Anhui', 'Beijing', 'Chongqing', 'Fujian', 'Gansu', 'Guangdong',\n        'Guangxi', 'Guizhou', 'Hainan', 'Hebei', 'Heilongjiang', 'Henan',\n        'Hubei', 'Hunan', 'Jiangsu', 'Jiangxi', 'Jilin', 'Ningxia',\n        'Qinghai', 'Shaanxi', 'Shandong', 'Shanxi', 'Sichuan', 'Tianjin',\n        'Tibet', 'Xinjiang', 'Yunnan', 'Zhejiang'], dtype=object),\n array([  72939.,   91405.,   96825.,  103041.,  107046.,  107687.,\n         119536.,  127819.,  133858.,  137190.,  142650.,  144839.,\n         147749.,  148812.,  160637.,  179235.,  181409.,  195580.,\n         207615.,  217707.,  223984.,  228043.,  237948.,  248903.,\n         251539.,  254002.,  262197.,  265770.,  271297.,  271499.,\n         281769.,  295133.,  319710.,  331999.,  337894.,  340372.,\n         353776.,  354775.,  359275.,  365437.,  367568.,  369552.,\n         370049.,  379186.,  391292.,  395775.,  397517.,  430577.,\n         434149.,  436189.,  447709.,  458201.,  469514.,  472786.,\n         484715.,  487155.,  510656.,  518022.,  531994.,  558569.,\n         575550.,  591088.,  601485.,  615593.,  619201.,  642581.,\n         675931.,  681676.,  684767.,  685732.,  688887.,  714572.,\n         752279.,  753552.,  761081.,  821750.,  833423.,  833430.,\n         844647.,  859482.,  875877.,  909559.,  978069.,  985851.,\n        1017303., 1035872., 1097470., 1188989., 1204547., 1207353.,\n        1224179., 1235386., 1246484., 1315102., 1331590., 1358528.,\n        1388043., 1457872., 1550764., 1562694., 1791403., 1890650.,\n        1897575., 1956261., 2018158., 2022957., 2045869., 2121243.,\n        2213991., 2225220., 2482173., 2663667., 2669238., 2922784.,\n        2981235., 3156087., 3847672., 3860764.]),\n array([      0.,    2990.,    8115.,   11755.,   11767.,   17400.,\n          20842.,   26300.,   27302.,   27387.,   29646.,   30705.,\n          32100.,   32119.,   32868.,   34842.,   36670.,   36946.,\n          40604.,   44623.,   45683.,   50097.,   52108.,   53900.,\n          56070.,   58533.,   59263.,   60560.,   60906.,   62000.,\n          66100.,   66529.,   68142.,   71807.,   80361.,   80609.,\n          81879.,   86256.,   88007.,   93323.,   95648.,  100000.,\n         100900.,  107658.,  108624.,  112137.,  113000.,  114418.,\n         116000.,  119658.,  123317.,  123546.,  124582.,  124647.,\n         129791.,  135765.,  143800.,  145000.,  149549.,  150000.,\n         153640.,  154364.,  165071.,  173552.,  173556.,  178705.,\n         179252.,  188633.,  197539.,  229895.,  241282.,  260313.,\n         264185.,  269596.,  279052.,  280277.,  302600.,  309582.,\n         317700.,  320627.,  321686.,  363054.,  394795.,  400000.,\n         405966.,  423049.,  429591.,  434318.,  447900.,  460668.,\n         498913.,  516342.,  527300.,  540479.,  564400.,  570723.,\n         581800.,  605400.,  655919.,  659400.,  694400.,  763953.,\n        1016400., 1023453., 1046700., 1081000., 1131615., 1187958.,\n        1197400., 1214100., 1239200., 1263500., 1272600., 1329200.,\n        1737800.]),\n array([2000, 2002, 2003, 2006, 2007]),\n array([  117.8 ,   162.04,   185.09,   263.68,   290.76,   295.02,\n          340.65,   341.43,   377.16,   390.2 ,   445.36,   725.9 ,\n          797.35,   919.11,  1029.92,  1052.88,  1232.03,  1243.43,\n         1254.17,  1363.56,  1426.34,  1612.65,  1672.96,  1804.  ,\n         1845.72,  1853.65,  1886.35,  2011.19,  2080.04,  2120.35,\n         2175.68,  2253.39,  2277.35,  2312.82,  2324.8 ,  2338.98,\n         2348.54,  2450.48,  2523.73,  2556.02,  2587.72,  2821.11,\n         2855.23,  2884.11,  2902.09,  3045.26,  3151.4 ,  3161.66,\n         3519.72,  3523.16,  3545.39,  3551.49,  3620.27,  3637.2 ,\n         3764.54,  3907.23,  3923.11,  3928.2 ,  3988.14,  4056.76,\n         4057.4 ,  4151.54,  4212.82,  4275.12,  4315.  ,  4462.74,\n         4467.55,  4659.99,  4676.13,  4725.01,  4743.61,  4746.16,\n         4757.45,  4772.52,  4820.53,  4983.67,  5007.21,  5043.96,\n         5052.99,  5333.09,  5757.29,  5823.41,  6035.48,  6112.5 ,\n         6211.8 ,  6867.7 ,  7104.  ,  7360.92,  7583.85,  7617.47,\n         7688.67,  7697.82,  8003.67,  8117.78,  8690.24,  9248.53,\n         9333.4 ,  9439.6 ,  9456.84,  9705.02,  9846.81, 10275.5 ,\n        10606.85, 10741.25, 12078.15, 12362.79, 13502.42, 13607.32,\n        15012.46, 15718.47, 15844.64, 18598.69, 18753.73, 21742.05,\n        21900.19, 25776.91, 26587.76, 31777.01]),\n array([      2,     293,     467,    1522,    1534,    1741,    1743,\n           1899,    1911,    2200,    2418,    2501,    2522,    2954,\n           3718,    3821,    4521,    4726,    5047,    6121,    6235,\n           8384,    9384,   10366,   11020,   11169,   12484,   12651,\n          12812,   21164,   21361,   22472,   24468,   28842,   30086,\n          30120,   30234,   31000,   31847,   32080,   32180,   33190,\n          33766,   35511,   36005,   36720,   38375,   39453,   39575,\n          40463,   41231,   41726,   41856,   43694,   44740,   52466,\n          53903,   55583,   56403,   66100,   67833,   67923,   68396,\n          69595,   76064,   90022,   92489,   94368,  101835,  108197,\n         108534,  112001,  119516,  120819,  139354,  142665,  156886,\n         168368,  170801,  172464,  184526,  208508,  219126,  241621,\n         242000,  244853,  259335,  259903,  276622,  280657,  299892,\n         306162,  307610,  322047,  327051,  343191,  383837,  406058,\n         413077,  455191,  473404,  498055,  506572,  601617,  607756,\n         691482,  782294,  888935, 1000069, 1018960, 1036576, 1101159,\n        1128091, 1133400, 1318339, 1451065, 1712603, 1743140]),\n array([0.        , 0.02702703, 0.03      , 0.03125   , 0.04761905,\n        0.09677419, 0.20512821, 0.22      , 0.4       , 1.21428571]),\n array([0.        , 0.03      , 0.03571429, 0.10869565, 0.11111111,\n        0.13      , 0.13888889, 0.15384615, 0.16      , 0.24      ,\n        0.27027027, 0.3       , 0.31      , 0.4       , 0.41025641,\n        0.4375    , 0.5       , 0.7948718 ]),\n array([0.        , 0.03571429, 0.05128205, 0.12820513, 0.13      ,\n        0.21621622, 0.22222222, 0.23076923, 0.27586207, 0.3       ,\n        0.32432432, 0.4       , 0.40625   , 0.47      , 0.51612903,\n        0.53      , 0.55      , 0.71052632, 0.8125    ]),\n array(['1060812', '1082935', '1089674', '1108348', '11537149', '1163113',\n        '11673659', '118013', '1212843', '123888', '1292604', '1310512',\n        '1321004', '1389153', '1443753', '147235', '14740022', '14926380',\n        '1514364', '1514799', '1543658', '1548155', '157652', '1600475',\n        '1601508', '16494981', '1667114', '16753980', '1675757', '1677840',\n        '16804703', '169770', '1710605', '1723026', '1755299', '1762409',\n        '1802055', '1807967', '1841592', '1851377', '1896479', '1913563',\n        '1925862', '1938812', '201412', '2018672', '2024337', '202761',\n        '2110577', '2125369', '2195820', '22377276', '2308652', '2329505',\n        '233299', '2373047', '2419708', '2450874', '2511249', '2523352',\n        '2525301', '2567976', '2648861', '27858007', '2823366', '2823413',\n        '2851375', '2858600', '2972212', '3206892', '3434548', '3444533',\n        '3816261', '3898510', '4032810', '4188265', '4247403', '4404689',\n        '4427000', '4468640', '447643', '4752398', '4830320', '4830392',\n        '4867146', '4958329', '505196', '50819', '5145006', '5596906',\n        '567083', '5903552', '597159', '59841', '6065508', '6166904',\n        '6212824', '6217715', '6879383', '693750', '6994577', '70048',\n        '7071605', '740947', '776120', '7891198', '800312', '830159',\n        '8620804', '8818088', '919235', '924080', '932549', '960708',\n        '966606', '971485', '974325', '9898522'], dtype=object),\n array(['East China', 'North China', 'Northeast China', 'Northwest China',\n        'South Central China', 'Southwest China'], dtype=object),\n array([  475184,   546541,   632880,   736165,   757990,   819028,\n          866691,   948521,   976396,  1047698,  1078754,  1109537,\n         1174622,  1184990,  1210637,  1216605,  1228569,  1258100,\n         1308445,  1333133,  1364344,  1364980,  1423771,  1426600,\n         1428990,  1440939,  1472622,  1492835,  1499110,  1554999,\n         1648826,  1658350,  1742585,  1782317,  1845611,  1873822,\n         1898911,  1927102,  1962192,  1962633,  1986738,  2017594,\n         2023674,  2047192,  2052220,  2053980,  2072426,  2135224,\n         2138158,  2138758,  2143190,  2150325,  2254281,  2261631,\n         2268499,  2339769,  2347862,  2355164,  2376983,  2378616,\n         2404936,  2444270,  2455900,  2545841,  2553268,  2649011,\n         2764053,  2815820,  2867525,  2907301,  2926542,  2939778,\n         2940367,  2977880,  3035767,  3051103,  3101537,  3114638,\n         3124234,  3343228,  3388449,  3545004,  3557071,  3586373,\n         3847158,  3893879,  3923569,  4039036,  4062020,  4073606,\n         4133488,  4229821,  4390259,  4559252,  4607955,  4613724,\n         4686125,  4947824,  5046865,  5167300,  5304833,  5502470,\n         5639838,  6003791,  6033279,  6185600,  6308151,  6349262,\n         6357869,  6832541,  7040099,  7537692,  7601825,  7646885,\n         7666512,  7968319,  8340692, 10533312])]\n\n\n\nord_encoder.transform(df)\n\narray([[  0.,   0.,  17., ...,  24.,   0.,  28.],\n       [  1.,   0.,  48., ...,  29.,   0.,  60.],\n       [  2.,   0.,  64., ...,  40.,   0.,  67.],\n       ...,\n       [115.,  27.,  44., ...,  97.,   0.,  53.],\n       [116.,  27.,  84., ...,   4.,   0.,  64.],\n       [117.,  27.,  78., ...,  25.,   0.,  71.]])\n\n\n\n\nCategories that are unknown during fit\n\n\nHow to handle unknown categories in OridinalEncoder?\n\n\nProvide all the categories in the constructor"
  },
  {
    "objectID": "posts/2021-05-31-pandas-interoperability.html#onehotencoder",
    "href": "posts/2021-05-31-pandas-interoperability.html#onehotencoder",
    "title": "Pandas Interoperability",
    "section": "OneHotEncoder",
    "text": "OneHotEncoder\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder()\nX_trans = ohe.fit_transform(df)\nX_trans\n\n<118x909 sparse matrix of type '<class 'numpy.float64'>'\n    with 1534 stored elements in Compressed Sparse Row format>\n\n\n\nX_trans.toarray()\n\narray([[1., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\nSwitch to dense\n\nohe = OneHotEncoder(sparse=False)\nohe.fit_transform(df)\n\narray([[1., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\n\nUnknown categories during transform?\n\n\nOHE can handle unknowns\n\nohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\nohe.fit(df)\n\nOneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)\n\n\n\nohe.transform(df)\n\narray([[1., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\nohe.categories_\n\n[array([  4,   6,   7,  10,  11,  16,  18,  19,  22,  23,  34,  35,  40,\n         42,  43,  46,  47,  52,  54,  58,  64,  66,  67,  70,  71,  76,\n         78,  79,  82,  83,  88,  90,  91,  94,  95, 107, 112, 119, 124,\n        126, 127, 130, 131, 136, 138, 139, 142, 143, 148, 150, 151, 154,\n        155, 160, 162, 163, 166, 167, 172, 174, 175, 178, 179, 184, 186,\n        187, 190, 191, 196, 198, 199, 202, 203, 220, 222, 223, 226, 227,\n        232, 234, 235, 239, 244, 246, 247, 250, 251, 258, 259, 262, 263,\n        280, 282, 283, 292, 294, 295, 298, 310, 316, 318, 319, 322, 323,\n        328, 330, 331, 334, 335, 340, 342, 343, 346, 347, 354, 355, 358,\n        359]),\n array(['Anhui', 'Beijing', 'Chongqing', 'Fujian', 'Gansu', 'Guangdong',\n        'Guangxi', 'Guizhou', 'Hainan', 'Hebei', 'Heilongjiang', 'Henan',\n        'Hubei', 'Hunan', 'Jiangsu', 'Jiangxi', 'Jilin', 'Ningxia',\n        'Qinghai', 'Shaanxi', 'Shandong', 'Shanxi', 'Sichuan', 'Tianjin',\n        'Tibet', 'Xinjiang', 'Yunnan', 'Zhejiang'], dtype=object),\n array([  72939.,   91405.,   96825.,  103041.,  107046.,  107687.,\n         119536.,  127819.,  133858.,  137190.,  142650.,  144839.,\n         147749.,  148812.,  160637.,  179235.,  181409.,  195580.,\n         207615.,  217707.,  223984.,  228043.,  237948.,  248903.,\n         251539.,  254002.,  262197.,  265770.,  271297.,  271499.,\n         281769.,  295133.,  319710.,  331999.,  337894.,  340372.,\n         353776.,  354775.,  359275.,  365437.,  367568.,  369552.,\n         370049.,  379186.,  391292.,  395775.,  397517.,  430577.,\n         434149.,  436189.,  447709.,  458201.,  469514.,  472786.,\n         484715.,  487155.,  510656.,  518022.,  531994.,  558569.,\n         575550.,  591088.,  601485.,  615593.,  619201.,  642581.,\n         675931.,  681676.,  684767.,  685732.,  688887.,  714572.,\n         752279.,  753552.,  761081.,  821750.,  833423.,  833430.,\n         844647.,  859482.,  875877.,  909559.,  978069.,  985851.,\n        1017303., 1035872., 1097470., 1188989., 1204547., 1207353.,\n        1224179., 1235386., 1246484., 1315102., 1331590., 1358528.,\n        1388043., 1457872., 1550764., 1562694., 1791403., 1890650.,\n        1897575., 1956261., 2018158., 2022957., 2045869., 2121243.,\n        2213991., 2225220., 2482173., 2663667., 2669238., 2922784.,\n        2981235., 3156087., 3847672., 3860764.]),\n array([      0.,    2990.,    8115.,   11755.,   11767.,   17400.,\n          20842.,   26300.,   27302.,   27387.,   29646.,   30705.,\n          32100.,   32119.,   32868.,   34842.,   36670.,   36946.,\n          40604.,   44623.,   45683.,   50097.,   52108.,   53900.,\n          56070.,   58533.,   59263.,   60560.,   60906.,   62000.,\n          66100.,   66529.,   68142.,   71807.,   80361.,   80609.,\n          81879.,   86256.,   88007.,   93323.,   95648.,  100000.,\n         100900.,  107658.,  108624.,  112137.,  113000.,  114418.,\n         116000.,  119658.,  123317.,  123546.,  124582.,  124647.,\n         129791.,  135765.,  143800.,  145000.,  149549.,  150000.,\n         153640.,  154364.,  165071.,  173552.,  173556.,  178705.,\n         179252.,  188633.,  197539.,  229895.,  241282.,  260313.,\n         264185.,  269596.,  279052.,  280277.,  302600.,  309582.,\n         317700.,  320627.,  321686.,  363054.,  394795.,  400000.,\n         405966.,  423049.,  429591.,  434318.,  447900.,  460668.,\n         498913.,  516342.,  527300.,  540479.,  564400.,  570723.,\n         581800.,  605400.,  655919.,  659400.,  694400.,  763953.,\n        1016400., 1023453., 1046700., 1081000., 1131615., 1187958.,\n        1197400., 1214100., 1239200., 1263500., 1272600., 1329200.,\n        1737800.]),\n array([2000, 2002, 2003, 2006, 2007]),\n array([  117.8 ,   162.04,   185.09,   263.68,   290.76,   295.02,\n          340.65,   341.43,   377.16,   390.2 ,   445.36,   725.9 ,\n          797.35,   919.11,  1029.92,  1052.88,  1232.03,  1243.43,\n         1254.17,  1363.56,  1426.34,  1612.65,  1672.96,  1804.  ,\n         1845.72,  1853.65,  1886.35,  2011.19,  2080.04,  2120.35,\n         2175.68,  2253.39,  2277.35,  2312.82,  2324.8 ,  2338.98,\n         2348.54,  2450.48,  2523.73,  2556.02,  2587.72,  2821.11,\n         2855.23,  2884.11,  2902.09,  3045.26,  3151.4 ,  3161.66,\n         3519.72,  3523.16,  3545.39,  3551.49,  3620.27,  3637.2 ,\n         3764.54,  3907.23,  3923.11,  3928.2 ,  3988.14,  4056.76,\n         4057.4 ,  4151.54,  4212.82,  4275.12,  4315.  ,  4462.74,\n         4467.55,  4659.99,  4676.13,  4725.01,  4743.61,  4746.16,\n         4757.45,  4772.52,  4820.53,  4983.67,  5007.21,  5043.96,\n         5052.99,  5333.09,  5757.29,  5823.41,  6035.48,  6112.5 ,\n         6211.8 ,  6867.7 ,  7104.  ,  7360.92,  7583.85,  7617.47,\n         7688.67,  7697.82,  8003.67,  8117.78,  8690.24,  9248.53,\n         9333.4 ,  9439.6 ,  9456.84,  9705.02,  9846.81, 10275.5 ,\n        10606.85, 10741.25, 12078.15, 12362.79, 13502.42, 13607.32,\n        15012.46, 15718.47, 15844.64, 18598.69, 18753.73, 21742.05,\n        21900.19, 25776.91, 26587.76, 31777.01]),\n array([      2,     293,     467,    1522,    1534,    1741,    1743,\n           1899,    1911,    2200,    2418,    2501,    2522,    2954,\n           3718,    3821,    4521,    4726,    5047,    6121,    6235,\n           8384,    9384,   10366,   11020,   11169,   12484,   12651,\n          12812,   21164,   21361,   22472,   24468,   28842,   30086,\n          30120,   30234,   31000,   31847,   32080,   32180,   33190,\n          33766,   35511,   36005,   36720,   38375,   39453,   39575,\n          40463,   41231,   41726,   41856,   43694,   44740,   52466,\n          53903,   55583,   56403,   66100,   67833,   67923,   68396,\n          69595,   76064,   90022,   92489,   94368,  101835,  108197,\n         108534,  112001,  119516,  120819,  139354,  142665,  156886,\n         168368,  170801,  172464,  184526,  208508,  219126,  241621,\n         242000,  244853,  259335,  259903,  276622,  280657,  299892,\n         306162,  307610,  322047,  327051,  343191,  383837,  406058,\n         413077,  455191,  473404,  498055,  506572,  601617,  607756,\n         691482,  782294,  888935, 1000069, 1018960, 1036576, 1101159,\n        1128091, 1133400, 1318339, 1451065, 1712603, 1743140]),\n array([0.        , 0.02702703, 0.03      , 0.03125   , 0.04761905,\n        0.09677419, 0.20512821, 0.22      , 0.4       , 1.21428571]),\n array([0.        , 0.03      , 0.03571429, 0.10869565, 0.11111111,\n        0.13      , 0.13888889, 0.15384615, 0.16      , 0.24      ,\n        0.27027027, 0.3       , 0.31      , 0.4       , 0.41025641,\n        0.4375    , 0.5       , 0.7948718 ]),\n array([0.        , 0.03571429, 0.05128205, 0.12820513, 0.13      ,\n        0.21621622, 0.22222222, 0.23076923, 0.27586207, 0.3       ,\n        0.32432432, 0.4       , 0.40625   , 0.47      , 0.51612903,\n        0.53      , 0.55      , 0.71052632, 0.8125    ]),\n array(['1060812', '1082935', '1089674', '1108348', '11537149', '1163113',\n        '11673659', '118013', '1212843', '123888', '1292604', '1310512',\n        '1321004', '1389153', '1443753', '147235', '14740022', '14926380',\n        '1514364', '1514799', '1543658', '1548155', '157652', '1600475',\n        '1601508', '16494981', '1667114', '16753980', '1675757', '1677840',\n        '16804703', '169770', '1710605', '1723026', '1755299', '1762409',\n        '1802055', '1807967', '1841592', '1851377', '1896479', '1913563',\n        '1925862', '1938812', '201412', '2018672', '2024337', '202761',\n        '2110577', '2125369', '2195820', '22377276', '2308652', '2329505',\n        '233299', '2373047', '2419708', '2450874', '2511249', '2523352',\n        '2525301', '2567976', '2648861', '27858007', '2823366', '2823413',\n        '2851375', '2858600', '2972212', '3206892', '3434548', '3444533',\n        '3816261', '3898510', '4032810', '4188265', '4247403', '4404689',\n        '4427000', '4468640', '447643', '4752398', '4830320', '4830392',\n        '4867146', '4958329', '505196', '50819', '5145006', '5596906',\n        '567083', '5903552', '597159', '59841', '6065508', '6166904',\n        '6212824', '6217715', '6879383', '693750', '6994577', '70048',\n        '7071605', '740947', '776120', '7891198', '800312', '830159',\n        '8620804', '8818088', '919235', '924080', '932549', '960708',\n        '966606', '971485', '974325', '9898522'], dtype=object),\n array(['East China', 'North China', 'Northeast China', 'Northwest China',\n        'South Central China', 'Southwest China'], dtype=object),\n array([  475184,   546541,   632880,   736165,   757990,   819028,\n          866691,   948521,   976396,  1047698,  1078754,  1109537,\n         1174622,  1184990,  1210637,  1216605,  1228569,  1258100,\n         1308445,  1333133,  1364344,  1364980,  1423771,  1426600,\n         1428990,  1440939,  1472622,  1492835,  1499110,  1554999,\n         1648826,  1658350,  1742585,  1782317,  1845611,  1873822,\n         1898911,  1927102,  1962192,  1962633,  1986738,  2017594,\n         2023674,  2047192,  2052220,  2053980,  2072426,  2135224,\n         2138158,  2138758,  2143190,  2150325,  2254281,  2261631,\n         2268499,  2339769,  2347862,  2355164,  2376983,  2378616,\n         2404936,  2444270,  2455900,  2545841,  2553268,  2649011,\n         2764053,  2815820,  2867525,  2907301,  2926542,  2939778,\n         2940367,  2977880,  3035767,  3051103,  3101537,  3114638,\n         3124234,  3343228,  3388449,  3545004,  3557071,  3586373,\n         3847158,  3893879,  3923569,  4039036,  4062020,  4073606,\n         4133488,  4229821,  4390259,  4559252,  4607955,  4613724,\n         4686125,  4947824,  5046865,  5167300,  5304833,  5502470,\n         5639838,  6003791,  6033279,  6185600,  6308151,  6349262,\n         6357869,  6832541,  7040099,  7537692,  7601825,  7646885,\n         7666512,  7968319,  8340692, 10533312])]"
  },
  {
    "objectID": "posts/2021-05-31-pandas-interoperability.html#two-categorical-features",
    "href": "posts/2021-05-31-pandas-interoperability.html#two-categorical-features",
    "title": "Pandas Interoperability",
    "section": "Two categorical features",
    "text": "Two categorical features\n\ndf_train = pd.DataFrame({\n    \"province\": [\"Zhejiang\", \"Beijing\", \"Shanghai\"],\n    \"region\": [\"East China\", \"North China\", \"Southwest China\"]\n})\n\n\nohe.fit(df_train)\n\nOneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)\n\n\n\nohe.categories_\n\n[array(['Beijing', 'Shanghai', 'Zhejiang'], dtype=object),\n array(['East China', 'North China', 'Southwest China'], dtype=object)]\n\n\n\nohe.transform(df_train)\n\narray([[0., 0., 1., 1., 0., 0.],\n       [1., 0., 0., 0., 1., 0.],\n       [0., 1., 0., 0., 0., 1.]])"
  },
  {
    "objectID": "posts/2021-05-31-pandas-interoperability.html#with-oridinalencoder",
    "href": "posts/2021-05-31-pandas-interoperability.html#with-oridinalencoder",
    "title": "Pandas Interoperability",
    "section": "With OridinalEncoder",
    "text": "With OridinalEncoder\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      province\n      specific\n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      reg\n      it\n    \n  \n  \n    \n      4\n      4\n      Anhui\n      195580.0\n      32100.0\n      2000\n      2902.09\n      31847\n      0.000000\n      0.000000\n      0.000000\n      1601508\n      East China\n      1499110\n    \n    \n      6\n      6\n      Anhui\n      434149.0\n      66529.0\n      2002\n      3519.72\n      38375\n      0.000000\n      0.000000\n      0.000000\n      1677840\n      East China\n      2404936\n    \n    \n      7\n      7\n      Anhui\n      619201.0\n      52108.0\n      2003\n      3923.11\n      36720\n      0.000000\n      0.000000\n      0.000000\n      1896479\n      East China\n      2815820\n    \n    \n      10\n      10\n      Anhui\n      1457872.0\n      279052.0\n      2006\n      6112.50\n      139354\n      0.000000\n      0.000000\n      0.324324\n      3434548\n      East China\n      5167300\n    \n    \n      11\n      11\n      Anhui\n      2213991.0\n      178705.0\n      2007\n      7360.92\n      299892\n      0.000000\n      0.000000\n      0.324324\n      4468640\n      East China\n      7040099\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      347\n      347\n      Yunnan\n      2482173.0\n      564400.0\n      2007\n      4772.52\n      39453\n      0.000000\n      0.000000\n      0.000000\n      4867146\n      Southwest China\n      6832541\n    \n    \n      354\n      354\n      Zhejiang\n      365437.0\n      321686.0\n      2002\n      8003.67\n      307610\n      0.000000\n      0.000000\n      0.000000\n      4958329\n      East China\n      1962633\n    \n    \n      355\n      355\n      Zhejiang\n      391292.0\n      260313.0\n      2003\n      9705.02\n      498055\n      1.214286\n      0.035714\n      0.035714\n      6217715\n      East China\n      2261631\n    \n    \n      358\n      358\n      Zhejiang\n      1017303.0\n      394795.0\n      2006\n      15718.47\n      888935\n      1.214286\n      0.035714\n      0.035714\n      11537149\n      East China\n      2553268\n    \n    \n      359\n      359\n      Zhejiang\n      844647.0\n      0.0\n      2007\n      18753.73\n      1036576\n      0.047619\n      0.000000\n      0.000000\n      16494981\n      East China\n      2939778\n    \n  \n\n118 rows  13 columns\n\n\n\n\nct = ColumnTransformer([\n    ('numerical', StandardScaler(), ['fdi', 'gdp']),\n    ('categorical', OrdinalEncoder(), ['reg'])\n])\n\nct.fit_transform(df)\n\narray([[-0.54088554, -0.48498461,  0.        ],\n       [-0.52313911, -0.37867596,  0.        ],\n       [-0.52763824, -0.30924305,  0.        ],\n       [-0.24862668,  0.06760246,  0.        ],\n       [ 0.18779748,  0.28248491,  0.        ],\n       [-0.16975183, -0.44030651,  1.        ],\n       [-0.15861681, -0.24178957,  1.        ],\n       [-0.0317657 , -0.12264395,  1.        ],\n       [ 0.60997938,  0.41275831,  1.        ],\n       [ 0.74965914,  0.71036505,  1.        ],\n       [-0.43826722, -0.31197638,  5.        ],\n       [-0.33241116, -0.1796306 ,  5.        ],\n       [ 0.30550625, -0.33653668,  0.        ],\n       [ 0.41600281, -0.21553212,  0.        ],\n       [ 0.07908699, -0.12669573,  0.        ],\n       [ 0.24802608,  0.3208564 ,  0.        ],\n       [ 0.47641082,  0.60738699,  0.        ],\n       [-0.61051202, -0.80327715,  3.        ],\n       [-0.61082193, -0.77244122,  3.        ],\n       [-0.61943145, -0.59251706,  3.        ],\n       [ 2.43926479,  0.86431921,  4.        ],\n       [ 2.45369736,  1.33958151,  4.        ],\n       [ 1.49921217,  1.74273266,  4.        ],\n       [ 3.31727285,  3.5918763 ,  4.        ],\n       [ 4.02826654,  4.48506828,  4.        ],\n       [-0.48483258, -0.62647875,  4.        ],\n       [-0.51402938, -0.55010927,  4.        ],\n       [-0.51367597, -0.49892317,  4.        ],\n       [-0.50583579, -0.16757679,  4.        ],\n       [-0.44152672,  0.01784327,  4.        ],\n       [-0.62066294, -0.8072291 ,  5.        ],\n       [-0.6170745 , -0.77047901,  5.        ],\n       [-0.61517155, -0.7389959 ,  5.        ],\n       [-0.60195143, -0.58190909,  5.        ],\n       [-0.59307006, -0.48807939,  5.        ],\n       [-0.32298609, -0.7686304 ,  4.        ],\n       [-0.44281257, -0.11631841,  1.        ],\n       [ 0.02938719,  1.35763727,  1.        ],\n       [-0.54567284, -0.4420725 ,  2.        ],\n       [-0.53092492, -0.35845489,  2.        ],\n       [-0.53998028, -0.28612859,  2.        ],\n       [-0.1631377 ,  0.08469433,  2.        ],\n       [-0.06063084,  0.23826293,  2.        ],\n       [-0.47412981, -0.11476413,  4.        ],\n       [-0.51746286,  0.05434551,  4.        ],\n       [-0.48092608,  0.19759014,  4.        ],\n       [-0.12582615,  1.14342438,  4.        ],\n       [ 0.20484254,  1.59949491,  4.        ],\n       [-0.37092157, -0.37425755,  4.        ],\n       [-0.23962569, -0.25937715,  4.        ],\n       [-0.20096576, -0.16563352,  4.        ],\n       [ 0.03817341,  0.3266432 ,  4.        ],\n       [ 0.12453776,  0.62199511,  4.        ],\n       [-0.44305724, -0.3732076 ,  4.        ],\n       [-0.38273622, -0.26992488,  4.        ],\n       [-0.35062246, -0.18240867,  4.        ],\n       [ 0.07754287,  0.33889839,  4.        ],\n       [ 0.2616295 ,  0.64027463,  4.        ],\n       [ 1.02472886,  0.34047332,  0.        ],\n       [ 1.25233884,  0.64324204,  0.        ],\n       [ 2.14259107,  0.84118581,  0.        ],\n       [ 2.95645589,  2.21676944,  0.        ],\n       [ 4.11128168,  2.75781563,  0.        ],\n       [-0.54025213, -0.6654458 ,  0.        ],\n       [-0.5198769 , -0.61001686,  0.        ],\n       [-0.3333273 , -0.56271731,  0.        ],\n       [ 0.0304175 , -0.28623875,  0.        ],\n       [ 0.13550694, -0.15477596,  0.        ],\n       [-0.54558041, -0.69654679,  2.        ],\n       [-0.53566872, -0.61954045,  2.        ],\n       [-0.56094543, -0.58026359,  2.        ],\n       [-0.44776842, -0.36136894,  2.        ],\n       [-0.42068118, -0.24865385,  2.        ],\n       [-0.62272901, -0.93372268,  3.        ],\n       [-0.62148121, -0.91958445,  3.        ],\n       [-0.62272357, -0.90784563,  3.        ],\n       [-0.61735451, -0.8595581 ,  3.        ],\n       [-0.61374161, -0.82630211,  3.        ],\n       [-0.59750395, -0.93911703,  3.        ],\n       [-0.61461425, -0.92586868,  3.        ],\n       [-0.62060585, -0.91733996,  3.        ],\n       [-0.54318812, -0.84725987,  3.        ],\n       [-0.54905466, -0.67399173,  3.        ],\n       [-0.52958198, -0.59664114,  3.        ],\n       [-0.53723458, -0.53909508,  3.        ],\n       [-0.37602966, -0.16801571,  3.        ],\n       [-0.30255648,  0.00646247,  3.        ],\n       [ 0.6594916 ,  0.78415268,  0.        ],\n       [ 1.00803993,  1.09443114,  0.        ],\n       [ 2.0912357 ,  2.78503525,  0.        ],\n       [ 2.36604988,  3.45230994,  0.        ],\n       [-0.56637157, -0.66681073,  1.        ],\n       [-0.56992738, -0.5843498 ,  1.        ],\n       [-0.56939184, -0.49305032,  1.        ],\n       [-0.50867935, -0.30836695,  5.        ],\n       [-0.47635899, -0.1712172 ,  5.        ],\n       [-0.51537504, -0.06655233,  5.        ],\n       [-0.29901427,  0.51129215,  5.        ],\n       [ 0.49549204, -0.21636004,  1.        ],\n       [-0.62745649, -0.96422641,  5.        ],\n       [-0.62666541, -0.95661166,  5.        ],\n       [-0.62619239, -0.95264422,  5.        ],\n       [-0.62332436, -0.93445592,  5.        ],\n       [-0.62088857, -0.92573443,  5.        ],\n       [-0.62226686, -0.74980181,  3.        ],\n       [-0.62229948, -0.70692756,  3.        ],\n       [-0.62329174, -0.65981736,  3.        ],\n       [-0.59928186, -0.46034169,  3.        ],\n       [-0.59352405, -0.37808386,  3.        ],\n       [-0.59263238, -0.63832946,  5.        ],\n       [-0.59709889, -0.58641184,  5.        ],\n       [-0.60466994, -0.5445514 ,  5.        ],\n       [-0.5452705 , -0.29804986,  5.        ],\n       [-0.52020855, -0.16303961,  5.        ],\n       [ 0.20877895,  0.3931173 ,  0.        ],\n       [ 0.72650559,  0.68595965,  0.        ],\n       [ 1.7891168 ,  1.72101584,  0.        ],\n       [ 2.19048034,  2.24345547,  0.        ]])"
  },
  {
    "objectID": "posts/2021-05-31-pandas-interoperability.html#with-onehotencoder",
    "href": "posts/2021-05-31-pandas-interoperability.html#with-onehotencoder",
    "title": "Pandas Interoperability",
    "section": "With OneHotEncoder",
    "text": "With OneHotEncoder\n\nct = ColumnTransformer([\n    ('numerical', StandardScaler(), ['fdi', 'gdp']),\n    ('categorical', OneHotEncoder(), ['reg'])\n])\n\n\nct.fit_transform(df)\n\narray([[-0.54088554, -0.48498461,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.52313911, -0.37867596,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.52763824, -0.30924305,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.24862668,  0.06760246,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.18779748,  0.28248491,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.16975183, -0.44030651,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.15861681, -0.24178957,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.0317657 , -0.12264395,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.60997938,  0.41275831,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.74965914,  0.71036505,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.43826722, -0.31197638,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.33241116, -0.1796306 ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [ 0.30550625, -0.33653668,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.41600281, -0.21553212,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.07908699, -0.12669573,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.24802608,  0.3208564 ,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.47641082,  0.60738699,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.61051202, -0.80327715,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.61082193, -0.77244122,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.61943145, -0.59251706,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [ 2.43926479,  0.86431921,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 2.45369736,  1.33958151,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 1.49921217,  1.74273266,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 3.31727285,  3.5918763 ,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 4.02826654,  4.48506828,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.48483258, -0.62647875,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.51402938, -0.55010927,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.51367597, -0.49892317,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.50583579, -0.16757679,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.44152672,  0.01784327,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.62066294, -0.8072291 ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.6170745 , -0.77047901,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.61517155, -0.7389959 ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.60195143, -0.58190909,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.59307006, -0.48807939,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.32298609, -0.7686304 ,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.44281257, -0.11631841,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.02938719,  1.35763727,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.54567284, -0.4420725 ,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.53092492, -0.35845489,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.53998028, -0.28612859,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.1631377 ,  0.08469433,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.06063084,  0.23826293,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.47412981, -0.11476413,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.51746286,  0.05434551,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.48092608,  0.19759014,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.12582615,  1.14342438,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 0.20484254,  1.59949491,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.37092157, -0.37425755,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.23962569, -0.25937715,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.20096576, -0.16563352,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 0.03817341,  0.3266432 ,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 0.12453776,  0.62199511,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.44305724, -0.3732076 ,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.38273622, -0.26992488,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.35062246, -0.18240867,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 0.07754287,  0.33889839,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 0.2616295 ,  0.64027463,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 1.02472886,  0.34047332,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 1.25233884,  0.64324204,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 2.14259107,  0.84118581,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 2.95645589,  2.21676944,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 4.11128168,  2.75781563,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.54025213, -0.6654458 ,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.5198769 , -0.61001686,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.3333273 , -0.56271731,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.0304175 , -0.28623875,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.13550694, -0.15477596,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.54558041, -0.69654679,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.53566872, -0.61954045,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.56094543, -0.58026359,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.44776842, -0.36136894,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.42068118, -0.24865385,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.62272901, -0.93372268,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.62148121, -0.91958445,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.62272357, -0.90784563,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.61735451, -0.8595581 ,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.61374161, -0.82630211,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.59750395, -0.93911703,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.61461425, -0.92586868,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.62060585, -0.91733996,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.54318812, -0.84725987,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.54905466, -0.67399173,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.52958198, -0.59664114,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.53723458, -0.53909508,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.37602966, -0.16801571,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.30255648,  0.00646247,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [ 0.6594916 ,  0.78415268,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 1.00803993,  1.09443114,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 2.0912357 ,  2.78503525,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 2.36604988,  3.45230994,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.56637157, -0.66681073,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.56992738, -0.5843498 ,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.56939184, -0.49305032,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.50867935, -0.30836695,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.47635899, -0.1712172 ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.51537504, -0.06655233,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.29901427,  0.51129215,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [ 0.49549204, -0.21636004,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.62745649, -0.96422641,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.62666541, -0.95661166,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.62619239, -0.95264422,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.62332436, -0.93445592,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.62088857, -0.92573443,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.62226686, -0.74980181,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.62229948, -0.70692756,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.62329174, -0.65981736,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.59928186, -0.46034169,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.59352405, -0.37808386,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.59263238, -0.63832946,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.59709889, -0.58641184,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.60466994, -0.5445514 ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.5452705 , -0.29804986,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.52020855, -0.16303961,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [ 0.20877895,  0.3931173 ,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.72650559,  0.68595965,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 1.7891168 ,  1.72101584,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 2.19048034,  2.24345547,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ]])\n\n\n\n# df\n\n\ny\n\n4       195580.0\n6       434149.0\n7       619201.0\n10     1457872.0\n11     2213991.0\n         ...    \n347    2482173.0\n354     365437.0\n355     391292.0\n358    1017303.0\n359     844647.0\nName: specific, Length: 118, dtype: float64\n\n\n\nX.head()\n\n\n\n\n\n  \n    \n      \n      province\n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      reg\n      it\n    \n  \n  \n    \n      4\n      Anhui\n      32100.0\n      2000\n      2902.09\n      31847\n      0.0\n      0.0\n      0.000000\n      1601508\n      East China\n      1499110\n    \n    \n      6\n      Anhui\n      66529.0\n      2002\n      3519.72\n      38375\n      0.0\n      0.0\n      0.000000\n      1677840\n      East China\n      2404936\n    \n    \n      7\n      Anhui\n      52108.0\n      2003\n      3923.11\n      36720\n      0.0\n      0.0\n      0.000000\n      1896479\n      East China\n      2815820\n    \n    \n      10\n      Anhui\n      279052.0\n      2006\n      6112.50\n      139354\n      0.0\n      0.0\n      0.324324\n      3434548\n      East China\n      5167300\n    \n    \n      11\n      Anhui\n      178705.0\n      2007\n      7360.92\n      299892\n      0.0\n      0.0\n      0.324324\n      4468640\n      East China\n      7040099\n    \n  \n\n\n\n\n\nAre three categories already encoded in the dataset?\n\nX.dtypes\n\nprovince     object\ngeneral     float64\nyear          int64\ngdp         float64\nfdi           int64\nrnr         float64\nrr          float64\ni           float64\nfr           object\nreg          object\nit            int64\ndtype: object\n\n\n\n\nAre there missing values in the dataset?\n\nmissing_values = pd.concat({\"na_cnt\": X.isna().sum(), \"dtypes\": X.dtypes}, axis='columns')\nmissing_values\n\n\n\n\n\n  \n    \n      \n      na_cnt\n      dtypes\n    \n  \n  \n    \n      province\n      0\n      object\n    \n    \n      general\n      0\n      float64\n    \n    \n      year\n      0\n      int64\n    \n    \n      gdp\n      0\n      float64\n    \n    \n      fdi\n      0\n      int64\n    \n    \n      rnr\n      0\n      float64\n    \n    \n      rr\n      0\n      float64\n    \n    \n      i\n      0\n      float64\n    \n    \n      fr\n      0\n      object\n    \n    \n      reg\n      0\n      object\n    \n    \n      it\n      0\n      int64\n    \n  \n\n\n\n\n\n\nSplit data into training and test set\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=42)"
  },
  {
    "objectID": "posts/2021-05-31-pandas-interoperability.html#columntransformer",
    "href": "posts/2021-05-31-pandas-interoperability.html#columntransformer",
    "title": "Pandas Interoperability",
    "section": "ColumnTransformer",
    "text": "ColumnTransformer\n\nmissing_values\n\n\n\n\n\n  \n    \n      \n      na_cnt\n      dtypes\n    \n  \n  \n    \n      province\n      0\n      object\n    \n    \n      general\n      0\n      float64\n    \n    \n      year\n      0\n      int64\n    \n    \n      gdp\n      0\n      float64\n    \n    \n      fdi\n      0\n      int64\n    \n    \n      rnr\n      0\n      float64\n    \n    \n      rr\n      0\n      float64\n    \n    \n      i\n      0\n      float64\n    \n    \n      fr\n      0\n      object\n    \n    \n      reg\n      0\n      object\n    \n    \n      it\n      0\n      int64\n    \n  \n\n\n\n\n\nX\n\n\n\n\n\n  \n    \n      \n      province\n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      reg\n      it\n    \n  \n  \n    \n      4\n      Anhui\n      32100.0\n      2000\n      2902.09\n      31847\n      0.000000\n      0.000000\n      0.000000\n      1601508\n      East China\n      1499110\n    \n    \n      6\n      Anhui\n      66529.0\n      2002\n      3519.72\n      38375\n      0.000000\n      0.000000\n      0.000000\n      1677840\n      East China\n      2404936\n    \n    \n      7\n      Anhui\n      52108.0\n      2003\n      3923.11\n      36720\n      0.000000\n      0.000000\n      0.000000\n      1896479\n      East China\n      2815820\n    \n    \n      10\n      Anhui\n      279052.0\n      2006\n      6112.50\n      139354\n      0.000000\n      0.000000\n      0.324324\n      3434548\n      East China\n      5167300\n    \n    \n      11\n      Anhui\n      178705.0\n      2007\n      7360.92\n      299892\n      0.000000\n      0.000000\n      0.324324\n      4468640\n      East China\n      7040099\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      347\n      Yunnan\n      564400.0\n      2007\n      4772.52\n      39453\n      0.000000\n      0.000000\n      0.000000\n      4867146\n      Southwest China\n      6832541\n    \n    \n      354\n      Zhejiang\n      321686.0\n      2002\n      8003.67\n      307610\n      0.000000\n      0.000000\n      0.000000\n      4958329\n      East China\n      1962633\n    \n    \n      355\n      Zhejiang\n      260313.0\n      2003\n      9705.02\n      498055\n      1.214286\n      0.035714\n      0.035714\n      6217715\n      East China\n      2261631\n    \n    \n      358\n      Zhejiang\n      394795.0\n      2006\n      15718.47\n      888935\n      1.214286\n      0.035714\n      0.035714\n      11537149\n      East China\n      2553268\n    \n    \n      359\n      Zhejiang\n      0.0\n      2007\n      18753.73\n      1036576\n      0.047619\n      0.000000\n      0.000000\n      16494981\n      East China\n      2939778\n    \n  \n\n118 rows  11 columns\n\n\n\n\nNumerical preprocessing\n\nnumerical_features = ['general', 'gdp', 'fdi', 'i', 'rr']\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nnum_prep = Pipeline([\n    ('imputer', SimpleImputer()),\n    ('scaler', StandardScaler())\n])\n\n\nnum_prep\n\nPipelinePipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler())])SimpleImputerSimpleImputer()StandardScalerStandardScaler()\n\n\n\nRunning only on numerical features\n\nnum_trans = num_prep.fit_transform(X_train[numerical_features])\nnum_trans\n\narray([[ 0.84445696, -0.23824571,  0.05133332, -0.46886997,  1.84019633],\n       [ 0.24766853,  2.07395803,  1.57444985, -0.46886997, -0.43195321],\n       [-0.72214407, -0.0446055 , -0.43939868, -0.46886997, -0.43195321],\n       [-0.48856958, -0.11992158, -0.34379901, -0.46886997, -0.43195321],\n       [ 0.85337791, -0.87758205, -0.54348651, -0.46886997, -0.43195321],\n       [-0.62992082, -0.83196322, -0.62382665, -0.46886997,  0.17004293],\n       [-0.83650183,  3.23074054,  4.28312405,  1.06326271, -0.43195321],\n       [ 1.76828095, -0.19541419, -0.41644878, -0.46886997, -0.43195321],\n       [ 1.78635605,  0.89747804,  0.79717537, -0.46886997,  3.97033652],\n       [-0.01481382, -0.57518987, -0.60442312,  3.47737968, -0.43195321],\n       [-0.33025804, -0.71766019, -0.62552372, -0.46886997, -0.43195321],\n       [-0.75781248, -0.70583032, -0.54596728, -0.46886997, -0.43195321],\n       [ 0.18859962, -0.10301925, -0.50475274, -0.46886997,  1.78339259],\n       [ 3.61705142,  1.91072514,  0.2322093 , -0.46886997, -0.43195321],\n       [-0.52047074, -0.52639929, -0.53731279, -0.46886997, -0.43195321],\n       [-0.35308192, -0.41382042, -0.15623907,  2.47473995, -0.43195321],\n       [ 0.51483816, -0.85369871, -0.61664935, -0.46886997, -0.43195321],\n       [ 2.42485843, -0.08843151,  0.16030939, -0.46886997,  1.84019633],\n       [-0.63055638, -0.3205428 , -0.53076977, -0.46886997, -0.43195321],\n       [-0.81570508, -0.2076344 , -0.22869714,  0.25314756, -0.43195321],\n       [-0.06101258, -0.78797637, -0.31514053, -0.46886997, -0.43195321],\n       [-0.022315  ,  0.78012485,  0.51382125, -0.46886997, -0.43195321],\n       [-0.37712312, -0.55331904, -0.3258642 ,  2.14150109, -0.43195321],\n       [ 2.33926246, -0.46826211, -0.59521328,  3.47737968, -0.43195321],\n       [ 0.62612061, -0.10351944, -0.37014586, -0.46886997, -0.43195321],\n       [-0.1455937 , -0.05181405, -0.01314966, -0.46886997,  3.97033652],\n       [-0.69280823, -0.04283426, -0.47187415, -0.46886997, -0.43195321],\n       [-0.15946076, -0.25170559, -0.5456459 , -0.46886997, -0.43195321],\n       [-0.80637666, -0.98226192, -0.59981116, -0.46886997, -0.43195321],\n       [-0.45208878, -0.48061962, -0.51288289, -0.46886997,  1.78339259],\n       [ 0.4867555 , -0.58727866, -0.62254962,  0.24317887, -0.43195321],\n       [-0.74721037, -0.26346288, -0.50770147, -0.46886997, -0.26580227],\n       [ 0.71498921, -0.96701118, -0.62406064,  0.76534802, -0.43195321],\n       [-0.6776111 , -0.6259812 , -0.48297275,  1.75272242, -0.43195321],\n       [ 0.20388901,  0.55832718,  0.6523297 , -0.46886997,  3.97033652],\n       [-0.68462536, -0.15766887,  0.45117911, -0.46886997,  1.22955614],\n       [-0.68130147, -0.59197843, -0.52937716, -0.46886997, -0.43195321],\n       [-0.1213603 ,  0.16498962, -0.23803102,  1.33242116, -0.43195321],\n       [-0.68649617, -0.8274596 , -0.61330032, -0.46886997,  0.42010287],\n       [-0.73244378, -0.41583293, -0.54606312, -0.46886997, -0.43195321],\n       [-0.75418874, -0.67194329, -0.56752738, -0.46886997, -0.43195321],\n       [-0.11822093, -0.18759169, -0.14469223,  2.47473995, -0.43195321],\n       [-0.5606009 , -0.89159703, -0.62039587, -0.46886997, -0.43195321],\n       [-0.70811556, -0.99767742, -0.6295606 ,  1.33242116, -0.43195321],\n       [-0.44276036, -0.47392695, -0.57065935, -0.46886997, -0.43195321],\n       [-0.61544895, -0.21965453, -0.37710046, -0.46886997, -0.43195321],\n       [-0.57791999, -0.10080471, -0.18860741, -0.46886997,  0.18342062],\n       [ 2.23213936, -0.34291185, -0.59568406, -0.46886997, -0.43195321],\n       [ 2.06354861,  1.39098988, -0.11068883, -0.46886997, -0.43195321],\n       [-0.79190991, -0.9574449 , -0.62376745,  0.24317887, -0.43195321],\n       [ 0.26443406,  2.61416753,  3.08558822,  1.06326271, -0.43195321],\n       [-0.01209987,  0.53594439,  0.23629128, -0.46886997, -0.43195321],\n       [-0.80634591, -0.96716418, -0.61755426, -0.46886997, -0.43195321],\n       [-0.16938374,  0.8696658 ,  0.77316551, -0.27051351, -0.23415447],\n       [-0.04311944,  0.18446741, -0.14938031,  0.73199078, -0.43195321],\n       [-0.68041476,  0.47595163,  1.08241827, -0.18405044, -0.43195321],\n       [-0.76631569, -0.33855142, -0.36484886, -0.46886997,  1.06490206],\n       [-0.71942755, -1.0021987 , -0.63005112, -0.46886997,  1.28493979],\n       [ 2.56991029,  0.81760341,  0.29109646, -0.46886997,  1.99108126],\n       [-0.74181833, -0.94662522, -0.62596349, -0.46886997, -0.43195321],\n       [-0.39172056, -0.75420507, -0.61813217,  0.73199078, -0.43195321],\n       [-0.74252565, -0.58032118, -0.59939112, -0.46886997, -0.43195321],\n       [-0.52984786,  0.82098506,  1.31844616, -0.46886997, -0.43195321],\n       [-0.75226924, -0.67038781, -0.54044193, -0.46886997, -0.43195321],\n       [-0.51722886, -0.15861235,  0.53360817, -0.46886997, -0.43195321],\n       [ 0.94307429,  1.63510561,  0.0502649 , -0.46886997,  2.33722904],\n       [ 2.27493734,  0.10828435, -0.43806527,  0.81281795, -0.43195321],\n       [-0.62666612, -0.76651942, -0.62548989, -0.46886997, -0.43195321],\n       [-0.66600439, -0.34358661, -0.52269601, -0.46886997, -0.43195321],\n       [ 2.40153737,  0.79677218,  0.14893452, -0.46886997,  0.18342062],\n       [ 0.09391613,  1.61452938,  2.56423569, -0.46886997, -0.43195321],\n       [-0.70296186, -0.26446128, -0.52736154, -0.46886997, -0.43195321],\n       [ 0.44208927,  4.18123112,  3.45974963, -0.46886997, -0.43195321],\n       [-0.78308892, -1.01087642, -0.63087146, -0.46886997,  0.89725427],\n       [-0.54691067,  0.98156598,  0.70367308, -0.46886997, -0.43195321],\n       [ 0.60991632, -0.09784872, -0.51965707, -0.46886997, -0.43195321],\n       [-0.76052643, -0.96000271, -0.62467519, -0.46886997,  0.33726408],\n       [ 1.93383484,  0.09531485, -0.29395536,  4.04373958, -0.43195321],\n       [-0.76653353, -0.6801267 , -0.54957002, -0.46886997, -0.43195321],\n       [-0.83650183,  2.64457877,  2.29128386, -0.46886997, -0.43195321],\n       [-0.51706228,  1.07292328,  2.54926935, -0.46886997, -0.43195321],\n       [-0.39173081,  1.0465606 ,  2.24162345, -0.46886997, -0.43195321],\n       [ 0.54861269, -0.32386364, -0.44453781, -0.46886997, -0.43195321],\n       [-0.41346552, -0.66397367, -0.62655268, -0.46886997, -0.43195321],\n       [-0.66187067, -0.05643144,  0.1018028 ,  1.19732432, -0.43195321],\n       [-0.61096157, -0.57797129, -0.5712147 , -0.46886997, -0.43195321],\n       [ 0.34407676,  0.4601909 ,  0.05937608, -0.46886997,  0.18342062],\n       [-0.46490255, -0.97695015, -0.62658651,  1.7874348 , -0.43195321]])\n\n\n\nnum_trans.shape\n\n(88, 5)\n\n\n\nX\n\n\n\n\n\n  \n    \n      \n      province\n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      reg\n      it\n    \n  \n  \n    \n      4\n      Anhui\n      32100.0\n      2000\n      2902.09\n      31847\n      0.000000\n      0.000000\n      0.000000\n      1601508\n      East China\n      1499110\n    \n    \n      6\n      Anhui\n      66529.0\n      2002\n      3519.72\n      38375\n      0.000000\n      0.000000\n      0.000000\n      1677840\n      East China\n      2404936\n    \n    \n      7\n      Anhui\n      52108.0\n      2003\n      3923.11\n      36720\n      0.000000\n      0.000000\n      0.000000\n      1896479\n      East China\n      2815820\n    \n    \n      10\n      Anhui\n      279052.0\n      2006\n      6112.50\n      139354\n      0.000000\n      0.000000\n      0.324324\n      3434548\n      East China\n      5167300\n    \n    \n      11\n      Anhui\n      178705.0\n      2007\n      7360.92\n      299892\n      0.000000\n      0.000000\n      0.324324\n      4468640\n      East China\n      7040099\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      347\n      Yunnan\n      564400.0\n      2007\n      4772.52\n      39453\n      0.000000\n      0.000000\n      0.000000\n      4867146\n      Southwest China\n      6832541\n    \n    \n      354\n      Zhejiang\n      321686.0\n      2002\n      8003.67\n      307610\n      0.000000\n      0.000000\n      0.000000\n      4958329\n      East China\n      1962633\n    \n    \n      355\n      Zhejiang\n      260313.0\n      2003\n      9705.02\n      498055\n      1.214286\n      0.035714\n      0.035714\n      6217715\n      East China\n      2261631\n    \n    \n      358\n      Zhejiang\n      394795.0\n      2006\n      15718.47\n      888935\n      1.214286\n      0.035714\n      0.035714\n      11537149\n      East China\n      2553268\n    \n    \n      359\n      Zhejiang\n      0.0\n      2007\n      18753.73\n      1036576\n      0.047619\n      0.000000\n      0.000000\n      16494981\n      East China\n      2939778\n    \n  \n\n118 rows  11 columns\n\n\n\n\n\n\nCategorical preprocessing\n\ncategorical_features = ['province', 'reg']\n\n\ncat_prep = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='sk_missing')),\n    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\n\ncat_prep\n\nPipelinePipeline(steps=[('imputer',\n                 SimpleImputer(fill_value='sk_missing', strategy='constant')),\n                ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))])SimpleImputerSimpleImputer(fill_value='sk_missing', strategy='constant')OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)\n\n\n\nRunning only on the categorical features\n\ncat_trans = cat_prep.fit_transform(X_train[categorical_features])\ncat_trans\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 1., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 1., 0.],\n       [0., 0., 0., ..., 0., 0., 1.]])\n\n\n\ncat_trans.shape\n\n(88, 33)"
  },
  {
    "objectID": "posts/2021-05-31-pandas-interoperability.html#columntransformer-1",
    "href": "posts/2021-05-31-pandas-interoperability.html#columntransformer-1",
    "title": "Pandas Interoperability",
    "section": "ColumnTransformer!",
    "text": "ColumnTransformer!\n\nct = ColumnTransformer([\n   ('numerical', num_prep, numerical_features),\n   ('categorical', cat_prep, categorical_features)\n])\n\n\nct\n\nColumnTransformerColumnTransformer(transformers=[('numerical',\n                                 Pipeline(steps=[('imputer', SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['general', 'gdp', 'fdi', 'i', 'rr']),\n                                ('categorical',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='sk_missing',\n                                                                strategy='constant')),\n                                                 ('ohe',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 ['province', 'reg'])])numerical['general', 'gdp', 'fdi', 'i', 'rr']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical['province', 'reg']SimpleImputerSimpleImputer(fill_value='sk_missing', strategy='constant')OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)\n\n\n\nX_trans = ct.fit_transform(X_train)\n\n\nX_trans[:, :5]\n\narray([[ 0.84445696, -0.23824571,  0.05133332, -0.46886997,  1.84019633],\n       [ 0.24766853,  2.07395803,  1.57444985, -0.46886997, -0.43195321],\n       [-0.72214407, -0.0446055 , -0.43939868, -0.46886997, -0.43195321],\n       [-0.48856958, -0.11992158, -0.34379901, -0.46886997, -0.43195321],\n       [ 0.85337791, -0.87758205, -0.54348651, -0.46886997, -0.43195321],\n       [-0.62992082, -0.83196322, -0.62382665, -0.46886997,  0.17004293],\n       [-0.83650183,  3.23074054,  4.28312405,  1.06326271, -0.43195321],\n       [ 1.76828095, -0.19541419, -0.41644878, -0.46886997, -0.43195321],\n       [ 1.78635605,  0.89747804,  0.79717537, -0.46886997,  3.97033652],\n       [-0.01481382, -0.57518987, -0.60442312,  3.47737968, -0.43195321],\n       [-0.33025804, -0.71766019, -0.62552372, -0.46886997, -0.43195321],\n       [-0.75781248, -0.70583032, -0.54596728, -0.46886997, -0.43195321],\n       [ 0.18859962, -0.10301925, -0.50475274, -0.46886997,  1.78339259],\n       [ 3.61705142,  1.91072514,  0.2322093 , -0.46886997, -0.43195321],\n       [-0.52047074, -0.52639929, -0.53731279, -0.46886997, -0.43195321],\n       [-0.35308192, -0.41382042, -0.15623907,  2.47473995, -0.43195321],\n       [ 0.51483816, -0.85369871, -0.61664935, -0.46886997, -0.43195321],\n       [ 2.42485843, -0.08843151,  0.16030939, -0.46886997,  1.84019633],\n       [-0.63055638, -0.3205428 , -0.53076977, -0.46886997, -0.43195321],\n       [-0.81570508, -0.2076344 , -0.22869714,  0.25314756, -0.43195321],\n       [-0.06101258, -0.78797637, -0.31514053, -0.46886997, -0.43195321],\n       [-0.022315  ,  0.78012485,  0.51382125, -0.46886997, -0.43195321],\n       [-0.37712312, -0.55331904, -0.3258642 ,  2.14150109, -0.43195321],\n       [ 2.33926246, -0.46826211, -0.59521328,  3.47737968, -0.43195321],\n       [ 0.62612061, -0.10351944, -0.37014586, -0.46886997, -0.43195321],\n       [-0.1455937 , -0.05181405, -0.01314966, -0.46886997,  3.97033652],\n       [-0.69280823, -0.04283426, -0.47187415, -0.46886997, -0.43195321],\n       [-0.15946076, -0.25170559, -0.5456459 , -0.46886997, -0.43195321],\n       [-0.80637666, -0.98226192, -0.59981116, -0.46886997, -0.43195321],\n       [-0.45208878, -0.48061962, -0.51288289, -0.46886997,  1.78339259],\n       [ 0.4867555 , -0.58727866, -0.62254962,  0.24317887, -0.43195321],\n       [-0.74721037, -0.26346288, -0.50770147, -0.46886997, -0.26580227],\n       [ 0.71498921, -0.96701118, -0.62406064,  0.76534802, -0.43195321],\n       [-0.6776111 , -0.6259812 , -0.48297275,  1.75272242, -0.43195321],\n       [ 0.20388901,  0.55832718,  0.6523297 , -0.46886997,  3.97033652],\n       [-0.68462536, -0.15766887,  0.45117911, -0.46886997,  1.22955614],\n       [-0.68130147, -0.59197843, -0.52937716, -0.46886997, -0.43195321],\n       [-0.1213603 ,  0.16498962, -0.23803102,  1.33242116, -0.43195321],\n       [-0.68649617, -0.8274596 , -0.61330032, -0.46886997,  0.42010287],\n       [-0.73244378, -0.41583293, -0.54606312, -0.46886997, -0.43195321],\n       [-0.75418874, -0.67194329, -0.56752738, -0.46886997, -0.43195321],\n       [-0.11822093, -0.18759169, -0.14469223,  2.47473995, -0.43195321],\n       [-0.5606009 , -0.89159703, -0.62039587, -0.46886997, -0.43195321],\n       [-0.70811556, -0.99767742, -0.6295606 ,  1.33242116, -0.43195321],\n       [-0.44276036, -0.47392695, -0.57065935, -0.46886997, -0.43195321],\n       [-0.61544895, -0.21965453, -0.37710046, -0.46886997, -0.43195321],\n       [-0.57791999, -0.10080471, -0.18860741, -0.46886997,  0.18342062],\n       [ 2.23213936, -0.34291185, -0.59568406, -0.46886997, -0.43195321],\n       [ 2.06354861,  1.39098988, -0.11068883, -0.46886997, -0.43195321],\n       [-0.79190991, -0.9574449 , -0.62376745,  0.24317887, -0.43195321],\n       [ 0.26443406,  2.61416753,  3.08558822,  1.06326271, -0.43195321],\n       [-0.01209987,  0.53594439,  0.23629128, -0.46886997, -0.43195321],\n       [-0.80634591, -0.96716418, -0.61755426, -0.46886997, -0.43195321],\n       [-0.16938374,  0.8696658 ,  0.77316551, -0.27051351, -0.23415447],\n       [-0.04311944,  0.18446741, -0.14938031,  0.73199078, -0.43195321],\n       [-0.68041476,  0.47595163,  1.08241827, -0.18405044, -0.43195321],\n       [-0.76631569, -0.33855142, -0.36484886, -0.46886997,  1.06490206],\n       [-0.71942755, -1.0021987 , -0.63005112, -0.46886997,  1.28493979],\n       [ 2.56991029,  0.81760341,  0.29109646, -0.46886997,  1.99108126],\n       [-0.74181833, -0.94662522, -0.62596349, -0.46886997, -0.43195321],\n       [-0.39172056, -0.75420507, -0.61813217,  0.73199078, -0.43195321],\n       [-0.74252565, -0.58032118, -0.59939112, -0.46886997, -0.43195321],\n       [-0.52984786,  0.82098506,  1.31844616, -0.46886997, -0.43195321],\n       [-0.75226924, -0.67038781, -0.54044193, -0.46886997, -0.43195321],\n       [-0.51722886, -0.15861235,  0.53360817, -0.46886997, -0.43195321],\n       [ 0.94307429,  1.63510561,  0.0502649 , -0.46886997,  2.33722904],\n       [ 2.27493734,  0.10828435, -0.43806527,  0.81281795, -0.43195321],\n       [-0.62666612, -0.76651942, -0.62548989, -0.46886997, -0.43195321],\n       [-0.66600439, -0.34358661, -0.52269601, -0.46886997, -0.43195321],\n       [ 2.40153737,  0.79677218,  0.14893452, -0.46886997,  0.18342062],\n       [ 0.09391613,  1.61452938,  2.56423569, -0.46886997, -0.43195321],\n       [-0.70296186, -0.26446128, -0.52736154, -0.46886997, -0.43195321],\n       [ 0.44208927,  4.18123112,  3.45974963, -0.46886997, -0.43195321],\n       [-0.78308892, -1.01087642, -0.63087146, -0.46886997,  0.89725427],\n       [-0.54691067,  0.98156598,  0.70367308, -0.46886997, -0.43195321],\n       [ 0.60991632, -0.09784872, -0.51965707, -0.46886997, -0.43195321],\n       [-0.76052643, -0.96000271, -0.62467519, -0.46886997,  0.33726408],\n       [ 1.93383484,  0.09531485, -0.29395536,  4.04373958, -0.43195321],\n       [-0.76653353, -0.6801267 , -0.54957002, -0.46886997, -0.43195321],\n       [-0.83650183,  2.64457877,  2.29128386, -0.46886997, -0.43195321],\n       [-0.51706228,  1.07292328,  2.54926935, -0.46886997, -0.43195321],\n       [-0.39173081,  1.0465606 ,  2.24162345, -0.46886997, -0.43195321],\n       [ 0.54861269, -0.32386364, -0.44453781, -0.46886997, -0.43195321],\n       [-0.41346552, -0.66397367, -0.62655268, -0.46886997, -0.43195321],\n       [-0.66187067, -0.05643144,  0.1018028 ,  1.19732432, -0.43195321],\n       [-0.61096157, -0.57797129, -0.5712147 , -0.46886997, -0.43195321],\n       [ 0.34407676,  0.4601909 ,  0.05937608, -0.46886997,  0.18342062],\n       [-0.46490255, -0.97695015, -0.62658651,  1.7874348 , -0.43195321]])\n\n\n\nX_trans[:, 5:]\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 1., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 1., 0.],\n       [0., 0., 0., ..., 0., 0., 1.]])\n\n\n\nX_trans.shape\n\n(88, 38)\n\n\n\nLinear model\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\n\nlin_reg = Pipeline([\n    ('preprocess', ct),\n    ('lin_reg', LinearRegression())\n])\nlin_reg\n\nPipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('numerical',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['general', 'gdp', 'fdi', 'i',\n                                                   'rr']),\n                                                 ('categorical',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value='sk_missing',\n                                                                                 strategy='constant')),\n                                                                  ('ohe',\n                                                                   OneHotEncoder(handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['province', 'reg'])])),\n                ('lin_reg', LinearRegression())])preprocess: ColumnTransformerColumnTransformer(transformers=[('numerical',\n                                 Pipeline(steps=[('imputer', SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['general', 'gdp', 'fdi', 'i', 'rr']),\n                                ('categorical',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='sk_missing',\n                                                                strategy='constant')),\n                                                 ('ohe',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 ['province', 'reg'])])numerical['general', 'gdp', 'fdi', 'i', 'rr']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical['province', 'reg']SimpleImputerSimpleImputer(fill_value='sk_missing', strategy='constant')OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)LinearRegressionLinearRegression()\n\n\n\nlin_reg.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('numerical',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['general', 'gdp', 'fdi', 'i',\n                                                   'rr']),\n                                                 ('categorical',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value='sk_missing',\n                                                                                 strategy='constant')),\n                                                                  ('ohe',\n                                                                   OneHotEncoder(handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['province', 'reg'])])),\n                ('lin_reg', LinearRegression())])preprocess: ColumnTransformerColumnTransformer(transformers=[('numerical',\n                                 Pipeline(steps=[('imputer', SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['general', 'gdp', 'fdi', 'i', 'rr']),\n                                ('categorical',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='sk_missing',\n                                                                strategy='constant')),\n                                                 ('ohe',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 ['province', 'reg'])])numerical['general', 'gdp', 'fdi', 'i', 'rr']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical['province', 'reg']SimpleImputerSimpleImputer(fill_value='sk_missing', strategy='constant')OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)LinearRegressionLinearRegression()\n\n\n\nlin_reg.score(X_train, y_train)\n\n0.9195619190476331"
  },
  {
    "objectID": "posts/2021-05-31-pandas-interoperability.html#random-forest",
    "href": "posts/2021-05-31-pandas-interoperability.html#random-forest",
    "title": "Pandas Interoperability",
    "section": "Random Forest",
    "text": "Random Forest\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nrf = Pipeline([\n    ('preprocess', ct),\n    ('log_reg', RandomForestRegressor(random_state=42))\n])\nrf\n\nPipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('numerical',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['general', 'gdp', 'fdi', 'i',\n                                                   'rr']),\n                                                 ('categorical',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value='sk_missing',\n                                                                                 strategy='constant')),\n                                                                  ('ohe',\n                                                                   OneHotEncoder(handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['province', 'reg'])])),\n                ('log_reg', RandomForestRegressor(random_state=42))])preprocess: ColumnTransformerColumnTransformer(transformers=[('numerical',\n                                 Pipeline(steps=[('imputer', SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['general', 'gdp', 'fdi', 'i', 'rr']),\n                                ('categorical',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='sk_missing',\n                                                                strategy='constant')),\n                                                 ('ohe',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 ['province', 'reg'])])numerical['general', 'gdp', 'fdi', 'i', 'rr']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical['province', 'reg']SimpleImputerSimpleImputer(fill_value='sk_missing', strategy='constant')OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\nrf.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('numerical',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['general', 'gdp', 'fdi', 'i',\n                                                   'rr']),\n                                                 ('categorical',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value='sk_missing',\n                                                                                 strategy='constant')),\n                                                                  ('ohe',\n                                                                   OneHotEncoder(handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['province', 'reg'])])),\n                ('log_reg', RandomForestRegressor(random_state=42))])preprocess: ColumnTransformerColumnTransformer(transformers=[('numerical',\n                                 Pipeline(steps=[('imputer', SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['general', 'gdp', 'fdi', 'i', 'rr']),\n                                ('categorical',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='sk_missing',\n                                                                strategy='constant')),\n                                                 ('ohe',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 ['province', 'reg'])])numerical['general', 'gdp', 'fdi', 'i', 'rr']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical['province', 'reg']SimpleImputerSimpleImputer(fill_value='sk_missing', strategy='constant')OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\nrf.score(X_train, y_train)\n\n0.9503718502648469"
  },
  {
    "objectID": "posts/2021-06-19-health_data.html",
    "href": "posts/2021-06-19-health_data.html",
    "title": "Forecasting running data",
    "section": "",
    "text": "from datetime import date\nimport os \ntoday = date.today()\n\n\nfor file in os.listdir():\n    if file.endswith('.ipynb'):\n        cd=today\n        os.rename(file, f'{today}-{file}')\n\n\nimport shutil\n\nshutil.copy(\n    os.path.join('2021-06-19-health_data.ipynb'),\n    os.path.join('../git-repos/Kearney_Data_Science/_notebooks')\n)\n\n'../git-repos/Kearney_Data_Science/_notebooks/2021-06-19-health_data.ipynb'\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\nengine = db.create_engine('sqlite:///../../Downloads/fitbit.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\n\nsql = \"\"\"\nselect DATE(date_time) as day\n, sum(distance_miles) as distance\nfrom distance_v\ngroup by DATE(date_time)\n\"\"\"\n\ncnxn = connection\n\ndf = pd.read_sql(sql, cnxn)\n\ndf\n\n\n\n\n\n  \n    \n      \n      day\n      distance\n    \n  \n  \n    \n      0\n      2020-12-02\n      11.238989\n    \n    \n      1\n      2020-12-03\n      7.615898\n    \n    \n      2\n      2020-12-04\n      11.392033\n    \n    \n      3\n      2020-12-05\n      9.929077\n    \n    \n      4\n      2020-12-06\n      10.442889\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      186\n      2021-06-08\n      0.935723\n    \n    \n      187\n      2021-06-09\n      4.844334\n    \n    \n      188\n      2021-06-10\n      8.554417\n    \n    \n      189\n      2021-06-11\n      6.167171\n    \n    \n      190\n      2021-06-12\n      5.006263\n    \n  \n\n191 rows  2 columns\n\n\n\n\ndf['ds'] = df.day\ndf['y'] = df.distance\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 191 entries, 0 to 190\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   day       191 non-null    object \n 1   distance  191 non-null    float64\n 2   ds        191 non-null    object \n 3   y         191 non-null    float64\ndtypes: float64(2), object(2)\nmemory usage: 6.1+ KB\n\n\n\nimport statsmodels.api as sm\nimport pandas as pd\nfrom prophet import Prophet\n\n\nimport pandas as pd\npd.set_option('compute.use_numexpr', False)\n\nm = Prophet()\nm.fit(df)\n\nINFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n<prophet.forecaster.Prophet at 0x7f0100bb6c70>\n\n\n\nfuture = m.make_future_dataframe(periods=365)\nfuture.tail()\n\n\n\n\n\n  \n    \n      \n      ds\n    \n  \n  \n    \n      551\n      2022-06-08\n    \n    \n      552\n      2022-06-09\n    \n    \n      553\n      2022-06-10\n    \n    \n      554\n      2022-06-11\n    \n    \n      555\n      2022-06-12\n    \n  \n\n\n\n\n\nforecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n\n\n\n\n\n  \n    \n      \n      ds\n      yhat\n      yhat_lower\n      yhat_upper\n    \n  \n  \n    \n      551\n      2022-06-08\n      9.954126\n      5.626830\n      14.090458\n    \n    \n      552\n      2022-06-09\n      11.067850\n      6.415623\n      15.269029\n    \n    \n      553\n      2022-06-10\n      9.524963\n      5.164365\n      13.816646\n    \n    \n      554\n      2022-06-11\n      10.393233\n      5.943450\n      14.634128\n    \n    \n      555\n      2022-06-12\n      11.320983\n      6.836854\n      15.647303\n    \n  \n\n\n\n\n\nfig1 = m.plot(forecast)\n\n\n\n\n\nfig2 = m.plot_components(forecast)\n\n\n\n\n\n# Python\nfig1 = m.plot(forecast)\n\n\n\n\n\n# Python\nfig2 = m.plot_components(forecast)\n\n\n\n\n\n# Python\nfrom prophet.plot import plot_plotly, plot_components_plotly\n\nplot_plotly(m, forecast)\n\n\n                                                \n\n\n\n# Python\nplot_components_plotly(m, forecast)\n\n\n                                                \n\n\n\n# Model fit\nm = Prophet() #Instanticate from Prophet class. \nm.fit(df) # Fit the Prophet model.\n\n# Predict\nfuture = m.make_future_dataframe(periods=365) # Make future date data frame for the next 365 days (it gives daily because it follows the frequency in input dataframe by default).\nforecast = m.predict(future) # Predict future value.\n\n# Plot results\nfig1 = m.plot(forecast) # Plot the fit to past data and future forcast.\nfig2 = m.plot_components(forecast) # Plot breakdown of components.\nplt.show()\nforecast # Displaying various results in table format.\n\nINFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      ds\n      trend\n      yhat_lower\n      yhat_upper\n      trend_lower\n      trend_upper\n      additive_terms\n      additive_terms_lower\n      additive_terms_upper\n      weekly\n      weekly_lower\n      weekly_upper\n      multiplicative_terms\n      multiplicative_terms_lower\n      multiplicative_terms_upper\n      yhat\n    \n  \n  \n    \n      0\n      2020-12-02\n      8.826908\n      4.690634\n      11.818085\n      8.826908\n      8.826908\n      -0.380921\n      -0.380921\n      -0.380921\n      -0.380921\n      -0.380921\n      -0.380921\n      0.0\n      0.0\n      0.0\n      8.445987\n    \n    \n      1\n      2020-12-03\n      8.805304\n      5.826039\n      12.902548\n      8.805304\n      8.805304\n      0.727109\n      0.727109\n      0.727109\n      0.727109\n      0.727109\n      0.727109\n      0.0\n      0.0\n      0.0\n      9.532413\n    \n    \n      2\n      2020-12-04\n      8.783700\n      4.373529\n      11.382860\n      8.783700\n      8.783700\n      -0.821473\n      -0.821473\n      -0.821473\n      -0.821473\n      -0.821473\n      -0.821473\n      0.0\n      0.0\n      0.0\n      7.962227\n    \n    \n      3\n      2020-12-05\n      8.762096\n      5.593958\n      12.411443\n      8.762096\n      8.762096\n      0.041102\n      0.041102\n      0.041102\n      0.041102\n      0.041102\n      0.041102\n      0.0\n      0.0\n      0.0\n      8.803198\n    \n    \n      4\n      2020-12-06\n      8.740492\n      6.342656\n      13.346898\n      8.740492\n      8.740492\n      0.963158\n      0.963158\n      0.963158\n      0.963158\n      0.963158\n      0.963158\n      0.0\n      0.0\n      0.0\n      9.703650\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      551\n      2022-06-08\n      10.335047\n      5.742947\n      14.393370\n      7.579529\n      13.222686\n      -0.380921\n      -0.380921\n      -0.380921\n      -0.380921\n      -0.380921\n      -0.380921\n      0.0\n      0.0\n      0.0\n      9.954126\n    \n    \n      552\n      2022-06-09\n      10.340742\n      6.769705\n      15.861835\n      7.575976\n      13.239863\n      0.727109\n      0.727109\n      0.727109\n      0.727109\n      0.727109\n      0.727109\n      0.0\n      0.0\n      0.0\n      11.067850\n    \n    \n      553\n      2022-06-10\n      10.346436\n      4.792225\n      14.159937\n      7.573519\n      13.261637\n      -0.821473\n      -0.821473\n      -0.821473\n      -0.821473\n      -0.821473\n      -0.821473\n      0.0\n      0.0\n      0.0\n      9.524963\n    \n    \n      554\n      2022-06-11\n      10.352131\n      5.946817\n      14.833565\n      7.570061\n      13.285497\n      0.041102\n      0.041102\n      0.041102\n      0.041102\n      0.041102\n      0.041102\n      0.0\n      0.0\n      0.0\n      10.393233\n    \n    \n      555\n      2022-06-12\n      10.357825\n      7.099368\n      15.701414\n      7.563656\n      13.302798\n      0.963158\n      0.963158\n      0.963158\n      0.963158\n      0.963158\n      0.963158\n      0.0\n      0.0\n      0.0\n      11.320983\n    \n  \n\n556 rows  16 columns\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Load test data: log-transformed daily page views for the Wikipedia page for Peyton Manning.\n\ndf['cap'] = 10 # Saturating maximum\ndf['floor'] = 7 # Saturating minimum\n\n# Model setup\nm = Prophet(growth='logistic')\nm.add_country_holidays(country_name='US') # Adding US holiday regressor\nm.fit(df) \n\n# Future data generation\nfuture = m.make_future_dataframe(periods=365*5)\nfuture['cap'] = 10 # Saturating maximum\nfuture['floor'] = 7 # Saturating minimum\n\n# Future forecast\nforecast = m.predict(future) \n\nINFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n\n\n# Visualize\nfig1 = m.plot(forecast) # Plot the fit to past data and future forcast.\nfig2 = m.plot_components(forecast) # Plot breakdown of components.\nplt.show()\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef is_nfl_season(ds):\n    date = pd.to_datetime(ds)\n    return (date.month > 8 or date.month < 2)\n\ndf['on_season'] = df['ds'].apply(is_nfl_season) #on_season dummy.\ndf['off_season'] = ~df['ds'].apply(is_nfl_season) #off_season dummy.\n\n# set user-defined seasonality and fit\nm = Prophet(weekly_seasonality=False)\nm.add_seasonality(name='weekly_on_season', period=7, fourier_order=3, condition_name='on_season')\nm.add_seasonality(name='weekly_off_season', period=7, fourier_order=3, condition_name='off_season')\nm.fit(df)\n\n# Make the same columns to future data.\nfuture = m.make_future_dataframe(periods=365*5) # Make future date data frame for the next 365 days (it gives daily because it follows the frequency in input dataframe by default).\nfuture['on_season'] = future['ds'].apply(is_nfl_season)\nfuture['off_season'] = ~future['ds'].apply(is_nfl_season)\n\n# Predict future value.\nforecast = m.predict(future)\n\nINFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n\n# Plot results\nfig1 = m.plot(forecast) # Plot the fit to past data and future forcast.\nfig2 = m.plot_components(forecast) # Plot breakdown of components.\nplt.show()\n\n\n\n\n\n\n\n\n# After getting forecast dataframe using user-defined seasonality \"on-season\"/\"off-season\" above...\n\nfrom statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n\ndf['ds'] = pd.to_datetime(df['ds'],format='%Y-%m-%d')\ndf_res = df.merge(forecast,how=\"inner\",on=\"ds\")\ndf_res['residual'] = df_res['y'] - df_res['yhat']\nplot_acf(df_res['residual'])\nplot_pacf(df_res['residual'])\nplt.show()"
  },
  {
    "objectID": "posts/2020-08-21-rdds and schemas and data types with pyspark.html",
    "href": "posts/2020-08-21-rdds and schemas and data types with pyspark.html",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "",
    "text": "from pyspark.sql import SparkSession\n\n# Load data from a CSV\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").option(\"inferSchema\", True).option(\"header\", True).load(file_location)\ndisplay(df.take(5))\n\n\n_c0provincespecificgeneralyeargdpfdirnrrrifrregit0Anhui147002.0null19962093.3506610.00.00.01128873East China6319301Anhui151981.0null19972347.32434430.00.00.01356287East China6578602Anhui174930.0null19982542.96276730.00.00.01518236East China8894633Anhui285324.0null19992712.3426131nullnullnull1646891East China12273644Anhui195580.032100.020002902.09318470.00.00.01601508East China1499110"
  },
  {
    "objectID": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#setting-data-schema-and-data-types",
    "href": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#setting-data-schema-and-data-types",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "Setting Data Schema and Data Types",
    "text": "Setting Data Schema and Data Types\n\nfrom pyspark.sql.types import StructField,StringType,IntegerType,StructType\n\n\n\n\n\n\ndata_schema = [\nStructField(\"_c0\", IntegerType(), True)\n,StructField(\"province\", StringType(), True)\n,StructField(\"specific\", IntegerType(), True)\n,StructField(\"general\", IntegerType(), True)\n,StructField(\"year\", IntegerType(), True)\n,StructField(\"gdp\", IntegerType(), True)\n,StructField(\"fdi\", IntegerType(), True)\n,StructField(\"rnr\", IntegerType(), True)\n,StructField(\"rr\", IntegerType(), True)\n,StructField(\"i\", IntegerType(), True)\n,StructField(\"fr\", IntegerType(), True)\n,StructField(\"reg\", StringType(), True)\n,StructField(\"it\", IntegerType(), True)\n]\n\n\n\n\n\n\nfinal_struc = StructType(fields=data_schema)"
  },
  {
    "objectID": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#applying-the-data-schemadata-types-while-reading-in-a-csv",
    "href": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#applying-the-data-schemadata-types-while-reading-in-a-csv",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "Applying the Data Schema/Data Types while reading in a CSV",
    "text": "Applying the Data Schema/Data Types while reading in a CSV\n\ndf = spark.read.format(\"CSV\").schema(final_struc).load(file_location)\n\n\n\n\n\n\ndf.printSchema()\n\n\nroot\n-- _c0: integer (nullable = true)\n-- province: string (nullable = true)\n-- specific: integer (nullable = true)\n-- general: integer (nullable = true)\n-- year: integer (nullable = true)\n-- gdp: integer (nullable = true)\n-- fdi: integer (nullable = true)\n-- rnr: integer (nullable = true)\n-- rr: integer (nullable = true)\n-- i: integer (nullable = true)\n-- fr: integer (nullable = true)\n-- reg: string (nullable = true)\n-- it: integer (nullable = true)\n\n\n\n\n\ndf.show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf['fr']\n\n\nOut[72]: Column<b'fr'>\n\n\n\ntype(df['fr'])\n\n\nOut[73]: pyspark.sql.column.Column\n\n\n\ndf.select('fr')\n\n\nOut[74]: DataFrame[fr: int]\n\n\n\ntype(df.select('fr'))\n\n\nOut[75]: pyspark.sql.dataframe.DataFrame\n\n\n\ndf.select('fr').show()\n\n\n+-------+\n     fr|\n+-------+\n   null|\n1128873|\n1356287|\n1518236|\n1646891|\n1601508|\n1672445|\n1677840|\n1896479|\n   null|\n   null|\n3434548|\n4468640|\n 634562|\n 634562|\n 938788|\n   null|\n1667114|\n2093925|\n2511249|\n+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.head(2)\n\n\nOut[77]: [Row(_c0=None, province='province', specific=None, general=None, year=None, gdp=None, fdi=None, rnr=None, rr=None, i=None, fr=None, reg='reg', it=None),\n Row(_c0=0, province='Anhui', specific=None, general=None, year=1996, gdp=None, fdi=50661, rnr=None, rr=None, i=None, fr=1128873, reg='East China', it=631930)]\n\n\n\ndf.select(['reg','fr'])\n\n\nOut[78]: DataFrame[reg: string, fr: int]"
  },
  {
    "objectID": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#using-select-with-rdds",
    "href": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#using-select-with-rdds",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "Using select with RDDs",
    "text": "Using select with RDDs\n\ndf.select(['reg','fr']).show()\n\n\n+-----------+-------+\n        reg|     fr|\n+-----------+-------+\n        reg|   null|\n East China|1128873|\n East China|1356287|\n East China|1518236|\n East China|1646891|\n East China|1601508|\n East China|1672445|\n East China|1677840|\n East China|1896479|\n East China|   null|\n East China|   null|\n East China|3434548|\n East China|4468640|\nNorth China| 634562|\nNorth China| 634562|\nNorth China| 938788|\nNorth China|   null|\nNorth China|1667114|\nNorth China|2093925|\nNorth China|2511249|\n+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.withColumn('fiscal_revenue',df['fr']).show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+--------------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|fiscal_revenue|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+--------------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|          null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|       1128873|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|       1356287|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|       1518236|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|       1646891|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|       1601508|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|       1672445|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|       1677840|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|       1896479|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|          null|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|          null|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|       3434548|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|       4468640|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|        634562|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|        634562|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|        938788|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|          null|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|       1667114|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|       2093925|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|       2511249|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+--------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#renaming-columns-using-withcolumnrenamed",
    "href": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#renaming-columns-using-withcolumnrenamed",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "Renaming Columns using withColumnRenamed",
    "text": "Renaming Columns using withColumnRenamed\n\ndf.withColumnRenamed('fr','new_fiscal_revenue').show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+------------------+-----------+-------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|new_fiscal_revenue|        reg|     it|\n+----+--------+--------+-------+----+----+------+----+----+----+------------------+-----------+-------+\nnull|province|    null|   null|null|null|  null|null|null|null|              null|        reg|   null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|           1128873| East China| 631930|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|           1356287| East China| 657860|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|           1518236| East China| 889463|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|           1646891| East China|1227364|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|           1601508| East China|1499110|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|           1672445| East China|2165189|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|           1677840| East China|2404936|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|           1896479| East China|2815820|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|              null| East China|3422176|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|              null| East China|3874846|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|           3434548| East China|5167300|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|           4468640| East China|7040099|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null|            634562|North China| 508135|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null|            634562|North China| 569283|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null|            938788|North China| 695528|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|              null|North China| 944047|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|           1667114|North China| 757990|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|           2093925|North China|1194728|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|           2511249|North China|1078754|\n+----+--------+--------+-------+----+----+------+----+----+----+------------------+-----------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#new-columns-by-transforming-extant-columns-using-withcolumn",
    "href": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#new-columns-by-transforming-extant-columns-using-withcolumn",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "New Columns by Transforming extant Columns using withColumn",
    "text": "New Columns by Transforming extant Columns using withColumn\n\ndf.withColumn('double_fiscal_revenue',df['fr']*2).show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+---------------------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|double_fiscal_revenue|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+---------------------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|                 null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|              2257746|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|              2712574|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|              3036472|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|              3293782|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|              3203016|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|              3344890|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|              3355680|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|              3792958|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|                 null|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|                 null|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|              6869096|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|              8937280|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|              1269124|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|              1269124|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|              1877576|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|                 null|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|              3334228|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|              4187850|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|              5022498|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+---------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.withColumn('add_fiscal_revenue',df['fr']+1).show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+------------------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|add_fiscal_revenue|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+------------------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|              null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|           1128874|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|           1356288|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|           1518237|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|           1646892|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|           1601509|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|           1672446|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|           1677841|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|           1896480|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|              null|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|              null|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|           3434549|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|           4468641|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|            634563|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|            634563|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|            938789|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|              null|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|           1667115|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|           2093926|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|           2511250|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.withColumn('half_fiscal_revenue',df['fr']/2).show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+-------------------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|half_fiscal_revenue|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+-------------------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|               null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|           564436.5|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|           678143.5|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|           759118.0|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|           823445.5|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|           800754.0|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|           836222.5|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|           838920.0|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|           948239.5|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|               null|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|               null|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|          1717274.0|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|          2234320.0|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|           317281.0|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|           317281.0|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|           469394.0|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|               null|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|           833557.0|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|          1046962.5|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|          1255624.5|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+-------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.withColumn('half_fr',df['fr']/2)\n\n\nOut[86]: DataFrame[_c0: int, province: string, specific: int, general: int, year: int, gdp: int, fdi: int, rnr: int, rr: int, i: int, fr: int, reg: string, it: int, half_fr: double]"
  },
  {
    "objectID": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#spark-sql-for-sql-functionality-using-createorreplacetempview",
    "href": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#spark-sql-for-sql-functionality-using-createorreplacetempview",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "Spark SQL for SQL functionality using createOrReplaceTempView",
    "text": "Spark SQL for SQL functionality using createOrReplaceTempView\n\ndf.createOrReplaceTempView(\"economic_data\")\n\n\n\n\n\n\nsql_results = spark.sql(\"SELECT * FROM economic_data\")\n\n\n\n\n\n\nsql_results\n\n\nOut[89]: DataFrame[_c0: int, province: string, specific: int, general: int, year: int, gdp: int, fdi: int, rnr: int, rr: int, i: int, fr: int, reg: string, it: int]\n\n\n\nsql_results.show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\nspark.sql(\"SELECT * FROM economic_data WHERE fr=634562\").show()\n\n\n+---+--------+--------+-------+----+----+------+----+----+----+------+-----------+------+\n_c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|    fr|        reg|    it|\n+---+--------+--------+-------+----+----+------+----+----+----+------+-----------+------+\n 12| Beijing|    null|   null|1996|null|155290|null|null|null|634562|North China|508135|\n 13| Beijing|    null|   null|1997|null|159286|null|null|null|634562|North China|569283|\n+---+--------+--------+-------+----+----+------+----+----+----+------+-----------+------+\n\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2021-05-29-pycaret-reg.html",
    "href": "posts/2021-05-29-pycaret-reg.html",
    "title": "Regression using Health Data with PyCaret",
    "section": "",
    "text": "1. Loading Dataset\n\nfrom pycaret.datasets import get_data\ndata = get_data('insurance')\n\n\n\n\n\n  \n    \n      \n      age\n      sex\n      bmi\n      children\n      smoker\n      region\n      charges\n    \n  \n  \n    \n      0\n      19\n      female\n      27.900\n      0\n      yes\n      southwest\n      16884.92400\n    \n    \n      1\n      18\n      male\n      33.770\n      1\n      no\n      southeast\n      1725.55230\n    \n    \n      2\n      28\n      male\n      33.000\n      3\n      no\n      southeast\n      4449.46200\n    \n    \n      3\n      33\n      male\n      22.705\n      0\n      no\n      northwest\n      21984.47061\n    \n    \n      4\n      32\n      male\n      28.880\n      0\n      no\n      northwest\n      3866.85520\n    \n  \n\n\n\n\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n  \n    \n      \n      id\n      gender\n      age\n      hypertension\n      heart_disease\n      ever_married\n      work_type\n      Residence_type\n      avg_glucose_level\n      bmi\n      smoking_status\n      stroke\n    \n  \n  \n    \n      0\n      30669\n      Male\n      3.0\n      0\n      0\n      No\n      children\n      Rural\n      95.12\n      18.0\n      NaN\n      0\n    \n    \n      1\n      30468\n      Male\n      58.0\n      1\n      0\n      Yes\n      Private\n      Urban\n      87.96\n      39.2\n      never smoked\n      0\n    \n    \n      2\n      16523\n      Female\n      8.0\n      0\n      0\n      No\n      Private\n      Urban\n      110.89\n      17.6\n      NaN\n      0\n    \n    \n      3\n      56543\n      Female\n      70.0\n      0\n      0\n      Yes\n      Private\n      Rural\n      69.04\n      35.9\n      formerly smoked\n      0\n    \n    \n      4\n      46136\n      Male\n      14.0\n      0\n      0\n      No\n      Never_worked\n      Rural\n      161.28\n      19.1\n      NaN\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      43395\n      56196\n      Female\n      10.0\n      0\n      0\n      No\n      children\n      Urban\n      58.64\n      20.4\n      never smoked\n      0\n    \n    \n      43396\n      5450\n      Female\n      56.0\n      0\n      0\n      Yes\n      Govt_job\n      Urban\n      213.61\n      55.4\n      formerly smoked\n      0\n    \n    \n      43397\n      28375\n      Female\n      82.0\n      1\n      0\n      Yes\n      Private\n      Urban\n      91.94\n      28.9\n      formerly smoked\n      0\n    \n    \n      43398\n      27973\n      Male\n      40.0\n      0\n      0\n      Yes\n      Private\n      Urban\n      99.16\n      33.2\n      never smoked\n      0\n    \n    \n      43399\n      36271\n      Female\n      82.0\n      0\n      0\n      Yes\n      Private\n      Urban\n      79.48\n      20.6\n      never smoked\n      0\n    \n  \n\n43400 rows  12 columns\n\n\n\n\n\n2. Initialize Setup\n\nfrom pycaret.regression import *\nreg1 = setup(df, target = 'avg_glucose_level', session_id=153, log_experiment=True, experiment_name='health1')\n\n\n                    Description        Value    \n                \n                        0\n                        session_id\n                        153\n            \n            \n                        1\n                        Target\n                        avg_glucose_level\n            \n            \n                        2\n                        Original Data\n                        (43400, 12)\n            \n            \n                        3\n                        Missing Values\n                        True\n            \n            \n                        4\n                        Numeric Features\n                        3\n            \n            \n                        5\n                        Categorical Features\n                        8\n            \n            \n                        6\n                        Ordinal Features\n                        False\n            \n            \n                        7\n                        High Cardinality Features\n                        False\n            \n            \n                        8\n                        High Cardinality Method\n                        None\n            \n            \n                        9\n                        Transformed Train Set\n                        (30379, 19)\n            \n            \n                        10\n                        Transformed Test Set\n                        (13021, 19)\n            \n            \n                        11\n                        Shuffle Train-Test\n                        True\n            \n            \n                        12\n                        Stratify Train-Test\n                        False\n            \n            \n                        13\n                        Fold Generator\n                        KFold\n            \n            \n                        14\n                        Fold Number\n                        10\n            \n            \n                        15\n                        CPU Jobs\n                        -1\n            \n            \n                        16\n                        Use GPU\n                        False\n            \n            \n                        17\n                        Log Experiment\n                        True\n            \n            \n                        18\n                        Experiment Name\n                        health1\n            \n            \n                        19\n                        USI\n                        2590\n            \n            \n                        20\n                        Imputation Type\n                        simple\n            \n            \n                        21\n                        Iterative Imputation Iteration\n                        None\n            \n            \n                        22\n                        Numeric Imputer\n                        mean\n            \n            \n                        23\n                        Iterative Imputation Numeric Model\n                        None\n            \n            \n                        24\n                        Categorical Imputer\n                        constant\n            \n            \n                        25\n                        Iterative Imputation Categorical Model\n                        None\n            \n            \n                        26\n                        Unknown Categoricals Handling\n                        least_frequent\n            \n            \n                        27\n                        Normalize\n                        False\n            \n            \n                        28\n                        Normalize Method\n                        None\n            \n            \n                        29\n                        Transformation\n                        False\n            \n            \n                        30\n                        Transformation Method\n                        None\n            \n            \n                        31\n                        PCA\n                        False\n            \n            \n                        32\n                        PCA Method\n                        None\n            \n            \n                        33\n                        PCA Components\n                        None\n            \n            \n                        34\n                        Ignore Low Variance\n                        False\n            \n            \n                        35\n                        Combine Rare Levels\n                        False\n            \n            \n                        36\n                        Rare Level Threshold\n                        None\n            \n            \n                        37\n                        Numeric Binning\n                        False\n            \n            \n                        38\n                        Remove Outliers\n                        False\n            \n            \n                        39\n                        Outliers Threshold\n                        None\n            \n            \n                        40\n                        Remove Multicollinearity\n                        False\n            \n            \n                        41\n                        Multicollinearity Threshold\n                        None\n            \n            \n                        42\n                        Clustering\n                        False\n            \n            \n                        43\n                        Clustering Iteration\n                        None\n            \n            \n                        44\n                        Polynomial Features\n                        False\n            \n            \n                        45\n                        Polynomial Degree\n                        None\n            \n            \n                        46\n                        Trignometry Features\n                        False\n            \n            \n                        47\n                        Polynomial Threshold\n                        None\n            \n            \n                        48\n                        Group Features\n                        False\n            \n            \n                        49\n                        Feature Selection\n                        False\n            \n            \n                        50\n                        Feature Selection Method\n                        classic\n            \n            \n                        51\n                        Features Selection Threshold\n                        None\n            \n            \n                        52\n                        Feature Interaction\n                        False\n            \n            \n                        53\n                        Feature Ratio\n                        False\n            \n            \n                        54\n                        Interaction Threshold\n                        None\n            \n            \n                        55\n                        Transform Target\n                        False\n            \n            \n                        56\n                        Transform Target Method\n                        box-cox\n            \n    \n\n\n\n\n3. Compare Baseline\n\nbest_model = compare_models(fold=5)\n\n\n                    Model        MAE        MSE        RMSE        R2        RMSLE        MAPE        TT (Sec)    \n                \n                        gbr\n                        Gradient Boosting Regressor\n                        29.9120\n                        1671.5862\n                        40.8836\n                        0.1111\n                        0.3360\n                        0.2897\n                        0.7480\n            \n            \n                        lightgbm\n                        Light Gradient Boosting Machine\n                        29.8101\n                        1680.0991\n                        40.9877\n                        0.1065\n                        0.3369\n                        0.2889\n                        0.1120\n            \n            \n                        ridge\n                        Ridge Regression\n                        30.3617\n                        1705.3918\n                        41.2947\n                        0.0931\n                        0.3394\n                        0.2936\n                        0.0400\n            \n            \n                        lar\n                        Least Angle Regression\n                        30.3618\n                        1705.3961\n                        41.2948\n                        0.0931\n                        0.3394\n                        0.2936\n                        0.0300\n            \n            \n                        br\n                        Bayesian Ridge\n                        30.3665\n                        1705.4186\n                        41.2951\n                        0.0931\n                        0.3394\n                        0.2936\n                        0.0400\n            \n            \n                        lr\n                        Linear Regression\n                        30.3618\n                        1705.3961\n                        41.2948\n                        0.0931\n                        0.3394\n                        0.2936\n                        0.4600\n            \n            \n                        en\n                        Elastic Net\n                        30.7817\n                        1744.7248\n                        41.7681\n                        0.0722\n                        0.3428\n                        0.2971\n                        0.0340\n            \n            \n                        lasso\n                        Lasso Regression\n                        30.8121\n                        1747.2784\n                        41.7986\n                        0.0709\n                        0.3431\n                        0.2974\n                        0.2980\n            \n            \n                        omp\n                        Orthogonal Matching Pursuit\n                        30.8859\n                        1772.4020\n                        42.0980\n                        0.0575\n                        0.3440\n                        0.2968\n                        0.0300\n            \n            \n                        rf\n                        Random Forest Regressor\n                        31.0628\n                        1783.7249\n                        42.2324\n                        0.0515\n                        0.3507\n                        0.3061\n                        2.4020\n            \n            \n                        llar\n                        Lasso Least Angle Regression\n                        31.0430\n                        1880.7945\n                        43.3662\n                        -0.0001\n                        0.3516\n                        0.2960\n                        0.0300\n            \n            \n                        et\n                        Extra Trees Regressor\n                        32.0922\n                        1941.3104\n                        44.0589\n                        -0.0324\n                        0.3645\n                        0.3153\n                        1.6780\n            \n            \n                        ada\n                        AdaBoost Regressor\n                        36.2249\n                        1942.8968\n                        44.0775\n                        -0.0335\n                        0.3835\n                        0.3864\n                        0.2040\n            \n            \n                        huber\n                        Huber Regressor\n                        30.5788\n                        1947.3049\n                        44.1187\n                        -0.0359\n                        0.3591\n                        0.2734\n                        0.2000\n            \n            \n                        knn\n                        K Neighbors Regressor\n                        32.9997\n                        2098.5319\n                        45.8082\n                        -0.1160\n                        0.3778\n                        0.3161\n                        0.0760\n            \n            \n                        dt\n                        Decision Tree Regressor\n                        40.5230\n                        3465.8436\n                        58.8670\n                        -0.8435\n                        0.4733\n                        0.3972\n                        0.0820\n            \n            \n                        par\n                        Passive Aggressive Regressor\n                        61.6977\n                        7002.6674\n                        77.1516\n                        -2.6714\n                        0.7598\n                        0.6310\n                        0.0500\n            \n    \n\n\n\n\n4. Create Model\n\ngbr = create_model('gbr')\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        29.4483\n                        1605.5183\n                        40.0689\n                        0.1065\n                        0.3327\n                        0.2893\n            \n            \n                        1\n                        30.1403\n                        1696.7591\n                        41.1917\n                        0.1159\n                        0.3351\n                        0.2869\n            \n            \n                        2\n                        29.9468\n                        1650.7967\n                        40.6300\n                        0.1151\n                        0.3359\n                        0.2924\n            \n            \n                        3\n                        30.9916\n                        1797.2590\n                        42.3941\n                        0.1164\n                        0.3429\n                        0.2943\n            \n            \n                        4\n                        29.7740\n                        1654.5201\n                        40.6758\n                        0.1206\n                        0.3339\n                        0.2886\n            \n            \n                        5\n                        29.4301\n                        1644.3258\n                        40.5503\n                        0.1098\n                        0.3356\n                        0.2880\n            \n            \n                        6\n                        29.8422\n                        1649.4613\n                        40.6136\n                        0.1063\n                        0.3369\n                        0.2940\n            \n            \n                        7\n                        30.1508\n                        1683.2950\n                        41.0280\n                        0.1142\n                        0.3346\n                        0.2861\n            \n            \n                        8\n                        29.8594\n                        1681.1659\n                        41.0020\n                        0.1008\n                        0.3383\n                        0.2920\n            \n            \n                        9\n                        29.4870\n                        1635.5078\n                        40.4414\n                        0.1104\n                        0.3323\n                        0.2853\n            \n            \n                        Mean\n                        29.9070\n                        1669.8609\n                        40.8596\n                        0.1116\n                        0.3358\n                        0.2897\n            \n            \n                        SD\n                        0.4393\n                        49.2434\n                        0.5964\n                        0.0056\n                        0.0029\n                        0.0031\n            \n    \n\n\n\nimport numpy as np\ngbrs = [create_model('gbr', learning_rate=i) for i in np.arange(0.1,1,0.1)]\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        30.0877\n                        1704.1490\n                        41.2813\n                        0.0517\n                        0.3429\n                        0.2952\n            \n            \n                        1\n                        31.1447\n                        1869.6863\n                        43.2399\n                        0.0258\n                        0.3573\n                        0.2956\n            \n            \n                        2\n                        30.5247\n                        1759.6697\n                        41.9484\n                        0.0568\n                        0.3475\n                        0.2990\n            \n            \n                        3\n                        31.8691\n                        1938.1533\n                        44.0245\n                        0.0471\n                        0.3581\n                        0.3021\n            \n            \n                        4\n                        30.6442\n                        1808.0313\n                        42.5210\n                        0.0390\n                        0.3494\n                        0.2982\n            \n            \n                        5\n                        30.0122\n                        1742.8764\n                        41.7478\n                        0.0564\n                        0.3456\n                        0.2943\n            \n            \n                        6\n                        30.5092\n                        1785.0937\n                        42.2504\n                        0.0328\n                        0.3503\n                        0.3013\n            \n            \n                        7\n                        30.6449\n                        1770.6888\n                        42.0796\n                        0.0682\n                        0.3457\n                        0.2922\n            \n            \n                        8\n                        30.6724\n                        1813.1471\n                        42.5811\n                        0.0302\n                        0.3533\n                        0.2999\n            \n            \n                        9\n                        30.3654\n                        1773.0299\n                        42.1074\n                        0.0356\n                        0.3480\n                        0.2961\n            \n            \n                        Mean\n                        30.6475\n                        1796.4526\n                        42.3781\n                        0.0444\n                        0.3498\n                        0.2974\n            \n            \n                        SD\n                        0.5070\n                        63.1980\n                        0.7402\n                        0.0131\n                        0.0048\n                        0.0030\n            \n    \n\n\n\nprint(len(gbrs))\n\n9\n\n\n\n\n5. Tune Hyperparameters\n\ntuned_gbr = tune_model(gbr, n_iter=50, optimize = 'RMSE')\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        29.4002\n                        1615.6882\n                        40.1956\n                        0.1009\n                        0.3328\n                        0.2882\n            \n            \n                        1\n                        30.0942\n                        1699.4660\n                        41.2246\n                        0.1145\n                        0.3346\n                        0.2859\n            \n            \n                        2\n                        29.9120\n                        1656.1440\n                        40.6957\n                        0.1123\n                        0.3358\n                        0.2914\n            \n            \n                        3\n                        30.9406\n                        1808.0163\n                        42.5208\n                        0.1111\n                        0.3430\n                        0.2928\n            \n            \n                        4\n                        29.8471\n                        1665.1412\n                        40.8061\n                        0.1149\n                        0.3346\n                        0.2890\n            \n            \n                        5\n                        29.4329\n                        1643.2520\n                        40.5370\n                        0.1104\n                        0.3351\n                        0.2880\n            \n            \n                        6\n                        29.6530\n                        1636.3363\n                        40.4517\n                        0.1134\n                        0.3355\n                        0.2918\n            \n            \n                        7\n                        30.1129\n                        1686.1293\n                        41.0625\n                        0.1127\n                        0.3343\n                        0.2850\n            \n            \n                        8\n                        29.8222\n                        1683.4829\n                        41.0303\n                        0.0996\n                        0.3380\n                        0.2913\n            \n            \n                        9\n                        29.3902\n                        1638.9886\n                        40.4844\n                        0.1085\n                        0.3319\n                        0.2840\n            \n            \n                        Mean\n                        29.8605\n                        1673.2645\n                        40.9009\n                        0.1098\n                        0.3356\n                        0.2887\n            \n            \n                        SD\n                        0.4414\n                        51.2132\n                        0.6187\n                        0.0051\n                        0.0029\n                        0.0029\n            \n    \n\n\n\ntuned_gbr\n\nGradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n                          init=None, learning_rate=0.01, loss='ls', max_depth=8,\n                          max_features='sqrt', max_leaf_nodes=None,\n                          min_impurity_decrease=0.2, min_impurity_split=None,\n                          min_samples_leaf=5, min_samples_split=10,\n                          min_weight_fraction_leaf=0.0, n_estimators=280,\n                          n_iter_no_change=None, presort='deprecated',\n                          random_state=153, subsample=0.65, tol=0.0001,\n                          validation_fraction=0.1, verbose=0, warm_start=False)\n\n\n\n\n6. Ensemble Model\n\ndt = create_model('dt')\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        38.8967\n                        3256.2671\n                        57.0637\n                        -0.8121\n                        0.4602\n                        0.3865\n            \n            \n                        1\n                        40.8905\n                        3506.9767\n                        59.2197\n                        -0.8272\n                        0.4737\n                        0.3927\n            \n            \n                        2\n                        39.8432\n                        3339.3332\n                        57.7870\n                        -0.7899\n                        0.4682\n                        0.3920\n            \n            \n                        3\n                        41.0738\n                        3547.0195\n                        59.5569\n                        -0.7439\n                        0.4741\n                        0.3947\n            \n            \n                        4\n                        41.0748\n                        3545.6724\n                        59.5455\n                        -0.8846\n                        0.4780\n                        0.4037\n            \n            \n                        5\n                        40.8333\n                        3518.6152\n                        59.3179\n                        -0.9049\n                        0.4776\n                        0.4027\n            \n            \n                        6\n                        40.5197\n                        3506.7571\n                        59.2179\n                        -0.9000\n                        0.4754\n                        0.4060\n            \n            \n                        7\n                        40.0449\n                        3418.0876\n                        58.4644\n                        -0.7987\n                        0.4696\n                        0.3896\n            \n            \n                        8\n                        40.7124\n                        3463.5019\n                        58.8515\n                        -0.8525\n                        0.4754\n                        0.4031\n            \n            \n                        9\n                        39.4837\n                        3352.3464\n                        57.8995\n                        -0.8234\n                        0.4671\n                        0.3819\n            \n            \n                        Mean\n                        40.3373\n                        3445.4577\n                        58.6924\n                        -0.8337\n                        0.4719\n                        0.3953\n            \n            \n                        SD\n                        0.7033\n                        94.8957\n                        0.8124\n                        0.0492\n                        0.0053\n                        0.0078\n            \n    \n\n\n\nbagged_dt = ensemble_model(dt, n_estimators=50)\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        30.4101\n                        1705.5270\n                        41.2980\n                        0.0509\n                        0.3458\n                        0.3031\n            \n            \n                        1\n                        31.1895\n                        1815.9909\n                        42.6144\n                        0.0538\n                        0.3495\n                        0.2995\n            \n            \n                        2\n                        31.1459\n                        1782.9179\n                        42.2246\n                        0.0443\n                        0.3522\n                        0.3090\n            \n            \n                        3\n                        32.1774\n                        1926.5104\n                        43.8920\n                        0.0528\n                        0.3581\n                        0.3104\n            \n            \n                        4\n                        31.4072\n                        1807.7021\n                        42.5171\n                        0.0392\n                        0.3534\n                        0.3106\n            \n            \n                        5\n                        30.8286\n                        1787.9650\n                        42.2843\n                        0.0320\n                        0.3533\n                        0.3073\n            \n            \n                        6\n                        31.0847\n                        1802.8904\n                        42.4605\n                        0.0232\n                        0.3557\n                        0.3122\n            \n            \n                        7\n                        31.2760\n                        1797.0518\n                        42.3916\n                        0.0544\n                        0.3507\n                        0.3029\n            \n            \n                        8\n                        31.1355\n                        1795.7085\n                        42.3758\n                        0.0396\n                        0.3535\n                        0.3108\n            \n            \n                        9\n                        30.7127\n                        1760.7991\n                        41.9619\n                        0.0423\n                        0.3486\n                        0.3020\n            \n            \n                        Mean\n                        31.1368\n                        1798.3063\n                        42.4020\n                        0.0432\n                        0.3521\n                        0.3068\n            \n            \n                        SD\n                        0.4454\n                        52.1778\n                        0.6116\n                        0.0097\n                        0.0034\n                        0.0043\n            \n    \n\n\n\nboosted_dt = ensemble_model(dt, method = 'Boosting')\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        32.0837\n                        2316.4857\n                        48.1299\n                        -0.2891\n                        0.3905\n                        0.3044\n            \n            \n                        1\n                        32.8474\n                        2379.9324\n                        48.7846\n                        -0.2400\n                        0.3920\n                        0.3000\n            \n            \n                        2\n                        32.5372\n                        2325.1765\n                        48.2201\n                        -0.2463\n                        0.3920\n                        0.3091\n            \n            \n                        3\n                        33.3153\n                        2448.9699\n                        49.4871\n                        -0.2040\n                        0.3948\n                        0.3026\n            \n            \n                        4\n                        32.6835\n                        2316.9897\n                        48.1351\n                        -0.2315\n                        0.3894\n                        0.3049\n            \n            \n                        5\n                        32.3224\n                        2299.7394\n                        47.9556\n                        -0.2450\n                        0.3899\n                        0.3068\n            \n            \n                        6\n                        32.6263\n                        2307.7303\n                        48.0388\n                        -0.2503\n                        0.3927\n                        0.3125\n            \n            \n                        7\n                        32.4531\n                        2287.3343\n                        47.8261\n                        -0.2036\n                        0.3883\n                        0.3019\n            \n            \n                        8\n                        32.1537\n                        2270.1737\n                        47.6463\n                        -0.2142\n                        0.3867\n                        0.3068\n            \n            \n                        9\n                        31.9558\n                        2247.6878\n                        47.4098\n                        -0.2225\n                        0.3851\n                        0.2979\n            \n            \n                        Mean\n                        32.4978\n                        2320.0220\n                        48.1633\n                        -0.2347\n                        0.3901\n                        0.3047\n            \n            \n                        SD\n                        0.3820\n                        54.4179\n                        0.5613\n                        0.0244\n                        0.0028\n                        0.0041\n            \n    \n\n\n\n\n9. Analyze Model\n\nplot_model(dt)\n\n\n\n\n\nplot_model(dt, plot = 'error')\n\n\n\n\n\nplot_model(dt, plot = 'feature')\n\n\n\n\n\nevaluate_model(dt)\n\n\n\n\n\n  \n    \n      \n      Parameters\n    \n  \n  \n    \n      ccp_alpha\n      0.0\n    \n    \n      criterion\n      mse\n    \n    \n      max_depth\n      None\n    \n    \n      max_features\n      None\n    \n    \n      max_leaf_nodes\n      None\n    \n    \n      min_impurity_decrease\n      0.0\n    \n    \n      min_impurity_split\n      None\n    \n    \n      min_samples_leaf\n      1\n    \n    \n      min_samples_split\n      2\n    \n    \n      min_weight_fraction_leaf\n      0.0\n    \n    \n      presort\n      deprecated\n    \n    \n      random_state\n      153\n    \n    \n      splitter\n      best\n    \n  \n\n\n\n\n\n\n10. Interpret Model\n\ninterpret_model(dt)\n\n\n\n\n\ninterpret_model(dt, plot = 'correlation')\n\n\n\n\n\ninterpret_model(dt, plot = 'reason', observation = 12)\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\n\n11. AutoML()\n\nbest = automl(optimize = 'MAE')\nbest\n\nLGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n              importance_type='split', learning_rate=0.1, max_depth=-1,\n              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n              random_state=153, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n\n\n\n\n12. Predict Model\n\npred_holdouts = predict_model(dt)\npred_holdouts.head()\n\n\n                    Model        MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        Decision Tree Regressor\n                        39.4954\n                        3317.2600\n                        57.5957\n                        -0.8368\n                        0.4665\n                        0.3924\n            \n    \n\n\n\n\n\n\n  \n    \n      \n      id\n      age\n      bmi\n      gender_Male\n      gender_Other\n      hypertension_1\n      heart_disease_0\n      ever_married_Yes\n      work_type_Govt_job\n      work_type_Never_worked\n      ...\n      work_type_Self-employed\n      work_type_children\n      Residence_type_Urban\n      smoking_status_formerly smoked\n      smoking_status_never smoked\n      smoking_status_not_available\n      smoking_status_smokes\n      stroke_0\n      avg_glucose_level\n      Label\n    \n  \n  \n    \n      0\n      22489.0\n      55.0\n      41.099998\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      99.110001\n      109.269997\n    \n    \n      1\n      26145.0\n      17.0\n      16.200001\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n      1.0\n      83.470001\n      73.949997\n    \n    \n      2\n      34373.0\n      0.4\n      20.200001\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      76.430000\n      92.430000\n    \n    \n      3\n      60599.0\n      69.0\n      32.500000\n      1.0\n      0.0\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      80.370003\n      86.099998\n    \n    \n      4\n      54268.0\n      35.0\n      27.200001\n      1.0\n      0.0\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      1.0\n      151.589996\n      175.320007\n    \n  \n\n5 rows  21 columns\n\n\n\n\nnew_data = df.copy()\nnew_data.drop(['avg_glucose_level'], axis=1, inplace=True)\npredict_new = predict_model(best, data=new_data)\npredict_new.head()\n\n\n\n\n\n  \n    \n      \n      id\n      gender\n      age\n      hypertension\n      heart_disease\n      ever_married\n      work_type\n      Residence_type\n      bmi\n      smoking_status\n      stroke\n      Label\n    \n  \n  \n    \n      0\n      30669\n      Male\n      3.0\n      0\n      0\n      No\n      children\n      Rural\n      18.0\n      NaN\n      0\n      91.930029\n    \n    \n      1\n      30468\n      Male\n      58.0\n      1\n      0\n      Yes\n      Private\n      Urban\n      39.2\n      never smoked\n      0\n      143.044226\n    \n    \n      2\n      16523\n      Female\n      8.0\n      0\n      0\n      No\n      Private\n      Urban\n      17.6\n      NaN\n      0\n      91.227274\n    \n    \n      3\n      56543\n      Female\n      70.0\n      0\n      0\n      Yes\n      Private\n      Rural\n      35.9\n      formerly smoked\n      0\n      123.386541\n    \n    \n      4\n      46136\n      Male\n      14.0\n      0\n      0\n      No\n      Never_worked\n      Rural\n      19.1\n      NaN\n      0\n      97.715823\n    \n  \n\n\n\n\n\n\n13. Save / Load Model\n\nsave_model(best, model_name='best-model')\n\nTransformation Pipeline and Model Succesfully Saved\n\n\n(Pipeline(memory=None,\n          steps=[('dtypes',\n                  DataTypes_Auto_infer(categorical_features=[],\n                                       display_types=True, features_todrop=[],\n                                       id_columns=[], ml_usecase='regression',\n                                       numerical_features=[],\n                                       target='avg_glucose_level',\n                                       time_features=[])),\n                 ('imputer',\n                  Simple_Imputer(categorical_strategy='not_available',\n                                 fill_value_categorical=None,\n                                 fill_value_numerical=None,\n                                 numeri...\n                  LGBMRegressor(boosting_type='gbdt', class_weight=None,\n                                colsample_bytree=1.0, importance_type='split',\n                                learning_rate=0.1, max_depth=-1,\n                                min_child_samples=20, min_child_weight=0.001,\n                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,\n                                num_leaves=31, objective=None, random_state=153,\n                                reg_alpha=0.0, reg_lambda=0.0, silent=True,\n                                subsample=1.0, subsample_for_bin=200000,\n                                subsample_freq=0)]],\n          verbose=False),\n 'best-model.pkl')\n\n\n\nloaded_bestmodel = load_model('best-model')\nprint(loaded_bestmodel)\n\nTransformation Pipeline and Model Successfully Loaded\nPipeline(memory=None,\n         steps=[('dtypes',\n                 DataTypes_Auto_infer(categorical_features=[],\n                                      display_types=True, features_todrop=[],\n                                      id_columns=[], ml_usecase='regression',\n                                      numerical_features=[],\n                                      target='avg_glucose_level',\n                                      time_features=[])),\n                ('imputer',\n                 Simple_Imputer(categorical_strategy='not_available',\n                                fill_value_categorical=None,\n                                fill_value_numerical=None,\n                                numeri...\n                 LGBMRegressor(boosting_type='gbdt', class_weight=None,\n                               colsample_bytree=1.0, importance_type='split',\n                               learning_rate=0.1, max_depth=-1,\n                               min_child_samples=20, min_child_weight=0.001,\n                               min_split_gain=0.0, n_estimators=100, n_jobs=-1,\n                               num_leaves=31, objective=None, random_state=153,\n                               reg_alpha=0.0, reg_lambda=0.0, silent=True,\n                               subsample=1.0, subsample_for_bin=200000,\n                               subsample_freq=0)]],\n         verbose=False)\n\n\n\nfrom sklearn import set_config\nset_config(display='diagram')\nloaded_bestmodel[0]\n\nDataTypes_Auto_inferDataTypes_Auto_infer(categorical_features=[], display_types=True,\n                     features_todrop=[], id_columns=[], ml_usecase='regression',\n                     numerical_features=[], target='avg_glucose_level',\n                     time_features=[])\n\n\n\nfrom sklearn import set_config\nset_config(display='text')\n\n\n\n14. Deploy Model\n\n#deploy_model(best, model_name = 'best-aws', authentication = {'bucket' : 'pycaret-test'})\n\n\n\n15. Get Config / Set Config\n\nX_train = get_config('X_train')\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      id\n      age\n      bmi\n      gender_Male\n      gender_Other\n      hypertension_1\n      heart_disease_0\n      ever_married_Yes\n      work_type_Govt_job\n      work_type_Never_worked\n      work_type_Private\n      work_type_Self-employed\n      work_type_children\n      Residence_type_Urban\n      smoking_status_formerly smoked\n      smoking_status_never smoked\n      smoking_status_not_available\n      smoking_status_smokes\n      stroke_0\n    \n  \n  \n    \n      41231\n      51687.0\n      55.0\n      25.799999\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n      1.0\n    \n    \n      21949\n      11886.0\n      38.0\n      19.500000\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n    \n    \n      12409\n      18294.0\n      4.0\n      18.000000\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n    \n    \n      9632\n      35206.0\n      37.0\n      29.000000\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n      1.0\n    \n    \n      15044\n      69458.0\n      41.0\n      38.299999\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n      1.0\n    \n  \n\n\n\n\n\nget_config('seed')\n\n153\n\n\n\nfrom pycaret.regression import set_config\nset_config('seed', 999)\n\n\nget_config('seed')\n\n999\n\n\n\n\n16. MLFlow UI\n\n!mlflow ui\n\n[2021-05-29 11:34:03 -0500] [17806] [INFO] Starting gunicorn 20.0.4\n[2021-05-29 11:34:03 -0500] [17806] [INFO] Listening at: http://127.0.0.1:5000 (17806)\n[2021-05-29 11:34:03 -0500] [17806] [INFO] Using worker: sync\n[2021-05-29 11:34:03 -0500] [17808] [INFO] Booting worker with pid: 17808"
  },
  {
    "objectID": "posts/2021-01-15-folium-maps.html",
    "href": "posts/2021-01-15-folium-maps.html",
    "title": "Maps and Folium",
    "section": "",
    "text": "folium_ipython_show(folium.Map(location=[42.079391, -87.815622], zoom_start=13))\n\n\n    \n    \n    \n        \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n            \n            \n        \n\n    \n    \n            \n        \n\n\n\n\n\nm3 = folium.Map(location=[42.079391, -87.815622], zoom_start=12, tiles=\"Stamen Terrain\")\n\nfolium_ipython_show(m3)\n\n\n    \n    \n    \n        \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n            \n            \n        \n\n    \n    \n            \n        \n\n\n\n\n\nm4 = folium.Map(location=[42.079391, -87.815622], zoom_start=12, tiles=\"Stamen Terrain\")\n\nfolium_ipython_show(m4)\n\n\n    \n    \n    \n        \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n            \n            \n        \n\n    \n    \n            \n        \n\n\n\n\n\nm5 = folium.Map(location=[42.079391, -87.815622], tiles=\"Stamen Toner\", zoom_start=13)\n\nfolium_ipython_show(m5)"
  },
  {
    "objectID": "posts/2020-08-30-nlp with pyspark.html",
    "href": "posts/2020-08-30-nlp with pyspark.html",
    "title": "NLP with Pyspark",
    "section": "",
    "text": "This post includes code from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-08-30-nlp with pyspark.html#using-tokenizer-and-regextokenizer",
    "href": "posts/2020-08-30-nlp with pyspark.html#using-tokenizer-and-regextokenizer",
    "title": "NLP with Pyspark",
    "section": "Using Tokenizer and RegexTokenizer",
    "text": "Using Tokenizer and RegexTokenizer\n\ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n\nregexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n# alternatively, pattern=\"\\\\w+\", gaps(False)\n\ncountTokens = udf(lambda words: len(words), IntegerType())\n\ntokenized = tokenizer.transform(sentenceDataFrame)\ntokenized.select(\"sentence\", \"words\")\\\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n\nregexTokenized = regexTokenizer.transform(sentenceDataFrame)\nregexTokenized.select(\"sentence\", \"words\") \\\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n\n\n+-----------------------------------+------------------------------------------+------+\nsentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\nHi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\nI wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\nLogistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n+-----------------------------------+------------------------------------------+------+\n\n+-----------------------------------+------------------------------------------+------+\nsentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\nHi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\nI wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\nLogistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n+-----------------------------------+------------------------------------------+------+"
  },
  {
    "objectID": "posts/2020-08-30-nlp with pyspark.html#removing-stop-words",
    "href": "posts/2020-08-30-nlp with pyspark.html#removing-stop-words",
    "title": "NLP with Pyspark",
    "section": "Removing Stop Words",
    "text": "Removing Stop Words\n\nfrom pyspark.ml.feature import StopWordsRemover\n\nsentenceData = spark.createDataFrame([\n    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n], [\"id\", \"raw\"])\n\nremover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\nremover.transform(sentenceData).show(truncate=False)\n\n\n+---+----------------------------+--------------------+\nid |raw                         |filtered            |\n+---+----------------------------+--------------------+\n0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n+---+----------------------------+--------------------+"
  },
  {
    "objectID": "posts/2020-08-30-nlp with pyspark.html#n-grams",
    "href": "posts/2020-08-30-nlp with pyspark.html#n-grams",
    "title": "NLP with Pyspark",
    "section": "n-grams",
    "text": "n-grams\n\nfrom pyspark.ml.feature import NGram\n\nwordDataFrame = spark.createDataFrame([\n    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n], [\"id\", \"words\"])\n\nngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n\nngramDataFrame = ngram.transform(wordDataFrame)\nngramDataFrame.select(\"ngrams\").show(truncate=False)\n\n\n+------------------------------------------------------------------+\nngrams                                                            |\n+------------------------------------------------------------------+\n[Hi I, I heard, heard about, about Spark]                         |\n[I wish, wish Java, Java could, could use, use case, case classes]|\n[Logistic regression, regression models, models are, are neat]    |\n+------------------------------------------------------------------+\n\n\n\n\n\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer\n\nsentenceData = spark.createDataFrame([\n    (0.0, \"Hi I heard about Spark\"),\n    (0.0, \"I wish Java could use case classes\"),\n    (1.0, \"Logistic regression models are neat\")\n], [\"label\", \"sentence\"])\n\nsentenceData.show()\n\n\n+-----+--------------------+\nlabel|            sentence|\n+-----+--------------------+\n  0.0|Hi I heard about ...|\n  0.0|I wish Java could...|\n  1.0|Logistic regressi...|\n+-----+--------------------+\n\n\n\n\n\ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\nwordsData = tokenizer.transform(sentenceData)\nwordsData.show()\n\n\n+-----+--------------------+--------------------+\nlabel|            sentence|               words|\n+-----+--------------------+--------------------+\n  0.0|Hi I heard about ...|[hi, i, heard, ab...|\n  0.0|I wish Java could...|[i, wish, java, c...|\n  1.0|Logistic regressi...|[logistic, regres...|\n+-----+--------------------+--------------------+\n\n\n\n\n\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\nfeaturizedData = hashingTF.transform(wordsData)\n# alternatively, CountVectorizer can also be used to get term frequency vectors\n\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData)\nrescaledData = idfModel.transform(featurizedData)\n\nrescaledData.select(\"label\", \"features\").show()\n\n\n+-----+--------------------+\nlabel|            features|\n+-----+--------------------+\n  0.0|(20,[6,8,13,16],[...|\n  0.0|(20,[0,2,7,13,15,...|\n  1.0|(20,[3,4,6,11,19]...|\n+-----+--------------------+"
  },
  {
    "objectID": "posts/2020-08-30-nlp with pyspark.html#countvectorizer",
    "href": "posts/2020-08-30-nlp with pyspark.html#countvectorizer",
    "title": "NLP with Pyspark",
    "section": "CountVectorizer",
    "text": "CountVectorizer\n\nfrom pyspark.ml.feature import CountVectorizer\n\n# Input data: Each row is a bag of words with a ID.\ndf = spark.createDataFrame([\n    (0, \"a b c\".split(\" \")),\n    (1, \"a b b c a\".split(\" \"))\n], [\"id\", \"words\"])\n\n# fit a CountVectorizerModel from the corpus.\ncv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n\nmodel = cv.fit(df)\n\nresult = model.transform(df)\nresult.show(truncate=False)\n\n\n+---+---------------+-------------------------+\nid |words          |features                 |\n+---+---------------+-------------------------+\n0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+---+---------------+-------------------------+\n\n\n\n\n\ndf = spark.read.load(\"/FileStore/tables/SMSSpamCollection\",\n                     format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"false\")\n\n\n\n\n\n\ndf.printSchema()\n\n\nroot\n-- _c0: string (nullable = true)\n-- _c1: string (nullable = true)\n\n\n\n\n\ndata = df.withColumnRenamed('_c0','class').withColumnRenamed('_c1','text')\n\n\n\n\n\n\ndata.printSchema()\n\n\nroot\n-- class: string (nullable = true)\n-- text: string (nullable = true)"
  },
  {
    "objectID": "posts/2020-08-30-nlp with pyspark.html#clean-and-prepare-the-data",
    "href": "posts/2020-08-30-nlp with pyspark.html#clean-and-prepare-the-data",
    "title": "NLP with Pyspark",
    "section": "Clean and Prepare the Data",
    "text": "Clean and Prepare the Data\n\nfrom pyspark.sql.functions import length\n\n\n\n\n\n\ndata = data.withColumn('length',length(data['text']))\n\n\n\n\n\n\ndata.printSchema()\n\n\nroot\n-- class: string (nullable = true)\n-- text: string (nullable = true)\n-- length: integer (nullable = true)\n\n\n\n\n\n# Pretty Clear Difference\ndata.groupby('class').mean().show()\n\n\n+-----+-----------------+\nclass|      avg(length)|\n+-----+-----------------+\n  ham| 71.4545266210897|\n spam|138.6706827309237|\n+-----+-----------------+\n\n\n\n\n\nfrom pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer\n\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\nstopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\ncount_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\nidf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\nham_spam_to_num = StringIndexer(inputCol='class',outputCol='label')\n\n\n\n\n\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.linalg import Vector\n\n\n\n\n\n\nclean_up = VectorAssembler(inputCols=['tf_idf','length'],outputCol='features')\n\n\n\n\n\n\nNaive Bayes\n\nfrom pyspark.ml.classification import NaiveBayes\n\n\n\n\n\n\n# Use defaults\nnb = NaiveBayes()\n\n\n\n\n\n\n### Pipeline\n\n\nfrom pyspark.ml import Pipeline\n\n\n\n\n\n\ndata_prep_pipe = Pipeline(stages=[ham_spam_to_num,tokenizer,stopremove,count_vec,idf,clean_up])\n\n\n\n\n\n\ncleaner = data_prep_pipe.fit(data)\n\n\n\n\n\n\nclean_data = cleaner.transform(data)\n\n\n\n\n\n\n\nTraining and Evaluation\n\nclean_data = clean_data.select(['label','features'])\n\n\n\n\n\n\nclean_data.show()\n\n\n+-----+--------------------+\nlabel|            features|\n+-----+--------------------+\n  0.0|(13424,[7,11,31,6...|\n  0.0|(13424,[0,24,297,...|\n  1.0|(13424,[2,13,19,3...|\n  0.0|(13424,[0,70,80,1...|\n  0.0|(13424,[36,134,31...|\n  1.0|(13424,[10,60,139...|\n  0.0|(13424,[10,53,103...|\n  0.0|(13424,[125,184,4...|\n  1.0|(13424,[1,47,118,...|\n  1.0|(13424,[0,1,13,27...|\n  0.0|(13424,[18,43,120...|\n  1.0|(13424,[8,17,37,8...|\n  1.0|(13424,[13,30,47,...|\n  0.0|(13424,[39,96,217...|\n  0.0|(13424,[552,1697,...|\n  1.0|(13424,[30,109,11...|\n  0.0|(13424,[82,214,47...|\n  0.0|(13424,[0,2,49,13...|\n  0.0|(13424,[0,74,105,...|\n  1.0|(13424,[4,30,33,5...|\n+-----+--------------------+\nonly showing top 20 rows\n\n\n\n\n\n(training,testing) = clean_data.randomSplit([0.7,0.3])\n\n\n\n\n\n\nspam_predictor = nb.fit(training)\n\n\n\n\n\n\ndata.printSchema()\n\n\nroot\n-- class: string (nullable = true)\n-- text: string (nullable = true)\n-- length: integer (nullable = true)\n\n\n\n\n\ntest_results = spam_predictor.transform(testing)\n\n\n\n\n\n\ntest_results.show()\n\n\n+-----+--------------------+--------------------+--------------------+----------+\nlabel|            features|       rawPrediction|         probability|prediction|\n+-----+--------------------+--------------------+--------------------+----------+\n  0.0|(13424,[0,1,2,13,...|[-605.26168264963...|[1.0,7.3447866033...|       0.0|\n  0.0|(13424,[0,1,2,41,...|[-1063.2170425771...|[1.0,9.8700382552...|       0.0|\n  0.0|(13424,[0,1,3,9,1...|[-569.95657733189...|[1.0,1.4498595638...|       0.0|\n  0.0|(13424,[0,1,5,15,...|[-998.87457222776...|[1.0,5.4020023412...|       0.0|\n  0.0|(13424,[0,1,7,15,...|[-658.37986687391...|[1.0,2.6912246466...|       0.0|\n  0.0|(13424,[0,1,14,31...|[-217.18809411711...|[1.0,3.3892033063...|       0.0|\n  0.0|(13424,[0,1,14,78...|[-688.50251926938...|[1.0,8.6317783323...|       0.0|\n  0.0|(13424,[0,1,17,19...|[-809.51840544334...|[1.0,1.3686507989...|       0.0|\n  0.0|(13424,[0,1,27,35...|[-1472.6804140726...|[0.99999999999983...|       0.0|\n  0.0|(13424,[0,1,31,43...|[-341.31126583915...|[1.0,3.4983325940...|       0.0|\n  0.0|(13424,[0,1,46,17...|[-1137.4942938439...|[5.99448563047616...|       1.0|\n  0.0|(13424,[0,1,72,10...|[-704.77256939631...|[1.0,1.2592610663...|       0.0|\n  0.0|(13424,[0,1,874,1...|[-96.404593207515...|[0.99999996015865...|       0.0|\n  0.0|(13424,[0,1,874,1...|[-98.086094104500...|[0.99999996999685...|       0.0|\n  0.0|(13424,[0,2,3,4,6...|[-1289.3891411076...|[1.0,1.3408017664...|       0.0|\n  0.0|(13424,[0,2,3,5,6...|[-2561.6651406471...|[1.0,2.6887776075...|       0.0|\n  0.0|(13424,[0,2,3,5,3...|[-490.88944126371...|[1.0,9.6538338828...|       0.0|\n  0.0|(13424,[0,2,4,5,1...|[-2493.1672898653...|[1.0,9.4058507096...|       0.0|\n  0.0|(13424,[0,2,4,7,2...|[-517.23267032348...|[1.0,2.8915589432...|       0.0|\n  0.0|(13424,[0,2,4,8,2...|[-1402.5570102185...|[1.0,6.7531061115...|       0.0|\n+-----+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n\n\n\n\n## Evaluating Model Accuracy\n\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n\n\n\n\n\nacc_eval = MulticlassClassificationEvaluator()\nacc = acc_eval.evaluate(test_results)\nprint(\"Accuracy of model at predicting spam was: {}\".format(acc))\n\n\nAccuracy of model at predicting spam was: 0.9204435112848836"
  },
  {
    "objectID": "posts/2021-06-10-regression-pycaret-2.html",
    "href": "posts/2021-06-10-regression-pycaret-2.html",
    "title": "Regression using Fiscal Data with PyCaret",
    "section": "",
    "text": "# %load solutions/regression_example.py\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\n\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\n\n\nX, y = df.drop(['specific', 'Unnamed: 0'], axis = 1), df['specific']\n\n\nX = X.select_dtypes(include='number')\nX\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\ndf=df.drop(['Unnamed: 0'], axis = 1)\n\n\n\n\n\nfrom pycaret.regression import *\nreg1 = setup(df, target = 'specific', session_id=153, log_experiment=True, experiment_name='fiscal')\n\n\n                    Description        Value    \n                \n                        0\n                        session_id\n                        153\n            \n            \n                        1\n                        Target\n                        specific\n            \n            \n                        2\n                        Original Data\n                        (118, 12)\n            \n            \n                        3\n                        Missing Values\n                        False\n            \n            \n                        4\n                        Numeric Features\n                        8\n            \n            \n                        5\n                        Categorical Features\n                        3\n            \n            \n                        6\n                        Ordinal Features\n                        False\n            \n            \n                        7\n                        High Cardinality Features\n                        False\n            \n            \n                        8\n                        High Cardinality Method\n                        None\n            \n            \n                        9\n                        Transformed Train Set\n                        (82, 47)\n            \n            \n                        10\n                        Transformed Test Set\n                        (36, 47)\n            \n            \n                        11\n                        Shuffle Train-Test\n                        True\n            \n            \n                        12\n                        Stratify Train-Test\n                        False\n            \n            \n                        13\n                        Fold Generator\n                        KFold\n            \n            \n                        14\n                        Fold Number\n                        10\n            \n            \n                        15\n                        CPU Jobs\n                        -1\n            \n            \n                        16\n                        Use GPU\n                        False\n            \n            \n                        17\n                        Log Experiment\n                        True\n            \n            \n                        18\n                        Experiment Name\n                        fiscal\n            \n            \n                        19\n                        USI\n                        0884\n            \n            \n                        20\n                        Imputation Type\n                        simple\n            \n            \n                        21\n                        Iterative Imputation Iteration\n                        None\n            \n            \n                        22\n                        Numeric Imputer\n                        mean\n            \n            \n                        23\n                        Iterative Imputation Numeric Model\n                        None\n            \n            \n                        24\n                        Categorical Imputer\n                        constant\n            \n            \n                        25\n                        Iterative Imputation Categorical Model\n                        None\n            \n            \n                        26\n                        Unknown Categoricals Handling\n                        least_frequent\n            \n            \n                        27\n                        Normalize\n                        False\n            \n            \n                        28\n                        Normalize Method\n                        None\n            \n            \n                        29\n                        Transformation\n                        False\n            \n            \n                        30\n                        Transformation Method\n                        None\n            \n            \n                        31\n                        PCA\n                        False\n            \n            \n                        32\n                        PCA Method\n                        None\n            \n            \n                        33\n                        PCA Components\n                        None\n            \n            \n                        34\n                        Ignore Low Variance\n                        False\n            \n            \n                        35\n                        Combine Rare Levels\n                        False\n            \n            \n                        36\n                        Rare Level Threshold\n                        None\n            \n            \n                        37\n                        Numeric Binning\n                        False\n            \n            \n                        38\n                        Remove Outliers\n                        False\n            \n            \n                        39\n                        Outliers Threshold\n                        None\n            \n            \n                        40\n                        Remove Multicollinearity\n                        False\n            \n            \n                        41\n                        Multicollinearity Threshold\n                        None\n            \n            \n                        42\n                        Clustering\n                        False\n            \n            \n                        43\n                        Clustering Iteration\n                        None\n            \n            \n                        44\n                        Polynomial Features\n                        False\n            \n            \n                        45\n                        Polynomial Degree\n                        None\n            \n            \n                        46\n                        Trignometry Features\n                        False\n            \n            \n                        47\n                        Polynomial Threshold\n                        None\n            \n            \n                        48\n                        Group Features\n                        False\n            \n            \n                        49\n                        Feature Selection\n                        False\n            \n            \n                        50\n                        Feature Selection Method\n                        classic\n            \n            \n                        51\n                        Features Selection Threshold\n                        None\n            \n            \n                        52\n                        Feature Interaction\n                        False\n            \n            \n                        53\n                        Feature Ratio\n                        False\n            \n            \n                        54\n                        Interaction Threshold\n                        None\n            \n            \n                        55\n                        Transform Target\n                        False\n            \n            \n                        56\n                        Transform Target Method\n                        box-cox\n            \n    \n\n\n\nbest_model = compare_models(fold=5)\n\n\n                    Model        MAE        MSE        RMSE        R2        RMSLE        MAPE        TT (Sec)    \n                \n                        ridge\n                        Ridge Regression\n                        203104.6812\n                        70889717760.0000\n                        264074.3844\n                        0.8726\n                        0.5540\n                        0.3956\n                        0.0080\n            \n            \n                        en\n                        Elastic Net\n                        214984.3531\n                        85422610841.6000\n                        290426.9344\n                        0.8517\n                        0.4986\n                        0.4002\n                        0.0100\n            \n            \n                        br\n                        Bayesian Ridge\n                        220166.1782\n                        95589994393.9887\n                        304401.6991\n                        0.8301\n                        0.4481\n                        0.3928\n                        0.0120\n            \n            \n                        huber\n                        Huber Regressor\n                        220856.7956\n                        112309915361.3680\n                        329346.7142\n                        0.8236\n                        0.4063\n                        0.3861\n                        0.0240\n            \n            \n                        lr\n                        Linear Regression\n                        232120.2812\n                        103810244608.0000\n                        317848.8281\n                        0.8138\n                        0.5000\n                        0.4204\n                        0.4660\n            \n            \n                        et\n                        Extra Trees Regressor\n                        221900.6764\n                        127140774212.6801\n                        338961.7060\n                        0.7945\n                        0.3815\n                        0.3499\n                        0.0580\n            \n            \n                        rf\n                        Random Forest Regressor\n                        237185.4749\n                        140134962286.5481\n                        359066.7448\n                        0.7677\n                        0.3845\n                        0.3603\n                        0.0680\n            \n            \n                        gbr\n                        Gradient Boosting Regressor\n                        238720.3298\n                        145838870195.5470\n                        366741.3828\n                        0.7624\n                        0.3810\n                        0.3619\n                        0.0200\n            \n            \n                        knn\n                        K Neighbors Regressor\n                        285577.7062\n                        149621195571.2000\n                        378386.5938\n                        0.7535\n                        0.4782\n                        0.4564\n                        0.0080\n            \n            \n                        omp\n                        Orthogonal Matching Pursuit\n                        238278.1124\n                        126779634746.9780\n                        340431.6364\n                        0.7507\n                        0.6785\n                        0.4087\n                        0.0060\n            \n            \n                        ada\n                        AdaBoost Regressor\n                        286133.9032\n                        178448925624.8169\n                        409351.8897\n                        0.7261\n                        0.4671\n                        0.4826\n                        0.0380\n            \n            \n                        par\n                        Passive Aggressive Regressor\n                        333654.6862\n                        255709611689.0604\n                        478365.7421\n                        0.6396\n                        0.6616\n                        0.4911\n                        0.0080\n            \n            \n                        lightgbm\n                        Light Gradient Boosting Machine\n                        333751.2407\n                        246645596801.5230\n                        489542.3762\n                        0.6196\n                        0.4881\n                        0.4594\n                        0.0140\n            \n            \n                        dt\n                        Decision Tree Regressor\n                        331466.4338\n                        251572931731.8265\n                        484401.1935\n                        0.5996\n                        0.4895\n                        0.4942\n                        0.0080\n            \n            \n                        lasso\n                        Lasso Regression\n                        472806.4594\n                        1744652831948.8000\n                        924353.6562\n                        -2.9647\n                        0.9793\n                        0.7323\n                        0.3020\n            \n            \n                        llar\n                        Lasso Least Angle Regression\n                        557614.3428\n                        2757565135711.8994\n                        1218269.0934\n                        -4.1517\n                        0.9882\n                        0.9808\n                        0.0120\n            \n            \n                        lar\n                        Least Angle Regression\n                        523505032166121.1875\n                        8777827809541126967434359603200.0000\n                        1651376809056778.0000\n                        -21875898822041108480.0000\n                        12.5860\n                        2953087708.0914\n                        0.0120\n            \n    \n\n\n\ngbr = create_model('gbr')\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        259539.4234\n                        188010814764.0497\n                        433602.1388\n                        0.2035\n                        0.3725\n                        0.3658\n            \n            \n                        1\n                        344439.6555\n                        261157479760.8762\n                        511035.6932\n                        0.7712\n                        0.2734\n                        0.2021\n            \n            \n                        2\n                        269448.7502\n                        89799376760.5959\n                        299665.4414\n                        0.2559\n                        0.5134\n                        0.5621\n            \n            \n                        3\n                        156389.8428\n                        56101010341.3762\n                        236856.5185\n                        0.9215\n                        0.3037\n                        0.2752\n            \n            \n                        4\n                        197734.1876\n                        68770895511.2340\n                        262242.0552\n                        0.8442\n                        0.4341\n                        0.4405\n            \n            \n                        5\n                        316382.5762\n                        190021156955.1915\n                        435914.1624\n                        0.8431\n                        0.3250\n                        0.3036\n            \n            \n                        6\n                        132877.7936\n                        48011457619.3648\n                        219115.1698\n                        0.9377\n                        0.1445\n                        0.1184\n            \n            \n                        7\n                        63780.4855\n                        6638335948.5179\n                        81475.9839\n                        0.9926\n                        0.2004\n                        0.1484\n            \n            \n                        8\n                        84622.6556\n                        19489890756.1842\n                        139606.1988\n                        0.8903\n                        0.5672\n                        0.5448\n            \n            \n                        9\n                        312499.9655\n                        219320548133.8201\n                        468316.7178\n                        0.6284\n                        0.4557\n                        0.4542\n            \n            \n                        Mean\n                        213771.5336\n                        114732096655.1211\n                        308783.0080\n                        0.7288\n                        0.3590\n                        0.3415\n            \n            \n                        SD\n                        95820.4252\n                        86484686219.4249\n                        139230.5665\n                        0.2673\n                        0.1285\n                        0.1501\n            \n    \n\n\n\nimport numpy as np\ngbrs = [create_model('gbr', learning_rate=i) for i in np.arange(0.1,1,0.1)]\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        214161.7167\n                        66589303052.3318\n                        258049.0323\n                        0.7179\n                        0.5064\n                        0.3315\n            \n            \n                        1\n                        383023.4934\n                        211259341300.2875\n                        459629.5697\n                        0.8149\n                        0.3382\n                        0.2795\n            \n            \n                        2\n                        239279.0742\n                        87706228150.9347\n                        296152.3732\n                        0.2732\n                        0.5829\n                        0.5918\n            \n            \n                        3\n                        192434.2246\n                        95811714139.4744\n                        309534.6736\n                        0.8660\n                        0.4334\n                        0.4130\n            \n            \n                        4\n                        142428.6249\n                        51054538060.0787\n                        225952.5128\n                        0.8843\n                        0.2410\n                        0.1901\n            \n            \n                        5\n                        367843.9369\n                        206432485479.9056\n                        454348.4186\n                        0.8295\n                        0.5008\n                        0.4725\n            \n            \n                        6\n                        228995.4083\n                        126313136112.6237\n                        355405.5938\n                        0.8362\n                        0.2683\n                        0.2024\n            \n            \n                        7\n                        158840.0937\n                        40230541391.6350\n                        200575.5254\n                        0.9550\n                        0.4181\n                        0.2684\n            \n            \n                        8\n                        195178.1770\n                        46865758291.9661\n                        216485.0071\n                        0.7363\n                        0.5635\n                        0.6440\n            \n            \n                        9\n                        371923.4898\n                        271120545391.5568\n                        520692.3712\n                        0.5406\n                        0.5683\n                        0.5885\n            \n            \n                        Mean\n                        249410.8239\n                        120338359137.0794\n                        329682.5078\n                        0.7454\n                        0.4421\n                        0.3982\n            \n            \n                        SD\n                        86305.1580\n                        77214568547.1943\n                        107924.9888\n                        0.1907\n                        0.1184\n                        0.1603\n            \n    \n\n\n\nprint(len(gbrs))\n\n9\n\n\n\ntuned_gbr = tune_model(gbr, n_iter=50, optimize = 'RMSE')\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        154942.6734\n                        74997708663.9170\n                        273857.0953\n                        0.6823\n                        0.2835\n                        0.2430\n            \n            \n                        1\n                        356026.5185\n                        259197991105.0069\n                        509114.9095\n                        0.7729\n                        0.3057\n                        0.2048\n            \n            \n                        2\n                        196245.5979\n                        50062838730.3433\n                        223747.2653\n                        0.5852\n                        0.4460\n                        0.4092\n            \n            \n                        3\n                        130813.1472\n                        28724978265.9940\n                        169484.4484\n                        0.9598\n                        0.3879\n                        0.3551\n            \n            \n                        4\n                        150379.1478\n                        58016543407.9028\n                        240866.2355\n                        0.8686\n                        0.3318\n                        0.2935\n            \n            \n                        5\n                        360842.5596\n                        256418976969.6155\n                        506378.2943\n                        0.7883\n                        0.3802\n                        0.3776\n            \n            \n                        6\n                        101664.5162\n                        23660563887.4224\n                        153819.9073\n                        0.9693\n                        0.1738\n                        0.1305\n            \n            \n                        7\n                        121078.1094\n                        22126447635.5874\n                        148749.6139\n                        0.9753\n                        0.3012\n                        0.2768\n            \n            \n                        8\n                        204892.4698\n                        59961129586.8548\n                        244869.6175\n                        0.6626\n                        0.7065\n                        0.9038\n            \n            \n                        9\n                        247798.4343\n                        119125737964.7928\n                        345145.9662\n                        0.7982\n                        0.3572\n                        0.3649\n            \n            \n                        Mean\n                        202468.3174\n                        95229291621.7437\n                        281603.3353\n                        0.8062\n                        0.3674\n                        0.3559\n            \n            \n                        SD\n                        88121.5888\n                        85677172457.2040\n                        126209.5604\n                        0.1300\n                        0.1326\n                        0.2000\n            \n    \n\n\n\ntuned_gbr\n\nGradientBoostingRegressorGradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n                          init=None, learning_rate=0.05, loss='ls', max_depth=8,\n                          max_features='sqrt', max_leaf_nodes=None,\n                          min_impurity_decrease=0.001, min_impurity_split=None,\n                          min_samples_leaf=3, min_samples_split=10,\n                          min_weight_fraction_leaf=0.0, n_estimators=260,\n                          n_iter_no_change=None, presort='deprecated',\n                          random_state=153, subsample=1.0, tol=0.0001,\n                          validation_fraction=0.1, verbose=0, warm_start=False)\n\n\n\ndt = create_model('dt')\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        207956.4444\n                        64209396573.1111\n                        253395.7312\n                        0.7280\n                        0.2948\n                        0.3014\n            \n            \n                        1\n                        524236.3333\n                        569991781768.5555\n                        754978.0009\n                        0.5006\n                        0.4063\n                        0.3254\n            \n            \n                        2\n                        329215.7500\n                        190393519675.5000\n                        436341.0589\n                        -0.5777\n                        0.5801\n                        0.6555\n            \n            \n                        3\n                        191451.2500\n                        124860637323.7500\n                        353356.2470\n                        0.8254\n                        0.4116\n                        0.3334\n            \n            \n                        4\n                        213423.5000\n                        76306439743.2500\n                        276236.2028\n                        0.8271\n                        0.5254\n                        0.3705\n            \n            \n                        5\n                        370690.3750\n                        199965742624.8750\n                        447175.2929\n                        0.8349\n                        0.5442\n                        0.4049\n            \n            \n                        6\n                        246669.8750\n                        129638363043.8750\n                        360053.2781\n                        0.8319\n                        0.2617\n                        0.2171\n            \n            \n                        7\n                        178672.0000\n                        77945893211.5000\n                        279187.9174\n                        0.9128\n                        0.3495\n                        0.3173\n            \n            \n                        8\n                        166587.8750\n                        46127063745.6250\n                        214772.1205\n                        0.7404\n                        0.4736\n                        0.4654\n            \n            \n                        9\n                        295026.8750\n                        185238337215.6250\n                        430393.2356\n                        0.6861\n                        0.3864\n                        0.3666\n            \n            \n                        Mean\n                        272393.0278\n                        166467717492.5666\n                        380588.9085\n                        0.6310\n                        0.4234\n                        0.3757\n            \n            \n                        SD\n                        105664.3855\n                        144523328671.4254\n                        147036.7308\n                        0.4171\n                        0.1010\n                        0.1121\n            \n    \n\n\n\nbagged_dt = ensemble_model(dt, n_estimators=50)\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        196727.1578\n                        90285018684.0473\n                        300474.6556\n                        0.6175\n                        0.3087\n                        0.2890\n            \n            \n                        1\n                        400549.3422\n                        340151201313.8998\n                        583224.8291\n                        0.7020\n                        0.3095\n                        0.2039\n            \n            \n                        2\n                        225912.9050\n                        72423782552.9881\n                        269116.6709\n                        0.3999\n                        0.4933\n                        0.5057\n            \n            \n                        3\n                        118783.1250\n                        22526053317.3862\n                        150086.8193\n                        0.9685\n                        0.3121\n                        0.2808\n            \n            \n                        4\n                        202532.9275\n                        80967074782.4978\n                        284547.1398\n                        0.8166\n                        0.4177\n                        0.4139\n            \n            \n                        5\n                        341289.0375\n                        234909197221.4275\n                        484674.3208\n                        0.8060\n                        0.3879\n                        0.3623\n            \n            \n                        6\n                        141661.2425\n                        37608256142.0725\n                        193928.4820\n                        0.9512\n                        0.1464\n                        0.1355\n            \n            \n                        7\n                        126158.8400\n                        34803232314.1118\n                        186556.2444\n                        0.9611\n                        0.3018\n                        0.2442\n            \n            \n                        8\n                        183361.3550\n                        44385100050.7344\n                        210677.7161\n                        0.7502\n                        0.5438\n                        0.5929\n            \n            \n                        9\n                        260316.8175\n                        176470763137.0123\n                        420084.2334\n                        0.7010\n                        0.3999\n                        0.3821\n            \n            \n                        Mean\n                        219729.2750\n                        113452967951.6178\n                        308337.1111\n                        0.7674\n                        0.3621\n                        0.3410\n            \n            \n                        SD\n                        87376.1209\n                        99179989784.9265\n                        135577.2615\n                        0.1680\n                        0.1066\n                        0.1322\n            \n    \n\n\n\nboosted_dt = ensemble_model(dt, method = 'Boosting')\n\n\n                    MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        261162.8889\n                        114911582041.3333\n                        338986.1089\n                        0.5132\n                        0.3872\n                        0.3925\n            \n            \n                        1\n                        422328.3333\n                        333782428795.6667\n                        577739.0664\n                        0.7076\n                        0.3211\n                        0.2542\n            \n            \n                        2\n                        232284.1250\n                        77562868468.1250\n                        278501.1104\n                        0.3573\n                        0.5087\n                        0.5015\n            \n            \n                        3\n                        197047.1250\n                        112221331803.8750\n                        334994.5250\n                        0.8431\n                        0.3502\n                        0.3395\n            \n            \n                        4\n                        285119.7500\n                        161644974606.0000\n                        402050.9602\n                        0.6338\n                        0.5596\n                        0.5738\n            \n            \n                        5\n                        473330.2500\n                        599114250508.0000\n                        774024.7092\n                        0.5053\n                        0.5119\n                        0.5217\n            \n            \n                        6\n                        108483.0000\n                        20435489036.7500\n                        142952.7511\n                        0.9735\n                        0.2098\n                        0.1500\n            \n            \n                        7\n                        157960.0000\n                        69455073830.5000\n                        263543.3054\n                        0.9223\n                        0.2735\n                        0.2394\n            \n            \n                        8\n                        120478.7500\n                        23354347455.5000\n                        152821.2925\n                        0.8686\n                        0.5655\n                        0.5893\n            \n            \n                        9\n                        231595.7500\n                        117602419885.0000\n                        342932.0922\n                        0.8007\n                        0.4401\n                        0.4379\n            \n            \n                        Mean\n                        248978.9972\n                        163008476643.0750\n                        360854.5921\n                        0.7125\n                        0.4128\n                        0.4000\n            \n            \n                        SD\n                        113864.7400\n                        167985632403.4423\n                        181086.8299\n                        0.1940\n                        0.1176\n                        0.1435\n            \n    \n\n\n\nplot_model(dt)\n\n\n\n\n\nplot_model(dt, plot = 'error')\n\n\n\n\n\nplot_model(dt, plot = 'feature')\n\n\n\n\n\nevaluate_model(dt)\n\n\n\n\n\n  \n    \n      \n      Parameters\n    \n  \n  \n    \n      ccp_alpha\n      0.0\n    \n    \n      criterion\n      mse\n    \n    \n      max_depth\n      None\n    \n    \n      max_features\n      None\n    \n    \n      max_leaf_nodes\n      None\n    \n    \n      min_impurity_decrease\n      0.0\n    \n    \n      min_impurity_split\n      None\n    \n    \n      min_samples_leaf\n      1\n    \n    \n      min_samples_split\n      2\n    \n    \n      min_weight_fraction_leaf\n      0.0\n    \n    \n      presort\n      deprecated\n    \n    \n      random_state\n      153\n    \n    \n      splitter\n      best\n    \n  \n\n\n\n\n\ninterpret_model(dt)\n\n\n\n\n\ninterpret_model(dt, plot = 'correlation')\n\n\n\n\n\ninterpret_model(dt, plot = 'reason', observation = 12)\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\nbest = automl(optimize = 'MAE')\nbest\n\nGradientBoostingRegressorGradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n                          init=None, learning_rate=0.05, loss='ls', max_depth=8,\n                          max_features='sqrt', max_leaf_nodes=None,\n                          min_impurity_decrease=0.001, min_impurity_split=None,\n                          min_samples_leaf=3, min_samples_split=10,\n                          min_weight_fraction_leaf=0.0, n_estimators=260,\n                          n_iter_no_change=None, presort='deprecated',\n                          random_state=153, subsample=1.0, tol=0.0001,\n                          validation_fraction=0.1, verbose=0, warm_start=False)\n\n\n\npred_holdouts = predict_model(dt)\npred_holdouts.head()\n\n\n                    Model        MAE        MSE        RMSE        R2        RMSLE        MAPE    \n                \n                        0\n                        Decision Tree Regressor\n                        330931.5556\n                        395886787646.2778\n                        629195.3494\n                        0.3277\n                        0.4581\n                        0.4602\n            \n    \n\n\n\n\n\n\n  \n    \n      \n      general\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      it\n      province_Anhui\n      province_Beijing\n      ...\n      year_2006\n      year_2007\n      reg_East China\n      reg_North China\n      reg_Northeast China\n      reg_Northwest China\n      reg_South Central China\n      reg_Southwest China\n      specific\n      Label\n    \n  \n  \n    \n      0\n      123546.0\n      2011.189941\n      12812.0\n      0.0\n      0.0\n      0.000000\n      1514364.0\n      2254281.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      436189.0\n      472786.0\n    \n    \n      1\n      36670.0\n      2312.820068\n      11169.0\n      0.0\n      0.0\n      0.000000\n      1600475.0\n      3035767.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      615593.0\n      601485.0\n    \n    \n      2\n      241282.0\n      6867.700195\n      53903.0\n      0.0\n      0.0\n      0.516129\n      2823413.0\n      3586373.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      685732.0\n      681676.0\n    \n    \n      3\n      581800.0\n      25776.910156\n      1101159.0\n      0.0\n      0.0\n      0.000000\n      16753980.0\n      6357869.0\n      0.0\n      0.0\n      ...\n      0.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2121243.0\n      3860764.0\n    \n    \n      4\n      36946.0\n      445.359985\n      1743.0\n      0.0\n      0.0\n      0.000000\n      233299.0\n      736165.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      133858.0\n      107687.0\n    \n  \n\n5 rows  49 columns\n\n\n\n\nnew_data = df.copy()\nnew_data.drop(['specific'], axis=1, inplace=True)\npredict_new = predict_model(best, data=new_data)\npredict_new.head()\n\n\n\n\n\n  \n    \n      \n      province\n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      reg\n      it\n      Label\n    \n  \n  \n    \n      4\n      Anhui\n      32100.0\n      2000\n      2902.09\n      31847\n      0.0\n      0.0\n      0.000000\n      1601508\n      East China\n      1499110\n      2.000834e+05\n    \n    \n      6\n      Anhui\n      66529.0\n      2002\n      3519.72\n      38375\n      0.0\n      0.0\n      0.000000\n      1677840\n      East China\n      2404936\n      4.365530e+05\n    \n    \n      7\n      Anhui\n      52108.0\n      2003\n      3923.11\n      36720\n      0.0\n      0.0\n      0.000000\n      1896479\n      East China\n      2815820\n      6.096731e+05\n    \n    \n      10\n      Anhui\n      279052.0\n      2006\n      6112.50\n      139354\n      0.0\n      0.0\n      0.324324\n      3434548\n      East China\n      5167300\n      1.455109e+06\n    \n    \n      11\n      Anhui\n      178705.0\n      2007\n      7360.92\n      299892\n      0.0\n      0.0\n      0.324324\n      4468640\n      East China\n      7040099\n      2.000116e+06\n    \n  \n\n\n\n\n\nsave_model(best, model_name='best-model')\n\nTransformation Pipeline and Model Succesfully Saved\n\n\n(Pipeline(memory=None,\n          steps=[('dtypes',\n                  DataTypes_Auto_infer(categorical_features=[],\n                                       display_types=True, features_todrop=[],\n                                       id_columns=[], ml_usecase='regression',\n                                       numerical_features=[], target='specific',\n                                       time_features=[])),\n                 ('imputer',\n                  Simple_Imputer(categorical_strategy='not_available',\n                                 fill_value_categorical=None,\n                                 fill_value_numerical=None,\n                                 numeric_strateg...\n                                            learning_rate=0.05, loss='ls',\n                                            max_depth=8, max_features='sqrt',\n                                            max_leaf_nodes=None,\n                                            min_impurity_decrease=0.001,\n                                            min_impurity_split=None,\n                                            min_samples_leaf=3,\n                                            min_samples_split=10,\n                                            min_weight_fraction_leaf=0.0,\n                                            n_estimators=260,\n                                            n_iter_no_change=None,\n                                            presort='deprecated',\n                                            random_state=153, subsample=1.0,\n                                            tol=0.0001, validation_fraction=0.1,\n                                            verbose=0, warm_start=False)]],\n          verbose=False),\n 'best-model.pkl')\n\n\n\nloaded_bestmodel = load_model('best-model')\nprint(loaded_bestmodel)\n\nTransformation Pipeline and Model Successfully Loaded\nPipeline(memory=None,\n         steps=[('dtypes',\n                 DataTypes_Auto_infer(categorical_features=[],\n                                      display_types=True, features_todrop=[],\n                                      id_columns=[], ml_usecase='regression',\n                                      numerical_features=[], target='specific',\n                                      time_features=[])),\n                ('imputer',\n                 Simple_Imputer(categorical_strategy='not_available',\n                                fill_value_categorical=None,\n                                fill_value_numerical=None,\n                                numeric_strateg...\n                                           learning_rate=0.05, loss='ls',\n                                           max_depth=8, max_features='sqrt',\n                                           max_leaf_nodes=None,\n                                           min_impurity_decrease=0.001,\n                                           min_impurity_split=None,\n                                           min_samples_leaf=3,\n                                           min_samples_split=10,\n                                           min_weight_fraction_leaf=0.0,\n                                           n_estimators=260,\n                                           n_iter_no_change=None,\n                                           presort='deprecated',\n                                           random_state=153, subsample=1.0,\n                                           tol=0.0001, validation_fraction=0.1,\n                                           verbose=0, warm_start=False)]],\n         verbose=False)\n\n\n\nfrom sklearn import set_config\nset_config(display='diagram')\nloaded_bestmodel[0]\n\nDataTypes_Auto_inferDataTypes_Auto_infer(categorical_features=[], display_types=True,\n                     features_todrop=[], id_columns=[], ml_usecase='regression',\n                     numerical_features=[], target='specific',\n                     time_features=[])\n\n\n\nfrom sklearn import set_config\nset_config(display='text')\n\n\nX_train = get_config('X_train')\nX_train.head()\n\n\n\n\n\n  \n    \n      \n      general\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      it\n      province_Anhui\n      province_Beijing\n      ...\n      year_2002\n      year_2003\n      year_2006\n      year_2007\n      reg_East China\n      reg_North China\n      reg_Northeast China\n      reg_Northwest China\n      reg_South Central China\n      reg_Southwest China\n    \n  \n  \n    \n      343\n      66100.0\n      2556.020020\n      8384.0\n      0.0\n      0.000000\n      0.000000\n      1807967.0\n      3388449.0\n      0.0\n      0.0\n      ...\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n    \n    \n      259\n      116000.0\n      12078.150391\n      601617.0\n      0.0\n      0.000000\n      0.000000\n      6166904.0\n      2940367.0\n      0.0\n      0.0\n      ...\n      0.0\n      1.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      190\n      655919.0\n      4056.760010\n      242000.0\n      0.0\n      0.410256\n      0.000000\n      2525301.0\n      3343228.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      1.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      319\n      50097.0\n      185.089996\n      467.0\n      0.0\n      0.000000\n      0.324324\n      70048.0\n      1333133.0\n      0.0\n      0.0\n      ...\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n    \n    \n      258\n      113000.0\n      10275.500000\n      473404.0\n      0.0\n      0.000000\n      0.000000\n      5145006.0\n      2455900.0\n      0.0\n      0.0\n      ...\n      1.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n5 rows  47 columns\n\n\n\n\nget_config('seed')\n\n153\n\n\n\nfrom pycaret.regression import set_config\nset_config('seed', 999)\n\n\nget_config('seed')\n\n999\n\n\n\n!mlflow ui \n\n[2021-05-31 20:13:02 -0500] [56453] [INFO] Starting gunicorn 20.0.4\n[2021-05-31 20:13:02 -0500] [56453] [INFO] Listening at: http://127.0.0.1:5000 (56453)\n[2021-05-31 20:13:02 -0500] [56453] [INFO] Using worker: sync\n[2021-05-31 20:13:02 -0500] [56455] [INFO] Booting worker with pid: 56455\n^C\n[2021-05-31 20:13:35 -0500] [56453] [INFO] Handling signal: int\n[2021-05-31 20:13:35 -0500] [56455] [INFO] Worker exiting (pid: 56455)"
  },
  {
    "objectID": "posts/2020-08-16-pandas-stats-fiscal.html",
    "href": "posts/2020-08-16-pandas-stats-fiscal.html",
    "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
    "section": "",
    "text": "# Import required packages\n\nimport numpy as np\nimport pandas as pd\n\n%matplotlib inline\nimport seaborn as sns\n\nimport statsmodels.formula.api as smf\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.lines import Line2D\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "posts/2020-08-16-pandas-stats-fiscal.html#read-required-datasets",
    "href": "posts/2020-08-16-pandas-stats-fiscal.html#read-required-datasets",
    "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
    "section": "Read required datasets",
    "text": "Read required datasets\n\ndf = pd.read_csv('ttb_county_clean.csv')\ndf1 = pd.read_csv('df_panel_fix.csv')"
  },
  {
    "objectID": "posts/2020-08-16-pandas-stats-fiscal.html#figure-1",
    "href": "posts/2020-08-16-pandas-stats-fiscal.html#figure-1",
    "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
    "section": "Figure 1",
    "text": "Figure 1\n\ndf.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n    s=df[\"specific\"]/100, label=\"Specific Purpose Transfers\", figsize=(12,8),\n    c=\"nightlights\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n    sharex=False)\n#save_fig(\"cn-spt-county-heat\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f0a4e16b4e0>"
  },
  {
    "objectID": "posts/2020-08-16-pandas-stats-fiscal.html#panel-regression-framework-with-year-and-province-fixed-effects",
    "href": "posts/2020-08-16-pandas-stats-fiscal.html#panel-regression-framework-with-year-and-province-fixed-effects",
    "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
    "section": "Panel regression framework with year and province fixed effects",
    "text": "Panel regression framework with year and province fixed effects\n\nlin_reg = smf.ols('np.log(specific) ~ np.log(gdp) + np.log(fdi) + i + rnr + rr + C(province) + C(year)', data=df1).fit()\n\n\n#lin_reg.summary()"
  },
  {
    "objectID": "posts/2020-08-16-pandas-stats-fiscal.html#figure-2",
    "href": "posts/2020-08-16-pandas-stats-fiscal.html#figure-2",
    "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
    "section": "Figure 2",
    "text": "Figure 2\n\ncoef_df = pd.read_csv('coef.csv')\n\nfig, ax = plt.subplots(figsize=(16, 10))\ncoef_df.plot(x='varname', y='coef', kind='bar', \n             ax=ax, color='none', \n             yerr='err', legend=False)\nax.set_ylabel('Specific Purpose Transfers (ln)')\nax.set_xlabel('Independant Variables')\nax.scatter(x=pd.np.arange(coef_df.shape[0]), \n           marker='s', s=120, \n           y=coef_df['coef'], color='black')\nax.axhline(y=0, linestyle='--', color='blue', linewidth=4)\nax.xaxis.set_ticks_position('none')\n\n_ = ax.set_xticklabels(['GDP', 'FDI', 'Incumbent', 'Non Relevant Rival', 'Relevant Rival'], \n                       rotation=0, fontsize=20)\n\nfs = 16\nax.annotate('Controls', xy=(0.2, -0.2), xytext=(0.2, -0.3), \n            xycoords='axes fraction', \n            textcoords='axes fraction', \n            fontsize=fs, ha='center', va='bottom',\n            bbox=dict(boxstyle='square', fc='white', ec='blue'),\n            arrowprops=dict(arrowstyle='-[, widthB=5.5, lengthB=1.2', lw=2.0, color='blue'))\n\n_ = ax.annotate('Connections', xy=(0.7, -0.2), xytext=(0.7, -0.3), \n                 xycoords='axes fraction', \n                 textcoords='axes fraction', \n                 fontsize=fs, ha='center', va='bottom',\n                 bbox=dict(boxstyle='square', fc='white', ec='red'),\n                 arrowprops=dict(arrowstyle='-[, widthB=10.5, lengthB=1.2', lw=2.0, color='red'))\n\n#save_fig(\"i-coef-plot\")"
  },
  {
    "objectID": "posts/2020-08-16-pandas-stats-fiscal.html#figure-3",
    "href": "posts/2020-08-16-pandas-stats-fiscal.html#figure-3",
    "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
    "section": "Figure 3",
    "text": "Figure 3\n\nimport numpy as np\nfrom plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot,iplot\nprint (__version__) # requires version >= 1.9.0\n\n\n#Always run this the command before at the start of notebook\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n\n\ntrace1 = go.Bar(\n    x=[1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003,\n       2004, 2005, 2006, 2007],\n    y=[188870900000.0, 185182900000.0, 237697500000.0, 347187900000.0, 296716700000.0,\n       397833100000.0, 440204800000.0, 514254300000.0, 686016600000.0, 677746300000.0, 940057900000.0,\n       1881304000000],\n    name='All Other Province Leaders',\n    marker=dict(\n        color='rgb(55, 83, 109)'\n    )\n)\ntrace2 = go.Bar(\n    x=[1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003,\n       2004, 2005, 2006, 2007],\n    y=[260376000000.0, 264934700000.0, 367350200000.0, 463861200000.0, 199068500000.0, 216582600000.0,\n  298631800000.0, 409759300000.0, 830363200000.0, 878158000000.0, 1143745000000.0, 2125891000000.0],\n    name='Incumbent Connected Province Leaders',\n    marker=dict(\n        color='rgb(26, 118, 255)'\n    )\n)\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title='Specific Purpose Transfers',\n    xaxis=dict(\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='RMB',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    legend=dict(\n        x=0,\n        y=1.0,\n        bgcolor='rgba(255, 255, 255, 0)',\n        bordercolor='rgba(255, 255, 255, 0)'\n    ),\n    barmode='group',\n    bargap=0.15,\n    bargroupgap=0.1\n)\n\nfig = go.Figure(data=data, layout=layout)\n\n#iplot(fig, filename='style-bar')\niplot(fig, image='png',filename='spt-i-bar')\n\n4.1.1"
  },
  {
    "objectID": "posts/2021-05-31-parameter-tuning.html",
    "href": "posts/2021-05-31-parameter-tuning.html",
    "title": "Parameter tuning",
    "section": "",
    "text": "import seaborn as sns\nsns.set_theme(context=\"notebook\", font_scale=1.4,\n              rc={\"figure.figsize\": [10, 6]})\n\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['it', 'Unnamed: 0'], axis = 1), df['it']\n\n\nX = X.select_dtypes(include='number')\nX\n\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\n\n\n\n\nX.fdi\n\n4        31847\n6        38375\n7        36720\n10      139354\n11      299892\n        ...   \n347      39453\n354     307610\n355     498055\n358     888935\n359    1036576\nName: fdi, Length: 118, dtype: int64\n\n\n\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\n\n\nparams = {\n    'max_depth': [2, 4, 8, 12, 16],\n    'max_features': [4, 8, 16, 32]\n}\n\n\ngrid_search = GridSearchCV(\n    RandomForestRegressor(random_state=42), param_grid=params,\n    verbose=1,\n    n_jobs=8)  # Update to the number of physical cpu cores\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n\ngrid_search.fit(X_train, y_train)\n\nFitting 5 folds for each of 20 candidates, totalling 100 fits\n\n\n[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    2.4s\n[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    3.6s finished\n\n\nGridSearchCVGridSearchCV(estimator=RandomForestRegressor(random_state=42), n_jobs=8,\n             param_grid={'max_depth': [2, 4, 8, 12, 16],\n                         'max_features': [4, 8, 16, 32]},\n             verbose=1)RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\ngrid_search.best_score_\n\n0.6771979740200512\n\n\n\ngrid_search.best_params_\n\n{'max_depth': 12, 'max_features': 4}\n\n\n\ngrid_search.score(X_test, y_test)\n\n0.7816726342611063\n\n\n\nimport pandas as pd\ncv_df = pd.DataFrame(grid_search.cv_results_)\n\n\nres = (cv_df.pivot(index='param_max_depth', columns='param_max_features', values='mean_test_score')\n            .rename_axis(index='max_depth', columns='max_features'))\n\n\nimport seaborn as sns\n_ = sns.heatmap(res, cmap='viridis')\n\n\n\n\n\n# %load solutions/02-ex01-solutions.py\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_dist = {\n    \"max_features\": randint(1, 11),\n    \"min_samples_split\": randint(2, 11)\n}\n\nrandom_search = RandomizedSearchCV(RandomForestRegressor(random_state=0),\n                                   param_distributions=param_dist,\n                                   verbose=1,\n                                   random_state=0)\n\nrandom_search.fit(X_train, y_train)\n\nrandom_search.best_params_\n\nrandom_search.best_score_\n\nrandom_search.score(X_test, y_test)\n\nfrom sklearn.svm import SVC\n\nsvm_grid = GridSearchCV(\n    SVC(random_state=42), param_grid={'kernel': ['linear', 'poly', 'rbf', 'sigmoid']},\n    verbose=1, n_jobs=8\n)\n\nsvm_grid.fit(X_train, y_train)\n\nsvm_grid.best_score_\n\nsvm_grid.best_params_\n\nsvm_grid.score(X_test, y_test)\n\nFitting 5 folds for each of 10 candidates, totalling 50 fits\n\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 386, in fit\n    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1048, in __call__\n    if self.dispatch_one_batch(iterator):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 866, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 784, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 168, in _parallel_build_trees\n    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 1242, in fit\n    super().fit(\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 279, in fit\n    raise ValueError(\"max_features must be in (0, n_features]\")\nValueError: max_features must be in (0, n_features]\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 386, in fit\n    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1048, in __call__\n    if self.dispatch_one_batch(iterator):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 866, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 784, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 168, in _parallel_build_trees\n    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 1242, in fit\n    super().fit(\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 279, in fit\n    raise ValueError(\"max_features must be in (0, n_features]\")\nValueError: max_features must be in (0, n_features]\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 386, in fit\n    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1048, in __call__\n    if self.dispatch_one_batch(iterator):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 866, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 784, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 168, in _parallel_build_trees\n    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 1242, in fit\n    super().fit(\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 279, in fit\n    raise ValueError(\"max_features must be in (0, n_features]\")\nValueError: max_features must be in (0, n_features]\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 386, in fit\n    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1048, in __call__\n    if self.dispatch_one_batch(iterator):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 866, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 784, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 168, in _parallel_build_trees\n    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 1242, in fit\n    super().fit(\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 279, in fit\n    raise ValueError(\"max_features must be in (0, n_features]\")\nValueError: max_features must be in (0, n_features]\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 386, in fit\n    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1048, in __call__\n    if self.dispatch_one_batch(iterator):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 866, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 784, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 168, in _parallel_build_trees\n    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 1242, in fit\n    super().fit(\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 279, in fit\n    raise ValueError(\"max_features must be in (0, n_features]\")\nValueError: max_features must be in (0, n_features]\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n\n\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n\n\n[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    4.7s finished\n\n\nValueError: n_splits=5 cannot be greater than the number of members in each class."
  },
  {
    "objectID": "posts/2020-09-02-nlp_ngram.html",
    "href": "posts/2020-09-02-nlp_ngram.html",
    "title": "NLP ngrams With Python",
    "section": "",
    "text": "import pandas as pd\ndf=pd.read_csv('../../processed_data/nf_complete.csv')"
  },
  {
    "objectID": "posts/2020-09-02-nlp_ngram.html#pre-processing-text",
    "href": "posts/2020-09-02-nlp_ngram.html#pre-processing-text",
    "title": "NLP ngrams With Python",
    "section": "Pre-processing text",
    "text": "Pre-processing text\n\ndef preprocessor(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n        ' '.join(emoticons).replace('-', '')\n    return text"
  },
  {
    "objectID": "posts/2020-09-02-nlp_ngram.html#find-total-word-count",
    "href": "posts/2020-09-02-nlp_ngram.html#find-total-word-count",
    "title": "NLP ngrams With Python",
    "section": "Find Total Word Count",
    "text": "Find Total Word Count\n\ntext = \" \".join(review for review in df.abstract)\nprint (\"There are {} words in the combination of all abstracts.\".format(len(text)))\n\nThere are 272025 words in the combination of all abstracts.\n\n\n\nfrom urllib.request import urlopen\nfrom random import randint\n\ndef wordListSum(wordList):\n    sum = 0\n    for word, value in wordList.items():\n        sum += value\n    return sum\n\ndef retrieveRandomWord(wordList):\n    randIndex = randint(1, wordListSum(wordList))\n    for word, value in wordList.items():\n        randIndex -= value\n        if randIndex <= 0:\n            return word\n\ndef buildWordDict(text):\n    # Remove newlines and quotes\n    text = text.replace('\\n', ' ');\n    text = text.replace('\"', '');\n\n    # Make sure punctuation marks are treated as their own \"words,\"\n    # so that they will be included in the Markov chain\n    punctuation = [',','.',';',':']\n    for symbol in punctuation:\n        text = text.replace(symbol, ' {} '.format(symbol));\n\n    words = text.split(' ')\n    # Filter out empty words\n    words = [word for word in words if word != '']\n\n    wordDict = {}\n    for i in range(1, len(words)):\n        if words[i-1] not in wordDict:\n                # Create a new dictionary for this word\n            wordDict[words[i-1]] = {}\n        if words[i] not in wordDict[words[i-1]]:\n            wordDict[words[i-1]][words[i]] = 0\n        wordDict[words[i-1]][words[i]] += 1\n    return wordDict\n\nwordDict = buildWordDict(text)\n\n#Generate a Markov chain of length 100\nlength = 100\nchain = ['Vietnam']\nfor i in range(0, length):\n    newWord = retrieveRandomWord(wordDict[chain[-1]])\n    chain.append(newWord)\n\nprint(' '.join(chain))\n\nVietnam (DRV) hampered the end it? This paper at all) and have on a viable combat jet aircraft into a corps composed overwhelmingly of radical visions of the argument in Iraqi Kurdistan , few reasons : the war experience . The group identified and how elite cues , the emphasis on the ongoing betrayal of 1971-79 under which I argue that expand our knowledge of biological weapons which it . This pushes against Axis material support for any single institutional prerogatives . My work fills an opposition organization at the generalizability of the Cold War strategy to escalate .  and\n\n\n\ndef getFirstSentenceContaining(ngram, text):\n    #print(ngram)\n    sentences = text.upper().split(\". \")\n    for sentence in sentences: \n        if ngram in sentence:\n            return sentence+'\\n'\n    return \"\"\n\n\nprint(getFirstSentenceContaining('I', text))\n\nCIVIL-MILITARY RELATIONS ARE FREQUENTLY STUDIED AS IF THEY OPERATE ON TWO DISTINCT LEVELS OF ANALYSIS\n\n\n\n\n#text\n\n\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport re\nimport string\nfrom collections import Counter\n\ndef cleanSentence(sentence):\n    sentence = sentence.split(' ')\n    sentence = [word.strip(string.punctuation+string.whitespace) for word in sentence]\n    sentence = [word for word in sentence if len(word) > 1 or (word.lower() == 'a' or word.lower() == 'i')]\n    return sentence\n\ndef cleanInput(content):\n    content = content.upper()\n    content = re.sub('\\n', ' ', content)\n    content = bytes(content, 'UTF-8')\n    content = content.decode('ascii', 'ignore')\n    sentences = content.split('. ')\n    return [cleanSentence(sentence) for sentence in sentences]\n\ndef getNgramsFromSentence(content, n):\n    output = []\n    for i in range(len(content)-n+1):\n        output.append(content[i:i+n])\n    return output\n\ndef getNgrams(content, n):\n    content = cleanInput(content)\n    ngrams = Counter()\n    ngrams_list = []\n    for sentence in content:\n        newNgrams = [' '.join(ngram) for ngram in getNgramsFromSentence(sentence, n)]\n        ngrams_list.extend(newNgrams)\n        ngrams.update(newNgrams)\n    return(ngrams)\n\n\ncontent = str(text)\n\nngrams = getNgrams(content, 3)\n#print(ngrams)\n\n\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport re\nimport string\nfrom collections import Counter\n\ndef isCommon(ngram):\n    commonWords = ['THE', 'BE', 'AND', 'OF', 'A', 'IN', 'TO', 'HAVE', 'IT', 'I', 'THAT', 'FOR', 'YOU', 'HE', 'WITH', 'ON', 'DO', 'SAY', 'THIS', 'THEY', 'IS', 'AN', 'AT', 'BUT', 'WE', 'HIS', 'FROM', 'THAT', 'NOT', 'BY', 'SHE', 'OR', 'AS', 'WHAT', 'GO', 'THEIR', 'CAN', 'WHO', 'GET', 'IF', 'WOULD', 'HER', 'ALL', 'MY', 'MAKE', 'ABOUT', 'KNOW', 'WILL', 'AS', 'UP', 'ONE', 'TIME', 'HAS', 'BEEN', 'THERE', 'YEAR', 'SO', 'THINK', 'WHEN', 'WHICH', 'THEM', 'SOME', 'ME', 'PEOPLE', 'TAKE', 'OUT', 'INTO', 'JUST', 'SEE', 'HIM', 'YOUR', 'COME', 'COULD', 'NOW', 'THAN', 'LIKE', 'OTHER', 'HOW', 'THEN', 'ITS', 'OUR', 'TWO', 'MORE', 'THESE', 'WANT', 'WAY', 'LOOK', 'FIRST', 'ALSO', 'NEW', 'BECAUSE', 'DAY', 'MORE', 'USE', 'NO', 'MAN', 'FIND', 'HERE', 'THING', 'GIVE', 'MANY', 'WELL']\n    for word in ngram:\n        if word in commonWords:\n            return True\n    return False\n\ndef getNgramsFromSentence(content, n):\n    output = []\n    for i in range(len(content)-n+1):\n        if not isCommon(content[i:i+n]):\n            output.append(content[i:i+n])\n    return output\n\nngrams = getNgrams(content, 3)\n#print(ngrams)\n\n\ndef getFirstSentenceContaining(ngram, content):\n    #print(ngram)\n    sentences = content.upper().split(\". \")\n    for sentence in sentences: \n        if ngram in sentence:\n            return sentence+'\\n'\n    return \"\"\n\n\nprint(getFirstSentenceContaining('SINO-JAPANESE WAR 1894-1895', content))\nprint(getFirstSentenceContaining('2ND VIETNAM WAR', content))\nprint(getFirstSentenceContaining('COLD WAR ARMY', content))\nprint(getFirstSentenceContaining('WORLD WAR II', content))\nprint(getFirstSentenceContaining('ARMS CONTROL AGREEMENTS', content))\n\n\n IN THE INTERNATIONAL SITUATION, THE GERMANS PROVIDED SUBSTANTIAL ADVANCES TO TECHNOLOGICAL DEVELOPMENT IN THE IMMEDIATE POST-WAR PERIOD.\n   THE HISTORIOGRAPHY ON THE 2ND VIETNAM WAR HAS FOCUSED MOSTLY ON THE AMERICAN SIDE, WHILE THE OTHER SIDE, ESPECIALLY FOR THE EARLY VIETNAM WAR, 1964-1966, HAS NOT ATTRACTED MUCH ATTENTION\n\nCOLD WAR ARMY DURING THE PERIOD 1949 AND 1953 BY EXAMINING HOW SENIOR ARMY LEADERS WERE ABLE TO FUNDAMENTALLY BROADEN THE INSTITUTIONS INTELLECTUAL AND HISTORICAL FRAMEWORK OF PREPAREDNESS TO DESIGN A BLUEPRINT FOR A NEW TYPE OF GROUND FORCE THAT WOULD BE MORE ADEPT TO MEET THE CHALLENGES OF THE NEW NATURE OF WAR IMPOSED BY THE COLD WAR\n\n I ARGUE THAT A NORM PROTECTING STATES TERRITORIAL SOVEREIGNTY IS ONLY ENTRENCHED AFTER WORLD WAR II, ALTHOUGH IT CAN BE TRACED AT LEAST AS FAR BACK AS THE FOUNDING OF THE LEAGUE OF NATIONS\n\n IN EACH CASE I USE RIGOROUS ANALYSIS ON ORIGINAL DATA TO EXPLAIN THE WHY, WHEN, AND HOW OF THEIR DECISIONS ON THE BOMB, AS WELL AS OF THEIR DECISIONS ON RELATED ISSUES SUCH AS WHETHER TO BUILD UP NUCLEAR TECHNOLOGY, TO SEEK NUCLEAR SECURITY GUARANTEES, AND TO SIGN INTERNATIONAL NUCLEAR ARMS CONTROL AGREEMENTS.\n         THE OVERALL APPROACH INTRODUCED HERE HAS WIDE POTENTIAL APPLICABILITY\n\n\n\n\nfrom urllib.request import urlopen\nfrom random import randint\n\ndef wordListSum(wordList):\n    sum = 0\n    for word, value in wordList.items():\n        sum += value\n    return sum\n\ndef retrieveRandomWord(wordList):\n    randIndex = randint(1, wordListSum(wordList))\n    for word, value in wordList.items():\n        randIndex -= value\n        if randIndex <= 0:\n            return word\n\ndef buildWordDict(text):\n    # Remove newlines and quotes\n    text = text.replace('\\n', ' ');\n    text = text.replace('\"', '');\n\n    # Make sure punctuation marks are treated as their own \"words,\"\n    # so that they will be included in the Markov chain\n    punctuation = [',','.',';',':']\n    for symbol in punctuation:\n        text = text.replace(symbol, ' {} '.format(symbol));\n\n    words = text.split(' ')\n    # Filter out empty words\n    words = [word for word in words if word != '']\n\n    wordDict = {}\n    for i in range(1, len(words)):\n        if words[i-1] not in wordDict:\n                # Create a new dictionary for this word\n            wordDict[words[i-1]] = {}\n        if words[i] not in wordDict[words[i-1]]:\n            wordDict[words[i-1]][words[i]] = 0\n        wordDict[words[i-1]][words[i]] += 1\n    return wordDict\n\nwordDict = buildWordDict(text)\n\n#Generate a Markov chain of length 100\nlength = 100\nchain = ['I']\nfor i in range(0, length):\n    newWord = retrieveRandomWord(wordDict[chain[-1]])\n    chain.append(newWord)\n\n#print(' '.join(chain))\n\n\nimport re\n\ndef getNgrams(content, n):\n    content = re.sub('\\n|[[\\d+\\]]', ' ', content)\n    content = bytes(content, 'UTF-8')\n    content = content.decode('ascii', 'ignore')\n    content = content.split(' ')\n    content = [word for word in content if word != '']\n    output = []\n    for i in range(len(content)-n+1):\n        output.append(content[i:i+n])\n    return output\n\n\nfrom collections import Counter\n\ndef getNgrams(content, n):\n    content = cleanInput(content)\n    ngrams = Counter()\n    ngrams_list = []\n    for sentence in content:\n        newNgrams = [' '.join(ngram) for ngram in getNgramsFromSentence(sentence, n)]\n        ngrams_list.extend(newNgrams)\n        ngrams.update(newNgrams)\n    return(ngrams)\n\n\n#print(getNgrams(content, 2))\n\n\ndef isCommon(ngram):\n    commonWords = ['THE', 'BE', 'AND', 'OF', 'A', 'IN', 'TO', 'HAVE', 'IT', 'I', 'THAT', 'FOR', 'YOU', 'HE', 'WITH', 'ON', 'DO', 'SAY', 'THIS', 'THEY', 'IS', 'AN', 'AT', 'BUT', 'WE', 'HIS', 'FROM', 'THAT', 'NOT', 'BY', 'SHE', 'OR', 'AS', 'WHAT', 'GO', 'THEIR', 'CAN', 'WHO', 'GET', 'IF', 'WOULD', 'HER', 'ALL', 'MY', 'MAKE', 'ABOUT', 'KNOW', 'WILL', 'AS', 'UP', 'ONE', 'TIME', 'HAS', 'BEEN', 'THERE', 'YEAR', 'SO', 'THINK', 'WHEN', 'WHICH', 'THEM', 'SOME', 'ME', 'PEOPLE', 'TAKE', 'OUT', 'INTO', 'JUST', 'SEE', 'HIM', 'YOUR', 'COME', 'COULD', 'NOW', 'THAN', 'LIKE', 'OTHER', 'HOW', 'THEN', 'ITS', 'OUR', 'TWO', 'MORE', 'THESE', 'WANT', 'WAY', 'LOOK', 'FIRST', 'ALSO', 'NEW', 'BECAUSE', 'DAY', 'MORE', 'USE', 'NO', 'MAN', 'FIND', 'HERE', 'THING', 'GIVE', 'MANY', 'WELL']\n    for word in ngram:\n        if word in commonWords:\n            return True\n    return False\n\ndef getNgramsFromSentence(text, n):\n    output = []\n    for i in range(len(text)-n+1):\n        if not isCommon(text[i:i+n]):\n            output.append(text[i:i+n])\n    return output\n\nngrams = getNgrams(text, 3)\n#print(ngrams)"
  },
  {
    "objectID": "posts/2020-08-18-pyspark-nas.html",
    "href": "posts/2020-08-18-pyspark-nas.html",
    "title": "Handling Missing Data with Pyspark",
    "section": "",
    "text": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import countDistinct, avg,stddev\n\n# Load data from a CSV\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").option(\"inferSchema\", True).option(\"header\", True).load(file_location)\ndisplay(df.take(5))\n\n\n_c0provincespecificgeneralyeargdpfdirnrrrifrregit0Anhui147002.0null19962093.3506610.00.00.01128873East China6319301Anhui151981.0null19972347.32434430.00.00.01356287East China6578602Anhui174930.0null19982542.96276730.00.00.01518236East China8894633Anhui285324.0null19992712.3426131nullnullnull1646891East China12273644Anhui195580.032100.020002902.09318470.00.00.01601508East China1499110"
  },
  {
    "objectID": "posts/2020-08-18-pyspark-nas.html#dropping-columns-without-non-null-values",
    "href": "posts/2020-08-18-pyspark-nas.html#dropping-columns-without-non-null-values",
    "title": "Handling Missing Data with Pyspark",
    "section": "Dropping Columns without non-null values",
    "text": "Dropping Columns without non-null values\n\n# Has to have at least 2 NON-null values\ndf.na.drop(thresh=2).show()\n\n\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000| 0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525| 0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-18-pyspark-nas.html#dropping-any-row-that-contains-missing-data",
    "href": "posts/2020-08-18-pyspark-nas.html#dropping-any-row-that-contains-missing-data",
    "title": "Handling Missing Data with Pyspark",
    "section": "Dropping any row that contains missing data",
    "text": "Dropping any row that contains missing data\n\ndf.na.drop().show()\n\n\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\n_c0| province| specific|  general|year|    gdp|   fdi|                rnr|         rr|          i|      fr|            reg|     it|\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\n  4|    Anhui| 195580.0|  32100.0|2000|2902.09| 31847|                0.0|        0.0|        0.0| 1601508|     East China|1499110|\n  6|    Anhui| 434149.0|  66529.0|2002|3519.72| 38375|                0.0|        0.0|        0.0| 1677840|     East China|2404936|\n  7|    Anhui| 619201.0|  52108.0|2003|3923.11| 36720|                0.0|        0.0|        0.0| 1896479|     East China|2815820|\n 10|    Anhui|1457872.0| 279052.0|2006| 6112.5|139354|                0.0|        0.0|0.324324324| 3434548|     East China|5167300|\n 11|    Anhui|2213991.0| 178705.0|2007|7360.92|299892|                0.0|        0.0|0.324324324| 4468640|     East China|7040099|\n 16|  Beijing| 281769.0| 188633.0|2000|3161.66|168368|                0.0|        0.0|       0.53| 1667114|    North China| 757990|\n 18|  Beijing| 558569.0| 280277.0|2002| 4315.0|172464|                0.0|        0.0|       0.53| 2511249|    North China|1078754|\n 19|  Beijing| 642581.0| 269596.0|2003|5007.21|219126|                0.0|0.794871795|        0.0| 2823366|    North China|1426600|\n 22|  Beijing|1315102.0| 405966.0|2006|8117.78|455191|                0.0|0.794871795|        0.0| 4830392|    North China|1782317|\n 23|  Beijing| 752279.0|1023453.0|2007|9846.81|506572|                0.0|0.794871795|        0.0|14926380|    North China|1962192|\n 34|Chongqing| 833423.0| 150000.0|2006|3907.23| 69595|0.09677419400000001|        0.0|        0.0| 1762409|Southwest China|3124234|\n 35|Chongqing|1791403.0| 447900.0|2007|4676.13|108534|0.09677419400000001|        0.0|        0.0| 4427000|Southwest China|3923569|\n 40|   Fujian| 142650.0|  71807.0|2000|3764.54|343191|                0.0|        0.0|        0.0| 2110577|     East China| 819028|\n 42|   Fujian| 137190.0|  59263.0|2002|4467.55|383837|               0.22|        0.3|        0.0| 2373047|     East China|1184990|\n 43|   Fujian| 148812.0|  68142.0|2003|4983.67|259903|                0.0|        0.0|        0.3| 2648861|     East China|1364980|\n 46|   Fujian| 397517.0| 149549.0|2006|7583.85|322047|                0.4|        0.0|        0.0| 4830320|     East China|2135224|\n 47|   Fujian| 753552.0| 317700.0|2007|9248.53|406058|                0.4|        0.0|        0.0| 6994577|     East China|2649011|\n 52|    Gansu| 223984.0|  58533.0|2000|1052.88|  6235|                0.0|0.153846154|        0.0|  505196|Northwest China|1258100|\n 54|    Gansu| 337894.0| 129791.0|2002|1232.03|  6121|                0.0|       0.13|        0.0|  597159|Northwest China|1898911|\n 58|    Gansu| 833430.0| 516342.0|2006|2277.35|  2954|                0.0|        0.0|0.128205128|  924080|Northwest China|3847158|\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.na.drop(subset=[\"general\"]).show()\n\n\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\n_c0| province| specific|  general|year|    gdp|   fdi|                rnr|         rr|          i|      fr|            reg|     it|\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\n  4|    Anhui| 195580.0|  32100.0|2000|2902.09| 31847|                0.0|        0.0|        0.0| 1601508|     East China|1499110|\n  6|    Anhui| 434149.0|  66529.0|2002|3519.72| 38375|                0.0|        0.0|        0.0| 1677840|     East China|2404936|\n  7|    Anhui| 619201.0|  52108.0|2003|3923.11| 36720|                0.0|        0.0|        0.0| 1896479|     East China|2815820|\n  8|    Anhui| 898441.0| 349699.0|2004| 4759.3| 54669|                0.0|        0.0|        0.0|    null|     East China|3422176|\n 10|    Anhui|1457872.0| 279052.0|2006| 6112.5|139354|                0.0|        0.0|0.324324324| 3434548|     East China|5167300|\n 11|    Anhui|2213991.0| 178705.0|2007|7360.92|299892|                0.0|        0.0|0.324324324| 4468640|     East China|7040099|\n 16|  Beijing| 281769.0| 188633.0|2000|3161.66|168368|                0.0|        0.0|       0.53| 1667114|    North China| 757990|\n 18|  Beijing| 558569.0| 280277.0|2002| 4315.0|172464|                0.0|        0.0|       0.53| 2511249|    North China|1078754|\n 19|  Beijing| 642581.0| 269596.0|2003|5007.21|219126|                0.0|0.794871795|        0.0| 2823366|    North China|1426600|\n 20|  Beijing|1009936.0| 309025.0|2004|6033.21|308354|                0.0|0.794871795|        0.0|    null|    North China|1644601|\n 22|  Beijing|1315102.0| 405966.0|2006|8117.78|455191|                0.0|0.794871795|        0.0| 4830392|    North China|1782317|\n 23|  Beijing| 752279.0|1023453.0|2007|9846.81|506572|                0.0|0.794871795|        0.0|14926380|    North China|1962192|\n 30|Chongqing| 311770.0|  41907.0|2002|2232.86| 19576|               null|       null|       null|  762806|Southwest China|1906968|\n 31|Chongqing| 335715.0|  18700.0|2003|2555.72| 26083|               null|       null|       null|  929935|Southwest China|1778125|\n 32|Chongqing| 568835.0|  97500.0|2004|3034.58| 40508|               null|       null|       null|    null|Southwest China|2197948|\n 34|Chongqing| 833423.0| 150000.0|2006|3907.23| 69595|0.09677419400000001|        0.0|        0.0| 1762409|Southwest China|3124234|\n 35|Chongqing|1791403.0| 447900.0|2007|4676.13|108534|0.09677419400000001|        0.0|        0.0| 4427000|Southwest China|3923569|\n 40|   Fujian| 142650.0|  71807.0|2000|3764.54|343191|                0.0|        0.0|        0.0| 2110577|     East China| 819028|\n 42|   Fujian| 137190.0|  59263.0|2002|4467.55|383837|               0.22|        0.3|        0.0| 2373047|     East China|1184990|\n 43|   Fujian| 148812.0|  68142.0|2003|4983.67|259903|                0.0|        0.0|        0.3| 2648861|     East China|1364980|\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.na.drop(how='any').show()\n\n\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\n_c0| province| specific|  general|year|    gdp|   fdi|                rnr|         rr|          i|      fr|            reg|     it|\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\n  4|    Anhui| 195580.0|  32100.0|2000|2902.09| 31847|                0.0|        0.0|        0.0| 1601508|     East China|1499110|\n  6|    Anhui| 434149.0|  66529.0|2002|3519.72| 38375|                0.0|        0.0|        0.0| 1677840|     East China|2404936|\n  7|    Anhui| 619201.0|  52108.0|2003|3923.11| 36720|                0.0|        0.0|        0.0| 1896479|     East China|2815820|\n 10|    Anhui|1457872.0| 279052.0|2006| 6112.5|139354|                0.0|        0.0|0.324324324| 3434548|     East China|5167300|\n 11|    Anhui|2213991.0| 178705.0|2007|7360.92|299892|                0.0|        0.0|0.324324324| 4468640|     East China|7040099|\n 16|  Beijing| 281769.0| 188633.0|2000|3161.66|168368|                0.0|        0.0|       0.53| 1667114|    North China| 757990|\n 18|  Beijing| 558569.0| 280277.0|2002| 4315.0|172464|                0.0|        0.0|       0.53| 2511249|    North China|1078754|\n 19|  Beijing| 642581.0| 269596.0|2003|5007.21|219126|                0.0|0.794871795|        0.0| 2823366|    North China|1426600|\n 22|  Beijing|1315102.0| 405966.0|2006|8117.78|455191|                0.0|0.794871795|        0.0| 4830392|    North China|1782317|\n 23|  Beijing| 752279.0|1023453.0|2007|9846.81|506572|                0.0|0.794871795|        0.0|14926380|    North China|1962192|\n 34|Chongqing| 833423.0| 150000.0|2006|3907.23| 69595|0.09677419400000001|        0.0|        0.0| 1762409|Southwest China|3124234|\n 35|Chongqing|1791403.0| 447900.0|2007|4676.13|108534|0.09677419400000001|        0.0|        0.0| 4427000|Southwest China|3923569|\n 40|   Fujian| 142650.0|  71807.0|2000|3764.54|343191|                0.0|        0.0|        0.0| 2110577|     East China| 819028|\n 42|   Fujian| 137190.0|  59263.0|2002|4467.55|383837|               0.22|        0.3|        0.0| 2373047|     East China|1184990|\n 43|   Fujian| 148812.0|  68142.0|2003|4983.67|259903|                0.0|        0.0|        0.3| 2648861|     East China|1364980|\n 46|   Fujian| 397517.0| 149549.0|2006|7583.85|322047|                0.4|        0.0|        0.0| 4830320|     East China|2135224|\n 47|   Fujian| 753552.0| 317700.0|2007|9248.53|406058|                0.4|        0.0|        0.0| 6994577|     East China|2649011|\n 52|    Gansu| 223984.0|  58533.0|2000|1052.88|  6235|                0.0|0.153846154|        0.0|  505196|Northwest China|1258100|\n 54|    Gansu| 337894.0| 129791.0|2002|1232.03|  6121|                0.0|       0.13|        0.0|  597159|Northwest China|1898911|\n 58|    Gansu| 833430.0| 516342.0|2006|2277.35|  2954|                0.0|        0.0|0.128205128|  924080|Northwest China|3847158|\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.na.drop(how='all').show()\n\n\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000| 0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525| 0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-18-pyspark-nas.html#imputation-of-null-values",
    "href": "posts/2020-08-18-pyspark-nas.html#imputation-of-null-values",
    "title": "Handling Missing Data with Pyspark",
    "section": "Imputation of Null Values",
    "text": "Imputation of Null Values\n\ndf.na.fill('example').show()\n\n\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|example| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000| 0.0|        0.0|0.324324324|example| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525| 0.0|        0.0|       0.53|example|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\nImputation of 0\n\ndf.na.fill(0).show()\n\n\n+---+--------+---------+--------+----+-------+------+---+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi|rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+---+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|     0.0|1996| 2093.3| 50661|0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|     0.0|1997|2347.32| 43443|0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|     0.0|1998|2542.96| 27673|0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|     0.0|1999|2712.34| 26131|0.0|        0.0|        0.0|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847|0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|     0.0|2001|3246.71| 33672|0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375|0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720|0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669|0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|     0.0|2005|5350.17| 69000|0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354|0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892|0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|     0.0|1996| 1789.2|155290|0.0|        0.0|        0.0| 634562|North China| 508135|\n 13| Beijing| 165957.0|     0.0|1997|2077.09|159286|0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|     0.0|1998|2377.18|216800|0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|     0.0|1999|2678.82|197525|0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368|0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|     0.0|2001|3707.96|176818|0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464|0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126|0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+---+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.na.fill('example',subset=['fr']).show()\n\n\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|example| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000| 0.0|        0.0|0.324324324|example| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525| 0.0|        0.0|       0.53|example|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.na.fill(0,subset=['general']).show()\n\n\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|     0.0|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|     0.0|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|     0.0|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|     0.0|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|     0.0|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|     0.0|2005|5350.17| 69000| 0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|     0.0|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|     0.0|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|     0.0|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|     0.0|1999|2678.82|197525| 0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|     0.0|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\n\nImputation of the Mean\n\nfrom pyspark.sql.functions import mean\nmean_val = df.select(mean(df['general'])).collect()\n\n\n\n\n\n\nmean_val[0][0]\n\n\nOut[19]: 309127.53846153844\n\n\n\nmean_gen = mean_val[0][0]\n\n\n\n\n\n\ndf.na.fill(mean_gen,[\"general\"]).show()\n\n\n+---+--------+---------+------------------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific|           general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+------------------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|309127.53846153844|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|309127.53846153844|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|309127.53846153844|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|309127.53846153844|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0|           32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|309127.53846153844|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0|           66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0|           52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|          349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|309127.53846153844|2005|5350.17| 69000| 0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|          279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|          178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|309127.53846153844|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|309127.53846153844|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|309127.53846153844|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|309127.53846153844|1999|2678.82|197525| 0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|          188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|309127.53846153844|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|          280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|          269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+------------------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.na.fill(df.select(mean(df['general'])).collect()[0][0],['general']).show()\n\n\n+---+--------+---------+------------------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific|           general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+------------------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|309127.53846153844|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|309127.53846153844|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|309127.53846153844|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|309127.53846153844|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0|           32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|309127.53846153844|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0|           66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0|           52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|          349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|309127.53846153844|2005|5350.17| 69000| 0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|          279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|          178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|309127.53846153844|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|309127.53846153844|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|309127.53846153844|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|309127.53846153844|1999|2678.82|197525| 0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|          188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|309127.53846153844|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|          280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|          269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+------------------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2021-01-02-snp-sma-ewma.html",
    "href": "posts/2021-01-02-snp-sma-ewma.html",
    "title": "Stock Market Analysis of the S&P 500 Index using Simple Moving Averages and Exponentially-weighted moving averages",
    "section": "",
    "text": "This post includes code and notes from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks."
  },
  {
    "objectID": "posts/2021-01-02-snp-sma-ewma.html#simple-moving-averages",
    "href": "posts/2021-01-02-snp-sma-ewma.html#simple-moving-averages",
    "title": "Stock Market Analysis of the S&P 500 Index using Simple Moving Averages and Exponentially-weighted moving averages",
    "section": "Simple Moving Averages",
    "text": "Simple Moving Averages\n\nSP500['6-month-SMA']=SP500['Value'].rolling(window=6).mean()\nSP500['12-month-SMA']=SP500['Value'].rolling(window=12).mean()\n\n\nSP500.head()\n\n\n\n\n\n  \n    \n      \n      Value\n      6-month-SMA\n      12-month-SMA\n    \n    \n      Date\n      \n      \n      \n    \n  \n  \n    \n      2019-01-01\n      2607.39\n      NaN\n      NaN\n    \n    \n      2019-02-01\n      2754.86\n      NaN\n      NaN\n    \n    \n      2019-03-01\n      2803.98\n      NaN\n      NaN\n    \n    \n      2019-04-01\n      2903.80\n      NaN\n      NaN\n    \n    \n      2019-05-01\n      2854.71\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nSP500.plot()\n\n<AxesSubplot:xlabel='Date'>"
  },
  {
    "objectID": "posts/2021-01-02-snp-sma-ewma.html#exponentially-weighted-moving-averages",
    "href": "posts/2021-01-02-snp-sma-ewma.html#exponentially-weighted-moving-averages",
    "title": "Stock Market Analysis of the S&P 500 Index using Simple Moving Averages and Exponentially-weighted moving averages",
    "section": "Exponentially-weighted moving averages",
    "text": "Exponentially-weighted moving averages\n\nSP500['EWMA12'] = SP500['Value'].ewm(span=12).mean()\n\n\nSP500[['Value','EWMA12']].plot()\n\n<AxesSubplot:xlabel='Date'>"
  },
  {
    "objectID": "posts/2020-08-29-clustering with pyspark.html",
    "href": "posts/2020-08-29-clustering with pyspark.html",
    "title": "Clustering with Pyspark",
    "section": "",
    "text": "This post includes code from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-08-29-clustering with pyspark.html#using-the-standardscaler",
    "href": "posts/2020-08-29-clustering with pyspark.html#using-the-standardscaler",
    "title": "Clustering with Pyspark",
    "section": "Using the StandardScaler",
    "text": "Using the StandardScaler\n\nfrom pyspark.ml.feature import StandardScaler\n\n\n\n\n\n\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)"
  },
  {
    "objectID": "posts/2020-08-29-clustering with pyspark.html#fitting-the-standardscaler",
    "href": "posts/2020-08-29-clustering with pyspark.html#fitting-the-standardscaler",
    "title": "Clustering with Pyspark",
    "section": "Fitting the StandardScaler",
    "text": "Fitting the StandardScaler\n\n# Compute summary statistics by fitting the StandardScaler\nscalerModel = scaler.fit(final_df)\n\n\n\n\n\n\n# Normalize each feature to have unit standard deviation.\ncluster_final_data = scalerModel.transform(final_df)\n\n\n\n\n\n\nkmeans3 = KMeans(featuresCol='scaledFeatures',k=3)\nkmeans2 = KMeans(featuresCol='scaledFeatures',k=2)\n\n\n\n\n\n\nmodel_k3 = kmeans3.fit(cluster_final_data)\nmodel_k2 = kmeans2.fit(cluster_final_data)\n\n\n\n\n\n\nmodel_k3.transform(cluster_final_data).groupBy('prediction').count().show()\n\n\n+----------+-----+\nprediction|count|\n+----------+-----+\n         1|   15|\n         2|   86|\n         0|  259|\n+----------+-----+\n\n\n\n\n\nmodel_k2.transform(cluster_final_data).groupBy('prediction').count().show()\n\n\n+----------+-----+\nprediction|count|\n+----------+-----+\n         1|  308|\n         0|   52|\n+----------+-----+"
  },
  {
    "objectID": "posts/2020-09-30-stockmarketportfolioanaylsis_snp.html",
    "href": "posts/2020-09-30-stockmarketportfolioanaylsis_snp.html",
    "title": "Stock Market and Portfolio Anaylsis of the S&P 500 with pandas and quandl",
    "section": "",
    "text": "import pandas as pd\nimport quandl\n\n\nstart = pd.to_datetime('2010-01-01')\nend = pd.to_datetime('today')\n\n\nSP500 = quandl.get(\"MULTPL/SP500_REAL_PRICE_MONTH\",start_date = start,end_date = end)\n\n\nSP500.head()\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-01-01\n      1123.58\n    \n    \n      2010-02-01\n      1089.16\n    \n    \n      2010-03-01\n      1152.05\n    \n    \n      2010-04-01\n      1197.32\n    \n    \n      2010-05-01\n      1125.06\n    \n  \n\n\n\n\n\nSP500.tail()\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2020-07-01\n      3207.62\n    \n    \n      2020-07-31\n      3271.12\n    \n    \n      2020-08-01\n      3391.71\n    \n    \n      2020-08-31\n      3500.31\n    \n    \n      2020-09-01\n      3526.65\n    \n  \n\n\n\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nSP500['Value'].plot(figsize = (12, 8))\nplt.title('Total S&P 500 from 2010 to 2020 Value')\n\nText(0.5, 1.0, 'Total S&P 500 from 2010 to 2020 Value')"
  },
  {
    "objectID": "posts/2020-10-01-stockmarketportfolioanaylsis_snp+.html",
    "href": "posts/2020-10-01-stockmarketportfolioanaylsis_snp+.html",
    "title": "Stock Market and Portfolio Anaylsis of the S&P 500 various metrics with pandas and quandl",
    "section": "",
    "text": "import pandas as pd\nimport quandl\n\n\nstart = pd.to_datetime('2010-01-01')\nend = pd.to_datetime('today')\n\n\nSP500 = quandl.get(\"MULTPL/SP500_REAL_PRICE_MONTH\",start_date = start,end_date = end)\n\n\nSP500 = quandl.get(\"MULTPL/SP500_DIV_YIELD_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-01-31\n      1.98\n    \n    \n      2010-02-28\n      2.03\n    \n    \n      2010-03-31\n      1.90\n    \n    \n      2010-04-30\n      1.83\n    \n    \n      2010-05-31\n      1.95\n    \n    \n      ...\n      ...\n    \n    \n      2020-07-01\n      1.91\n    \n    \n      2020-07-31\n      1.86\n    \n    \n      2020-08-31\n      1.76\n    \n    \n      2020-09-01\n      1.69\n    \n    \n      2020-09-30\n      1.69\n    \n  \n\n136 rows  1 columns\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_PE_RATIO_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-01-01\n      20.70\n    \n    \n      2010-02-01\n      18.91\n    \n    \n      2010-03-01\n      18.91\n    \n    \n      2010-04-01\n      19.01\n    \n    \n      2010-05-01\n      17.30\n    \n    \n      ...\n      ...\n    \n    \n      2020-07-01\n      27.57\n    \n    \n      2020-07-31\n      28.12\n    \n    \n      2020-08-01\n      29.16\n    \n    \n      2020-08-31\n      30.09\n    \n    \n      2020-09-01\n      30.32\n    \n  \n\n138 rows  1 columns\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_PE_RATIO_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-01-01\n      20.70\n    \n    \n      2010-02-01\n      18.91\n    \n    \n      2010-03-01\n      18.91\n    \n    \n      2010-04-01\n      19.01\n    \n    \n      2010-05-01\n      17.30\n    \n    \n      ...\n      ...\n    \n    \n      2020-07-01\n      27.57\n    \n    \n      2020-07-31\n      28.12\n    \n    \n      2020-08-01\n      29.16\n    \n    \n      2020-08-31\n      30.09\n    \n    \n      2020-09-01\n      30.32\n    \n  \n\n138 rows  1 columns\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_EARNINGS_YIELD_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-01-01\n      4.83\n    \n    \n      2010-02-01\n      5.29\n    \n    \n      2010-03-01\n      5.29\n    \n    \n      2010-04-01\n      5.26\n    \n    \n      2010-05-01\n      5.78\n    \n    \n      ...\n      ...\n    \n    \n      2020-07-01\n      3.63\n    \n    \n      2020-07-31\n      3.56\n    \n    \n      2020-08-01\n      3.43\n    \n    \n      2020-08-31\n      3.32\n    \n    \n      2020-09-01\n      3.30\n    \n  \n\n138 rows  1 columns\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_INFLADJ_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-01-01\n      1343.51\n    \n    \n      2010-02-01\n      1302.03\n    \n    \n      2010-03-01\n      1371.58\n    \n    \n      2010-04-01\n      1423.00\n    \n    \n      2010-05-01\n      1336.08\n    \n    \n      ...\n      ...\n    \n    \n      2020-07-01\n      3207.62\n    \n    \n      2020-07-31\n      3271.12\n    \n    \n      2020-08-01\n      3391.71\n    \n    \n      2020-08-31\n      3500.31\n    \n    \n      2020-09-01\n      3526.65\n    \n  \n\n138 rows  1 columns\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_DIV_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-01-31\n      26.59\n    \n    \n      2010-02-28\n      26.38\n    \n    \n      2010-03-31\n      26.08\n    \n    \n      2010-04-30\n      26.09\n    \n    \n      2010-05-31\n      26.12\n    \n    \n      ...\n      ...\n    \n    \n      2020-02-29\n      59.23\n    \n    \n      2020-03-31\n      59.81\n    \n    \n      2020-04-30\n      60.25\n    \n    \n      2020-05-31\n      60.28\n    \n    \n      2020-06-30\n      59.99\n    \n  \n\n126 rows  1 columns\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_DIV_YEAR\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-12-31\n      26.87\n    \n    \n      2011-12-31\n      30.34\n    \n    \n      2012-12-31\n      35.26\n    \n    \n      2013-12-31\n      38.90\n    \n    \n      2014-12-31\n      43.52\n    \n    \n      2015-12-31\n      47.53\n    \n    \n      2016-12-31\n      49.05\n    \n    \n      2017-12-31\n      51.43\n    \n    \n      2018-12-31\n      55.43\n    \n    \n      2019-03-31\n      55.35\n    \n    \n      2019-06-30\n      56.17\n    \n    \n      2019-09-30\n      57.32\n    \n    \n      2019-12-31\n      58.72\n    \n    \n      2020-03-31\n      59.18\n    \n    \n      2020-06-30\n      59.99\n    \n  \n\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_DIV_GROWTH_YEAR\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-12-31\n      1.45\n    \n    \n      2011-12-31\n      16.26\n    \n    \n      2012-12-31\n      18.25\n    \n    \n      2013-12-31\n      11.99\n    \n    \n      2014-12-31\n      12.72\n    \n    \n      2015-12-31\n      10.00\n    \n    \n      2016-12-31\n      5.33\n    \n    \n      2017-12-31\n      7.07\n    \n    \n      2018-12-31\n      9.84\n    \n    \n      2019-03-31\n      9.87\n    \n    \n      2019-06-30\n      9.98\n    \n    \n      2019-09-30\n      9.32\n    \n    \n      2019-12-31\n      8.36\n    \n    \n      2020-03-31\n      8.45\n    \n    \n      2020-06-30\n      6.43\n    \n  \n\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_DIV_GROWTH_QUARTER\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-03-31\n      -19.63\n    \n    \n      2010-06-30\n      -13.90\n    \n    \n      2010-09-30\n      -6.48\n    \n    \n      2010-12-31\n      1.45\n    \n    \n      2011-03-31\n      6.97\n    \n    \n      2011-06-30\n      10.46\n    \n    \n      2011-09-30\n      12.65\n    \n    \n      2011-12-31\n      16.26\n    \n    \n      2012-03-31\n      16.74\n    \n    \n      2012-06-30\n      16.35\n    \n    \n      2012-09-30\n      17.51\n    \n    \n      2012-12-31\n      18.25\n    \n    \n      2013-03-31\n      17.40\n    \n    \n      2013-06-30\n      17.47\n    \n    \n      2013-09-30\n      16.27\n    \n    \n      2013-12-31\n      11.99\n    \n    \n      2014-03-31\n      12.82\n    \n    \n      2014-06-30\n      12.37\n    \n    \n      2014-09-30\n      11.89\n    \n    \n      2014-12-31\n      12.72\n    \n    \n      2015-03-31\n      12.64\n    \n    \n      2015-06-30\n      11.67\n    \n    \n      2015-09-30\n      10.43\n    \n    \n      2015-12-31\n      10.00\n    \n    \n      2016-03-31\n      7.52\n    \n    \n      2016-06-30\n      6.51\n    \n    \n      2016-09-30\n      5.92\n    \n    \n      2016-12-31\n      5.33\n    \n    \n      2017-03-31\n      5.71\n    \n    \n      2017-06-30\n      6.21\n    \n    \n      2017-09-30\n      6.99\n    \n    \n      2017-12-31\n      7.07\n    \n    \n      2018-03-31\n      7.81\n    \n    \n      2018-06-30\n      7.99\n    \n    \n      2018-09-30\n      8.65\n    \n    \n      2018-12-31\n      9.84\n    \n    \n      2019-03-31\n      9.87\n    \n    \n      2019-06-30\n      9.98\n    \n    \n      2019-09-30\n      9.32\n    \n    \n      2019-12-31\n      8.36\n    \n    \n      2020-03-31\n      8.45\n    \n    \n      2020-06-30\n      6.43\n    \n  \n\n\n\n\n\nSP500.head()\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-01-01\n      1123.58\n    \n    \n      2010-02-01\n      1089.16\n    \n    \n      2010-03-01\n      1152.05\n    \n    \n      2010-04-01\n      1197.32\n    \n    \n      2010-05-01\n      1125.06\n    \n  \n\n\n\n\n\nSP500.tail()\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2020-07-01\n      3207.62\n    \n    \n      2020-07-31\n      3271.12\n    \n    \n      2020-08-01\n      3391.71\n    \n    \n      2020-08-31\n      3500.31\n    \n    \n      2020-09-01\n      3526.65\n    \n  \n\n\n\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nSP500['Value'].plot(figsize = (12, 8))\nplt.title('Total S&P 500 from 2010 to 2020 Value')\n\nText(0.5, 1.0, 'Total S&P 500 from 2010 to 2020 Value')"
  },
  {
    "objectID": "posts/2021-01-18-datasette-csv-reading.html",
    "href": "posts/2021-01-18-datasette-csv-reading.html",
    "title": "Reading CSV from Datasette SQL Database",
    "section": "",
    "text": "df = pd.read_csv('https://stocks-snp-500.herokuapp.com/stocks/index_stock.csv?_size=max')\n\nHTTPError: HTTP Error 404: Not Found\n\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      rowid\n      Date\n      MSFT_stock\n      ZOOM_stock\n      SNOW_stock\n      FXAIX_stock\n    \n  \n  \n    \n      0\n      1\n      2020-01-02 00:00:00.000000\n      158.779999\n      68.800003\n      NaN\n      112.980003\n    \n    \n      1\n      2\n      2020-01-03 00:00:00.000000\n      158.320007\n      67.620003\n      NaN\n      112.190002\n    \n    \n      2\n      3\n      2020-01-06 00:00:00.000000\n      157.080002\n      66.629997\n      NaN\n      112.589996\n    \n    \n      3\n      4\n      2020-01-07 00:00:00.000000\n      159.320007\n      70.290001\n      NaN\n      112.290001\n    \n    \n      4\n      5\n      2020-01-08 00:00:00.000000\n      158.929993\n      71.809998\n      NaN\n      112.839996\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      258\n      259\n      2021-01-11 00:00:00.000000\n      218.470001\n      344.980011\n      295.000000\n      131.750000\n    \n    \n      259\n      260\n      2021-01-12 00:00:00.000000\n      216.500000\n      333.200012\n      298.000000\n      131.800003\n    \n    \n      260\n      261\n      2021-01-13 00:00:00.000000\n      214.020004\n      360.000000\n      295.000000\n      132.100006\n    \n    \n      261\n      262\n      2021-01-14 00:00:00.000000\n      215.910004\n      371.000000\n      305.000000\n      131.619995\n    \n    \n      262\n      263\n      2021-01-15 00:00:00.000000\n      213.520004\n      397.709991\n      306.820007\n      130.679993\n    \n  \n\n263 rows  6 columns\n\n\n\n\ndf['FXAIX_stock'].plot()\n\n<AxesSubplot:>"
  },
  {
    "objectID": "posts/2021-02-07-portfolio-optimization.html",
    "href": "posts/2021-02-07-portfolio-optimization.html",
    "title": "Portfolio Optimization",
    "section": "",
    "text": "This post includes code and notes from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks."
  },
  {
    "objectID": "posts/2021-02-07-portfolio-optimization.html#log-returns-vs-arithmetic-returns",
    "href": "posts/2021-02-07-portfolio-optimization.html#log-returns-vs-arithmetic-returns",
    "title": "Portfolio Optimization",
    "section": "Log Returns vs Arithmetic Returns",
    "text": "Log Returns vs Arithmetic Returns\nWe will now switch over to using log returns instead of arithmetic returns, for many of our use cases they are almost the same,but most technical analyses require detrending/normalizing the time series and using log returns is a nice way to do that. Log returns are convenient to work with in many of the algorithms we will encounter.\nFor a full analysis of why we use log returns, check this great article.\n\nlog_ret = np.log(stocks/stocks.shift(1))\nlog_ret.head()\n\n\n\n\n\n  \n    \n      \n      FXAIX_stock\n      VRTTX_stock\n      FNCMX_stock\n      FSMAX_stock\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2018-01-02\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2018-01-03\n      0.006347\n      0.005843\n      0.008336\n      0.003353\n    \n    \n      2018-01-04\n      0.004209\n      0.003780\n      0.001831\n      0.001752\n    \n    \n      2018-01-05\n      0.007011\n      0.006406\n      0.008252\n      0.003811\n    \n    \n      2018-01-08\n      0.001667\n      0.001852\n      0.002877\n      0.002849\n    \n  \n\n\n\n\n\nlog_ret.hist(bins=100,figsize=(12,6));\nplt.tight_layout()\n\n\n\n\n\nlog_ret.describe().transpose()\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n  \n  \n    \n      FXAIX_stock\n      784.0\n      0.000474\n      0.014648\n      -0.127150\n      -0.003955\n      0.001085\n      0.006955\n      0.089894\n    \n    \n      VRTTX_stock\n      784.0\n      0.000517\n      0.014807\n      -0.131282\n      -0.004037\n      0.001219\n      0.007279\n      0.090501\n    \n    \n      FNCMX_stock\n      784.0\n      0.000835\n      0.016046\n      -0.131396\n      -0.004628\n      0.001768\n      0.008337\n      0.089514\n    \n    \n      FSMAX_stock\n      784.0\n      0.000537\n      0.016836\n      -0.154994\n      -0.004773\n      0.001653\n      0.008149\n      0.095016\n    \n  \n\n\n\n\n\nlog_ret.mean() * 252\n\nFXAIX_stock    0.119352\nVRTTX_stock    0.130180\nFNCMX_stock    0.210329\nFSMAX_stock    0.135276\ndtype: float64\n\n\n\n# Compute pairwise covariance of columns\nlog_ret.cov()\n\n\n\n\n\n  \n    \n      \n      FXAIX_stock\n      VRTTX_stock\n      FNCMX_stock\n      FSMAX_stock\n    \n  \n  \n    \n      FXAIX_stock\n      0.000215\n      0.000216\n      0.000223\n      0.000229\n    \n    \n      VRTTX_stock\n      0.000216\n      0.000219\n      0.000226\n      0.000236\n    \n    \n      FNCMX_stock\n      0.000223\n      0.000226\n      0.000257\n      0.000241\n    \n    \n      FSMAX_stock\n      0.000229\n      0.000236\n      0.000241\n      0.000283\n    \n  \n\n\n\n\n\nlog_ret.cov()*252 # multiply by days\n\n\n\n\n\n  \n    \n      \n      FXAIX_stock\n      VRTTX_stock\n      FNCMX_stock\n      FSMAX_stock\n    \n  \n  \n    \n      FXAIX_stock\n      0.054072\n      0.054451\n      0.056310\n      0.057654\n    \n    \n      VRTTX_stock\n      0.054451\n      0.055251\n      0.056959\n      0.059382\n    \n    \n      FNCMX_stock\n      0.056310\n      0.056959\n      0.064882\n      0.060762\n    \n    \n      FSMAX_stock\n      0.057654\n      0.059382\n      0.060762\n      0.071430"
  },
  {
    "objectID": "posts/2021-02-07-portfolio-optimization.html#single-run-for-some-random-allocation",
    "href": "posts/2021-02-07-portfolio-optimization.html#single-run-for-some-random-allocation",
    "title": "Portfolio Optimization",
    "section": "Single Run for Some Random Allocation",
    "text": "Single Run for Some Random Allocation\n\n# Set seed (optional)\nnp.random.seed(101)\n\n# Stock Columns\nprint('Stocks')\nprint(stocks.columns)\nprint('\\n')\n\n# Create Random Weights\nprint('Creating Random Weights')\nweights = np.array(np.random.random(4))\nprint(weights)\nprint('\\n')\n\n# Rebalance Weights\nprint('Rebalance to sum to 1.0')\nweights = weights / np.sum(weights)\nprint(weights)\nprint('\\n')\n\n# Expected Return\nprint('Expected Portfolio Return')\nexp_ret = np.sum(log_ret.mean() * weights) *252\nprint(exp_ret)\nprint('\\n')\n\n# Expected Variance\nprint('Expected Volatility')\nexp_vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\nprint(exp_vol)\nprint('\\n')\n\n# Sharpe Ratio\nSR = exp_ret/exp_vol\nprint('Sharpe Ratio')\nprint(SR)\n\nStocks\nIndex(['FXAIX_stock', 'VRTTX_stock', 'FNCMX_stock', 'FSMAX_stock'], dtype='object')\n\n\nCreating Random Weights\n[0.51639863 0.57066759 0.02847423 0.17152166]\n\n\nRebalance to sum to 1.0\n[0.40122278 0.44338777 0.02212343 0.13326603]\n\n\nExpected Portfolio Return\n0.12828800153609307\n\n\nExpected Volatility\n0.23642865366590426\n\n\nSharpe Ratio\n0.5426076727458592\n\n\n\nnum_ports = 15000\n\nall_weights = np.zeros((num_ports,len(stocks.columns)))\nret_arr = np.zeros(num_ports)\nvol_arr = np.zeros(num_ports)\nsharpe_arr = np.zeros(num_ports)\n\nfor ind in range(num_ports):\n\n    # Create Random Weights\n    weights = np.array(np.random.random(4))\n\n    # Rebalance Weights\n    weights = weights / np.sum(weights)\n    \n    # Save Weights\n    all_weights[ind,:] = weights\n\n    # Expected Return\n    ret_arr[ind] = np.sum((log_ret.mean() * weights) *252)\n\n    # Expected Variance\n    vol_arr[ind] = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n\n    # Sharpe Ratio\n    sharpe_arr[ind] = ret_arr[ind]/vol_arr[ind]\n\n\nsharpe_arr.max()\n\n0.8049153800614341\n\n\n\nsharpe_arr.argmax()\n\n9077\n\n\n\nall_weights[1419,:]\n\narray([0.26188068, 0.20759516, 0.00110226, 0.5294219 ])\n\n\n\nmax_sr_ret = ret_arr[1419]\nmax_sr_vol = vol_arr[1419]"
  },
  {
    "objectID": "posts/2021-02-07-portfolio-optimization.html#plotting-the-data",
    "href": "posts/2021-02-07-portfolio-optimization.html#plotting-the-data",
    "title": "Portfolio Optimization",
    "section": "Plotting the data",
    "text": "Plotting the data\n\nplt.figure(figsize=(12,8))\nplt.scatter(vol_arr,ret_arr,c=sharpe_arr,cmap='plasma')\nplt.colorbar(label='Sharpe Ratio')\nplt.xlabel('Volatility')\nplt.ylabel('Return')\n\n# Add red dot for max SR\nplt.scatter(max_sr_vol,max_sr_ret,c='red',s=50,edgecolors='black')\n\n<matplotlib.collections.PathCollection at 0x7f3f7467de50>\n\n\n\n\n\n\ndef get_ret_vol_sr(weights):\n    \"\"\"\n    Takes in weights, returns array or return,volatility, sharpe ratio\n    \"\"\"\n    weights = np.array(weights)\n    ret = np.sum(log_ret.mean() * weights) * 252\n    vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n    sr = ret/vol\n    return np.array([ret,vol,sr])\n\n\nfrom scipy.optimize import minimize\n\nTo fully understand all the parameters, check out: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n\n#help(minimize)\n\nOptimization works as a minimization function, since we actually want to maximize the Sharpe Ratio, we will need to turn it negative so we can minimize the negative sharpe (same as maximizing the postive sharpe)\n\ndef neg_sharpe(weights):\n    return  get_ret_vol_sr(weights)[2] * -1\n\n\n# Contraints\ndef check_sum(weights):\n    '''\n    Returns 0 if sum of weights is 1.0\n    '''\n    return np.sum(weights) - 1\n\n\n# By convention of minimize function it should be a function that returns zero for conditions\ncons = ({'type':'eq','fun': check_sum})\n\n\n# 0-1 bounds for each weight\nbounds = ((0, 1), (0, 1), (0, 1), (0, 1))\n\n\n# Initial Guess (equal distribution)\ninit_guess = [0.25,0.25,0.25,0.25]\n\n\n# Sequential Least SQuares Programming (SLSQP).\nopt_results = minimize(neg_sharpe,init_guess,method='SLSQP',bounds=bounds,constraints=cons)\n\n\nopt_results\n\n     fun: -0.8257252387825378\n     jac: array([ 2.48069711e-01,  2.13815376e-01, -7.45058060e-09,  2.42212258e-01])\n message: 'Optimization terminated successfully'\n    nfev: 15\n     nit: 3\n    njev: 3\n  status: 0\n success: True\n       x: array([0.00000000e+00, 2.77555756e-16, 1.00000000e+00, 0.00000000e+00])\n\n\n\nopt_results.x\n\narray([0.00000000e+00, 2.77555756e-16, 1.00000000e+00, 0.00000000e+00])\n\n\n\nget_ret_vol_sr(opt_results.x)\n\narray([0.21032899, 0.25472031, 0.82572524])"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html",
    "href": "posts/2021-06-01-model-evaluation.html",
    "title": "Model Evaluation",
    "section": "",
    "text": "#code adapted from https://github.com/thomasjpfan/ml-workshop-intermediate-1-of-2"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#is-this-data-imbalanced",
    "href": "posts/2021-06-01-model-evaluation.html#is-this-data-imbalanced",
    "title": "Model Evaluation",
    "section": "Is this data imbalanced?",
    "text": "Is this data imbalanced?\n\ny.value_counts()\n\n0    28524\n1      548\nName: stroke, dtype: int64\n\n\n\ny = (y == 1).astype('int')\n\n\ny.value_counts()\n\n0    28524\n1      548\nName: stroke, dtype: int64"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#train-models-for-prediction",
    "href": "posts/2021-06-01-model-evaluation.html#train-models-for-prediction",
    "title": "Model Evaluation",
    "section": "Train models for prediction",
    "text": "Train models for prediction\n\nLinear model\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, random_state=42)\n\n\nlog_reg = Pipeline([\n    ('scaler', StandardScaler()),\n    ('log_reg', LogisticRegression(random_state=42))])\nlog_reg.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('log_reg', LogisticRegression(random_state=42))])StandardScalerStandardScaler()LogisticRegressionLogisticRegression(random_state=42)\n\n\n\ny_pred = log_reg.predict(X_test)\n\n\ny_pred\n\narray([0, 0, 0, ..., 0, 0, 0])\n\n\n\nlog_reg.score(X_test, y_test)\n\n0.9811502476609797\n\n\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7131\n           1       0.00      0.00      0.00       137\n\n    accuracy                           0.98      7268\n   macro avg       0.49      0.50      0.50      7268\nweighted avg       0.96      0.98      0.97      7268\n\n\n\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n# %load solutions/01-ex01-solutions.py\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\nrf.score(X_test, y_test)\n\ny_pred = rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7131\n           1       0.00      0.00      0.00       137\n\n    accuracy                           0.98      7268\n   macro avg       0.49      0.50      0.50      7268\nweighted avg       0.96      0.98      0.97      7268"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#thresholds",
    "href": "posts/2021-06-01-model-evaluation.html#thresholds",
    "title": "Model Evaluation",
    "section": "Thresholds",
    "text": "Thresholds"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#default",
    "href": "posts/2021-06-01-model-evaluation.html#default",
    "title": "Model Evaluation",
    "section": "Default",
    "text": "Default\n\ny_pred = log_reg.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7131\n           1       0.00      0.00      0.00       137\n\n    accuracy                           0.98      7268\n   macro avg       0.49      0.50      0.50      7268\nweighted avg       0.96      0.98      0.97      7268\n\n\n\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\nUsing probabilities\n\ny_proba = log_reg.predict_proba(X_test)\n\n\ny_proba[65:70]\n\narray([[9.99513156e-01, 4.86844076e-04],\n       [9.82052253e-01, 1.79477470e-02],\n       [9.94416208e-01, 5.58379215e-03],\n       [8.50081179e-01, 1.49918821e-01],\n       [9.95569630e-01, 4.43036992e-03]])\n\n\n\ny_pred[65:70]\n\narray([0, 0, 0, 0, 0])\n\n\n\nThreshold at 0.50\n\ny_pred_50 = y_proba[:, 1] > 0.5\nprint(classification_report(y_test, y_pred_50))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7131\n           1       0.00      0.00      0.00       137\n\n    accuracy                           0.98      7268\n   macro avg       0.49      0.50      0.50      7268\nweighted avg       0.96      0.98      0.97      7268\n\n\n\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\nThreshold at 0.25\n\ny_pred_25 = y_proba[:, 1] > 0.25\nprint(classification_report(y_test, y_pred_25))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7131\n           1       0.00      0.00      0.00       137\n\n    accuracy                           0.98      7268\n   macro avg       0.49      0.50      0.50      7268\nweighted avg       0.96      0.98      0.97      7268\n\n\n\n\n\nThreshold at 0.75\n\ny_pred_75 = y_proba[:, 1] > 0.75\nprint(classification_report(y_test, y_pred_75))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7131\n           1       0.00      0.00      0.00       137\n\n    accuracy                           0.98      7268\n   macro avg       0.49      0.50      0.50      7268\nweighted avg       0.96      0.98      0.97      7268\n\n\n\n\nfrom sklearn.metrics import plot_precision_recall_curve\nplot_precision_recall_curve(log_reg, X_test, y_test, name=\"LogisticRegression\")\n\n<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f6fc27e78b0>\n\n\n\n\n\n\nfrom sklearn.metrics import plot_roc_curve\nplot_roc_curve(log_reg, X_test, y_test, name=\"LogisticRegression\")\n\n<sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7f6fc075e790>\n\n\n\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\nplot_precision_recall_curve(log_reg, X_test, y_test, name=\"LogisticRegression\", ax=ax1)\nplot_roc_curve(log_reg, X_test, y_test, name=\"LogisticRegression\", ax=ax2)\n\n<sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7f6fc06594f0>\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\nRandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\nfig, ax = plt.subplots()\nplot_precision_recall_curve(log_reg, X_test, y_test, ax=ax, name=\"Logistic Regression\")\nplot_precision_recall_curve(rf, X_test, y_test, ax=ax, name=\"Random Forest\")\n\n<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f6fc05cd940>"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#exercise-2",
    "href": "posts/2021-06-01-model-evaluation.html#exercise-2",
    "title": "Model Evaluation",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nPlot the roc curve of the logistic regression model and the random forest model on the same axes.\nTrain a sklearn.dummy.DummyClassifier(strategy='prior') on the training dataset and plot the precision recall curve and the roc curve with the test dataset.\nWhat is the ROC AUC and the average precision for the dummy classifer?\nExtra: Compute the f1 score for three models we have trained so far. While model performs the best according to the f1 score? Hint: f1_score is in sklearn.metrics"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#decision-function",
    "href": "posts/2021-06-01-model-evaluation.html#decision-function",
    "title": "Model Evaluation",
    "section": "Decision function",
    "text": "Decision function\n\nComparing decision function vs predictions\n\nlog_reg_decision = log_reg.decision_function(X_test)\n\n\nnp.all((log_reg_decision > 0) ==  log_reg.predict(X_test))\n\nTrue\n\n\n\nlog_reg_pred = log_reg.predict_proba(X_test)\n\n\nlog_reg_pred\n\narray([[0.99375825, 0.00624175],\n       [0.99785633, 0.00214367],\n       [0.98641586, 0.01358414],\n       ...,\n       [0.9772597 , 0.0227403 ],\n       [0.93760339, 0.06239661],\n       [0.99690984, 0.00309016]])\n\n\n\n\nComputing the predict_proba from the decision function\n\n1/(1 + np.exp(-log_reg_decision))\n\narray([0.00624175, 0.00214367, 0.01358414, ..., 0.0227403 , 0.06239661,\n       0.00309016])\n\n\n\nlog_reg_pred[:, 1]\n\narray([0.00624175, 0.00214367, 0.01358414, ..., 0.0227403 , 0.06239661,\n       0.00309016])\n\n\n\n\nRanking metrics\n\nfrom sklearn.metrics import average_precision_score\n\n\nUsing the decision function to compute the average precision\n\naverage_precision_score(y_test, log_reg_decision)\n\n0.09421224656746816\n\n\n\n\nUsing predict_proba to compute the average precision\n\naverage_precision_score(y_test, log_reg_pred[:, 1])\n\n0.09421224656746816"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#exercise-3",
    "href": "posts/2021-06-01-model-evaluation.html#exercise-3",
    "title": "Model Evaluation",
    "section": "Exercise 3",
    "text": "Exercise 3\n\nCompute the roc_auc_score for the random forest. Hint: Use predict_proba.\nTrain a sklearn.svm.SVC model on the training datast and compute the average precision. Hint: Use decision_function.\n\n\n# %load solutions/01-ex03-solutions.py\nfrom sklearn.metrics import roc_auc_score\n\nrf_proba = rf.predict_proba(X_test)\n\nroc_auc_score(y_test, rf_proba[:, 1])\n\nfrom sklearn.svm import SVC\n\nsvc = SVC(random_state=0)\nsvc.fit(X_train, y_train)\n\nsvc_decision = svc.decision_function(X_test)\n\nroc_auc_score(y_test, svc_decision)\n\n0.538190915167353"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#multiclass",
    "href": "posts/2021-06-01-model-evaluation.html#multiclass",
    "title": "Model Evaluation",
    "section": "Multiclass",
    "text": "Multiclass\n\n## Reading the dataset using pandas\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/CTG.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n  \n    \n      \n      FileName\n      Date\n      SegFile\n      b\n      e\n      LBE\n      LB\n      AC\n      FM\n      UC\n      ...\n      C\n      D\n      E\n      AD\n      DE\n      LD\n      FS\n      SUSP\n      CLASS\n      NSP\n    \n  \n  \n    \n      0\n      Variab10.txt\n      12/1/1996\n      CTG0001.txt\n      240.0\n      357.0\n      120.0\n      120.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      9.0\n      2.0\n    \n    \n      1\n      Fmcs_1.txt\n      5/3/1996\n      CTG0002.txt\n      5.0\n      632.0\n      132.0\n      132.0\n      4.0\n      0.0\n      4.0\n      ...\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      6.0\n      1.0\n    \n    \n      2\n      Fmcs_1.txt\n      5/3/1996\n      CTG0003.txt\n      177.0\n      779.0\n      133.0\n      133.0\n      2.0\n      0.0\n      5.0\n      ...\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      6.0\n      1.0\n    \n    \n      3\n      Fmcs_1.txt\n      5/3/1996\n      CTG0004.txt\n      411.0\n      1192.0\n      134.0\n      134.0\n      2.0\n      0.0\n      6.0\n      ...\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      6.0\n      1.0\n    \n    \n      4\n      Fmcs_1.txt\n      5/3/1996\n      CTG0005.txt\n      533.0\n      1147.0\n      132.0\n      132.0\n      4.0\n      0.0\n      5.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2.0\n      1.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2124\n      S8001045.dsp\n      6/6/1998\n      CTG2127.txt\n      1576.0\n      3049.0\n      140.0\n      140.0\n      1.0\n      0.0\n      9.0\n      ...\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      5.0\n      2.0\n    \n    \n      2125\n      S8001045.dsp\n      6/6/1998\n      CTG2128.txt\n      2796.0\n      3415.0\n      142.0\n      142.0\n      1.0\n      1.0\n      5.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n    \n    \n      2126\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2127\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2128\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      564.0\n      23.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n2129 rows  40 columns\n\n\n\n\nsklearn.set_config(display='diagram')\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\n\n\nX, y = df.drop(['NSP', 'FileName', 'Date', 'SegFile'], axis = 1), df['NSP']\n\nX\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, random_state=42)\n\n\nrf = RandomForestClassifier(random_state=42).fit(X_train, y_train)\n\n\nfrom sklearn.metrics import plot_confusion_matrix\n\n\nplot_confusion_matrix(rf, X_test, y_test, cmap='gray_r')\n\n<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f6fb9cb03a0>\n\n\n\n\n\n\ny_pred = rf.predict(X_test)\n\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n         1.0       0.97      1.00      0.98       414\n         2.0       0.98      0.84      0.91        74\n         3.0       1.00      1.00      1.00        44\n\n    accuracy                           0.98       532\n   macro avg       0.99      0.95      0.96       532\nweighted avg       0.98      0.98      0.97       532\n\n\n\n\nfrom sklearn.metrics import roc_auc_score\n\n\nrf_y_pred_proba = rf.predict_proba(X_test)\n\n\nroc_auc_score(y_test, rf_y_pred_proba, multi_class='ovo')\n\n0.9980180279093323\n\n\n\nroc_auc_score(y_test, rf_y_pred_proba, multi_class='ovr')\n\n0.9985885196707779"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#regression",
    "href": "posts/2021-06-01-model-evaluation.html#regression",
    "title": "Model Evaluation",
    "section": "Regression",
    "text": "Regression\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\n\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\n\n\nX, y = df.drop(['specific', 'Unnamed: 0'], axis = 1), df['specific']\n\nX = X.select_dtypes(include='number')\nX\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=42)\n\n\nfrom sklearn.pipeline import make_pipeline\n\nridge = make_pipeline(StandardScaler(), Ridge())\nridge.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('standardscaler', StandardScaler()), ('ridge', Ridge())])StandardScalerStandardScaler()RidgeRidge()\n\n\n\nridge.score(X_test, y_test)\n\n0.7482100751181446\n\n\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\n\nridge_pred = ridge.predict(X_test)\n\n\nLook at predictions\n\nridge_pred[:10]\n\narray([2027961.35376795, 1530619.74443469, 1639626.98468177,\n        965135.26705736,  438579.62245477,  690017.03701487,\n        337043.03751396,  630817.75885244, 2231143.82393699,\n        -50934.54700348])\n\n\n\nridge_r2 = r2_score(y_test, ridge_pred)\nridge_r2\n\n0.7482100751181446\n\n\n\nridge_mse = mean_squared_error(y_test, ridge_pred)\nridge_mse\n\n165402066384.76834\n\n\n\nridge_mae = mean_absolute_error(y_test, ridge_pred)\nridge_mae\n\n275823.0351447304"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#prediction-plots",
    "href": "posts/2021-06-01-model-evaluation.html#prediction-plots",
    "title": "Model Evaluation",
    "section": "Prediction plots",
    "text": "Prediction plots\n\nfig, ax = plt.subplots()\ndelta = y_test - ridge_pred\nax.plot(ridge_pred, delta, 'o', alpha=0.5)\nax.axhline(y=0, c='k', ls='--')\nax.set(xlabel='predicted', ylabel='y_true - predicited', aspect='equal');"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#prediction-plots-histogram",
    "href": "posts/2021-06-01-model-evaluation.html#prediction-plots-histogram",
    "title": "Model Evaluation",
    "section": "Prediction plots histogram",
    "text": "Prediction plots histogram\n\nfig, ax = plt.subplots()\nax.hist(delta, bins=30)\nax.set(xlabel=\"y_true - predicted\", ylabel=\"Counts\");\n\n\n\n\n\n# %load solutions/01-ex04-solutions.py\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(random_state=42).fit(X_train, y_train)\nrf_pred = rf.predict(X_test)\n\n\nr2_score(y_test, rf_pred)\n\nmean_squared_error(y_test, rf_pred)\n\nmean_absolute_error(y_test, rf_pred)\n\n243725.82533333328"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#prediction-plots-per-feature",
    "href": "posts/2021-06-01-model-evaluation.html#prediction-plots-per-feature",
    "title": "Model Evaluation",
    "section": "Prediction plots per feature",
    "text": "Prediction plots per feature\n\nfrom sklearn.datasets import load_boston\nimport pandas as pd\n\n\nX_df = pd.DataFrame(X, columns=boston.feature_names)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=42)\n\n\nridge = make_pipeline(StandardScaler(), Ridge(random_state=42))\nridge.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('ridge', Ridge(random_state=42))])StandardScalerStandardScaler()RidgeRidge(random_state=42)\n\n\n\nridge_pred = ridge.predict(X_test)\n\n\nX_test.head()\n\n\n\n\n\n  \n    \n      \n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      it\n    \n  \n  \n    \n      166\n      763953.0\n      2006\n      7688.67\n      259335\n      0.0\n      0.4375\n      0.000000\n      6349262\n    \n    \n      262\n      112137.0\n      2006\n      21900.19\n      1000069\n      0.0\n      0.0000\n      0.000000\n      5304833\n    \n    \n      11\n      178705.0\n      2007\n      7360.92\n      299892\n      0.0\n      0.0000\n      0.324324\n      7040099\n    \n    \n      139\n      241282.0\n      2003\n      6867.70\n      53903\n      0.0\n      0.0000\n      0.516129\n      3586373\n    \n    \n      78\n      100000.0\n      2002\n      2523.73\n      41726\n      0.0\n      0.0000\n      0.400000\n      2138758\n    \n  \n\n\n\n\n\nX_analysis = X_test.assign(\n    delta=y_test - ridge_pred\n)\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncolumns = X_analysis.columns\nn_features = X.shape[1]\n\nfig, axes = plt.subplots(3, 5, figsize=(20, 10), constrained_layout=True)\nfor i, ax in enumerate(axes.ravel()):\n    if i >= n_features:\n        ax.set_visible(False)\n        continue\n    sns.scatterplot(x=columns[i], y='delta', ax=ax, data=X_analysis)\n    ax.axhline(y=0, c='k', ls='--')"
  },
  {
    "objectID": "posts/2020-10-26-working_with_sqlitedbs_in_jupyter_soccer_pred.html",
    "href": "posts/2020-10-26-working_with_sqlitedbs_in_jupyter_soccer_pred.html",
    "title": "Working with sqlite databases in Jupyter for European Soccer Match Data",
    "section": "",
    "text": "import sqlalchemy as db\nimport sqlite3\nimport pandas as pd\nimport numpy as np\n\n\nengine = db.create_engine('sqlite:///database.sqlite')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nconnection\n\n<sqlalchemy.engine.base.Connection at 0x7fb9c3356780>\n\n\n\nengine.execute(\"SELECT * FROM Country LIMIT 10\").fetchall()\n\n[(1, 'Belgium'),\n (1729, 'England'),\n (4769, 'France'),\n (7809, 'Germany'),\n (10257, 'Italy'),\n (13274, 'Netherlands'),\n (15722, 'Poland'),\n (17642, 'Portugal'),\n (19694, 'Scotland'),\n (21518, 'Spain')]\n\n\n\n%load_ext sql\n\nThe sql extension is already loaded. To reload it, use:\n  %reload_ext sql\n\n\n\n%sql sqlite:///database.sqlite\n\n\n%%sql\nSELECT *\nFROM Country\nLIMIT 10\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n    \n        id\n        name\n    \n    \n        1\n        Belgium\n    \n    \n        1729\n        England\n    \n    \n        4769\n        France\n    \n    \n        7809\n        Germany\n    \n    \n        10257\n        Italy\n    \n    \n        13274\n        Netherlands\n    \n    \n        15722\n        Poland\n    \n    \n        17642\n        Portugal\n    \n    \n        19694\n        Scotland\n    \n    \n        21518\n        Spain\n    \n\n\n\n\n%%sql\nSELECT id\n,name\nFROM Country\nWHERE name = \"England\"\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n    \n        id\n        name\n    \n    \n        1729\n        England\n    \n\n\n\n\n%%sql\nSELECT * FROM League LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n    \n        id\n        country_id\n        name\n    \n    \n        1\n        1\n        Belgium Jupiler League\n    \n    \n        1729\n        1729\n        England Premier League\n    \n    \n        4769\n        4769\n        France Ligue 1\n    \n    \n        7809\n        7809\n        Germany 1. Bundesliga\n    \n    \n        10257\n        10257\n        Italy Serie A\n    \n    \n        13274\n        13274\n        Netherlands Eredivisie\n    \n    \n        15722\n        15722\n        Poland Ekstraklasa\n    \n    \n        17642\n        17642\n        Portugal Liga ZON Sagres\n    \n    \n        19694\n        19694\n        Scotland Premier League\n    \n    \n        21518\n        21518\n        Spain LIGA BBVA\n    \n\n\n\n\n%%sql\nSELECT * FROM Match LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n    \n        id\n        country_id\n        league_id\n        season\n        stage\n        date\n        match_api_id\n        home_team_api_id\n        away_team_api_id\n        home_team_goal\n        away_team_goal\n        home_player_X1\n        home_player_X2\n        home_player_X3\n        home_player_X4\n        home_player_X5\n        home_player_X6\n        home_player_X7\n        home_player_X8\n        home_player_X9\n        home_player_X10\n        home_player_X11\n        away_player_X1\n        away_player_X2\n        away_player_X3\n        away_player_X4\n        away_player_X5\n        away_player_X6\n        away_player_X7\n        away_player_X8\n        away_player_X9\n        away_player_X10\n        away_player_X11\n        home_player_Y1\n        home_player_Y2\n        home_player_Y3\n        home_player_Y4\n        home_player_Y5\n        home_player_Y6\n        home_player_Y7\n        home_player_Y8\n        home_player_Y9\n        home_player_Y10\n        home_player_Y11\n        away_player_Y1\n        away_player_Y2\n        away_player_Y3\n        away_player_Y4\n        away_player_Y5\n        away_player_Y6\n        away_player_Y7\n        away_player_Y8\n        away_player_Y9\n        away_player_Y10\n        away_player_Y11\n        home_player_1\n        home_player_2\n        home_player_3\n        home_player_4\n        home_player_5\n        home_player_6\n        home_player_7\n        home_player_8\n        home_player_9\n        home_player_10\n        home_player_11\n        away_player_1\n        away_player_2\n        away_player_3\n        away_player_4\n        away_player_5\n        away_player_6\n        away_player_7\n        away_player_8\n        away_player_9\n        away_player_10\n        away_player_11\n        goal\n        shoton\n        shotoff\n        foulcommit\n        card\n        cross\n        corner\n        possession\n        B365H\n        B365D\n        B365A\n        BWH\n        BWD\n        BWA\n        IWH\n        IWD\n        IWA\n        LBH\n        LBD\n        LBA\n        PSH\n        PSD\n        PSA\n        WHH\n        WHD\n        WHA\n        SJH\n        SJD\n        SJA\n        VCH\n        VCD\n        VCA\n        GBH\n        GBD\n        GBA\n        BSH\n        BSD\n        BSA\n    \n    \n        1\n        1\n        1\n        2008/2009\n        1\n        2008-08-17 00:00:00\n        492473\n        9987\n        9993\n        1\n        1\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        1.73\n        3.4\n        5\n        1.75\n        3.35\n        4.2\n        1.85\n        3.2\n        3.5\n        1.8\n        3.3\n        3.75\n        None\n        None\n        None\n        1.7\n        3.3\n        4.33\n        1.9\n        3.3\n        4\n        1.65\n        3.4\n        4.5\n        1.78\n        3.25\n        4\n        1.73\n        3.4\n        4.2\n    \n    \n        2\n        1\n        1\n        2008/2009\n        1\n        2008-08-16 00:00:00\n        492474\n        10000\n        9994\n        0\n        0\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        1.95\n        3.2\n        3.6\n        1.8\n        3.3\n        3.95\n        1.9\n        3.2\n        3.5\n        1.9\n        3.2\n        3.5\n        None\n        None\n        None\n        1.83\n        3.3\n        3.6\n        1.95\n        3.3\n        3.8\n        2\n        3.25\n        3.25\n        1.85\n        3.25\n        3.75\n        1.91\n        3.25\n        3.6\n    \n    \n        3\n        1\n        1\n        2008/2009\n        1\n        2008-08-16 00:00:00\n        492475\n        9984\n        8635\n        0\n        3\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        2.38\n        3.3\n        2.75\n        2.4\n        3.3\n        2.55\n        2.6\n        3.1\n        2.3\n        2.5\n        3.2\n        2.5\n        None\n        None\n        None\n        2.5\n        3.25\n        2.4\n        2.63\n        3.3\n        2.5\n        2.35\n        3.25\n        2.65\n        2.5\n        3.2\n        2.5\n        2.3\n        3.2\n        2.75\n    \n    \n        4\n        1\n        1\n        2008/2009\n        1\n        2008-08-17 00:00:00\n        492476\n        9991\n        9998\n        5\n        0\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        1.44\n        3.75\n        7.5\n        1.4\n        4\n        6.8\n        1.4\n        3.9\n        6\n        1.44\n        3.6\n        6.5\n        None\n        None\n        None\n        1.44\n        3.75\n        6\n        1.44\n        4\n        7.5\n        1.45\n        3.75\n        6.5\n        1.5\n        3.75\n        5.5\n        1.44\n        3.75\n        6.5\n    \n    \n        5\n        1\n        1\n        2008/2009\n        1\n        2008-08-16 00:00:00\n        492477\n        7947\n        9985\n        1\n        3\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        5\n        3.5\n        1.65\n        5\n        3.5\n        1.6\n        4\n        3.3\n        1.7\n        4\n        3.4\n        1.72\n        None\n        None\n        None\n        4.2\n        3.4\n        1.7\n        4.5\n        3.5\n        1.73\n        4.5\n        3.4\n        1.65\n        4.5\n        3.5\n        1.65\n        4.75\n        3.3\n        1.67\n    \n    \n        6\n        1\n        1\n        2008/2009\n        1\n        2008-09-24 00:00:00\n        492478\n        8203\n        8342\n        1\n        1\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        4.75\n        3.4\n        1.67\n        4.85\n        3.4\n        1.65\n        3.7\n        3.2\n        1.8\n        5\n        3.25\n        1.62\n        None\n        None\n        None\n        4.2\n        3.4\n        1.7\n        5.5\n        3.75\n        1.67\n        4.35\n        3.4\n        1.7\n        4.5\n        3.4\n        1.7\n        None\n        None\n        None\n    \n    \n        7\n        1\n        1\n        2008/2009\n        1\n        2008-08-16 00:00:00\n        492479\n        9999\n        8571\n        2\n        2\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        2.1\n        3.2\n        3.3\n        2.05\n        3.25\n        3.15\n        1.85\n        3.2\n        3.5\n        1.83\n        3.3\n        3.6\n        None\n        None\n        None\n        1.83\n        3.3\n        3.6\n        1.91\n        3.4\n        3.6\n        2.1\n        3.25\n        3\n        1.85\n        3.25\n        3.75\n        2.1\n        3.25\n        3.1\n    \n    \n        8\n        1\n        1\n        2008/2009\n        1\n        2008-08-16 00:00:00\n        492480\n        4049\n        9996\n        1\n        2\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        3.2\n        3.4\n        2.2\n        2.55\n        3.3\n        2.4\n        2.4\n        3.2\n        2.4\n        2.5\n        3.2\n        2.5\n        None\n        None\n        None\n        2.7\n        3.25\n        2.25\n        2.6\n        3.4\n        2.4\n        2.8\n        3.25\n        2.25\n        2.8\n        3.2\n        2.25\n        2.88\n        3.25\n        2.2\n    \n    \n        9\n        1\n        1\n        2008/2009\n        1\n        2008-08-16 00:00:00\n        492481\n        10001\n        9986\n        1\n        0\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        2.25\n        3.25\n        2.88\n        2.3\n        3.25\n        2.7\n        2.1\n        3.1\n        3\n        2.25\n        3.2\n        2.75\n        None\n        None\n        None\n        2.2\n        3.25\n        2.75\n        2.2\n        3.3\n        3.1\n        2.25\n        3.25\n        2.8\n        2.2\n        3.3\n        2.8\n        2.25\n        3.2\n        2.8\n    \n    \n        10\n        1\n        1\n        2008/2009\n        10\n        2008-11-01 00:00:00\n        492564\n        8342\n        8571\n        4\n        1\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        1.3\n        5.25\n        9.5\n        1.25\n        5\n        10\n        1.3\n        4.2\n        8\n        1.25\n        4.5\n        10\n        None\n        None\n        None\n        1.35\n        4.2\n        7\n        1.27\n        5\n        10\n        1.3\n        4.35\n        8.5\n        1.25\n        5\n        10\n        1.29\n        4.5\n        9\n    \n\n\n\n\n%%sql\nSELECT * FROM Player LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n    \n        id\n        player_api_id\n        player_name\n        player_fifa_api_id\n        birthday\n        height\n        weight\n    \n    \n        1\n        505942\n        Aaron Appindangoye\n        218353\n        1992-02-29 00:00:00\n        182.88\n        187\n    \n    \n        2\n        155782\n        Aaron Cresswell\n        189615\n        1989-12-15 00:00:00\n        170.18\n        146\n    \n    \n        3\n        162549\n        Aaron Doran\n        186170\n        1991-05-13 00:00:00\n        170.18\n        163\n    \n    \n        4\n        30572\n        Aaron Galindo\n        140161\n        1982-05-08 00:00:00\n        182.88\n        198\n    \n    \n        5\n        23780\n        Aaron Hughes\n        17725\n        1979-11-08 00:00:00\n        182.88\n        154\n    \n    \n        6\n        27316\n        Aaron Hunt\n        158138\n        1986-09-04 00:00:00\n        182.88\n        161\n    \n    \n        7\n        564793\n        Aaron Kuhl\n        221280\n        1996-01-30 00:00:00\n        172.72\n        146\n    \n    \n        8\n        30895\n        Aaron Lennon\n        152747\n        1987-04-16 00:00:00\n        165.1\n        139\n    \n    \n        9\n        528212\n        Aaron Lennox\n        206592\n        1993-02-19 00:00:00\n        190.5\n        181\n    \n    \n        10\n        101042\n        Aaron Meijers\n        188621\n        1987-10-28 00:00:00\n        175.26\n        170\n    \n\n\n\n\n%%sql\nSELECT * FROM Player_Attributes LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n    \n        id\n        player_fifa_api_id\n        player_api_id\n        date\n        overall_rating\n        potential\n        preferred_foot\n        attacking_work_rate\n        defensive_work_rate\n        crossing\n        finishing\n        heading_accuracy\n        short_passing\n        volleys\n        dribbling\n        curve\n        free_kick_accuracy\n        long_passing\n        ball_control\n        acceleration\n        sprint_speed\n        agility\n        reactions\n        balance\n        shot_power\n        jumping\n        stamina\n        strength\n        long_shots\n        aggression\n        interceptions\n        positioning\n        vision\n        penalties\n        marking\n        standing_tackle\n        sliding_tackle\n        gk_diving\n        gk_handling\n        gk_kicking\n        gk_positioning\n        gk_reflexes\n    \n    \n        1\n        218353\n        505942\n        2016-02-18 00:00:00\n        67\n        71\n        right\n        medium\n        medium\n        49\n        44\n        71\n        61\n        44\n        51\n        45\n        39\n        64\n        49\n        60\n        64\n        59\n        47\n        65\n        55\n        58\n        54\n        76\n        35\n        71\n        70\n        45\n        54\n        48\n        65\n        69\n        69\n        6\n        11\n        10\n        8\n        8\n    \n    \n        2\n        218353\n        505942\n        2015-11-19 00:00:00\n        67\n        71\n        right\n        medium\n        medium\n        49\n        44\n        71\n        61\n        44\n        51\n        45\n        39\n        64\n        49\n        60\n        64\n        59\n        47\n        65\n        55\n        58\n        54\n        76\n        35\n        71\n        70\n        45\n        54\n        48\n        65\n        69\n        69\n        6\n        11\n        10\n        8\n        8\n    \n    \n        3\n        218353\n        505942\n        2015-09-21 00:00:00\n        62\n        66\n        right\n        medium\n        medium\n        49\n        44\n        71\n        61\n        44\n        51\n        45\n        39\n        64\n        49\n        60\n        64\n        59\n        47\n        65\n        55\n        58\n        54\n        76\n        35\n        63\n        41\n        45\n        54\n        48\n        65\n        66\n        69\n        6\n        11\n        10\n        8\n        8\n    \n    \n        4\n        218353\n        505942\n        2015-03-20 00:00:00\n        61\n        65\n        right\n        medium\n        medium\n        48\n        43\n        70\n        60\n        43\n        50\n        44\n        38\n        63\n        48\n        60\n        64\n        59\n        46\n        65\n        54\n        58\n        54\n        76\n        34\n        62\n        40\n        44\n        53\n        47\n        62\n        63\n        66\n        5\n        10\n        9\n        7\n        7\n    \n    \n        5\n        218353\n        505942\n        2007-02-22 00:00:00\n        61\n        65\n        right\n        medium\n        medium\n        48\n        43\n        70\n        60\n        43\n        50\n        44\n        38\n        63\n        48\n        60\n        64\n        59\n        46\n        65\n        54\n        58\n        54\n        76\n        34\n        62\n        40\n        44\n        53\n        47\n        62\n        63\n        66\n        5\n        10\n        9\n        7\n        7\n    \n    \n        6\n        189615\n        155782\n        2016-04-21 00:00:00\n        74\n        76\n        left\n        high\n        medium\n        80\n        53\n        58\n        71\n        40\n        73\n        70\n        69\n        68\n        71\n        79\n        78\n        78\n        67\n        90\n        71\n        85\n        79\n        56\n        62\n        68\n        67\n        60\n        66\n        59\n        76\n        75\n        78\n        14\n        7\n        9\n        9\n        12\n    \n    \n        7\n        189615\n        155782\n        2016-04-07 00:00:00\n        74\n        76\n        left\n        high\n        medium\n        80\n        53\n        58\n        71\n        32\n        73\n        70\n        69\n        68\n        71\n        79\n        78\n        78\n        67\n        90\n        71\n        85\n        79\n        56\n        60\n        68\n        67\n        60\n        66\n        59\n        76\n        75\n        78\n        14\n        7\n        9\n        9\n        12\n    \n    \n        8\n        189615\n        155782\n        2016-01-07 00:00:00\n        73\n        75\n        left\n        high\n        medium\n        79\n        52\n        57\n        70\n        29\n        71\n        68\n        69\n        68\n        70\n        79\n        78\n        78\n        67\n        90\n        71\n        84\n        79\n        56\n        59\n        67\n        66\n        58\n        65\n        59\n        76\n        75\n        78\n        14\n        7\n        9\n        9\n        12\n    \n    \n        9\n        189615\n        155782\n        2015-12-24 00:00:00\n        73\n        75\n        left\n        high\n        medium\n        79\n        51\n        57\n        70\n        29\n        71\n        68\n        69\n        68\n        70\n        79\n        78\n        78\n        67\n        90\n        71\n        84\n        79\n        56\n        58\n        67\n        66\n        58\n        65\n        59\n        76\n        75\n        78\n        14\n        7\n        9\n        9\n        12\n    \n    \n        10\n        189615\n        155782\n        2015-12-17 00:00:00\n        73\n        75\n        left\n        high\n        medium\n        79\n        51\n        57\n        70\n        29\n        71\n        68\n        69\n        68\n        70\n        79\n        78\n        78\n        67\n        90\n        71\n        84\n        79\n        56\n        58\n        67\n        66\n        58\n        65\n        59\n        76\n        75\n        78\n        14\n        7\n        9\n        9\n        12\n    \n\n\n\n\n%%sql\nSELECT * FROM Team LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n    \n        id\n        team_api_id\n        team_fifa_api_id\n        team_long_name\n        team_short_name\n    \n    \n        1\n        9987\n        673\n        KRC Genk\n        GEN\n    \n    \n        2\n        9993\n        675\n        Beerschot AC\n        BAC\n    \n    \n        3\n        10000\n        15005\n        SV Zulte-Waregem\n        ZUL\n    \n    \n        4\n        9994\n        2007\n        Sporting Lokeren\n        LOK\n    \n    \n        5\n        9984\n        1750\n        KSV Cercle Brugge\n        CEB\n    \n    \n        6\n        8635\n        229\n        RSC Anderlecht\n        AND\n    \n    \n        7\n        9991\n        674\n        KAA Gent\n        GEN\n    \n    \n        8\n        9998\n        1747\n        RAEC Mons\n        MON\n    \n    \n        9\n        7947\n        None\n        FCV Dender EH\n        DEN\n    \n    \n        10\n        9985\n        232\n        Standard de Lige\n        STL\n    \n\n\n\n\n%%sql\nSELECT * FROM Team_Attributes LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n    \n        id\n        team_fifa_api_id\n        team_api_id\n        date\n        buildUpPlaySpeed\n        buildUpPlaySpeedClass\n        buildUpPlayDribbling\n        buildUpPlayDribblingClass\n        buildUpPlayPassing\n        buildUpPlayPassingClass\n        buildUpPlayPositioningClass\n        chanceCreationPassing\n        chanceCreationPassingClass\n        chanceCreationCrossing\n        chanceCreationCrossingClass\n        chanceCreationShooting\n        chanceCreationShootingClass\n        chanceCreationPositioningClass\n        defencePressure\n        defencePressureClass\n        defenceAggression\n        defenceAggressionClass\n        defenceTeamWidth\n        defenceTeamWidthClass\n        defenceDefenderLineClass\n    \n    \n        1\n        434\n        9930\n        2010-02-22 00:00:00\n        60\n        Balanced\n        None\n        Little\n        50\n        Mixed\n        Organised\n        60\n        Normal\n        65\n        Normal\n        55\n        Normal\n        Organised\n        50\n        Medium\n        55\n        Press\n        45\n        Normal\n        Cover\n    \n    \n        2\n        434\n        9930\n        2014-09-19 00:00:00\n        52\n        Balanced\n        48\n        Normal\n        56\n        Mixed\n        Organised\n        54\n        Normal\n        63\n        Normal\n        64\n        Normal\n        Organised\n        47\n        Medium\n        44\n        Press\n        54\n        Normal\n        Cover\n    \n    \n        3\n        434\n        9930\n        2015-09-10 00:00:00\n        47\n        Balanced\n        41\n        Normal\n        54\n        Mixed\n        Organised\n        54\n        Normal\n        63\n        Normal\n        64\n        Normal\n        Organised\n        47\n        Medium\n        44\n        Press\n        54\n        Normal\n        Cover\n    \n    \n        4\n        77\n        8485\n        2010-02-22 00:00:00\n        70\n        Fast\n        None\n        Little\n        70\n        Long\n        Organised\n        70\n        Risky\n        70\n        Lots\n        70\n        Lots\n        Organised\n        60\n        Medium\n        70\n        Double\n        70\n        Wide\n        Cover\n    \n    \n        5\n        77\n        8485\n        2011-02-22 00:00:00\n        47\n        Balanced\n        None\n        Little\n        52\n        Mixed\n        Organised\n        53\n        Normal\n        48\n        Normal\n        52\n        Normal\n        Organised\n        47\n        Medium\n        47\n        Press\n        52\n        Normal\n        Cover\n    \n    \n        6\n        77\n        8485\n        2012-02-22 00:00:00\n        58\n        Balanced\n        None\n        Little\n        62\n        Mixed\n        Organised\n        45\n        Normal\n        70\n        Lots\n        55\n        Normal\n        Organised\n        40\n        Medium\n        40\n        Press\n        60\n        Normal\n        Cover\n    \n    \n        7\n        77\n        8485\n        2013-09-20 00:00:00\n        62\n        Balanced\n        None\n        Little\n        45\n        Mixed\n        Organised\n        40\n        Normal\n        50\n        Normal\n        55\n        Normal\n        Organised\n        42\n        Medium\n        42\n        Press\n        60\n        Normal\n        Cover\n    \n    \n        8\n        77\n        8485\n        2014-09-19 00:00:00\n        58\n        Balanced\n        64\n        Normal\n        62\n        Mixed\n        Organised\n        56\n        Normal\n        68\n        Lots\n        57\n        Normal\n        Organised\n        41\n        Medium\n        42\n        Press\n        60\n        Normal\n        Cover\n    \n    \n        9\n        77\n        8485\n        2015-09-10 00:00:00\n        59\n        Balanced\n        64\n        Normal\n        53\n        Mixed\n        Organised\n        51\n        Normal\n        72\n        Lots\n        63\n        Normal\n        Free Form\n        49\n        Medium\n        45\n        Press\n        63\n        Normal\n        Cover\n    \n    \n        10\n        614\n        8576\n        2010-02-22 00:00:00\n        60\n        Balanced\n        None\n        Little\n        40\n        Mixed\n        Organised\n        45\n        Normal\n        35\n        Normal\n        55\n        Normal\n        Organised\n        30\n        Deep\n        70\n        Double\n        30\n        Narrow\n        Offside Trap\n    \n\n\n\n\n%%sql\nCREATE TABLE Team_table AS\nSELECT * FROM Team_Attributes LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n[]\n\n\n\n%%sql\nDROP TABLE IF EXISTS Team_table\n\n * sqlite:///database.sqlite\nDone.\n\n\n[]\n\n\n\nsql_query = %sql SELECT * FROM Team LIMIT 10\ndf = sql_query.DataFrame()\n\n * sqlite:///database.sqlite\nDone.\n\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      id\n      team_api_id\n      team_fifa_api_id\n      team_long_name\n      team_short_name\n    \n  \n  \n    \n      0\n      1\n      9987\n      673.0\n      KRC Genk\n      GEN\n    \n    \n      1\n      2\n      9993\n      675.0\n      Beerschot AC\n      BAC\n    \n    \n      2\n      3\n      10000\n      15005.0\n      SV Zulte-Waregem\n      ZUL\n    \n    \n      3\n      4\n      9994\n      2007.0\n      Sporting Lokeren\n      LOK\n    \n    \n      4\n      5\n      9984\n      1750.0\n      KSV Cercle Brugge\n      CEB\n    \n    \n      5\n      6\n      8635\n      229.0\n      RSC Anderlecht\n      AND\n    \n    \n      6\n      7\n      9991\n      674.0\n      KAA Gent\n      GEN\n    \n    \n      7\n      8\n      9998\n      1747.0\n      RAEC Mons\n      MON\n    \n    \n      8\n      9\n      7947\n      NaN\n      FCV Dender EH\n      DEN\n    \n    \n      9\n      10\n      9985\n      232.0\n      Standard de Lige\n      STL\n    \n  \n\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,7))\n\nplot = %sql SELECT team_short_name, count(*) FROM Team GROUP BY team_short_name ORDER BY team_short_name\nplot.bar();\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\n\nplot.pie();\n\n\n\n\n\ntype(plot)\n\nsql.run.ResultSet"
  },
  {
    "objectID": "posts/2020-10-04-stockmarketportfolioanaylsis_snp_pandas_datareader_sqlite-copy1.html",
    "href": "posts/2020-10-04-stockmarketportfolioanaylsis_snp_pandas_datareader_sqlite-copy1.html",
    "title": "Stock Market and Portfolio Anaylsis Tech Stocks and the S&P 500 in 2020 with pandas_datareader and writing to at sqlite database",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport pandas_datareader.data as web\nimport datetime\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n# # start = datetime.datetime(2016, 1, 1)\n# # end = datetime.datetime(2017, 5, 17)\n\n# start = datetime.datetime(2010, 1, 1)\n# end = datetime.datetime(2020, 1, 1)\n\n\nstart = pd.to_datetime('2020-01-01')\nend = pd.to_datetime('today')\n\n\nFXAIX_stock = web.DataReader('FXAIX', 'yahoo', start, end)\nFXAIX_stock.head()\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nSNOW_stock = web.DataReader('SNOW', 'yahoo', start, end)\nSNOW_stock.head()\n\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Open')\n\nMSFT_stock['Open'].plot(label='Microsoft')\nZOOM_stock['Open'].plot(label='Zoom')\nSNOW_stock['Open'].plot(label='Snowflake')\nFXAIX_stock['Open'].plot(label='SNP_500')\nplt.legend()\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Volume')\n\nMSFT_stock['Volume'].plot(label='Microsoft')\nZOOM_stock['Volume'].plot(label='Zoom')\nSNOW_stock['Volume'].plot(label='Snowflake')\nFXAIX_stock['Volume'].plot(label='SNP_500')\n\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7fae549b8ba8>\n\n\n\n\n\n\n\n\n\nFXAIX_stock = web.DataReader('FXAIX', 'yahoo', start, end)\nFXAIX_stock.head()\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nSNOW_stock = web.DataReader('SNOW', 'yahoo', start, end)\nSNOW_stock.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-09-16\n      319.0\n      231.110001\n      245.000000\n      253.929993\n      36099700\n      253.929993\n    \n    \n      2020-09-17\n      241.5\n      215.240005\n      230.759995\n      227.539993\n      11907500\n      227.539993\n    \n    \n      2020-09-18\n      249.0\n      218.589996\n      235.000000\n      240.000000\n      7475400\n      240.000000\n    \n    \n      2020-09-21\n      241.5\n      218.600006\n      230.000000\n      228.850006\n      5524900\n      228.850006\n    \n    \n      2020-09-22\n      239.0\n      225.149994\n      238.500000\n      235.160004\n      3889100\n      235.160004\n    \n  \n\n\n\n\n\nstocks = pd.concat([MSFT_stock['Open'], ZOOM_stock['Open'], SNOW_stock['Open'], FXAIX_stock['Open']],\n                   axis = 1)\n\n\nstocks\n\n\n\n\n\n  \n    \n      \n      Open\n      Open\n      Open\n      Open\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-02\n      158.779999\n      68.800003\n      NaN\n      112.980003\n    \n    \n      2020-01-03\n      158.320007\n      67.620003\n      NaN\n      112.190002\n    \n    \n      2020-01-06\n      157.080002\n      66.629997\n      NaN\n      112.589996\n    \n    \n      2020-01-07\n      159.320007\n      70.290001\n      NaN\n      112.290001\n    \n    \n      2020-01-08\n      158.929993\n      71.809998\n      NaN\n      112.839996\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2020-09-28\n      210.880005\n      502.410004\n      235.929993\n      116.650002\n    \n    \n      2020-09-29\n      209.350006\n      488.130005\n      255.000000\n      116.099998\n    \n    \n      2020-09-30\n      207.729996\n      464.209991\n      261.500000\n      117.070000\n    \n    \n      2020-10-01\n      213.490005\n      477.000000\n      255.250000\n      117.699997\n    \n    \n      2020-10-02\n      208.000000\n      485.005005\n      232.440002\n      116.120003\n    \n  \n\n191 rows  4 columns\n\n\n\n\nstocks.columns = ['MSFT_stock','ZOOM_stock','SNOW_stock','FXAIX_stock']\n\n\nstocks\n\n\n\n\n\n  \n    \n      \n      MSFT_stock\n      ZOOM_stock\n      SNOW_stock\n      FXAIX_stock\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-02\n      158.779999\n      68.800003\n      NaN\n      112.980003\n    \n    \n      2020-01-03\n      158.320007\n      67.620003\n      NaN\n      112.190002\n    \n    \n      2020-01-06\n      157.080002\n      66.629997\n      NaN\n      112.589996\n    \n    \n      2020-01-07\n      159.320007\n      70.290001\n      NaN\n      112.290001\n    \n    \n      2020-01-08\n      158.929993\n      71.809998\n      NaN\n      112.839996\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2020-09-28\n      210.880005\n      502.410004\n      235.929993\n      116.650002\n    \n    \n      2020-09-29\n      209.350006\n      488.130005\n      255.000000\n      116.099998\n    \n    \n      2020-09-30\n      207.729996\n      464.209991\n      261.500000\n      117.070000\n    \n    \n      2020-10-01\n      213.490005\n      477.000000\n      255.250000\n      117.699997\n    \n    \n      2020-10-02\n      208.000000\n      485.005005\n      232.440002\n      116.120003\n    \n  \n\n191 rows  4 columns\n\n\n\n\nmean_daily_ret = stocks.pct_change(1).mean()\nmean_daily_ret\n\nMSFT_stock     0.001751\nZOOM_stock     0.011973\nSNOW_stock    -0.002546\nFXAIX_stock    0.000440\ndtype: float64\n\n\n\nstocks.pct_change(1).corr()\n\n\n\n\n\n  \n    \n      \n      MSFT_stock\n      ZOOM_stock\n      SNOW_stock\n      FXAIX_stock\n    \n  \n  \n    \n      MSFT_stock\n      1.000000\n      0.209041\n      0.661827\n      0.382807\n    \n    \n      ZOOM_stock\n      0.209041\n      1.000000\n      0.095052\n      0.127526\n    \n    \n      SNOW_stock\n      0.661827\n      0.095052\n      1.000000\n      0.292117\n    \n    \n      FXAIX_stock\n      0.382807\n      0.127526\n      0.292117\n      1.000000\n    \n  \n\n\n\n\n\nstock_normed = stocks/stocks.iloc[0]\nstock_normed.plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7fae54a74a90>\n\n\n\n\n\n\nstock_daily_ret = stocks.pct_change(1)\nstock_daily_ret.head()\n\n\n\n\n\n  \n    \n      \n      MSFT_stock\n      ZOOM_stock\n      SNOW_stock\n      FXAIX_stock\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-02\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2020-01-03\n      -0.002897\n      -0.017151\n      NaN\n      -0.006992\n    \n    \n      2020-01-06\n      -0.007832\n      -0.014641\n      NaN\n      0.003565\n    \n    \n      2020-01-07\n      0.014260\n      0.054930\n      NaN\n      -0.002664\n    \n    \n      2020-01-08\n      -0.002448\n      0.021625\n      NaN\n      0.004898\n    \n  \n\n\n\n\n\nlog_ret = np.log(stocks / stocks.shift(1))\nlog_ret.head()\n\n\n\n\n\n  \n    \n      \n      MSFT_stock\n      ZOOM_stock\n      SNOW_stock\n      FXAIX_stock\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-02\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2020-01-03\n      -0.002901\n      -0.017300\n      NaN\n      -0.007017\n    \n    \n      2020-01-06\n      -0.007863\n      -0.014749\n      NaN\n      0.003559\n    \n    \n      2020-01-07\n      0.014160\n      0.053475\n      NaN\n      -0.002668\n    \n    \n      2020-01-08\n      -0.002451\n      0.021394\n      NaN\n      0.004886\n    \n  \n\n\n\n\n\nlog_ret.hist(bins = 100,\n             figsize = (12, 6));\nplt.tight_layout()\n\n\n\n\n\nlog_ret.describe().transpose()\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n  \n  \n    \n      MSFT_stock\n      190.0\n      0.001421\n      0.025752\n      -0.087821\n      -0.012115\n      0.004000\n      0.016980\n      0.081248\n    \n    \n      ZOOM_stock\n      190.0\n      0.010279\n      0.056461\n      -0.142569\n      -0.017014\n      0.011119\n      0.035968\n      0.368600\n    \n    \n      SNOW_stock\n      12.0\n      -0.004386\n      0.063753\n      -0.131433\n      -0.033113\n      0.019477\n      0.034320\n      0.077728\n    \n    \n      FXAIX_stock\n      190.0\n      0.000144\n      0.024461\n      -0.127150\n      -0.007774\n      0.002806\n      0.010082\n      0.089894\n    \n  \n\n\n\n\n\nlog_ret.mean() * 252\n\nMSFT_stock     0.358130\nZOOM_stock     2.590236\nSNOW_stock    -1.105148\nFXAIX_stock    0.036359\ndtype: float64\n\n\n\nlog_ret.cov()\n\n\n\n\n\n  \n    \n      \n      MSFT_stock\n      ZOOM_stock\n      SNOW_stock\n      FXAIX_stock\n    \n  \n  \n    \n      MSFT_stock\n      0.000663\n      0.000323\n      0.001291\n      0.000245\n    \n    \n      ZOOM_stock\n      0.000323\n      0.003188\n      0.000290\n      0.000184\n    \n    \n      SNOW_stock\n      0.001291\n      0.000290\n      0.004064\n      0.000231\n    \n    \n      FXAIX_stock\n      0.000245\n      0.000184\n      0.000231\n      0.000598\n    \n  \n\n\n\n\n\n# Set seed (optional)\nnp.random.seed(101)\n\n# Stock Columns\nprint('Stocks')\nprint(stocks.columns)\nprint('\\n')\n\n# Create Random Weights\nprint('Creating Random Weights')\nweights = np.array(np.random.random(4))\nprint(weights)\nprint('\\n')\n\n# Rebalance Weights\nprint('Rebalance to sum to 1.0')\nweights = weights / np.sum(weights)\nprint(weights)\nprint('\\n')\n\n# Expected Return\nprint('Expected Portfolio Return')\nexp_ret = np.sum(log_ret.mean() * weights) *252\nprint(exp_ret)\nprint('\\n')\n\n# Expected Variance\nprint('Expected Volatility')\nexp_vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\nprint(exp_vol)\nprint('\\n')\n\n# Sharpe Ratio\nSR = exp_ret/exp_vol\nprint('Sharpe Ratio')\nprint(SR)\n\nStocks\nIndex(['MSFT_stock', 'ZOOM_stock', 'SNOW_stock', 'FXAIX_stock'], dtype='object')\n\n\nCreating Random Weights\n[0.51639863 0.57066759 0.02847423 0.17152166]\n\n\nRebalance to sum to 1.0\n[0.40122278 0.44338777 0.02212343 0.13326603]\n\n\nExpected Portfolio Return\n1.272564336318203\n\n\nExpected Volatility\n0.4864366288684257\n\n\nSharpe Ratio\n2.6160948020680697\n\n\n\nnum_ports = 15000\n\nall_weights = np.zeros((num_ports, len(stocks.columns)))\nret_arr = np.zeros(num_ports)\nvol_arr = np.zeros(num_ports)\nsharpe_arr = np.zeros(num_ports)\n\nfor ind in range(num_ports):\n\n    # Create Random Weights\n    weights = np.array(np.random.random(4))\n\n    # Rebalance Weights\n    weights = weights / np.sum(weights)\n    \n    # Save Weights\n    all_weights[ind,:] = weights\n\n    # Expected Return\n    ret_arr[ind] = np.sum((log_ret.mean() * weights) *252)\n\n    # Expected Variance\n    vol_arr[ind] = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n\n    # Sharpe Ratio\n    sharpe_arr[ind] = ret_arr[ind] / vol_arr[ind]\n\n\nsharpe_arr.max()\n\n2.8667995807841824\n\n\n\nsharpe_arr.argmax()\n\n5483\n\n\n\nall_weights[10619,:]\n\narray([5.06395348e-01, 4.67772019e-04, 2.64242193e-01, 2.28894687e-01])\n\n\n\nmax_sr_ret = ret_arr[1419]\nmax_sr_vol = vol_arr[1419]\n\n\nplt.figure(figsize = (12, 8))\nplt.scatter(vol_arr,\n            ret_arr,\n            c = sharpe_arr,\n            cmap = 'plasma')\nplt.colorbar(label = 'Sharpe Ratio')\nplt.xlabel('Volatility')\nplt.ylabel('Return')\n\n# Add red dot for max SR\nplt.scatter(max_sr_vol,\n            max_sr_ret,\n            c = 'red',\n            s = 50,\n            edgecolors = 'black')\n\n<matplotlib.collections.PathCollection at 0x7fae54366048>\n\n\n\n\n\n\ndef get_ret_vol_sr(weights):\n    \"\"\"\n    Takes in weights, returns array or return,volatility, sharpe ratio\n    \"\"\"\n    weights = np.array(weights)\n    ret = np.sum(log_ret.mean() * weights) * 252\n    vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n    sr = ret/vol\n    return np.array([ret, vol, sr])\n\nfrom scipy.optimize import minimize\nimport numpy as np\n\ndef neg_sharpe(weights):\n    return  get_ret_vol_sr(weights)[2] * -1\n\n# Contraints\ndef check_sum(weights):\n    '''\n    Returns 0 if sum of weights is 1.0\n    '''\n    return np.sum(weights) - 1\n\n# By convention of minimize function it should be a function that returns zero for conditions\ncons = ({'type' : 'eq', 'fun': check_sum})\n\n# 0-1 bounds for each weight\nbounds = ((0, 1), (0, 1), (0, 1), (0, 1))\n\n# Initial Guess (equal distribution)\ninit_guess = [0.25, 0.25, 0.25, 0.25]\n\n# Sequential Least Squares \nopt_results = minimize(neg_sharpe,\n                       init_guess,\n                       method = 'SLSQP',\n                       bounds = bounds,\n                       constraints = cons)\n\nopt_results\n\n     fun: -2.8998675936504807\n     jac: array([-3.57061625e-04,  6.75618649e-05,  1.98669076e+00,  1.90789163e-01])\n message: 'Optimization terminated successfully.'\n    nfev: 42\n     nit: 7\n    njev: 7\n  status: 0\n success: True\n       x: array([1.59222977e-01, 8.40777023e-01, 7.68699340e-16, 0.00000000e+00])\n\n\n\nopt_results.x\n\nget_ret_vol_sr(opt_results.x)\n\narray([2.23483308, 0.77066728, 2.89986759])\n\n\n\nfrontier_y = np.linspace(0, 0.3, 100)\n\n\ndef minimize_volatility(weights):\n    return  get_ret_vol_sr(weights)[1] \n\nfrontier_volatility = []\n\nfor possible_return in frontier_y:\n    # function for return\n    cons = ({'type':'eq','fun': check_sum},\n            {'type':'eq','fun': lambda w: get_ret_vol_sr(w)[0] - possible_return})\n    \n    result = minimize(minimize_volatility,\n                      init_guess,\n                      method = 'SLSQP',\n                      bounds = bounds,\n                      constraints = cons)\n    \n    frontier_volatility.append(result['fun'])\n\n\nplt.figure(figsize = (12, 8))\nplt.scatter(vol_arr,\n            ret_arr,\n            c = sharpe_arr,\n            cmap = 'plasma')\nplt.colorbar(label = 'Sharpe Ratio')\nplt.xlabel('Volatility')\nplt.ylabel('Return')\n\n\n\n# Add frontier line\nplt.plot(frontier_volatility,\n         frontier_y,\n         'g--',\n         linewidth = 3)\n\n\n\n\n\nstocks['FXAIX_stock'].plot(figsize = (12, 8))\nplt.title('Total S&P 500 in 2020 Value')\n\nText(0.5, 1.0, 'Total S&P 500 in 2020 Value')\n\n\n\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nstocks\n\n\n\n\n\n  \n    \n      \n      MSFT_stock\n      ZOOM_stock\n      SNOW_stock\n      FXAIX_stock\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-02\n      158.779999\n      68.800003\n      NaN\n      112.980003\n    \n    \n      2020-01-03\n      158.320007\n      67.620003\n      NaN\n      112.190002\n    \n    \n      2020-01-06\n      157.080002\n      66.629997\n      NaN\n      112.589996\n    \n    \n      2020-01-07\n      159.320007\n      70.290001\n      NaN\n      112.290001\n    \n    \n      2020-01-08\n      158.929993\n      71.809998\n      NaN\n      112.839996\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2020-09-28\n      210.880005\n      502.410004\n      235.929993\n      116.650002\n    \n    \n      2020-09-29\n      209.350006\n      488.130005\n      255.000000\n      116.099998\n    \n    \n      2020-09-30\n      207.729996\n      464.209991\n      261.500000\n      117.070000\n    \n    \n      2020-10-01\n      213.490005\n      477.000000\n      255.250000\n      117.699997\n    \n    \n      2020-10-02\n      208.000000\n      485.005005\n      232.440002\n      116.120003\n    \n  \n\n191 rows  4 columns\n\n\n\n\nengine = db.create_engine('sqlite:///stocks.sqlite')\n\n\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nstocks.to_sql('stocks', con=engine, if_exists='append', index=True)\n\n\nengine.execute(\"SELECT * FROM stocks LIMIT 10\").fetchall()\n\n[(158.77999877929688, 68.80000305175781, None, 112.9800033569336),\n (158.32000732421875, 67.62000274658203, None, 112.19000244140625),\n (157.0800018310547, 66.62999725341797, None, 112.58999633789062),\n (159.32000732421875, 70.29000091552734, None, 112.29000091552734),\n (158.92999267578125, 71.80999755859375, None, 112.83999633789062),\n (161.83999633789062, 73.98999786376953, None, 113.62000274658203),\n (162.82000732421875, 73.08000183105469, None, 113.30000305175781),\n (161.75999450683594, 73.88999938964844, None, 114.08999633789062),\n (163.38999938964844, 74.31999969482422, None, 113.93000030517578),\n (162.6199951171875, 73.27999877929688, None, 114.13999938964844)]\n\n\n\nengine.execute(\"SELECT FXAIX_stock FROM stocks LIMIT 10\").fetchall()\n\n[(112.9800033569336,),\n (112.19000244140625,),\n (112.58999633789062,),\n (112.29000091552734,),\n (112.83999633789062,),\n (113.62000274658203,),\n (113.30000305175781,),\n (114.08999633789062,),\n (113.93000030517578,),\n (114.13999938964844,)]\n\n\n\n# df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n# df\n\n# df.to_sql('users', con=engine)\n\n# engine.execute(\"SELECT * FROM users\").fetchall()"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesnt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2021-05-31-cross-validation.html",
    "href": "posts/2021-05-31-cross-validation.html",
    "title": "Cross-Validation in scikit-learn example",
    "section": "",
    "text": "#code adapted from https://github.com/thomasjpfan/ml-workshop-intermediate-1-of-2"
  },
  {
    "objectID": "posts/2021-05-31-cross-validation.html#load-sample-data",
    "href": "posts/2021-05-31-cross-validation.html#load-sample-data",
    "title": "Cross-Validation in scikit-learn example",
    "section": "Load sample data",
    "text": "Load sample data\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['it', 'Unnamed: 0'], axis = 1), df['it']\n\n\nX = X.select_dtypes(include='number')\nX\n\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\n\n\n\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
  },
  {
    "objectID": "posts/2021-05-31-cross-validation.html#cross-validation-for-model-selection",
    "href": "posts/2021-05-31-cross-validation.html#cross-validation-for-model-selection",
    "title": "Cross-Validation in scikit-learn example",
    "section": "Cross validation for model selection",
    "text": "Cross validation for model selection\n\nTry DummyClassifier\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.dummy import DummyClassifier\n\n\ndummy_clf = DummyClassifier(strategy=\"prior\")\ndummy_scores = cross_val_score(dummy_clf, X_train, y_train)\n\n\ndummy_scores\n\narray([0.98119697, 0.98119697, 0.98119697, 0.98096767, 0.98119266])\n\n\n\ndummy_scores.mean()\n\n0.981150249606079\n\n\n\n\nTry KNeighborsClassifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nknc = make_pipeline(StandardScaler(), KNeighborsClassifier())\nknc_scores = cross_val_score(knc, X_train, y_train)\n\n\nknc_scores\n\narray([0.98073836, 0.98096767, 0.98050906, 0.98096767, 0.9809633 ])\n\n\n\nknc_scores.mean()\n\n0.980829211800172\n\n\n\n\nTry LogisticRegression\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n\nlog_reg = make_pipeline(\n    StandardScaler(),\n    LogisticRegression(random_state=0)\n)\n\n\nlog_reg_scores = cross_val_score(log_reg, X_train, y_train)\n\n\nlog_reg_scores\n\narray([0.98119697, 0.98119697, 0.98119697, 0.98096767, 0.98119266])\n\n\n\nlog_reg_scores.mean()\n\n0.981150249606079\n\n\n\n\nWhich model do we choose?\n\nDummy\nKNeighborsClassifier\nLogisticRegression\n\n\ny.value_counts()\n\n\ndummy_scores = cross_val_score(dummy_clf, X_train, y_train, scoring=\"roc_auc\")\ndummy_scores.mean()\n\nknc_scores = cross_val_score(knc, X_train, y_train, scoring=\"roc_auc\")\nknc_scores.mean()\n\n0.5880984927926187"
  },
  {
    "objectID": "posts/2021-05-31-cross-validation.html#cross-validation-strategies",
    "href": "posts/2021-05-31-cross-validation.html#cross-validation-strategies",
    "title": "Cross-Validation in scikit-learn example",
    "section": "Cross validation Strategies",
    "text": "Cross validation Strategies\n\nKFold\n\nfrom sklearn.model_selection import KFold\n\ncross_val_score(log_reg, X_train, y_train, cv=KFold(n_splits=4))\n\narray([0.97982022, 0.98275546, 0.98183819, 0.98018712])"
  },
  {
    "objectID": "posts/2021-05-31-cross-validation.html#repeated-kfold",
    "href": "posts/2021-05-31-cross-validation.html#repeated-kfold",
    "title": "Cross-Validation in scikit-learn example",
    "section": "Repeated KFold",
    "text": "Repeated KFold\n\nfrom sklearn.model_selection import RepeatedKFold\n\nscores = cross_val_score(log_reg, X_train, y_train,\n                         cv=RepeatedKFold(n_splits=4, n_repeats=2))\n\n\nscores\n\narray([0.98238855, 0.9787195 , 0.98348927, 0.98000367, 0.98018712,\n       0.98018712, 0.98128784, 0.98293891])\n\n\n\nscores.shape\n\n(8,)"
  },
  {
    "objectID": "posts/2021-05-31-cross-validation.html#stratifiedkfold",
    "href": "posts/2021-05-31-cross-validation.html#stratifiedkfold",
    "title": "Cross-Validation in scikit-learn example",
    "section": "StratifiedKFold",
    "text": "StratifiedKFold\n\nfrom sklearn.model_selection import StratifiedKFold\n\nscores = cross_val_score(log_reg, X_train, y_train,\n                         cv=StratifiedKFold(n_splits=4))\n\n\nscores\n\narray([0.98128784, 0.98110438, 0.98110438, 0.98110438])\n\n\nThis is a binary classification problem:\n\ny.value_counts()\n\n0    28524\n1      548\nName: stroke, dtype: int64\n\n\nScikit-learn will use StratifiedKFold by default:\n\ncross_val_score(log_reg, X_train, y_train, cv=4)\n\narray([0.98128784, 0.98110438, 0.98110438, 0.98110438])"
  },
  {
    "objectID": "posts/2021-05-31-cross-validation.html#repeatedstratifiedkfold",
    "href": "posts/2021-05-31-cross-validation.html#repeatedstratifiedkfold",
    "title": "Cross-Validation in scikit-learn example",
    "section": "RepeatedStratifiedKFold",
    "text": "RepeatedStratifiedKFold\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nscores = cross_val_score(\n    log_reg, X_train, y_train,\n    cv=RepeatedStratifiedKFold(n_splits=4, n_repeats=3))\n\n\nscores\n\narray([0.98128784, 0.98110438, 0.98110438, 0.98110438, 0.98128784,\n       0.98110438, 0.98110438, 0.98110438, 0.98128784, 0.98110438,\n       0.98110438, 0.98110438])\n\n\n\nscores.shape\n\n(12,)\n\n\n\n# %load solutions/01-ex02-solutions.py\nfrom sklearn.model_selection import cross_validate\n\nresults = cross_validate(log_reg, X_train, y_train, cv=4)\nresults\n\nimport pandas as pd\npd.DataFrame(results)\n\nmore_results = cross_validate(log_reg, X_train, y_train, cv=4, scoring=[\"f1\", \"accuracy\"])\n\npd.DataFrame(more_results)\n\n\n\n\n\n  \n    \n      \n      fit_time\n      score_time\n      test_f1\n      test_accuracy\n    \n  \n  \n    \n      0\n      0.039663\n      0.007991\n      0.0\n      0.981288\n    \n    \n      1\n      0.038163\n      0.007970\n      0.0\n      0.981104\n    \n    \n      2\n      0.040289\n      0.008049\n      0.0\n      0.981104\n    \n    \n      3\n      0.038140\n      0.007756\n      0.0\n      0.981104\n    \n  \n\n\n\n\n\nAppendix: TimeSeriesSplit\n\nfrom sklearn.model_selection import TimeSeriesSplit\nimport numpy as np\n\nX = np.arange(10)\n\n\ntscv = TimeSeriesSplit(n_splits=3)\nfor train_index, test_index in tscv.split(X):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n\nTRAIN: [0 1 2 3] TEST: [4 5]\nTRAIN: [0 1 2 3 4 5] TEST: [6 7]\nTRAIN: [0 1 2 3 4 5 6 7] TEST: [8 9]\n\n\nWith gap=2:"
  },
  {
    "objectID": "posts/2020-08-17-pyspark-group-by.html",
    "href": "posts/2020-08-17-pyspark-group-by.html",
    "title": "Group By and Aggregation with Pyspark",
    "section": "",
    "text": "Group By and Aggregation with Pyspark"
  },
  {
    "objectID": "posts/2020-08-17-pyspark-group-by.html#read-csv-and-inferschema",
    "href": "posts/2020-08-17-pyspark-group-by.html#read-csv-and-inferschema",
    "title": "Group By and Aggregation with Pyspark",
    "section": "Read CSV and inferSchema",
    "text": "Read CSV and inferSchema\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import countDistinct, avg,stddev\n\n\n# Load data from a CSV\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").option(\"inferSchema\", True).option(\"header\", True).load(file_location)\ndisplay(df.take(5))\n\n\n_c0provincespecificgeneralyeargdpfdirnrrrifrregit0Anhui147002.0null19962093.3506610.00.00.01128873East China6319301Anhui151981.0null19972347.32434430.00.00.01356287East China6578602Anhui174930.0null19982542.96276730.00.00.01518236East China8894633Anhui285324.0null19992712.3426131nullnullnull1646891East China12273644Anhui195580.032100.020002902.09318470.00.00.01601508East China1499110\n\n\n\ndf.printSchema()\n\n\nroot\n-- _c0: integer (nullable = true)\n-- province: string (nullable = true)\n-- specific: double (nullable = true)\n-- general: double (nullable = true)\n-- year: integer (nullable = true)\n-- gdp: double (nullable = true)\n-- fdi: integer (nullable = true)\n-- rnr: double (nullable = true)\n-- rr: double (nullable = true)\n-- i: double (nullable = true)\n-- fr: string (nullable = true)\n-- reg: string (nullable = true)\n-- it: integer (nullable = true)"
  },
  {
    "objectID": "posts/2020-08-17-pyspark-group-by.html#using-groupby-for-averages-and-counts",
    "href": "posts/2020-08-17-pyspark-group-by.html#using-groupby-for-averages-and-counts",
    "title": "Group By and Aggregation with Pyspark",
    "section": "Using groupBy for Averages and Counts",
    "text": "Using groupBy for Averages and Counts\n\ndf.groupBy(\"province\")\n\n\nOut[8]: <pyspark.sql.group.GroupedData at 0x7f939a0aada0>\n\n\n\ndf.groupBy(\"province\").mean().show()\n\n\n+------------+--------+------------------+------------------+---------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\n    province|avg(_c0)|     avg(specific)|      avg(general)|avg(year)|          avg(gdp)|          avg(fdi)|            avg(rnr)|             avg(rr)|              avg(i)|           avg(it)|\n+------------+--------+------------------+------------------+---------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\n   Guangdong|    65.5|1123328.0833333333|          312308.0|   2001.5|15358.781666666668|        1194950.25|0.011261261250000001|                 0.0|                 0.0|        3099014.25|\n       Hunan|   161.5| 824676.9166666666| 480788.3333333333|   2001.5| 4765.891666666666|         132110.25|                 0.0| 0.07291666666666667|                 0.0|         3215128.5|\n      Shanxi|   281.5| 577540.4166666666|          351680.0|   2001.5| 2817.210833333333|38628.833333333336|                 0.0|                 0.0|                 0.0|1983718.3333333333|\n       Tibet|   317.5|189219.91666666666|165365.33333333334|   2001.5|170.42666666666665|            839.75| 0.03030303033333333| 0.15583333333333335| 0.20278090583333333|1174175.5833333333|\n       Hubei|   149.5|         595463.25|          391326.5|   2001.5| 4772.503333333333|         149713.25|         0.045045045| 0.11386386375000002| 0.06230392158333333|        2904659.75|\n     Tianjin|   305.5| 76884.16666666667|          126636.0|   2001.5|2528.6650000000004|250173.33333333334|                 0.0|                 0.0|                 0.0| 831028.4166666666|\n     Beijing|    17.5| 581440.8333333334|          412825.0|   2001.5| 4673.453333333333|257369.33333333334|                 0.0|  0.3613053613636364| 0.29545454545454547|1175965.4166666667|\nHeilongjiang|   125.5|1037878.1666666666| 315925.3333333333|   2001.5| 4041.241666666667| 82719.33333333333|                 0.0|                 0.0| 0.03931203927272728|3230451.1666666665|\n    Liaoning|   209.5|        1111002.75|185280.83333333334|   2001.5| 5231.135000000001| 285925.3333333333| 0.11469534044444446|                 0.0|                null|2628358.4166666665|\n       Henan|   137.5| 955407.4166666666|          673392.5|   2001.5| 7208.966666666667|           94426.0|                 0.0|                0.04| 0.08602150533333335|3671970.6666666665|\n       Anhui|     5.5| 643984.1666666666|159698.83333333334|   2001.5|3905.8700000000003| 70953.08333333333|                 0.0|                 0.0| 0.08845208836363637|2649674.4166666665|\n    Xinjiang|   329.5| 345334.3333333333|          412906.0|   2001.5|1828.8966666666665| 4433.083333333333|                 0.0|                 0.0|                 0.0|         2251012.0|\n      Fujian|    41.5|246144.16666666666|140619.33333333334|   2001.5|4864.0233333333335| 374466.4166666667|  0.1366666666666667|0.049999999999999996| 0.09999999999999999|        1274116.75|\n     Jiangxi|   185.5| 592906.3333333334| 458268.6666666667|   2001.5|         2460.7825|         103735.25|                 0.0|  0.1491841490909091|0.042727272727272725|        1760613.25|\n       Jilin|   197.5|         711132.25|          348186.0|   2001.5|2274.8541666666665|41226.583333333336|                 0.0|                 0.0|                 0.0|2136634.9166666665|\n   Chongqing|    29.5| 561854.1111111111|          151201.4|   2001.5|         2477.7125|41127.833333333336| 0.09677419400000001|                 0.0|                 0.0|1636146.4166666667|\n     Shaanxi|   245.5| 387167.1666666667|          386760.5|   2001.5| 2658.034166666667|50892.583333333336|0.002840909090909091|                 0.0| 0.07386363636363637|2474031.4166666665|\n     Sichuan|   293.5|         1194640.5| 707032.8333333334|   2001.5|           5377.79|62197.166666666664| 0.00818181818181818| 0.00818181818181818|                 0.2|4016479.5833333335|\n      Yunnan|   341.5| 802151.1666666666|          200426.0|   2001.5| 2604.054166666667|17048.333333333332|                 0.0|                 0.0|                 0.0|3165418.9166666665|\n       Gansu|    53.5| 498930.9166666667| 382092.6666666667|   2001.5|1397.8325000000002|            5295.5| 0.11111111120000002|         0.088974359| 0.13038461533333334|         2045347.0|\n+------------+--------+------------------+------------------+---------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.groupBy(\"reg\").mean().show()\n\n\n+-------------------+------------------+------------------+------------------+---------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\n                reg|          avg(_c0)|     avg(specific)|      avg(general)|avg(year)|          avg(gdp)|          avg(fdi)|            avg(rnr)|             avg(rr)|              avg(i)|           avg(it)|\n+-------------------+------------------+------------------+------------------+---------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\n    Southwest China|             214.3| 648086.8070175438|          327627.0|   2001.5|2410.3988333333336|25405.083333333332| 0.01764440930612245|0.053185448081632655| 0.13679739081632653|         2424971.4|\n         East China|183.78571428571428|517524.90476190473|230217.37142857144|   2001.5| 7126.732976190476|414659.03571428574| 0.08284508739240506| 0.05701117448101268| 0.09036240282278483|1949130.4761904762|\n    Northeast China|             177.5| 953337.7222222222|283130.72222222225|   2001.5| 3849.076944444444|         136623.75| 0.03686635942857143|                 0.0| 0.02275960168421053|2665148.1666666665|\n        North China|             179.5|506433.57446808513|334689.14285714284|   2001.5| 4239.038541666667|169600.58333333334|                 0.0| 0.15428824051724138| 0.11206896551724138|1733718.7291666667|\n    Northwest China|             216.7|324849.06666666665|293066.73333333334|   2001.5|1340.0261666666668|15111.133333333333|0.022847222240000003|0.033887245249999996|0.048179240615384616|        1703537.75|\nSouth Central China|             115.5| 690125.8333333334| 382414.8888888889|   2001.5| 5952.826944444445|281785.59722222225|0.014928879322033899| 0.07324349771186443| 0.06797753142372882|       2626299.875|\n+-------------------+------------------+------------------+------------------+---------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\n\n\n\n\n\n# Count\ndf.groupBy(\"reg\").count().show()\n\n\n+-------------------+-----+\n                reg|count|\n+-------------------+-----+\n    Southwest China|   60|\n         East China|   84|\n    Northeast China|   36|\n        North China|   48|\n    Northwest China|   60|\nSouth Central China|   72|\n+-------------------+-----+\n\n\n\n\n\n# Max\ndf.groupBy(\"reg\").max().show()\n\n\n+-------------------+--------+-------------+------------+---------+--------+--------+------------------+-----------+-------------------+--------+\n                reg|max(_c0)|max(specific)|max(general)|max(year)|max(gdp)|max(fdi)|          max(rnr)|    max(rr)|             max(i)| max(it)|\n+-------------------+--------+-------------+------------+---------+--------+--------+------------------+-----------+-------------------+--------+\n    Southwest China|     347|    3937966.0|   1725100.0|     2007|10562.39|  149322|       0.181818182|       0.84|               0.75|10384846|\n         East China|     359|    2213991.0|   1272600.0|     2007|25776.91| 1743140|       1.214285714|       0.53|                0.6| 7040099|\n    Northeast China|     215|    3847672.0|   1046700.0|     2007| 9304.52|  598554|       0.516129032|        0.0|0.21621621600000002| 7968319|\n        North China|     311|    2981235.0|   1023453.0|     2007|13607.32|  527776|               0.0|0.794871795|                0.6| 7537692|\n    Northwest China|     335|    2669238.0|   1197400.0|     2007| 5757.29|  119516|0.5555555560000001|        0.5|               1.05| 6308151|\nSouth Central China|     167|    3860764.0|   1737800.0|     2007|31777.01| 1712603|        0.27027027|     0.4375| 0.6176470589999999|10533312|\n+-------------------+--------+-------------+------------+---------+--------+--------+------------------+-----------+-------------------+--------+\n\n\n\n\n\n# Min\ndf.groupBy(\"reg\").min().show()\n\n\n+-------------------+--------+-------------+------------+---------+--------+--------+--------+-------+------+-------+\n                reg|min(_c0)|min(specific)|min(general)|min(year)|min(gdp)|min(fdi)|min(rnr)|min(rr)|min(i)|min(it)|\n+-------------------+--------+-------------+------------+---------+--------+--------+--------+-------+------+-------+\n    Southwest China|      24|      18829.0|     18700.0|     1996|   64.98|       2|     0.0|    0.0|   0.0| 176802|\n         East China|       0|       8964.0|         0.0|     1996| 1169.73|   22724|     0.0|    0.0|   0.0| 489132|\n    Northeast China|     120|      80595.0|     19360.0|     1996| 1137.23|   19059|     0.0|    0.0|   0.0| 625471|\n        North China|      12|      35084.0|     32119.0|     1996| 1121.93|   13802|     0.0|    0.0|   0.0| 303992|\n    Northwest China|      48|      32088.0|      2990.0|     1996|  184.17|     247|     0.0|    0.0|   0.0| 178668|\nSouth Central China|      60|      54462.0|         0.0|     1996|  389.68|   29579|     0.0|    0.0|   0.0| 147897|\n+-------------------+--------+-------------+------------+---------+--------+--------+--------+-------+------+-------+\n\n\n\n\n\n# Sum\ndf.groupBy(\"reg\").sum().show()\n\n\n+-------------------+--------+-------------+------------+---------+------------------+--------+------------------+------------------+-------------------+---------+\n                reg|sum(_c0)|sum(specific)|sum(general)|sum(year)|          sum(gdp)|sum(fdi)|          sum(rnr)|           sum(rr)|             sum(i)|  sum(it)|\n+-------------------+--------+-------------+------------+---------+------------------+--------+------------------+------------------+-------------------+---------+\n    Southwest China|   12858|  3.6940948E7|   9501183.0|   120090|144623.93000000002| 1524305|       0.864576056|       2.606086956|         6.70307215|145498284|\n         East China|   15438|  4.3472092E7|   8057608.0|   168126|         598645.57|34831359|       6.544761904| 4.503882784000002|  7.138629823000002|163726960|\n    Northeast China|    6390|  3.4320158E7|   5096353.0|    72054|         138566.77| 4918455|       1.032258064|               0.0|0.43243243200000003| 95945334|\n        North China|    8616|  2.3802378E7|   7028472.0|    96072|         203473.85| 8140828|               0.0|       4.474358975|               3.25| 83218499|\n    Northwest China|   13002|  1.9490944E7|   8792002.0|   120090|          80401.57|  906668|1.1423611120000001|1.7621367529999998|        2.505320512|102212265|\nSouth Central China|    8316|   4.968906E7| 1.3766936E7|   144108|428603.54000000004|20288563|        0.88080388| 4.321366365000001|  4.010674354000001|189093591|\n+-------------------+--------+-------------+------------+---------+------------------+--------+------------------+------------------+-------------------+---------+\n\n\n\n\n\n# Max it across everything\ndf.agg({'specific':'max'}).show()\n\n\n+-------------+\nmax(specific)|\n+-------------+\n    3937966.0|\n+-------------+\n\n\n\n\n\ngrouped = df.groupBy(\"reg\")\ngrouped.agg({\"it\":'max'}).show()\n\n\n+-------------------+--------+\n                reg| max(it)|\n+-------------------+--------+\n    Southwest China|10384846|\n         East China| 7040099|\n    Northeast China| 7968319|\n        North China| 7537692|\n    Northwest China| 6308151|\nSouth Central China|10533312|\n+-------------------+--------+\n\n\n\n\n\ndf.select(countDistinct(\"reg\")).show()\n\n\n+-------------------+\ncount(DISTINCT reg)|\n+-------------------+\n                  6|\n+-------------------+\n\n\n\n\n\ndf.select(countDistinct(\"reg\").alias(\"Distinct Region\")).show()\n\n\n+---------------+\nDistinct Region|\n+---------------+\n              6|\n+---------------+\n\n\n\n\n\ndf.select(avg('specific')).show()\n\n\n+-----------------+\n    avg(specific)|\n+-----------------+\n583470.7303370787|\n+-----------------+\n\n\n\n\n\ndf.select(stddev(\"specific\")).show()\n\n\n+---------------------+\nstddev_samp(specific)|\n+---------------------+\n    654055.3290782663|\n+---------------------+"
  },
  {
    "objectID": "posts/2020-08-17-pyspark-group-by.html#choosing-significant-digits-with-format_number",
    "href": "posts/2020-08-17-pyspark-group-by.html#choosing-significant-digits-with-format_number",
    "title": "Group By and Aggregation with Pyspark",
    "section": "Choosing Significant Digits with format_number",
    "text": "Choosing Significant Digits with format_number\n\nfrom pyspark.sql.functions import format_number\n\n\nspecific_std = df.select(stddev(\"specific\").alias('std'))\nspecific_std.show()\n\n\n+-----------------+\n              std|\n+-----------------+\n654055.3290782663|\n+-----------------+\n\n\n\n\n\nspecific_std.select(format_number('std',0)).show()\n\n\n+---------------------+\nformat_number(std, 0)|\n+---------------------+\n              654,055|\n+---------------------+"
  },
  {
    "objectID": "posts/2020-08-17-pyspark-group-by.html#using-orderby",
    "href": "posts/2020-08-17-pyspark-group-by.html#using-orderby",
    "title": "Group By and Aggregation with Pyspark",
    "section": "Using orderBy",
    "text": "Using orderBy\n\ndf.orderBy(\"specific\").show()\n\n\n+---+---------+--------+-------+----+--------+------+-----------+----+----+-------+---------------+-------+\n_c0| province|specific|general|year|     gdp|   fdi|        rnr|  rr|   i|     fr|            reg|     it|\n+---+---------+--------+-------+----+--------+------+-----------+----+----+-------+---------------+-------+\n 28|Chongqing|    null|   null|2000|  1791.0| 24436|       null|null|null|   null|Southwest China|1022148|\n109|    Hebei|    null|   null|1997| 3953.78|110064|       null|null|null|   null|    North China| 826734|\n 24|Chongqing|    null|   null|1996| 1315.12| 21878|       null|null|null|   null|Southwest China| 176802|\n 25|Chongqing|    null|   null|1997| 1509.75| 38675|       null|null|null|   null|Southwest China| 383402|\n268| Shanghai|  8964.0|   null|2000| 4771.17|316014|        0.0| 0.0|0.44|2224124|     East China|1212473|\n269| Shanghai|  9834.0|   null|2001| 5210.12|429159|        0.0| 0.0|0.44|2947285|     East China|1053917|\n312|    Tibet| 18829.0|   null|1996|   64.98|   679|0.181818182| 0.0| 0.0|  27801|Southwest China| 306114|\n270| Shanghai| 19985.0|   null|2002| 5741.03|427229|        0.0| 0.0|0.44|3380397|     East China|1572208|\n271| Shanghai| 23547.0|   null|2003| 6694.23|546849|        0.0|0.53| 0.0|4461153|     East China|2031496|\n313|    Tibet| 25185.0|   null|1997|   77.24|    63|0.181818182| 0.0| 0.0|  33787|Southwest China| 346368|\n273| Shanghai| 29943.0|   null|2005| 9247.66|685000|        0.0|0.53| 0.0|   null|     East China|2140461|\n272| Shanghai| 29943.0|   null|2004| 8072.83|654100|        0.0|0.53| 0.0|   null|     East China|2703643|\n216|  Ningxia| 32088.0|   null|1996|   202.9|  2826|       null|null|null|  90805|Northwest China| 178668|\n305|  Tianjin| 35084.0|   null|2001| 1919.09|213348|        0.0| 0.0| 0.0| 942763|    North China| 688810|\n228|  Qinghai| 37976.0|   null|1996|  184.17|   576|       null|null|null|  73260|Northwest China| 218361|\n302|  Tianjin| 39364.0|   null|1998|  1374.6|211361|       null|null|null| 540178|    North China| 361723|\n274| Shanghai| 42928.0|   null|2006|10572.24|710700|        0.0|0.53| 0.0|8175966|     East China|2239987|\n217|  Ningxia| 44267.0|   null|1997|  224.59|   671|       null|null|null| 102083|Northwest China| 195295|\n303|  Tianjin| 45463.0|   null|1999| 1500.95|176399|        0.0| 0.0| 0.0| 605662|    North China| 422522|\n314|    Tibet| 48197.0|   null|1998|    91.5|   481|        0.0|0.24| 0.0|   3810|Southwest China| 415547|\n+---+---------+--------+-------+----+--------+------+-----------+----+----+-------+---------------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.orderBy(df[\"specific\"].desc()).show()\n\n\n+---+------------+---------+---------+----+--------+-------+-----------+-----------+-------------------+--------+-------------------+--------+\n_c0|    province| specific|  general|year|     gdp|    fdi|        rnr|         rr|                  i|      fr|                reg|      it|\n+---+------------+---------+---------+----+--------+-------+-----------+-----------+-------------------+--------+-------------------+--------+\n299|     Sichuan|3937966.0|1725100.0|2007|10562.39| 149322|       null|       null|               null| 8508606|    Southwest China|10384846|\n143|       Henan|3860764.0|1737800.0|2007|15012.46| 306162|        0.0|        0.0|                0.0| 8620804|South Central China|10533312|\n131|Heilongjiang|3847672.0|1046700.0|2007|  7104.0| 208508|        0.0|        0.0|0.21621621600000002| 4404689|    Northeast China| 7968319|\n215|    Liaoning|3396397.0| 599600.0|2007| 9304.52| 598554|0.516129032|        0.0|               null|10826948|    Northeast China| 5502192|\n167|       Hunan|3156087.0|1329200.0|2007|  9439.6| 327051|        0.0|     0.4375|                0.0| 6065508|South Central China| 8340692|\n119|       Hebei|2981235.0| 694400.0|2007|13607.32| 241621|        0.0|        0.5|                0.0| 7891198|        North China| 7537692|\n155|       Hubei|2922784.0|1263500.0|2007|  9333.4| 276622|        0.0|0.111111111|                0.0| 5903552|South Central China| 7666512|\n251|     Shaanxi|2669238.0|1081000.0|2007| 5757.29| 119516|    0.03125|        0.0|             0.8125| 4752398|    Northwest China| 6308151|\n203|       Jilin|2663667.0|1016400.0|2007| 4275.12|  76064|        0.0|        0.0|                0.0| 3206892|    Northeast China| 4607955|\n347|      Yunnan|2482173.0| 564400.0|2007| 4772.52|  39453|        0.0|        0.0|                0.0| 4867146|    Southwest China| 6832541|\n298|     Sichuan|2225220.0|1187958.0|2006| 8690.24| 120819|        0.0|        0.0|               0.55| 4247403|    Southwest China| 7646885|\n 11|       Anhui|2213991.0| 178705.0|2007| 7360.92| 299892|        0.0|        0.0|        0.324324324| 4468640|         East China| 7040099|\n287|      Shanxi|2189020.0| 661200.0|2007| 6024.45| 134283|       null|       null|               null| 5978870|        North China| 5070166|\n263|    Shandong|2121243.0| 581800.0|2007|25776.91|1101159|        0.0|        0.0|                0.0|16753980|         East China| 6357869|\n191|     Jiangxi|2045869.0|1272600.0|2007| 4820.53| 280657|        0.0| 0.41025641|                0.0| 3898510|         East China| 4229821|\n 83|     Guangxi|2022957.0|1214100.0|2007| 5823.41|  68396|0.205128205|        0.0|0.23076923100000002| 4188265|South Central China| 6185600|\n142|       Henan|2018158.0|1131615.0|2006|12362.79| 184526|        0.0|        0.0|                0.0| 6212824|South Central China| 7601825|\n 59|       Gansu|2010553.0|1039400.0|2007| 2703.98|  11802|       null|        0.0|               1.05| 1909107|    Northwest China| 5111059|\n 95|     Guizhou|1956261.0|1239200.0|2007| 2884.11|  12651|        0.0|        0.0| 0.7105263159999999| 2851375|    Southwest China| 5639838|\n214|    Liaoning|1947031.0| 179893.0|2006| 8047.26| 359000|0.516129032|        0.0|               null| 6530236|    Northeast China| 4605917|\n+---+------------+---------+---------+----+--------+-------+-----------+-----------+-------------------+--------+-------------------+--------+\nonly showing top 20 rows\n\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2021-05-31-sklearn-preprocessing-example.html",
    "href": "posts/2021-05-31-sklearn-preprocessing-example.html",
    "title": "Preprocessing example in Sklearn",
    "section": "",
    "text": "import seaborn as sns\nsns.set_theme(context=\"notebook\", font_scale=1.4,\n              rc={\"figure.constrained_layout.use\": True,\n                  \"figure.figsize\": [10, 6]})\n\n\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n\nX\n\n\n\n\n\n  \n    \n      \n      age\n      hypertension\n      heart_disease\n      avg_glucose_level\n      bmi\n    \n  \n  \n    \n      1\n      58.0\n      1\n      0\n      87.96\n      39.2\n    \n    \n      3\n      70.0\n      0\n      0\n      69.04\n      35.9\n    \n    \n      6\n      52.0\n      0\n      0\n      77.59\n      17.7\n    \n    \n      7\n      75.0\n      0\n      1\n      243.53\n      27.0\n    \n    \n      8\n      32.0\n      0\n      0\n      77.67\n      32.3\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      43395\n      10.0\n      0\n      0\n      58.64\n      20.4\n    \n    \n      43396\n      56.0\n      0\n      0\n      213.61\n      55.4\n    \n    \n      43397\n      82.0\n      1\n      0\n      91.94\n      28.9\n    \n    \n      43398\n      40.0\n      0\n      0\n      99.16\n      33.2\n    \n    \n      43399\n      82.0\n      0\n      0\n      79.48\n      20.6\n    \n  \n\n29072 rows  5 columns\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(3, 4, figsize=(20, 10))\n\nfor name, ax in zip(df.drop(['id'], axis = 1), axes.ravel()):\n    sns.scatterplot(x=name, y='age', ax=ax, data=df)"
  },
  {
    "objectID": "posts/2021-05-31-sklearn-preprocessing-example.html#model-without-scaling",
    "href": "posts/2021-05-31-sklearn-preprocessing-example.html#model-without-scaling",
    "title": "Preprocessing example in Sklearn",
    "section": "Model without scaling",
    "text": "Model without scaling\nRemove categories for this example\n\nfeature_names = X.columns\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknr = KNeighborsClassifier().fit(X_train, y_train)\nknr.score(X_train, y_train)\n\n0.9810126582278481\n\n\n\nknr.score(X_test, y_test)\n\n0.9821133736929004"
  },
  {
    "objectID": "posts/2021-05-31-sklearn-preprocessing-example.html#model-with-scaling",
    "href": "posts/2021-05-31-sklearn-preprocessing-example.html#model-with-scaling",
    "title": "Preprocessing example in Sklearn",
    "section": "Model with scaling",
    "text": "Model with scaling\n\nScale first!\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\n\n\nimport pandas as pd\nX_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_names)\n\n\nX_train_scaled_df.plot(kind='box');\n\n\n\n\n\n\nTrain model on scaled data\n\nknr = KNeighborsClassifier().fit(X_train_scaled, y_train)\nknr.score(X_train_scaled, y_train)\n\n0.9805081636396992\n\n\n\nX_test_scaled = scaler.transform(X_test)\nknr.score(X_test_scaled, y_test)\n\n0.9815630159603742\n\n\n\n# %load solutions/03-ex1-solution.py\nfrom sklearn.svm import SVR\n\nsvr_unscaled = SVR()\n\nsvr_unscaled.fit(X_train, y_train)\n\nsvr_unscaled.score(X_test, y_test)\n\nsvr_scaled = SVR()\nsvr_scaled.fit(X_train_scaled, y_train)\n\nsvr_scaled.score(X_test_scaled, y_test)\n\n-0.38405905025130793"
  },
  {
    "objectID": "posts/2021-05-31-sklearn-preprocessing-example.html#tree-based-models",
    "href": "posts/2021-05-31-sklearn-preprocessing-example.html#tree-based-models",
    "title": "Preprocessing example in Sklearn",
    "section": "Tree based models",
    "text": "Tree based models\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ntree = DecisionTreeClassifier(random_state=0, max_depth=3).fit(X_train, y_train)\ntree.score(X_test, y_test)\n\n0.9822509631260319\n\n\n\ntree_scaled = DecisionTreeClassifier(random_state=0, max_depth=3).fit(X_train_scaled, y_train)\ntree_scaled.score(X_test_scaled, y_test)\n\n0.9822509631260319\n\n\n\nWhy are the scores the same?\n\nfrom sklearn.tree import plot_tree\nsns.reset_orig()\nfig, ax = plt.subplots(figsize=(20, 10))\n_ = plot_tree(tree, ax=ax, fontsize=16, feature_names=feature_names)\n\n\n\n\n\nfrom sklearn.tree import plot_tree\nsns.reset_orig()\nfig, ax = plt.subplots(figsize=(20, 10))\n_ = plot_tree(tree_scaled, ax=ax, fontsize=16, feature_names=feature_names)"
  },
  {
    "objectID": "posts/2020-10-17-dask-nlp-gutenberg-books.html",
    "href": "posts/2020-10-17-dask-nlp-gutenberg-books.html",
    "title": "Using Dask with dask.bag and regex to parse Notes from the Underground from project gutenberg",
    "section": "",
    "text": "import dask.bag as db\nimport re\n\n\nbook_bag = db.from_url('https://www.gutenberg.org/cache/epub/600/pg600.txt')\n\n\nbook_bag.take(5)\n\n(b\"\\xef\\xbb\\xbfProject Gutenberg's Notes from the Underground, by Feodor Dostoevsky\\r\\n\",\n b'\\r\\n',\n b'This eBook is for the use of anyone anywhere at no cost and with\\r\\n',\n b'almost no restrictions whatsoever.  You may copy it, give it away or\\r\\n',\n b're-use it under the terms of the Project Gutenberg License included\\r\\n')\n\n\n\nremove_spaces = book_bag.map(lambda x:x.strip())\n\n\nremove_spaces.take(10)\n\n(b\"\\xef\\xbb\\xbfProject Gutenberg's Notes from the Underground, by Feodor Dostoevsky\",\n b'',\n b'This eBook is for the use of anyone anywhere at no cost and with',\n b'almost no restrictions whatsoever.  You may copy it, give it away or',\n b're-use it under the terms of the Project Gutenberg License included',\n b'with this eBook or online at www.gutenberg.net',\n b'',\n b'',\n b'Title: Notes from the Underground',\n b'')\n\n\n\ndef decode_to_ascii(x):\n    return x.decode(\"ascii\",\"ignore\") \n\n\nascii_text = remove_spaces.map(decode_to_ascii)\n\n\nascii_text.take(10)\n\n(\"Project Gutenberg's Notes from the Underground, by Feodor Dostoevsky\",\n '',\n 'This eBook is for the use of anyone anywhere at no cost and with',\n 'almost no restrictions whatsoever.  You may copy it, give it away or',\n 're-use it under the terms of the Project Gutenberg License included',\n 'with this eBook or online at www.gutenberg.net',\n '',\n '',\n 'Title: Notes from the Underground',\n '')\n\n\n\ndef remove_punctuation(x):\n    return re.sub(r'[^\\w\\s]','',x)\n\n\nremove_punctuation = ascii_text.map(remove_punctuation)\n\n\nremove_punctuation.take(10)\n\n('Project Gutenbergs Notes from the Underground by Feodor Dostoevsky',\n '',\n 'This eBook is for the use of anyone anywhere at no cost and with',\n 'almost no restrictions whatsoever  You may copy it give it away or',\n 'reuse it under the terms of the Project Gutenberg License included',\n 'with this eBook or online at wwwgutenbergnet',\n '',\n '',\n 'Title Notes from the Underground',\n '')\n\n\n\nlower_text = remove_punctuation.map(str.lower)\n\n\nlower_text.take(10)\n\n('project gutenbergs notes from the underground by feodor dostoevsky',\n '',\n 'this ebook is for the use of anyone anywhere at no cost and with',\n 'almost no restrictions whatsoever  you may copy it give it away or',\n 'reuse it under the terms of the project gutenberg license included',\n 'with this ebook or online at wwwgutenbergnet',\n '',\n '',\n 'title notes from the underground',\n '')\n\n\n\nsplit_word_list = lower_text.map(lambda x: x.split(' '))\n\n\nsplit_word_list.take(10)\n\n(['project',\n  'gutenbergs',\n  'notes',\n  'from',\n  'the',\n  'underground',\n  'by',\n  'feodor',\n  'dostoevsky'],\n [''],\n ['this',\n  'ebook',\n  'is',\n  'for',\n  'the',\n  'use',\n  'of',\n  'anyone',\n  'anywhere',\n  'at',\n  'no',\n  'cost',\n  'and',\n  'with'],\n ['almost',\n  'no',\n  'restrictions',\n  'whatsoever',\n  '',\n  'you',\n  'may',\n  'copy',\n  'it',\n  'give',\n  'it',\n  'away',\n  'or'],\n ['reuse',\n  'it',\n  'under',\n  'the',\n  'terms',\n  'of',\n  'the',\n  'project',\n  'gutenberg',\n  'license',\n  'included'],\n ['with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergnet'],\n [''],\n [''],\n ['title', 'notes', 'from', 'the', 'underground'],\n [''])\n\n\n\ndef remove_empty_words(word_list):\n    return list(filter(lambda a: a != '', word_list))\n\nnon_empty_words = split_word_list.filter(remove_empty_words)\n\n\nnon_empty_words.take(10)\n\n(['project',\n  'gutenbergs',\n  'notes',\n  'from',\n  'the',\n  'underground',\n  'by',\n  'feodor',\n  'dostoevsky'],\n ['this',\n  'ebook',\n  'is',\n  'for',\n  'the',\n  'use',\n  'of',\n  'anyone',\n  'anywhere',\n  'at',\n  'no',\n  'cost',\n  'and',\n  'with'],\n ['almost',\n  'no',\n  'restrictions',\n  'whatsoever',\n  '',\n  'you',\n  'may',\n  'copy',\n  'it',\n  'give',\n  'it',\n  'away',\n  'or'],\n ['reuse',\n  'it',\n  'under',\n  'the',\n  'terms',\n  'of',\n  'the',\n  'project',\n  'gutenberg',\n  'license',\n  'included'],\n ['with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergnet'],\n ['title', 'notes', 'from', 'the', 'underground'],\n ['author', 'feodor', 'dostoevsky'],\n ['posting', 'date', 'september', '13', '2008', 'ebook', '600'],\n ['release', 'date', 'july', '1996'],\n ['language', 'english'])\n\n\n\nall_words = non_empty_words.flatten()\n\n\ntype(all_words)\n\ndask.bag.core.Bag\n\n\n\nall_words.take(30)\n\n('project',\n 'gutenbergs',\n 'notes',\n 'from',\n 'the',\n 'underground',\n 'by',\n 'feodor',\n 'dostoevsky',\n 'this',\n 'ebook',\n 'is',\n 'for',\n 'the',\n 'use',\n 'of',\n 'anyone',\n 'anywhere',\n 'at',\n 'no',\n 'cost',\n 'and',\n 'with',\n 'almost',\n 'no',\n 'restrictions',\n 'whatsoever',\n '',\n 'you',\n 'may')\n\n\n\nchange_to_key_value = all_words.map(lambda x: (x, 1))\n\n\nchange_to_key_value.take(4)\n\n(('project', 1), ('gutenbergs', 1), ('notes', 1), ('from', 1))\n\n\n\ngrouped_words = all_words.groupby(lambda x:x)\n\n\ngrouped_words.take(1)\n\n(('project',\n  ['project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project']),)\n\n\n\nword_count = grouped_words.map(lambda x: (x[0], len(x[1])))\n\n\nword_count.take(10)\n\n(('project', 87),\n ('gutenbergs', 2),\n ('notes', 11),\n ('from', 186),\n ('the', 1555),\n ('underground', 26),\n ('by', 153),\n ('feodor', 3),\n ('dostoevsky', 3),\n ('this', 237))\n\n\n\nchange_to_key_value.take(10)\n\n(('project', 1),\n ('gutenbergs', 1),\n ('notes', 1),\n ('from', 1),\n ('the', 1),\n ('underground', 1),\n ('by', 1),\n ('feodor', 1),\n ('dostoevsky', 1),\n ('this', 1))\n\n\n\n# Take a running count of a word\n# In this case, the default value of \n# count needs to be provided\ndef add_bin_op(count, x):\n    return count + x[1]\n\n# Take the output from multiple bin_op(s)\n# and add them to get the total count of\n# a word\ndef add_combine_op(x, y):\n    return x + y\n\nword_count = change_to_key_value.foldby(lambda x: x[0],\n                                       add_bin_op, 0,\n                                       add_combine_op)\n\n\nword_count.take(10)\n\n(('project', 87),\n ('gutenbergs', 2),\n ('notes', 11),\n ('from', 186),\n ('the', 1555),\n ('underground', 26),\n ('by', 153),\n ('feodor', 3),\n ('dostoevsky', 3),\n ('this', 237))\n\n\n\nmuch_easier = all_words.frequencies()\n\n\nmuch_easier.take(10)\n\n(('project', 87),\n ('gutenbergs', 2),\n ('notes', 11),\n ('from', 186),\n ('the', 1555),\n ('underground', 26),\n ('by', 153),\n ('feodor', 3),\n ('dostoevsky', 3),\n ('this', 237))\n\n\n\nRemoving stop words in top word frequency counts\n\nfrom spacy.lang.en import STOP_WORDS\n\n\nwithout_stopwords = all_words.filter(lambda x: x not in STOP_WORDS)\n\n\nnew_freq = without_stopwords.frequencies()\n\n\nnew_freq.take(20)\n\n(('project', 87),\n ('gutenbergs', 2),\n ('notes', 11),\n ('underground', 26),\n ('feodor', 3),\n ('dostoevsky', 3),\n ('ebook', 9),\n ('use', 18),\n ('cost', 5),\n ('restrictions', 3),\n ('whatsoever', 2),\n ('', 1896),\n ('copy', 12),\n ('away', 59),\n ('reuse', 2),\n ('terms', 24),\n ('gutenberg', 28),\n ('license', 15),\n ('included', 6),\n ('online', 4))\n\n\n\nnew_freq.topk(10)\n\ndask.bag<topk-aggregate, npartitions=1>\n\n\n\nnew_freq.topk(10, key=lambda x: x[1]).compute()\n\n[('', 1896),\n ('man', 122),\n ('know', 90),\n ('project', 87),\n ('time', 83),\n ('like', 82),\n ('come', 74),\n ('course', 73),\n ('love', 72),\n ('life', 69)]"
  },
  {
    "objectID": "posts/2020-10-02-databases_sqllite_sqlalchemy_27-copy1.html",
    "href": "posts/2020-10-02-databases_sqllite_sqlalchemy_27-copy1.html",
    "title": "Creating and accessing SQL databases with python using sqlalchemy and sqlite",
    "section": "",
    "text": "import sqlalchemy as db\nimport sqlite3\nimport pandas as pd\n\n\nfrom sqlalchemy import Column, Integer, String, ForeignKey, create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import relationship, backref, sessionmaker, joinedload\n\n# For this example we will use an in-memory sqlite DB.\n# Let's also configure it to echo everything it does to the screen.\nengine = create_engine('sqlite:///:memory:', echo=True)\n\n\n# The base class which our objects will be defined on.\nBase = declarative_base()\n\n# Our User object, mapped to the 'users' table\nclass User(Base):\n    __tablename__ = 'users'\n\n    # Every SQLAlchemy table should have a primary key named 'id'\n    id = Column(Integer, primary_key=True)\n\n    name = Column(String)\n    fullname = Column(String)\n    password = Column(String)\n\n    # Lets us print out a user object conveniently.\n    def __repr__(self):\n       return \"<User(name='%s', fullname='%s', password'%s')>\" % (\n                               self.name, self.fullname, self.password)\n\n\n# The Address object stores the addresses \n# of a user in the 'adressess' table.\nclass Address(Base):\n    __tablename__ = 'addresses'\n    id = Column(Integer, primary_key=True)\n    email_address = Column(String, nullable=False)\n\n    # Since we have a 1:n relationship, we need to store a foreign key \n    # to the users table.\n    user_id = Column(Integer, ForeignKey('users.id'))\n\n    # Defines the 1:n relationship between users and addresses.\n    # Also creates a backreference which is accessible from a User object.\n    user = relationship(\"User\", backref=backref('addresses'))\n\n    # Lets us print out an address object conveniently.\n    def __repr__(self):\n        return \"<Address(email_address='%s')>\" % self.email_address\n\n\n\n# Create all tables by issuing CREATE TABLE commands to the DB.\nBase.metadata.create_all(engine) \n\n# Creates a new session to the database by using the engine we described.\nSession = sessionmaker(bind=engine)\nsession = Session()\n\n# Let's create a user and add two e-mail addresses to that user.\nexample_user = User(name='example', fullname='example last_name_example', password='examplepassword')\nexample_user.addresses = [Address(email_address='example@gmail.com'), Address(email_address='example@yahoo.com')]\n\n# Let's add the user and its addresses we've created to the DB and commit.\nsession.add(example_user)\nsession.commit()\n\n# Now let's query the user that has the e-mail address ed@google.com\n# SQLAlchemy will construct a JOIN query automatically.\nuser_by_email = session.query(User)\\\n    .filter(Address.email_address=='example@gmail.com')\\\n    .first()\n\n2020-10-02 08:55:48,507 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info(\"users\")\n2020-10-02 08:55:48,508 INFO sqlalchemy.engine.base.Engine ()\n2020-10-02 08:55:48,510 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info(\"addresses\")\n2020-10-02 08:55:48,510 INFO sqlalchemy.engine.base.Engine ()\n2020-10-02 08:55:48,513 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)\n2020-10-02 08:55:48,514 INFO sqlalchemy.engine.base.Engine INSERT INTO users (name, fullname, password) VALUES (?, ?, ?)\n2020-10-02 08:55:48,515 INFO sqlalchemy.engine.base.Engine ('example', 'example last_name_example', 'examplepassword')\n2020-10-02 08:55:48,517 INFO sqlalchemy.engine.base.Engine INSERT INTO addresses (email_address, user_id) VALUES (?, ?)\n2020-10-02 08:55:48,517 INFO sqlalchemy.engine.base.Engine ('example@gmail.com', 1)\n2020-10-02 08:55:48,519 INFO sqlalchemy.engine.base.Engine INSERT INTO addresses (email_address, user_id) VALUES (?, ?)\n2020-10-02 08:55:48,519 INFO sqlalchemy.engine.base.Engine ('example@yahoo.com', 1)\n2020-10-02 08:55:48,520 INFO sqlalchemy.engine.base.Engine COMMIT\n2020-10-02 08:55:48,522 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)\n2020-10-02 08:55:48,523 INFO sqlalchemy.engine.base.Engine SELECT users.id AS users_id, users.name AS users_name, users.fullname AS users_fullname, users.password AS users_password \nFROM users, addresses \nWHERE addresses.email_address = ?\n LIMIT ? OFFSET ?\n2020-10-02 08:55:48,524 INFO sqlalchemy.engine.base.Engine ('example@gmail.com', 1, 0)\n\n\n\nprint(user_by_email)\n\n<User(name='example', fullname='example last_name_example', password'examplepassword')>\n\n\n\n# This will cause an additional query by lazy loading from the DB.\nprint(user_by_email.addresses)\n\n2020-10-02 08:55:53,081 INFO sqlalchemy.engine.base.Engine SELECT addresses.id AS addresses_id, addresses.email_address AS addresses_email_address, addresses.user_id AS addresses_user_id \nFROM addresses \nWHERE ? = addresses.user_id\n2020-10-02 08:55:53,083 INFO sqlalchemy.engine.base.Engine (1,)\n[<Address(email_address='example@gmail.com')>, <Address(email_address='example@yahoo.com')>]\n\n\n\n# To avoid querying again when getting all addresses of a user,\n# we use the joinedload option. SQLAlchemy will load all results and hide\n# the duplicate entries from us, so we can then get for\n# the user's addressess without an additional query to the DB.\nuser_by_email = session.query(User)\\\n    .filter(Address.email_address=='example@gmail.com')\\\n    .options(joinedload(User.addresses))\\\n    .first()\n\n2020-10-02 08:56:04,305 INFO sqlalchemy.engine.base.Engine SELECT anon_1.users_id AS anon_1_users_id, anon_1.users_name AS anon_1_users_name, anon_1.users_fullname AS anon_1_users_fullname, anon_1.users_password AS anon_1_users_password, addresses_1.id AS addresses_1_id, addresses_1.email_address AS addresses_1_email_address, addresses_1.user_id AS addresses_1_user_id \nFROM (SELECT users.id AS users_id, users.name AS users_name, users.fullname AS users_fullname, users.password AS users_password \nFROM users, addresses \nWHERE addresses.email_address = ?\n LIMIT ? OFFSET ?) AS anon_1 LEFT OUTER JOIN addresses AS addresses_1 ON anon_1.users_id = addresses_1.user_id\n2020-10-02 08:56:04,306 INFO sqlalchemy.engine.base.Engine ('example@gmail.com', 1, 0)\n\n\n\nprint(user_by_email)\n\n<User(name='example', fullname='example last_name_example', password'examplepassword')>\n\n\n\nprint(user_by_email.addresses)\n\n[<Address(email_address='example@gmail.com')>, <Address(email_address='example@yahoo.com')>]"
  },
  {
    "objectID": "posts/2020-06-07-kwargs-decorators.html",
    "href": "posts/2020-06-07-kwargs-decorators.html",
    "title": "A timer for ML functions",
    "section": "",
    "text": "toc: true- branch: master- badges: true\ncomments: true\nauthor: David Kearney\ncategories: [timer, jupyter]\ndescription: A timer for ML functions\ntitle: A timer for ML functions\n\n\n\nCode\nfrom functools import wraps\nimport time\n\n\ndef timer(func):\n    \"\"\"[This decorator is a timer for functions]\n\n    Args:\n        func ([function]): [This decorator takes a function as argument]\n\n    Returns:\n        [string]: [states the duration of time between the function begining and ending]\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        print(f\"{func.__name__!r} begins\")\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        print(f\"{func.__name__!r} ends in {time.time()-start_time}  secs\")\n        return result\n    return wrapper\n\n\n\n@timer\ndef model_metrics(*args, **kwargs):\n    \"\"\"[This is a function to print model metrics of interest]\n    \"\"\"\n    print(\"Model ID Number:\", args)\n    print(\"Metric of Interest:\", kwargs)\n\n\nmodel_metrics(1, 2, 10, key=\"word\", key2=\"word2\", numtrees=\"200\")\n\n\nfrom collections import Counter\nimport math, random\n\n#\n# data splitting\n#\n\ndef split_data(data, prob):\n    \"\"\"split data into fractions [prob, 1 - prob]\"\"\"\n    results = [], []\n    for row in data:\n        results[0 if random.random() < prob else 1].append(row)\n    return results\n\ndef train_test_split(x, y, test_pct):\n    data = list(zip(x, y))                        # pair corresponding values\n    train, test = split_data(data, 1 - test_pct)  # split the dataset of pairs\n    x_train, y_train = list(zip(*train))          # magical un-zip trick\n    x_test, y_test = list(zip(*test))\n    return x_train, x_test, y_train, y_test\n\n#\n# correctness\n#\n\ndef accuracy(tp, fp, fn, tn):\n    correct = tp + tn\n    total = tp + fp + fn + tn\n    return correct / total\n\ndef precision(tp, fp, fn, tn):\n    return tp / (tp + fp)\n\ndef recall(tp, fp, fn, tn):\n    return tp / (tp + fn)\n\ndef f1_score(tp, fp, fn, tn):\n    p = precision(tp, fp, fn, tn)\n    r = recall(tp, fp, fn, tn)\n\n    return 2 * p * r / (p + r)\n\nif __name__ == \"__main__\":\n\n    print(\"accuracy(70, 4930, 13930, 981070)\", accuracy(70, 4930, 13930, 981070))\n    print(\"precision(70, 4930, 13930, 981070)\", precision(70, 4930, 13930, 981070))\n    print(\"recall(70, 4930, 13930, 981070)\", recall(70, 4930, 13930, 981070))\n    print(\"f1_score(70, 4930, 13930, 981070)\", f1_score(70, 4930, 13930, 981070))\n\n\nfavorite_number = 7\n\n\ndef add(a, b):\n    return a + b\n\n\ndef sub(a, b):\n    return a - b\n\n\ndef multiply(a, b):\n    return a * b\n\n\ndef divide(a, b):\n    return a / b\n\n\ndef count_vowels(word):\n    count = 0\n    for letter in word.lower():\n        count += letter in 'aeiou'\n\n    return count\n\n\n# import example_module as sm\n\n# print(sm.favorite_number)\n\n# # add two numbers together\n# print(sm.add(3, 8))\n\n# # count the number of vowels in a string\n# print(sm.count_vowels('Testing'))\n\n\nimport pandas as pd\nfrom alive_progress import alive_bar, showtime, show_bars, show_spinners, config_handler\nconfig_handler.set_global(theme='ascii', spinner='notes', bar='solid')\n\nwith alive_bar(3) as bar:\n    df = pd.read_csv('https://gist.githubusercontent.com/davidrkearney/bb461ba351da484336a19bd00a2612e2/raw/18dd90b57fec46a247248d161ffd8085de2a00db/china_province_economicdata_1996_2007.csv')\n    bar('file read, printing file')\n    print(df.head)\n    bar('data printed ok, printing methods of data')\n    print(dir(df))\n    bar('process complete')\n\n\nfrom functools import wraps\nimport time\n\n\ndef timer(func):\n    \"\"\"[This decorator is a timer for functions]\n\n    Args:\n        func ([function]): [This decorator takes a function as argument]\n\n    Returns:\n        [string]: [states the duration of time between the function begining and ending]\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        print(f\"{func.__name__!r} begins\")\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        print(f\"{func.__name__!r} ends in {time.time()-start_time}  secs\")\n        return result\n    return wrapper\n\n\n\n@timer\ndef model_metrics(*args, **kwargs):\n    \"\"\"[This is a function to print model metrics of interest]\n    \"\"\"\n    print(\"Model ID Number:\", args)\n    print(\"Metric of Interest:\", kwargs)\n\n\nmodel_metrics(1, 2, 10, key=\"word\", key2=\"word2\", numtrees=\"200\")\n\nThis post includes code adapted from Data Science from Scratch"
  },
  {
    "objectID": "posts/2021-06-05-causalml-uplift_tree_visualization.html",
    "href": "posts/2021-06-05-causalml-uplift_tree_visualization.html",
    "title": "CausalML Uplift Tree Visualization",
    "section": "",
    "text": "# Code from https://github.com/uber/causalml/tree/master/examples"
  },
  {
    "objectID": "posts/2021-06-05-causalml-uplift_tree_visualization.html#introduction",
    "href": "posts/2021-06-05-causalml-uplift_tree_visualization.html#introduction",
    "title": "CausalML Uplift Tree Visualization",
    "section": "Introduction",
    "text": "Introduction\nThis example notebooks illustrates how to visualize uplift trees for interpretation and diagnosis.\n\nSupported Models\nThese visualization functions work only for tree-based algorithms:\n\nUplift tree/random forests on KL divergence, Euclidean Distance, and Chi-Square\nUplift tree/random forests on Contextual Treatment Selection\n\nCurrently, they are NOT supporting Meta-learner algorithms\n\nS-learner\nT-learner\nX-learner\nR-learner\n\n\n\nSupported Usage\nThis notebook will show how to use visualization for:\n\nUplift Tree and Uplift Random Forest\n\nVisualize a trained uplift classification tree model\nVisualize an uplift tree in a trained uplift random forests\n\nTraining and Validation Data\n\nVisualize the validation tree: fill the trained uplift classification tree with validation (or testing) data, and show the statistics for both training data and validation data\n\nOne Treatment Group and Multiple Treatment Groups\n\nVisualize the case where there are one control group and one treatment group\nVisualize the case where there are one control group and multiple treatment groups"
  },
  {
    "objectID": "posts/2021-06-05-causalml-uplift_tree_visualization.html#step-1-load-modules",
    "href": "posts/2021-06-05-causalml-uplift_tree_visualization.html#step-1-load-modules",
    "title": "CausalML Uplift Tree Visualization",
    "section": "Step 1 Load Modules",
    "text": "Step 1 Load Modules\n\nLoad CausalML modules\n\nfrom causalml.dataset import make_uplift_classification\nfrom causalml.inference.tree import UpliftTreeClassifier, UpliftRandomForestClassifier\nfrom causalml.inference.tree import uplift_tree_string, uplift_tree_plot\n\n\n\nLoad standard modules\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\nfrom sklearn.model_selection import train_test_split"
  },
  {
    "objectID": "posts/2021-06-05-causalml-uplift_tree_visualization.html#one-control-one-treatment-for-uplift-classification-tree",
    "href": "posts/2021-06-05-causalml-uplift_tree_visualization.html#one-control-one-treatment-for-uplift-classification-tree",
    "title": "CausalML Uplift Tree Visualization",
    "section": "One Control + One Treatment for Uplift Classification Tree",
    "text": "One Control + One Treatment for Uplift Classification Tree\n\n# Data generation\ndf, x_names = make_uplift_classification()\n\n# Rename features for easy interpretation of visualization\nx_names_new = ['feature_%s'%(i) for i in range(len(x_names))]\nrename_dict = {x_names[i]:x_names_new[i] for i in range(len(x_names))}\ndf = df.rename(columns=rename_dict)\nx_names = x_names_new\n\ndf.head()\n\ndf = df[df['treatment_group_key'].isin(['control','treatment1'])]\n\n# Look at the conversion rate and sample size in each group\ndf.pivot_table(values='conversion',\n               index='treatment_group_key',\n               aggfunc=[np.mean, np.size],\n               margins=True)\n\n\n\n\n\n  \n    \n      \n      mean\n      size\n    \n    \n      \n      conversion\n      conversion\n    \n    \n      treatment_group_key\n      \n      \n    \n  \n  \n    \n      control\n      0.5110\n      1000\n    \n    \n      treatment1\n      0.5140\n      1000\n    \n    \n      All\n      0.5125\n      2000\n    \n  \n\n\n\n\n\n# Split data to training and testing samples for model validation (next section)\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=111)\n\n# Train uplift tree\nuplift_model = UpliftTreeClassifier(max_depth = 4, min_samples_leaf = 200, min_samples_treatment = 50, n_reg = 100, evaluationFunction='KL', control_name='control')\n\nuplift_model.fit(df_train[x_names].values,\n                 treatment=df_train['treatment_group_key'].values,\n                 y=df_train['conversion'].values)\n\n\n# Print uplift tree as a string\nresult = uplift_tree_string(uplift_model.fitted_uplift_tree, x_names)\n\nfeature_17 >= -0.44234212654232735?\nyes -> feature_10 >= 1.020659213325515?\n        yes -> {'treatment1': 0.606557, 'control': 0.381356}\n        no  -> {'treatment1': 0.526786, 'control': 0.507812}\nno  -> feature_9 >= 0.8142773340486678?\n        yes -> {'treatment1': 0.61, 'control': 0.459677}\n        no  -> feature_4 >= 0.280545459525536?\n                yes -> {'treatment1': 0.41433, 'control': 0.552288}\n                no  -> {'treatment1': 0.574803, 'control': 0.507042}\n\n\n\nRead the tree\n\nFirst line: node split condition\nimpurity: the value for the loss function\ntotal_sample: total sample size in this node\ngroup_sample: sample size by treatment group\nuplift score: the treatment effect between treatment and control (when there are multiple treatment groups, this is the maximum of the treatment effects)\nuplift p_value: the p_value for the treatment effect\nvalidation uplift score: when validation data is filled in the tree, this reflects the uplift score based on the - validation data. It can be compared with the uplift score (for training data) to check if there are over-fitting issue.\n\n\n# Plot uplift tree\ngraph = uplift_tree_plot(uplift_model.fitted_uplift_tree,x_names)\nImage(graph.create_png())\n\n\n\n\n\n\nVisualize Validation Tree: One Control + One Treatment for Uplift Classification Tree\nNote the validation uplift score will update.\n\n### Fill the trained tree with testing data set \n# The uplift score based on testing dataset is shown as validation uplift score in the tree nodes\nuplift_model.fill(X=df_test[x_names].values, treatment=df_test['treatment_group_key'].values, y=df_test['conversion'].values)\n\n# Plot uplift tree\ngraph = uplift_tree_plot(uplift_model.fitted_uplift_tree,x_names)\nImage(graph.create_png())\n\n\n\n\n\n\nVisualize a Tree in Random Forest\n\n# Split data to training and testing samples for model validation (next section)\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=111)\n\n# Train uplift tree\nuplift_model = UpliftRandomForestClassifier(n_estimators=5, max_depth = 5, min_samples_leaf = 200, min_samples_treatment = 50, n_reg = 100, evaluationFunction='KL', control_name='control')\n\nuplift_model.fit(df_train[x_names].values,\n                 treatment=df_train['treatment_group_key'].values,\n                 y=df_train['conversion'].values)\n\n\n# Specify a tree in the random forest (the index can be any integer from 0 to n_estimators-1)\nuplift_tree = uplift_model.uplift_forest[0]\n# Print uplift tree as a string\nresult = uplift_tree_string(uplift_tree.fitted_uplift_tree, x_names)\n\nfeature_9 >= 0.7626607142400706?\nyes -> {'treatment1': 0.621795, 'control': 0.481707}\nno  -> feature_12 >= 0.5486596851987631?\n        yes -> feature_4 >= 0.9956888137470166?\n                yes -> {'treatment1': 0.496815, 'control': 0.398773}\n                no  -> {'treatment1': 0.483871, 'control': 0.551515}\n        no  -> feature_9 >= -0.15675867904794422?\n                yes -> {'treatment1': 0.456376, 'control': 0.48538}\n                no  -> {'treatment1': 0.40625, 'control': 0.625}\n\n\n\n# Plot uplift tree\ngraph = uplift_tree_plot(uplift_tree.fitted_uplift_tree,x_names)\nImage(graph.create_png())\n\n\n\n\n\nFill the tree with validation data\n\n### Fill the trained tree with testing data set \n# The uplift score based on testing dataset is shown as validation uplift score in the tree nodes\nuplift_tree.fill(X=df_test[x_names].values, treatment=df_test['treatment_group_key'].values, y=df_test['conversion'].values)\n\n# Plot uplift tree\ngraph = uplift_tree_plot(uplift_tree.fitted_uplift_tree,x_names)\nImage(graph.create_png())"
  },
  {
    "objectID": "posts/2021-06-05-causalml-uplift_tree_visualization.html#one-control-multiple-treatments",
    "href": "posts/2021-06-05-causalml-uplift_tree_visualization.html#one-control-multiple-treatments",
    "title": "CausalML Uplift Tree Visualization",
    "section": "One Control + Multiple Treatments",
    "text": "One Control + Multiple Treatments\n\n# Data generation\ndf, x_names = make_uplift_classification()\n# Look at the conversion rate and sample size in each group\ndf.pivot_table(values='conversion',\n               index='treatment_group_key',\n               aggfunc=[np.mean, np.size],\n               margins=True)\n\n\n\n\n\n  \n    \n      \n      mean\n      size\n    \n    \n      \n      conversion\n      conversion\n    \n    \n      treatment_group_key\n      \n      \n    \n  \n  \n    \n      control\n      0.511\n      1000\n    \n    \n      treatment1\n      0.514\n      1000\n    \n    \n      treatment2\n      0.559\n      1000\n    \n    \n      treatment3\n      0.600\n      1000\n    \n    \n      All\n      0.546\n      4000\n    \n  \n\n\n\n\n\n# Split data to training and testing samples for model validation (next section)\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=111)\n\n# Train uplift tree\nuplift_model = UpliftTreeClassifier(max_depth = 3, min_samples_leaf = 200, min_samples_treatment = 50, n_reg = 100, evaluationFunction='KL', control_name='control')\n\nuplift_model.fit(df_train[x_names].values,\n                 treatment=df_train['treatment_group_key'].values,\n                 y=df_train['conversion'].values)\n\n\n# Plot uplift tree\n# The uplift score represents the best uplift score among all treatment effects\ngraph = uplift_tree_plot(uplift_model.fitted_uplift_tree,x_names)\nImage(graph.create_png())\n\n\n\n\n\nSave the Plot\n\n# Save the graph as pdf\ngraph.write_pdf(\"tbc.pdf\")\n# Save the graph as png\ngraph.write_png(\"tbc.png\")\n\nTrue"
  },
  {
    "objectID": "posts/2021-02-01-newsplease.html",
    "href": "posts/2021-02-01-newsplease.html",
    "title": "Using NewsPlease an open source, news crawler that extracts structured information",
    "section": "",
    "text": "from newsplease import NewsPlease\narticle = NewsPlease.from_url('https://www.bbc.com/vietnamese/vert-earth-55751040')\nprint(article.title)\n\nNhng loi vt chu c  lnh lo khng khip nht hnh tinh\n\n\n\narticle.authors\n\n['Https', 'Www.Facebook.Com Bbcnews']\n\n\n\narticle.date_publish\n\ndatetime.datetime(2021, 1, 25, 0, 0)\n\n\n\narticle.description\n\n'Mt s loi ng vt k l  c chin lc thng minh v tin ha  sinh tn c trong nhng iu kin kh hu khng khip nht.'\n\n\n\narticle.filename\n\n'https%3A%2F%2Fwww.bbc.com%2Fvietnamese%2Fvert-earth-55751040.json'\n\n\n\narticle.language\n\n'vi'\n\n\n\narticle.maintext\n\n'Nhng loi vt chu c  lnh lo khng khip nht hnh tinh\\nDale Shaw\\nBBC Earth\\nNgun hnh nh, Getty Images\\nLoi ng vt no c th sng st trong iu kin nhit  thp nht?\\nChng ta ang ni ti nhng ng vt sng  x hn i v i nht trn hnh tinh.\\nng vy, nhng loi  x lnh, hay ni cch khc, l nhng sinh vt c th thch nghi v pht trin trong thi tit lnh (t chionophile trong ting Anh c ngha l \"nhng k m tuyt\").\\nNhng loi no c th pht trin trong iu kin nhit  thp nht? Sau y l mt trong s nhng loi hay ho nht trn Tri t lm c vy\\nNgun hnh nh, Getty Images Chp li hnh nh, Chim cnh ct hong  sng st nh kh nng hot ng tp hp sc mnh tp th - chng m nhau  gi m\\nChim cnh ct Hong \\nGi lnh  Nam Cc lm nhit  gim xung n -60  C khin ta run cm cp, y l thch thc vi chim cnh ct hong .\\ny l loi chim cnh ct ln nht trn hnh tinh - v v cao ln nh vy nn chng d b hng nhng t gi lnh but hn so vi cc loi kch c nh hn sng  .\\nNhng chng vn c kh nng sng st nh vo hot ng ng i, m nhau gi m trong sut nhng thng lnh nht  Nam Cc.\\nNhng ch chim cnh ct ng gia nhm m nhau l nhng con m nht, v khi  m p ri, chng s lun phin chui ra ngoi  cc ch chim bn ngoi c th thay phin i vo gia si m.\\nNgun hnh nh, Getty Images Chp li hnh nh, Sc Bc Cc trnh ci lnh khc nghit nh o hang su vo lng t\\nSc t Bc Cc\\nNu tnh hnh hin thi ca th gii khin bn mun i ng ng cho ri, th bn c th xem xt loi sc t Bc Cc  chn lm hnh tng noi theo.\\nSng  vng lnh nguyn Bc Cc  Bc M, ni nhit  c th xung di -63  C, chng c th thot khi ma ng ti t nht nh vo vic o hang di lng t v ng mt mch tm thng trong nm.\\nKhi ng ng, nhit  trong no cc ch sc ny c th gim xung ch va trn mc ng bng, trong khi nhit  c th c th gim ti mc -2,9  C v nhp tim gim xung ch cn mt nhp mi pht.\\nKhi thi gian ng ng kt thc, cc ch sc ny cn khong ba gi  lm m c th tr li.\\nNgun hnh nh, Getty Images Chp li hnh nh, Hi cu Weddel dng rng  o h trong khi bng i dng  th\\nHi cu Weddell\\nNu bn mun tm mt loi ng vt c v yu thch ci lnh trong tng giy pht, ti gii thiu bn vi hi cu Weddel.\\nChng l loi sng  min nam xa xi nht so vi bt k ging hi cu no khc, v dnh hu ht thi gian sng bn di lp bng Nam Cc, ni chng c th sn mi v trnh b c voi st th bt.\\nLn su xung  su hn 2.000 feet, chng c th  di nc n 45 pht v nu khng th p v bng  ngp kh oxy cn thit, chng s dng rng  o cho mnh mt l ly kh.\\nNc bin Nam Cc thc ra m hn so vi khng kh trn b mt (vn c th gim xung -70  C), v vy c bit trong nhng trn bo ma ng d di, hi cu s gi m bng cch ln xung bin.\\nNgun hnh nh, Getty Images Chp li hnh nh, C mp khng l Greenland c th chu c nc bin lnh but nh nhng ni th ny\\nSng  vng nc su trong khu vc Bc i Ty Dng v Bc Cc, nhng con c mp khng l ny khng ch sng st trong ln nc lnh cm, m cn c tui th di nht so vi bt c loi c xng sng no trn hnh tinh.\\nChng c tui th trung bnh t 300 n 500 nm tui.\\nNhp trao i cht cc k chm ca chng gip kim sot nhit  v ko di tui th, v chng cng c th l loi c mp ln nht, c th t ti 6,4m chiu di v nng khong 1,000kg.\\nD vy, Usain Bolt khng c g phi lo lng, v chng ch c th tng tc  bi n 1,6 dm/gi, v do vy, khng c g ngc nhin khi chng thng sn con mi  i ng.\\nNgun hnh nh, Getty Images Chp li hnh nh, Lp lng dy mn gip chim sheathbill tuyt gi nhit tt\\nChim sheathbill tuyt\\nLoi chim trng ging b cu, trng phau v dng cm ny l loi chim bn a duy nht sng trn b mt Nam Cc v l loi duy nht sinh sn ti y.\\nVi lp lng dy gip gi m, chng hu nh sng trn mt t, c gng lm lt thc n tha ri vi ca cc loi chim khc. iu ny tht hay.\\nChng cng l loi chim duy nht  Nam Cc khng c mng chn.\\nVy lm sao chng c th gi ngn chn m p, trong khi li dnh qu nhiu thi gian trn b mt t lnh cng?\\nKhng c gii php sinh hc k diu no  y c, chng ch dnh nhiu thi gian nhy l c t chn ny i sang chn kia.\\nNgun hnh nh, Getty Images Chp li hnh nh, B kp  loi b x hng c th sng st l nh lp lng rm\\nB x hng\\nMt trong nhng sinh vt sng trn lnh nguyn xut hin  khu vc t Siberia n Greenland l loi b x hng. Loi ny c ci tn ni bt, c t theo mi hng kh chu thot ra trong ma ng c.\\nNhng con th k v ny  sng st gia mi trng khc nghit nht trong hng ngn nm qua nh vo lp lng dy rm.\\nLp lng rm rp v dy ni bt c lm t nhiu lp, vi phn bn ngoi - c gi l lp lng bo v - che ph cho lp lng th hai bn di, ngn hn, em li kh nng gi m tng cng trong nhng thng lnh nht.\\nNgun hnh nh, Getty Images Chp li hnh nh, Nh sng bn di lp b cy, nhng ch b cnh cng can trng ny c th sng st qua thng lnh nht trong ma ng\\nB cnh cng dt v cy \\nLoi b cnh cng di khong na inch ny c khu vc sinh sng t North Carolina n Vng Cc Bc.\\nChng sng bn di lp v cy v c th c cu to c ch ch  sinh tn trong iu kin khc nghit nht gia ma ng.\\nKh hu  vng Bc Cc vo thng Tm c tc dng nh kh hu ngh dng i vi loi ny.\\nTrong cc phng th nghim, chng c th chng chi c nhit  lnh cng n mc -150  C. Mc ny ng l cc lnh!'\n\n\n\narticle.url\n\n'https://www.bbc.com/vietnamese/vert-earth-55751040'"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html",
    "title": "Regression and Classification with Pyspark ML",
    "section": "",
    "text": "from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructField,StringType,IntegerType,StructType, DoubleType, FloatType\nfrom pyspark.sql.functions import *\n\ndata_schema = [\nStructField(\"_c0\", IntegerType(), True)\n,StructField(\"province\", StringType(), True)\n,StructField(\"specific\", DoubleType(), True)\n,StructField(\"general\", DoubleType(), True)\n,StructField(\"year\", IntegerType(), True)\n,StructField(\"gdp\", FloatType(), True)\n,StructField(\"fdi\", FloatType(), True)\n,StructField(\"rnr\", DoubleType(), True)\n,StructField(\"rr\", FloatType(), True)\n,StructField(\"i\", FloatType(), True)\n,StructField(\"fr\", IntegerType(), True)\n,StructField(\"reg\", StringType(), True)\n,StructField(\"it\", IntegerType(), True)\n]\n\nfinal_struc = StructType(fields=data_schema)\n\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").schema(final_struc).option(\"header\", True).load(file_location)\n\n#df.printSchema()\n\ndf.show()\n\n\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|     fdi| rnr|       rr|        i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661.0| 0.0|      0.0|      0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443.0| 0.0|      0.0|      0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673.0| 0.0|      0.0|      0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131.0|null|     null|     null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0|      0.0|      0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672.0| 0.0|      0.0|      0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0|      0.0|      0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0|      0.0|      0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669.0| 0.0|      0.0|      0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000.0| 0.0|      0.0|0.3243243|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354.0| 0.0|      0.0|0.3243243|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892.0| 0.0|      0.0|0.3243243|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290.0|null|     null|     null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286.0| 0.0|      0.0|      0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800.0| 0.0|      0.0|     0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525.0| 0.0|      0.0|     0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368.0| 0.0|      0.0|     0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818.0| 0.0|      0.0|     0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464.0| 0.0|      0.0|     0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126.0| 0.0|0.7948718|      0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.groupBy('province').count().show()\n\n\n+------------+-----+\n    province|count|\n+------------+-----+\n   Guangdong|   12|\n       Hunan|   12|\n      Shanxi|   12|\n       Tibet|   12|\n       Hubei|   12|\n     Tianjin|   12|\n     Beijing|   12|\nHeilongjiang|   12|\n    Liaoning|   12|\n       Henan|   12|\n       Anhui|   12|\n    Xinjiang|   12|\n      Fujian|   12|\n     Jiangxi|   12|\n       Jilin|   12|\n   Chongqing|   12|\n     Shaanxi|   12|\n     Sichuan|   12|\n      Yunnan|   12|\n       Gansu|   12|\n+------------+-----+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#imputation-of-mean-values-to-prepare-the-data",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#imputation-of-mean-values-to-prepare-the-data",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Imputation of mean values to prepare the data",
    "text": "Imputation of mean values to prepare the data\n\nmean_val = df.select(mean(df['general'])).collect()\nmean_val[0][0]\nmean_gen = mean_val[0][0]\ndf = df.na.fill(mean_gen,[\"general\"])\n\n\n\n\n\n\nmean_val = df.select(mean(df['specific'])).collect()\nmean_val[0][0]\nmean_gen = mean_val[0][0]\ndf = df.na.fill(mean_gen,[\"specific\"])\n\n\n\n\n\n\nmean_val = df.select(mean(df['rr'])).collect()\nmean_val[0][0]\nmean_gen = mean_val[0][0]\ndf = df.na.fill(mean_gen,[\"rr\"])\n\n\n\n\n\n\nmean_val = df.select(mean(df['fr'])).collect()\nmean_val[0][0]\nmean_gen = mean_val[0][0]\ndf = df.na.fill(mean_gen,[\"fr\"])\n\n\n\n\n\n\nmean_val = df.select(mean(df['rnr'])).collect()\nmean_val[0][0]\nmean_gen = mean_val[0][0]\ndf = df.na.fill(mean_gen,[\"rnr\"])\n\n\n\n\n\n\nmean_val = df.select(mean(df['i'])).collect()\nmean_val[0][0]\nmean_gen = mean_val[0][0]\ndf = df.na.fill(mean_gen,[\"i\"])"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#creating-binary-target-feature-from-extant-column-for-classification",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#creating-binary-target-feature-from-extant-column-for-classification",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Creating binary target feature from extant column for classification",
    "text": "Creating binary target feature from extant column for classification\n\nfrom pyspark.sql.functions import *\ndf = df.withColumn('specific_classification',when(df.specific >= 583470.7303370787,1).otherwise(0))"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#using-stringindexer-for-categorical-encoding-of-string-type-columns",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#using-stringindexer-for-categorical-encoding-of-string-type-columns",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Using StringIndexer for categorical encoding of string type columns",
    "text": "Using StringIndexer for categorical encoding of string type columns\n\nfrom pyspark.ml.feature import StringIndexer\n\n\n\n\n\n\nindexer = StringIndexer(inputCol=\"province\", outputCol=\"provinceIndex\")\ndf = indexer.fit(df).transform(df)\n\n\n\n\n\n\nindexer = StringIndexer(inputCol=\"reg\", outputCol=\"regionIndex\")\ndf = indexer.fit(df).transform(df)\n\n\n\n\n\n\ndf.show()\n\n\n+---+--------+---------+------------------+----+-------+--------+------------------+----------+----------+-------+-----------+-------+-----------------------+-------------+-----------+\n_c0|province| specific|           general|year|    gdp|     fdi|               rnr|        rr|         i|     fr|        reg|     it|specific_classification|provinceIndex|regionIndex|\n+---+--------+---------+------------------+----+-------+--------+------------------+----------+----------+-------+-----------+-------+-----------------------+-------------+-----------+\n  0|   Anhui| 147002.0|309127.53846153844|1996| 2093.3| 50661.0|               0.0|       0.0|       0.0|1128873| East China| 631930|                      0|          0.0|        0.0|\n  1|   Anhui| 151981.0|309127.53846153844|1997|2347.32| 43443.0|               0.0|       0.0|       0.0|1356287| East China| 657860|                      0|          0.0|        0.0|\n  2|   Anhui| 174930.0|309127.53846153844|1998|2542.96| 27673.0|               0.0|       0.0|       0.0|1518236| East China| 889463|                      0|          0.0|        0.0|\n  3|   Anhui| 285324.0|309127.53846153844|1999|2712.34| 26131.0|0.0355944252244898|0.05968862|0.08376352|1646891| East China|1227364|                      0|          0.0|        0.0|\n  4|   Anhui| 195580.0|           32100.0|2000|2902.09| 31847.0|               0.0|       0.0|       0.0|1601508| East China|1499110|                      0|          0.0|        0.0|\n  5|   Anhui| 250898.0|309127.53846153844|2001|3246.71| 33672.0|               0.0|       0.0|       0.0|1672445| East China|2165189|                      0|          0.0|        0.0|\n  6|   Anhui| 434149.0|           66529.0|2002|3519.72| 38375.0|               0.0|       0.0|       0.0|1677840| East China|2404936|                      0|          0.0|        0.0|\n  7|   Anhui| 619201.0|           52108.0|2003|3923.11| 36720.0|               0.0|       0.0|       0.0|1896479| East China|2815820|                      1|          0.0|        0.0|\n  8|   Anhui| 898441.0|          349699.0|2004| 4759.3| 54669.0|               0.0|       0.0|       0.0|2522449| East China|3422176|                      1|          0.0|        0.0|\n  9|   Anhui| 898441.0|309127.53846153844|2005|5350.17| 69000.0|               0.0|       0.0| 0.3243243|2522449| East China|3874846|                      1|          0.0|        0.0|\n 10|   Anhui|1457872.0|          279052.0|2006| 6112.5|139354.0|               0.0|       0.0| 0.3243243|3434548| East China|5167300|                      1|          0.0|        0.0|\n 11|   Anhui|2213991.0|          178705.0|2007|7360.92|299892.0|               0.0|       0.0| 0.3243243|4468640| East China|7040099|                      1|          0.0|        0.0|\n 12| Beijing| 165957.0|309127.53846153844|1996| 1789.2|155290.0|0.0355944252244898|0.05968862|0.08376352| 634562|North China| 508135|                      0|          1.0|        4.0|\n 13| Beijing| 165957.0|309127.53846153844|1997|2077.09|159286.0|               0.0|       0.0|       0.6| 634562|North China| 569283|                      0|          1.0|        4.0|\n 14| Beijing| 245198.0|309127.53846153844|1998|2377.18|216800.0|               0.0|       0.0|      0.53| 938788|North China| 695528|                      0|          1.0|        4.0|\n 15| Beijing| 388083.0|309127.53846153844|1999|2678.82|197525.0|               0.0|       0.0|      0.53|2522449|North China| 944047|                      0|          1.0|        4.0|\n 16| Beijing| 281769.0|          188633.0|2000|3161.66|168368.0|               0.0|       0.0|      0.53|1667114|North China| 757990|                      0|          1.0|        4.0|\n 17| Beijing| 441923.0|309127.53846153844|2001|3707.96|176818.0|               0.0|       0.0|      0.53|2093925|North China|1194728|                      0|          1.0|        4.0|\n 18| Beijing| 558569.0|          280277.0|2002| 4315.0|172464.0|               0.0|       0.0|      0.53|2511249|North China|1078754|                      0|          1.0|        4.0|\n 19| Beijing| 642581.0|          269596.0|2003|5007.21|219126.0|               0.0| 0.7948718|       0.0|2823366|North China|1426600|                      1|          1.0|        4.0|\n+---+--------+---------+------------------+----+-------+--------+------------------+----------+----------+-------+-----------+-------+-----------------------+-------------+-----------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#using-vectorassembler-to-prepare-features-for-machine-learning",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#using-vectorassembler-to-prepare-features-for-machine-learning",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Using VectorAssembler to prepare features for machine learning",
    "text": "Using VectorAssembler to prepare features for machine learning\n\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\n\n\n\n\n\ndf.columns\n\n\nOut[375]: ['_c0',\n 'province',\n 'specific',\n 'general',\n 'year',\n 'gdp',\n 'fdi',\n 'rnr',\n 'rr',\n 'i',\n 'fr',\n 'reg',\n 'it',\n 'specific_classification',\n 'provinceIndex',\n 'regionIndex']\n\n\n\nassembler = VectorAssembler(\n inputCols=[\n 'provinceIndex',\n# 'specific',\n 'general',\n 'year',\n 'gdp',\n 'fdi',\n #'rnr',\n #'rr',\n #'i',\n #'fr',\n 'regionIndex',\n 'it'\n ],\n outputCol=\"features\")\n\n\n\n\n\n\noutput = assembler.transform(df)\n\n\n\n\n\n\nfinal_data = output.select(\"features\", \"specific\")"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#spliting-data-into-train-and-test",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#spliting-data-into-train-and-test",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Spliting data into train and test",
    "text": "Spliting data into train and test\n\ntrain_data,test_data = final_data.randomSplit([0.7,0.3])"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#regression-with-pyspark-ml",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#regression-with-pyspark-ml",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Regression with Pyspark ML",
    "text": "Regression with Pyspark ML\n\nfrom pyspark.ml.regression import LinearRegression\nlr = LinearRegression(labelCol='specific')"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#fitting-the-linear-regression-model-to-the-training-data",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#fitting-the-linear-regression-model-to-the-training-data",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Fitting the linear regression model to the training data",
    "text": "Fitting the linear regression model to the training data\n\nlrModel = lr.fit(train_data)"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#coefficients-and-intercept-of-the-linear-regression-model",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#coefficients-and-intercept-of-the-linear-regression-model",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Coefficients and Intercept of the linear regression model",
    "text": "Coefficients and Intercept of the linear regression model\n\nprint(\"Coefficients: {} Intercept: {}\".format(lrModel.coefficients,lrModel.intercept))\n\n\nCoefficients: [-4936.461707001148,0.8007702471080539,-3994.683052325085,-7.5033201950338,0.42095493334994133,50994.51222529955,0.2531915644818595] Intercept: 7695214.561654471"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#evaluating-trained-linear-regression-model-on-the-test-data",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#evaluating-trained-linear-regression-model-on-the-test-data",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Evaluating trained linear regression model on the test data",
    "text": "Evaluating trained linear regression model on the test data\n\ntest_results = lrModel.evaluate(test_data)"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#metrics-of-trained-linear-regression-model-on-the-test-data-rmse-mse-r2",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#metrics-of-trained-linear-regression-model-on-the-test-data-rmse-mse-r2",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Metrics of trained linear regression model on the test data (RMSE, MSE, R2)",
    "text": "Metrics of trained linear regression model on the test data (RMSE, MSE, R2)\n\nprint(\"RMSE: {}\".format(test_results.rootMeanSquaredError))\nprint(\"MSE: {}\".format(test_results.meanSquaredError))\nprint(\"R2: {}\".format(test_results.r2))\n\n\nRMSE: 292695.0825058327\nMSE: 85670411323.0962\nR2: 0.7853651103073853"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#looking-at-correlations-with-corr",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#looking-at-correlations-with-corr",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Looking at correlations with corr",
    "text": "Looking at correlations with corr\n\nfrom pyspark.sql.functions import corr\n\n\n\n\n\n\ndf.select(corr('specific','gdp')).show()\n\n\n+-------------------+\ncorr(specific, gdp)|\n+-------------------+\n 0.5141876884991972|\n+-------------------+"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#classification-with-pyspark-ml",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#classification-with-pyspark-ml",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Classification with Pyspark ML",
    "text": "Classification with Pyspark ML\n\nfrom pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\nfrom pyspark.ml import Pipeline"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#decisiontreeclassifier-randomforestclassifier-and-gbtclassifier",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#decisiontreeclassifier-randomforestclassifier-and-gbtclassifier",
    "title": "Regression and Classification with Pyspark ML",
    "section": "DecisionTreeClassifier, RandomForestClassifier and GBTClassifier",
    "text": "DecisionTreeClassifier, RandomForestClassifier and GBTClassifier\n\ndtc = DecisionTreeClassifier(labelCol='specific_classification',featuresCol='features')\nrfc = RandomForestClassifier(labelCol='specific_classification',featuresCol='features')\ngbt = GBTClassifier(labelCol='specific_classification',featuresCol='features')"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#selecting-features-and-binary-target",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#selecting-features-and-binary-target",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Selecting features and binary target",
    "text": "Selecting features and binary target\n\nfinal_data = output.select(\"features\", \"specific_classification\")\ntrain_data,test_data = final_data.randomSplit([0.7,0.3])"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#fitting-the-classifiers-to-the-training-data",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#fitting-the-classifiers-to-the-training-data",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Fitting the Classifiers to the Training Data",
    "text": "Fitting the Classifiers to the Training Data\n\nrfc_model = rfc.fit(train_data)\ngbt_model = gbt.fit(train_data)\ndtc_model = dtc.fit(train_data)"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#classifier-predictions-on-test-data",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#classifier-predictions-on-test-data",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Classifier predictions on test data",
    "text": "Classifier predictions on test data\n\ndtc_predictions = dtc_model.transform(test_data)\nrfc_predictions = rfc_model.transform(test_data)\ngbt_predictions = gbt_model.transform(test_data)"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#evaluating-classifiers-using-pyspark.ml.evaluation-and-multiclassclassificationevaluator",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#evaluating-classifiers-using-pyspark.ml.evaluation-and-multiclassclassificationevaluator",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Evaluating Classifiers using pyspark.ml.evaluation and MulticlassClassificationEvaluator",
    "text": "Evaluating Classifiers using pyspark.ml.evaluation and MulticlassClassificationEvaluator\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n\n\n\n\n\nClassifier Accuracy\n\nacc_evaluator = MulticlassClassificationEvaluator(labelCol=\"specific_classification\", predictionCol=\"prediction\", metricName=\"accuracy\")"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#classifier-accuracy-metrics",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#classifier-accuracy-metrics",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Classifier Accuracy Metrics",
    "text": "Classifier Accuracy Metrics\n\ndtc_acc = acc_evaluator.evaluate(dtc_predictions)\nrfc_acc = acc_evaluator.evaluate(rfc_predictions)\ngbt_acc = acc_evaluator.evaluate(gbt_predictions)\n\n\n\n\n\n\nprint('-'*80)\nprint('Decision tree accuracy: {0:2.2f}%'.format(dtc_acc*100))\nprint('-'*80)\nprint('Random forest ensemble accuracy: {0:2.2f}%'.format(rfc_acc*100))\nprint('-'*80)\nprint('GBT accuracy: {0:2.2f}%'.format(gbt_acc*100))\nprint('-'*80)\n\n\n--------------------------------------------------------------------------------\nDecision tree accuracy: 81.98%\n--------------------------------------------------------------------------------\nRandom forest ensemble accuracy: 88.29%\n--------------------------------------------------------------------------------\nGBT accuracy: 81.08%\n--------------------------------------------------------------------------------"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#classification-correlation-with-corr",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#classification-correlation-with-corr",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Classification Correlation with Corr",
    "text": "Classification Correlation with Corr\n\ndf.select(corr('specific_classification','fdi')).show()\n\n\n+----------------------------------+\ncorr(specific_classification, fdi)|\n+----------------------------------+\n                 0.307429849493392|\n+----------------------------------+\n\n\n\n\n\ndf.select(corr('specific_classification','gdp')).show()\n\n\n+----------------------------------+\ncorr(specific_classification, gdp)|\n+----------------------------------+\n                 0.492176921599151|\n+----------------------------------+\n\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-10-07-fiscal_data-sqlitedb-copy1.html",
    "href": "posts/2020-10-07-fiscal_data-sqlitedb-copy1.html",
    "title": "Using sqlalchemy and pandas to read and write from and to a local sqlite database for Fiscal Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport pandas_datareader.data as web\nimport datetime\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndf=pd.read_csv('df_panel_fix.csv')\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      province\n      specific\n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      reg\n      it\n    \n  \n  \n    \n      0\n      0\n      Anhui\n      147002.0\n      NaN\n      1996\n      2093.30\n      50661\n      0.000000\n      0.000000\n      0.000000\n      1128873\n      East China\n      631930\n    \n    \n      1\n      1\n      Anhui\n      151981.0\n      NaN\n      1997\n      2347.32\n      43443\n      0.000000\n      0.000000\n      0.000000\n      1356287\n      East China\n      657860\n    \n    \n      2\n      2\n      Anhui\n      174930.0\n      NaN\n      1998\n      2542.96\n      27673\n      0.000000\n      0.000000\n      0.000000\n      1518236\n      East China\n      889463\n    \n    \n      3\n      3\n      Anhui\n      285324.0\n      NaN\n      1999\n      2712.34\n      26131\n      NaN\n      NaN\n      NaN\n      1646891\n      East China\n      1227364\n    \n    \n      4\n      4\n      Anhui\n      195580.0\n      32100.0\n      2000\n      2902.09\n      31847\n      0.000000\n      0.000000\n      0.000000\n      1601508\n      East China\n      1499110\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      355\n      Zhejiang\n      391292.0\n      260313.0\n      2003\n      9705.02\n      498055\n      1.214286\n      0.035714\n      0.035714\n      6217715\n      East China\n      2261631\n    \n    \n      356\n      356\n      Zhejiang\n      656175.0\n      276652.0\n      2004\n      11648.70\n      668128\n      1.214286\n      0.035714\n      0.035714\n      NaN\n      East China\n      3162299\n    \n    \n      357\n      357\n      Zhejiang\n      656175.0\n      NaN\n      2005\n      13417.68\n      772000\n      1.214286\n      0.035714\n      0.035714\n      NaN\n      East China\n      2370200\n    \n    \n      358\n      358\n      Zhejiang\n      1017303.0\n      394795.0\n      2006\n      15718.47\n      888935\n      1.214286\n      0.035714\n      0.035714\n      11537149\n      East China\n      2553268\n    \n    \n      359\n      359\n      Zhejiang\n      844647.0\n      0.0\n      2007\n      18753.73\n      1036576\n      0.047619\n      0.000000\n      0.000000\n      16494981\n      East China\n      2939778\n    \n  \n\n360 rows  13 columns\n\n\n\n\ndf_subset = df[[\"year\", \"reg\", \"province\", \"it\", \"specific\", 'gdp',\"fdi\"]]\ndf_subset\n\n\n\n\n\n  \n    \n      \n      year\n      reg\n      province\n      it\n      specific\n      gdp\n      fdi\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      631930\n      147002.0\n      2093.30\n      50661\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      657860\n      151981.0\n      2347.32\n      43443\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      889463\n      174930.0\n      2542.96\n      27673\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      1227364\n      285324.0\n      2712.34\n      26131\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      1499110\n      195580.0\n      2902.09\n      31847\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      2261631\n      391292.0\n      9705.02\n      498055\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      3162299\n      656175.0\n      11648.70\n      668128\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      2370200\n      656175.0\n      13417.68\n      772000\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      2553268\n      1017303.0\n      15718.47\n      888935\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      2939778\n      844647.0\n      18753.73\n      1036576\n    \n  \n\n360 rows  7 columns\n\n\n\n\ndf_subset.columns = [\"year\", \"region\", \"province\", \"it\", \"specific\", 'gdp',\"fdi\"]\n\n\ndf_subset\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      it\n      specific\n      gdp\n      fdi\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      631930\n      147002.0\n      2093.30\n      50661\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      657860\n      151981.0\n      2347.32\n      43443\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      889463\n      174930.0\n      2542.96\n      27673\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      1227364\n      285324.0\n      2712.34\n      26131\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      1499110\n      195580.0\n      2902.09\n      31847\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      2261631\n      391292.0\n      9705.02\n      498055\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      3162299\n      656175.0\n      11648.70\n      668128\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      2370200\n      656175.0\n      13417.68\n      772000\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      2553268\n      1017303.0\n      15718.47\n      888935\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      2939778\n      844647.0\n      18753.73\n      1036576\n    \n  \n\n360 rows  7 columns\n\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///fiscal.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nfiscal_table = db.Table('fiscal_table', metadata, \n    db.Column('year',db.Integer, nullable=True, index=False),\n    db.Column('region',db.Integer, nullable=True),\n    db.Column('province',db.Integer, nullable=True),\n    db.Column('it',db.Integer, nullable=True),\n    db.Column('specific',db.Integer, nullable=True),\n    db.Column('gdp',db.Integer, nullable=True),\n    db.Column('fdi', db.Numeric, nullable=True)\n)\n\n\nmetadata.create_all(engine) #Creates the table\n\n\ndf_subset.to_sql('fiscal_table', con=engine, if_exists='append', index=False)\n\n\nengine.execute(\"SELECT * FROM fiscal_table LIMIT 10\").fetchall()\n\n[(1996, 'East China', 'Anhui', 631930, 147002, 2093.3, 50661),\n (1997, 'East China', 'Anhui', 657860, 151981, 2347.32, 43443),\n (1998, 'East China', 'Anhui', 889463, 174930, 2542.96, 27673),\n (1999, 'East China', 'Anhui', 1227364, 285324, 2712.34, 26131),\n (2000, 'East China', 'Anhui', 1499110, 195580, 2902.09, 31847),\n (2001, 'East China', 'Anhui', 2165189, 250898, 3246.71, 33672),\n (2002, 'East China', 'Anhui', 2404936, 434149, 3519.72, 38375),\n (2003, 'East China', 'Anhui', 2815820, 619201, 3923.11, 36720),\n (2004, 'East China', 'Anhui', 3422176, 898441, 4759.3, 54669),\n (2005, 'East China', 'Anhui', 3874846, 898441, 5350.17, 69000)]\n\n\n\nsql = \"\"\"\nSELECT\n  year\n, region\n, province\n, it\n--, CURRENT_DATE()\nFROM fiscal_table\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf.tail(30)\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      it\n    \n  \n  \n    \n      330\n      2002\n      Northwest China\n      Xinjiang\n      2150325\n    \n    \n      331\n      2003\n      Northwest China\n      Xinjiang\n      2355164\n    \n    \n      332\n      2004\n      Northwest China\n      Xinjiang\n      2838346\n    \n    \n      333\n      2005\n      Northwest China\n      Xinjiang\n      3421743\n    \n    \n      334\n      2006\n      Northwest China\n      Xinjiang\n      4686125\n    \n    \n      335\n      2007\n      Northwest China\n      Xinjiang\n      5502470\n    \n    \n      336\n      1996\n      Southwest China\n      Yunnan\n      1374111\n    \n    \n      337\n      1997\n      Southwest China\n      Yunnan\n      1452425\n    \n    \n      338\n      1998\n      Southwest China\n      Yunnan\n      1617463\n    \n    \n      339\n      1999\n      Southwest China\n      Yunnan\n      1888666\n    \n    \n      340\n      2000\n      Southwest China\n      Yunnan\n      2254281\n    \n    \n      341\n      2001\n      Southwest China\n      Yunnan\n      2856307\n    \n    \n      342\n      2002\n      Southwest China\n      Yunnan\n      3035767\n    \n    \n      343\n      2003\n      Southwest China\n      Yunnan\n      3388449\n    \n    \n      344\n      2004\n      Southwest China\n      Yunnan\n      3957158\n    \n    \n      345\n      2005\n      Southwest China\n      Yunnan\n      4280994\n    \n    \n      346\n      2006\n      Southwest China\n      Yunnan\n      5046865\n    \n    \n      347\n      2007\n      Southwest China\n      Yunnan\n      6832541\n    \n    \n      348\n      1996\n      East China\n      Zhejiang\n      740327\n    \n    \n      349\n      1997\n      East China\n      Zhejiang\n      814253\n    \n    \n      350\n      1998\n      East China\n      Zhejiang\n      923455\n    \n    \n      351\n      1999\n      East China\n      Zhejiang\n      1001703\n    \n    \n      352\n      2000\n      East China\n      Zhejiang\n      1135215\n    \n    \n      353\n      2001\n      East China\n      Zhejiang\n      1203372\n    \n    \n      354\n      2002\n      East China\n      Zhejiang\n      1962633\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      2261631\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      3162299\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      2370200\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      2553268\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      2939778\n    \n  \n\n\n\n\n\n#df['it'].plot(figsize = (12, 8))"
  },
  {
    "objectID": "posts/2020-10-06-nlp-sqlitedb.html",
    "href": "posts/2020-10-06-nlp-sqlitedb.html",
    "title": "Using sqlalchemy and pandas to read and write from and to a local sqlite database for NLP",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport pandas_datareader.data as web\nimport datetime\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndf=pd.read_csv('nf_complete.csv')\n\n\ndf.columns\n\nIndex(['Unnamed: 0', 'year', 'title', 'abstract', 'theme', 'China', 'Russia',\n       'War', 'President', 'US', 'Vietnam', 'Cold War', 'World War',\n       'Vietnam War', 'Korean War', 'Survey', 'Case Study', 'Trade',\n       'Humanitarian', 'fixed_effects', 'instrumental_variable', 'regression',\n       'experimental'],\n      dtype='object')\n\n\n\ndf[[\"year\",\"title\"]]\n\n\n\n\n\n  \n    \n      \n      year\n      title\n    \n  \n  \n    \n      0\n      2000\n      \"Institutions at the Domestic/International Ne...\n    \n    \n      1\n      2000\n      Born to Lose and Doomed to Survive: State Deat...\n    \n    \n      2\n      2000\n      The significance of allegiance in internatio...\n    \n    \n      3\n      2000\n      The significance of allegiance in internatio...\n    \n    \n      4\n      2000\n      Truth-Telling and Mythmaking in Post-Soviet Ru...\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      121\n      2018\n      Planning for the Short Haul: Trade Among Belli...\n    \n    \n      122\n      2018\n      Clinging to the Anti-Imperial Mantle: The Repu...\n    \n    \n      123\n      2018\n      The New Navy's Pacific Wars:  Peripheral Confl...\n    \n    \n      124\n      2018\n      Stop or I'll Shoot, Comply and I Won't: The Di...\n    \n    \n      125\n      2018\n      Unexpected Humanitarians: Albania, the U.S. Mi...\n    \n  \n\n126 rows  2 columns\n\n\n\n\ndf_subset = df[[\"year\", \"title\", \"abstract\", \"theme\", \"War\", 'Cold War',\"Trade\"]]\ndf_subset\n\n\n\n\n\n  \n    \n      \n      year\n      title\n      abstract\n      theme\n      War\n      Cold War\n      Trade\n    \n  \n  \n    \n      0\n      2000\n      \"Institutions at the Domestic/International Ne...\n      Civil-military relations are frequently studie...\n      IR scholarship\n      1\n      0\n      0\n    \n    \n      1\n      2000\n      Born to Lose and Doomed to Survive: State Deat...\n      Under what conditions do states die, or exit t...\n      IR scholarship\n      1\n      1\n      0\n    \n    \n      2\n      2000\n      The significance of allegiance in internatio...\n      My dissertation employs original and secondary...\n      IR scholarship\n      1\n      0\n      0\n    \n    \n      3\n      2000\n      The significance of allegiance in internatio...\n      \\nThis study revises prevailing interpretation...\n      Conflit Between States\n      0\n      1\n      0\n    \n    \n      4\n      2000\n      Truth-Telling and Mythmaking in Post-Soviet Ru...\n      Can distorted and pernicious ideas about histo...\n      Conflict Between States\n      1\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      121\n      2018\n      Planning for the Short Haul: Trade Among Belli...\n      In times of war, why do belligerents continue ...\n      Conflict between states\n      1\n      0\n      1\n    \n    \n      122\n      2018\n      Clinging to the Anti-Imperial Mantle: The Repu...\n      My dissertation project, Clinging to the Anti-...\n      Cold War\n      0\n      1\n      0\n    \n    \n      123\n      2018\n      The New Navy's Pacific Wars:  Peripheral Confl...\n      Using a transnational methodology and sources ...\n      Military History\n      1\n      0\n      0\n    \n    \n      124\n      2018\n      Stop or I'll Shoot, Comply and I Won't: The Di...\n      There is a dilemma at the heart of coercion. S...\n      IR Scholarship\n      0\n      0\n      1\n    \n    \n      125\n      2018\n      Unexpected Humanitarians: Albania, the U.S. Mi...\n      Using archives and oral history, this disserta...\n      Military History\n      0\n      0\n      0\n    \n  \n\n126 rows  7 columns\n\n\n\n\ndf_subset.columns = [\"year\", \"title\", \"abstract\", \"theme\", \"War\", 'Cold War',\"Trade\"]\n\n\ndf_subset\n\n\n\n\n\n  \n    \n      \n      year\n      title\n      abstract\n      theme\n      War\n      Cold War\n      Trade\n    \n  \n  \n    \n      0\n      2000\n      \"Institutions at the Domestic/International Ne...\n      Civil-military relations are frequently studie...\n      IR scholarship\n      1\n      0\n      0\n    \n    \n      1\n      2000\n      Born to Lose and Doomed to Survive: State Deat...\n      Under what conditions do states die, or exit t...\n      IR scholarship\n      1\n      1\n      0\n    \n    \n      2\n      2000\n      The significance of allegiance in internatio...\n      My dissertation employs original and secondary...\n      IR scholarship\n      1\n      0\n      0\n    \n    \n      3\n      2000\n      The significance of allegiance in internatio...\n      \\nThis study revises prevailing interpretation...\n      Conflit Between States\n      0\n      1\n      0\n    \n    \n      4\n      2000\n      Truth-Telling and Mythmaking in Post-Soviet Ru...\n      Can distorted and pernicious ideas about histo...\n      Conflict Between States\n      1\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      121\n      2018\n      Planning for the Short Haul: Trade Among Belli...\n      In times of war, why do belligerents continue ...\n      Conflict between states\n      1\n      0\n      1\n    \n    \n      122\n      2018\n      Clinging to the Anti-Imperial Mantle: The Repu...\n      My dissertation project, Clinging to the Anti-...\n      Cold War\n      0\n      1\n      0\n    \n    \n      123\n      2018\n      The New Navy's Pacific Wars:  Peripheral Confl...\n      Using a transnational methodology and sources ...\n      Military History\n      1\n      0\n      0\n    \n    \n      124\n      2018\n      Stop or I'll Shoot, Comply and I Won't: The Di...\n      There is a dilemma at the heart of coercion. S...\n      IR Scholarship\n      0\n      0\n      1\n    \n    \n      125\n      2018\n      Unexpected Humanitarians: Albania, the U.S. Mi...\n      Using archives and oral history, this disserta...\n      Military History\n      0\n      0\n      0\n    \n  \n\n126 rows  7 columns\n\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///nf_nlp.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nnf_nlp_table = db.Table('nf_nlp_table', metadata, \n    db.Column('year',db.Integer, nullable=True, index=False),\n    db.Column('title',db.String, nullable=True),\n    db.Column('abstract',db.String, nullable=True),\n    db.Column('theme',db.String, nullable=True),\n    db.Column('War',db.Integer, nullable=True),\n    db.Column('Cold War',db.Integer, nullable=True),\n    db.Column('Trade', db.Integer, nullable=True)\n)\n\n\nmetadata.create_all(engine) #Creates the table\n\n\nnf_nlp_table\n\nTable('nf_nlp_table', MetaData(bind=None), Column('year', Integer(), table=<nf_nlp_table>), Column('title', String(), table=<nf_nlp_table>), Column('abstract', String(), table=<nf_nlp_table>), Column('theme', String(), table=<nf_nlp_table>), Column('War', Integer(), table=<nf_nlp_table>), Column('Cold War', Integer(), table=<nf_nlp_table>), Column('Trade', Integer(), table=<nf_nlp_table>), schema=None)\n\n\n\ndf_subset.to_sql('nf_nlp_table', con=engine, if_exists='append', index=False)\n\n\nengine.execute(\"SELECT year, theme, title FROM nf_nlp_table LIMIT 10\").fetchall()\n\n[(2000, 'IR scholarship', '\"Institutions at the Domestic/International Nexus: the political-military  origins of military effectiveness, strategic integration and war'),\n (2000, 'IR scholarship', 'Born to Lose and Doomed to Survive: State Death and Survival in the International System'),\n (2000, 'IR scholarship', 'The significance of allegiance in international relations'),\n (2000, 'Conflit Between States', 'The significance of allegiance in international relations'),\n (2000, 'Conflict Between States', 'Truth-Telling and Mythmaking in Post-Soviet Russia: Historical Ideas, Mass Education, and Interstate Conflict'),\n (2000, 'Domestic Military History', 'Building a Cape Fear Metropolis: Fort Bragg, Fayetteville, and the  Sandhills of North Carolina'),\n (2000, 'Culture', 'The Glories and the Sadness: Shaping the national Memory of the First World War in Great Britain, Canada and Australia'),\n (2000, 'Culture / Peace Process', 'What leads longstanding adversaries to engage in conflict resolution'),\n (2001, 'Military History', 'A School for the Nation: Military  Institutions and the Boundaries of Nationality'),\n (2001, 'Military History', \"The 'American Century' Army:  The Origins of the U.S. Cold War Army, 1949-1959\")]\n\n\n\nsql = \"\"\"\nSELECT\n  year\n, theme\n, title\nFROM nf_nlp_table\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf.tail(30)\n\n\n\n\n\n  \n    \n      \n      year\n      theme\n      title\n    \n  \n  \n    \n      96\n      2014\n      IR Scholarship\n      Multiparty Mediation: Identifying Characteris...\n    \n    \n      97\n      2014\n      IR Scholarship\n      The Justice Dilemma: International Criminal Ac...\n    \n    \n      98\n      2014\n      IR Scholarship\n      Beyond Revolution and Repression: U.S. Foreign...\n    \n    \n      99\n      2014\n      IR Scholarship\n      Protection States Trust?: Major Power Patronag...\n    \n    \n      100\n      2014\n      Nuclear Weapons\n      The Constraining Power of the Nuclear Nonproli...\n    \n    \n      101\n      2015\n      Military History\n      Selling Her the Military: Recruiting Women int...\n    \n    \n      102\n      2015\n      IR Scholarship\n      American Evangelicals, Israel, and Modern Chri...\n    \n    \n      103\n      2015\n      Non-state\n      Who Can Keep the Peace? Insurgent Organization...\n    \n    \n      104\n      2015\n      IR Scholarship\n      Credibility in Crisis: The Role of Leadership ...\n    \n    \n      105\n      2015\n      IR Scholarship\n      Evaluating the Changing of the Guards: Survey ...\n    \n    \n      106\n      2015\n      Soviet Union\n      Extracting the Eagles Talons: The Soviet Unio...\n    \n    \n      107\n      2015\n      IR Scholarship\n      The Control War: Communist Revolutionary Warfa...\n    \n    \n      108\n      2015\n      Nuclear Weapons\n      Nuclear Weapons and Foreign Policy\n    \n    \n      109\n      2016\n      Civ-Mil\n      Securing Control and Controlling Security: Civ...\n    \n    \n      110\n      2016\n      Military History\n      Digging for Victory: The Stalinist States Mob...\n    \n    \n      111\n      2016\n      Non-state\n      Persuading Power: Insurgent Diplomacy and the ...\n    \n    \n      112\n      2016\n      Conflict between states\n      A Prelude to Violence? The Effect of Nationali...\n    \n    \n      113\n      2016\n      Conflict between states\n      Engaging the Evil Empire: East  West Relati...\n    \n    \n      114\n      2017\n      IR Scholarship\n      More Talk, Less Action: Why Costless Diplomacy...\n    \n    \n      115\n      2017\n      Cold War\n      Experiments in Peace: Asian Neutralism, Human ...\n    \n    \n      116\n      2017\n      IR Scholarship\n      Fully Committed? Religiously Committed State P...\n    \n    \n      117\n      2017\n      Military History\n      Straddling the Threshold of Two Worlds: Soldie...\n    \n    \n      118\n      2017\n      Military History\n      U.S. Armys Investigation and Adjudication of ...\n    \n    \n      119\n      2017\n      IR Scholarship\n      Grand Strategic Crucibles: The Lasting Effects...\n    \n    \n      120\n      2018\n      Nuclear Weapons\n      Trust in International Politics: The Role of L...\n    \n    \n      121\n      2018\n      Conflict between states\n      Planning for the Short Haul: Trade Among Belli...\n    \n    \n      122\n      2018\n      Cold War\n      Clinging to the Anti-Imperial Mantle: The Repu...\n    \n    \n      123\n      2018\n      Military History\n      The New Navy's Pacific Wars:  Peripheral Confl...\n    \n    \n      124\n      2018\n      IR Scholarship\n      Stop or I'll Shoot, Comply and I Won't: The Di...\n    \n    \n      125\n      2018\n      Military History\n      Unexpected Humanitarians: Albania, the U.S. Mi..."
  },
  {
    "objectID": "posts/2021-02-06-portfolio-allocation-sharpe-ratio.html",
    "href": "posts/2021-02-06-portfolio-allocation-sharpe-ratio.html",
    "title": "Sharpe Ratio and Portfolio Values",
    "section": "",
    "text": "This post includes code and notes from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks."
  },
  {
    "objectID": "posts/2021-02-06-portfolio-allocation-sharpe-ratio.html#create-a-portfolio",
    "href": "posts/2021-02-06-portfolio-allocation-sharpe-ratio.html#create-a-portfolio",
    "title": "Sharpe Ratio and Portfolio Values",
    "section": "Create a Portfolio",
    "text": "Create a Portfolio\n\ndf = pd.read_csv('https://stocks-snp-500.herokuapp.com/stocks/index_stocks_table.csv?_size=max')\ndf.head()\n\ndf['Date'] = pd.to_datetime(df.Date)\ndf.index = pd.to_datetime(df.Date)\ndf2 = df.drop('Date', axis=1)\ndf3 = df2.drop('rowid', axis=1)\ndf3 = df3.drop('Russell_2000_stock', axis=1)\ndf3 = df3.drop('SP500_stock', axis=1)\ndf3\n\nstocks = df3\n\n\nstocks\n\n\n\n\n\n  \n    \n      \n      FXAIX_stock\n      VRTTX_stock\n      FNCMX_stock\n      FSMAX_stock\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2018-01-02\n      94.230003\n      238.889999\n      91.989998\n      62.529999\n    \n    \n      2018-01-03\n      94.830002\n      240.289993\n      92.760002\n      62.740002\n    \n    \n      2018-01-04\n      95.230003\n      241.199997\n      92.930000\n      62.849998\n    \n    \n      2018-01-05\n      95.900002\n      242.750000\n      93.699997\n      63.090000\n    \n    \n      2018-01-08\n      96.059998\n      243.199997\n      93.970001\n      63.270000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2021-02-08\n      135.880005\n      355.709991\n      175.610001\n      93.940002\n    \n    \n      2021-02-09\n      135.750000\n      355.730011\n      175.839996\n      94.400002\n    \n    \n      2021-02-10\n      135.699997\n      355.579987\n      175.380005\n      94.309998\n    \n    \n      2021-02-11\n      135.960007\n      356.440002\n      176.100006\n      94.790001\n    \n    \n      2021-02-12\n      136.600006\n      358.170013\n      176.979996\n      95.250000\n    \n  \n\n785 rows  4 columns\n\n\n\n\n# stocks\n\n# stocks['FXAIX_stock Daily Return'] = stocks['FXAIX_stock'].pct_change(1)\n\n# cum_ret = 100 * (stocks['FXAIX_stock'][-1] / stocks['FXAIX_stock'][0] - 1 )\n# print('Our return {} was percent!'.format(cum_ret))\n\n# stocks['FXAIX_stock Daily Return'].mean()\n\n# stocks['FXAIX_stock Daily Return'].std()\n\n# stocks['FXAIX_stock Daily Return'].plot(kind = 'kde')\n\n# portfolio_val=stocks\n\n# SR = portfolio_val['FXAIX_stock Daily Return'].mean() / portfolio_val['FXAIX_stock Daily Return'].std()\n\n# SR\n\n# ASR = (252 ** 0.5) * SR\n\n# ASR\n\n# portfolio_val['FXAIX_stock Daily Return'].std()\n\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n\n\n# fig = plt.figure(figsize = (12, 8))\n# portfolio_val['FXAIX_stock Daily Return'].plot(kind = 'kde')\n\n\nstart = pd.to_datetime('2020-01-01')\nend = pd.to_datetime('today')\n\nFXAIX_stock = web.DataReader('FXAIX', 'yahoo', start, end)\nFXAIX_stock.head()\n\nVRTTX_stock = web.DataReader('VRTTX', 'yahoo', start, end)\nVRTTX_stock.head()\n\n\nFNCMX_stock = web.DataReader('FNCMX', 'yahoo', start, end)\nFNCMX_stock.head()\n\nFSMAX_stock = web.DataReader('FSMAX', 'yahoo', start, end)\nFSMAX_stock.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-02\n      65.269997\n      65.269997\n      65.269997\n      65.269997\n      0.0\n      65.192108\n    \n    \n      2020-01-03\n      65.129997\n      65.129997\n      65.129997\n      65.129997\n      0.0\n      65.052277\n    \n    \n      2020-01-06\n      65.279999\n      65.279999\n      65.279999\n      65.279999\n      0.0\n      65.202103\n    \n    \n      2020-01-07\n      65.180000\n      65.180000\n      65.180000\n      65.180000\n      0.0\n      65.102219\n    \n    \n      2020-01-08\n      65.410004\n      65.410004\n      65.410004\n      65.410004\n      0.0\n      65.331947\n    \n  \n\n\n\n\n\nVRTTX_stock\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-02\n      285.730011\n      285.730011\n      285.730011\n      285.730011\n      0.0\n      280.905823\n    \n    \n      2020-01-03\n      283.959991\n      283.959991\n      283.959991\n      283.959991\n      0.0\n      279.165680\n    \n    \n      2020-01-06\n      284.910004\n      284.910004\n      284.910004\n      284.910004\n      0.0\n      280.099640\n    \n    \n      2020-01-07\n      284.200012\n      284.200012\n      284.200012\n      284.200012\n      0.0\n      279.401642\n    \n    \n      2020-01-08\n      285.559998\n      285.559998\n      285.559998\n      285.559998\n      0.0\n      280.738678\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2021-02-08\n      355.709991\n      355.709991\n      355.709991\n      355.709991\n      0.0\n      355.709991\n    \n    \n      2021-02-09\n      355.730011\n      355.730011\n      355.730011\n      355.730011\n      0.0\n      355.730011\n    \n    \n      2021-02-10\n      355.579987\n      355.579987\n      355.579987\n      355.579987\n      0.0\n      355.579987\n    \n    \n      2021-02-11\n      356.440002\n      356.440002\n      356.440002\n      356.440002\n      0.0\n      356.440002\n    \n    \n      2021-02-12\n      358.170013\n      358.170013\n      358.170013\n      358.170013\n      0.0\n      358.170013\n    \n  \n\n282 rows  6 columns\n\n\n\n\nFNCMX_stock\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-02\n      114.180000\n      114.180000\n      114.180000\n      114.180000\n      0.0\n      113.419273\n    \n    \n      2020-01-03\n      113.269997\n      113.269997\n      113.269997\n      113.269997\n      0.0\n      112.515327\n    \n    \n      2020-01-06\n      113.849998\n      113.849998\n      113.849998\n      113.849998\n      0.0\n      113.091469\n    \n    \n      2020-01-07\n      113.839996\n      113.839996\n      113.839996\n      113.839996\n      0.0\n      113.081535\n    \n    \n      2020-01-08\n      114.650002\n      114.650002\n      114.650002\n      114.650002\n      0.0\n      113.886139\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2021-02-08\n      175.610001\n      175.610001\n      175.610001\n      175.610001\n      0.0\n      175.610001\n    \n    \n      2021-02-09\n      175.839996\n      175.839996\n      175.839996\n      175.839996\n      0.0\n      175.839996\n    \n    \n      2021-02-10\n      175.380005\n      175.380005\n      175.380005\n      175.380005\n      0.0\n      175.380005\n    \n    \n      2021-02-11\n      176.100006\n      176.100006\n      176.100006\n      176.100006\n      0.0\n      176.100006\n    \n    \n      2021-02-12\n      176.979996\n      176.979996\n      176.979996\n      176.979996\n      0.0\n      176.979996\n    \n  \n\n282 rows  6 columns\n\n\n\n\nfor stock_df in (FXAIX_stock, VRTTX_stock, FNCMX_stock, FSMAX_stock):\n    stock_df['Normed Return'] = stock_df['Adj Close'] / stock_df.iloc[0]['Adj Close']\n\n\nstock_df\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n      Normed Return\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-02\n      65.269997\n      65.269997\n      65.269997\n      65.269997\n      0.0\n      65.192108\n      1.000000\n    \n    \n      2020-01-03\n      65.129997\n      65.129997\n      65.129997\n      65.129997\n      0.0\n      65.052277\n      0.997855\n    \n    \n      2020-01-06\n      65.279999\n      65.279999\n      65.279999\n      65.279999\n      0.0\n      65.202103\n      1.000153\n    \n    \n      2020-01-07\n      65.180000\n      65.180000\n      65.180000\n      65.180000\n      0.0\n      65.102219\n      0.998621\n    \n    \n      2020-01-08\n      65.410004\n      65.410004\n      65.410004\n      65.410004\n      0.0\n      65.331947\n      1.002145\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2021-02-08\n      93.940002\n      93.940002\n      93.940002\n      93.940002\n      0.0\n      93.940002\n      1.440972\n    \n    \n      2021-02-09\n      94.400002\n      94.400002\n      94.400002\n      94.400002\n      0.0\n      94.400002\n      1.448028\n    \n    \n      2021-02-10\n      94.309998\n      94.309998\n      94.309998\n      94.309998\n      0.0\n      94.309998\n      1.446647\n    \n    \n      2021-02-11\n      94.790001\n      94.790001\n      94.790001\n      94.790001\n      0.0\n      94.790001\n      1.454010\n    \n    \n      2021-02-12\n      95.250000\n      95.250000\n      95.250000\n      95.250000\n      0.0\n      95.250000\n      1.461066\n    \n  \n\n282 rows  7 columns\n\n\n\n\n## Allocations\n\n\nfor stock_df,allo in zip([FXAIX_stock, VRTTX_stock, FNCMX_stock, FSMAX_stock],[.7, .1, .1, .1]):\n    stock_df['Allocation'] = stock_df['Normed Return'] * allo\n\n\nstock_df\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n      Normed Return\n      Allocation\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-02\n      65.269997\n      65.269997\n      65.269997\n      65.269997\n      0.0\n      65.192108\n      1.000000\n      0.100000\n    \n    \n      2020-01-03\n      65.129997\n      65.129997\n      65.129997\n      65.129997\n      0.0\n      65.052277\n      0.997855\n      0.099786\n    \n    \n      2020-01-06\n      65.279999\n      65.279999\n      65.279999\n      65.279999\n      0.0\n      65.202103\n      1.000153\n      0.100015\n    \n    \n      2020-01-07\n      65.180000\n      65.180000\n      65.180000\n      65.180000\n      0.0\n      65.102219\n      0.998621\n      0.099862\n    \n    \n      2020-01-08\n      65.410004\n      65.410004\n      65.410004\n      65.410004\n      0.0\n      65.331947\n      1.002145\n      0.100215\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2021-02-08\n      93.940002\n      93.940002\n      93.940002\n      93.940002\n      0.0\n      93.940002\n      1.440972\n      0.144097\n    \n    \n      2021-02-09\n      94.400002\n      94.400002\n      94.400002\n      94.400002\n      0.0\n      94.400002\n      1.448028\n      0.144803\n    \n    \n      2021-02-10\n      94.309998\n      94.309998\n      94.309998\n      94.309998\n      0.0\n      94.309998\n      1.446647\n      0.144665\n    \n    \n      2021-02-11\n      94.790001\n      94.790001\n      94.790001\n      94.790001\n      0.0\n      94.790001\n      1.454010\n      0.145401\n    \n    \n      2021-02-12\n      95.250000\n      95.250000\n      95.250000\n      95.250000\n      0.0\n      95.250000\n      1.461066\n      0.146107\n    \n  \n\n282 rows  8 columns\n\n\n\n\n## Investment\n\n\nfor stock_df in [FXAIX_stock, VRTTX_stock, FNCMX_stock, FSMAX_stock]:\n    stock_df['Position Values'] = stock_df['Allocation'] * 100000\n\n\nstock_df\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n      Normed Return\n      Allocation\n      Position Values\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-02\n      65.269997\n      65.269997\n      65.269997\n      65.269997\n      0.0\n      65.192108\n      1.000000\n      0.100000\n      10000.000000\n    \n    \n      2020-01-03\n      65.129997\n      65.129997\n      65.129997\n      65.129997\n      0.0\n      65.052277\n      0.997855\n      0.099786\n      9978.550848\n    \n    \n      2020-01-06\n      65.279999\n      65.279999\n      65.279999\n      65.279999\n      0.0\n      65.202103\n      1.000153\n      0.100015\n      10001.533085\n    \n    \n      2020-01-07\n      65.180000\n      65.180000\n      65.180000\n      65.180000\n      0.0\n      65.102219\n      0.998621\n      0.099862\n      9986.211594\n    \n    \n      2020-01-08\n      65.410004\n      65.410004\n      65.410004\n      65.410004\n      0.0\n      65.331947\n      1.002145\n      0.100215\n      10021.450322\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2021-02-08\n      93.940002\n      93.940002\n      93.940002\n      93.940002\n      0.0\n      93.940002\n      1.440972\n      0.144097\n      14409.719995\n    \n    \n      2021-02-09\n      94.400002\n      94.400002\n      94.400002\n      94.400002\n      0.0\n      94.400002\n      1.448028\n      0.144803\n      14480.280543\n    \n    \n      2021-02-10\n      94.309998\n      94.309998\n      94.309998\n      94.309998\n      0.0\n      94.309998\n      1.446647\n      0.144665\n      14466.474582\n    \n    \n      2021-02-11\n      94.790001\n      94.790001\n      94.790001\n      94.790001\n      0.0\n      94.790001\n      1.454010\n      0.145401\n      14540.103641\n    \n    \n      2021-02-12\n      95.250000\n      95.250000\n      95.250000\n      95.250000\n      0.0\n      95.250000\n      1.461066\n      0.146107\n      14610.664189\n    \n  \n\n282 rows  9 columns\n\n\n\n\n\n\n## Total Portfolio Value\n\nportfolio_val = pd.concat([FXAIX_stock['Position Values'],\n                           VRTTX_stock['Position Values'],\n                           FNCMX_stock['Position Values'],\n                           FSMAX_stock['Position Values']],\n                          axis = 1)\n\nportfolio_val.head()\n\nportfolio_val.columns = ['FXAIX_stock Pos', 'VRTTX_stock Pos', 'FNCMX_stock Pos', 'FSMAX_stock Pos']\n\nportfolio_val.head()\n\nportfolio_val['Total Pos'] = portfolio_val.sum(axis = 1)\n\nportfolio_val.head()\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nportfolio_val['Total Pos'].plot(figsize = (12, 8))\nplt.title('Total Portfolio Value')\n\nportfolio_val.drop('Total Pos',\n                   axis = 1).plot(kind = 'line', figsize = (12, 8))\n\nportfolio_val.tail()\n\n\n\n\n\n  \n    \n      \n      FXAIX_stock Pos\n      VRTTX_stock Pos\n      FNCMX_stock Pos\n      FSMAX_stock Pos\n      Total Pos\n    \n    \n      Date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2021-02-08\n      85012.841550\n      12662.962553\n      15483.259184\n      14409.719995\n      127568.783283\n    \n    \n      2021-02-09\n      84931.504458\n      12663.675231\n      15503.537547\n      14480.280543\n      127578.997779\n    \n    \n      2021-02-10\n      84900.220227\n      12658.334494\n      15462.980820\n      14466.474582\n      127488.010123\n    \n    \n      2021-02-11\n      85062.894411\n      12688.950302\n      15526.462202\n      14540.103641\n      127818.410556\n    \n    \n      2021-02-12\n      85463.307752\n      12750.537170\n      15604.049511\n      14610.664189\n      128428.558622\n    \n  \n\n\n\n\n\n\n\n\n\n\n\nportfolio_val['Daily Return'] = portfolio_val['Total Pos'].pct_change(1)\n\n\ncum_ret = 100 * (portfolio_val['Total Pos'][-1] / portfolio_val['Total Pos'][0] - 1 )\nprint('Our return {} was percent!'.format(cum_ret))\n\nOur return 28.428558621835933 was percent!\n\n\n\nportfolio_val['Daily Return'].mean()\n\n0.001112432148531155\n\n\n\nportfolio_val['Daily Return'].std()\n\n0.020973776003181118\n\n\n\nportfolio_val['Daily Return'].plot(kind = 'kde')\n\n<AxesSubplot:ylabel='Density'>\n\n\n\n\n\n\nSR = portfolio_val['Daily Return'].mean() / portfolio_val['Daily Return'].std()\n\nSR\n\nASR = (252 ** 0.5) * SR\n\nASR\n\nportfolio_val['Daily Return'].std()\n\nportfolio_val['Daily Return'].mean()\n\n0.001112432148531155\n\n\n\nfig = plt.figure(figsize = (12, 8))\nportfolio_val['Daily Return'].plot(kind = 'kde')\n\n<AxesSubplot:ylabel='Density'>\n\n\n\n\n\n\nfig = plt.figure(figsize = (12, 8))\nFXAIX_stock['Adj Close'].pct_change(1).plot(kind ='kde', label = 'FXAIX_stock')\nVRTTX_stock['Adj Close'].pct_change(1).plot(kind ='kde', label = 'VRTTX_stock')\nFNCMX_stock['Adj Close'].pct_change(1).plot(kind ='kde', label = 'FNCMX_stock')\nFSMAX_stock['Adj Close'].pct_change(1).plot(kind ='kde', label = 'FSMAX_stock')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f617d8dcf40>"
  },
  {
    "objectID": "posts/2021-02-08-capital-asset-pricing-model.html",
    "href": "posts/2021-02-08-capital-asset-pricing-model.html",
    "title": "Daily and Cumulative Returns, CAPM",
    "section": "",
    "text": "This post includes code and notes from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks."
  },
  {
    "objectID": "posts/2021-02-08-capital-asset-pricing-model.html#compare-cumulative-return",
    "href": "posts/2021-02-08-capital-asset-pricing-model.html#compare-cumulative-return",
    "title": "Daily and Cumulative Returns, CAPM",
    "section": "Compare Cumulative Return",
    "text": "Compare Cumulative Return\n\nFXAIX_stock['Cumulative'] = FXAIX_stock['Close']/FXAIX_stock['Close'].iloc[0]\nVRTTX_stock['Cumulative'] = VRTTX_stock['Close']/VRTTX_stock['Close'].iloc[0]\nFNCMX_stock['Cumulative'] = FNCMX_stock['Close']/FNCMX_stock['Close'].iloc[0]\nFSMAX_stock['Cumulative'] = FSMAX_stock['Close']/FSMAX_stock['Close'].iloc[0]\n\n\nFXAIX_stock['Cumulative'].plot(label='FXAIX_stock',figsize=(10,8))\nVRTTX_stock['Cumulative'].plot(label='VRTTX_stock',figsize=(10,8))\nFNCMX_stock['Cumulative'].plot(label='FNCMX_stock',figsize=(10,8))\nFSMAX_stock['Cumulative'].plot(label='FSMAX_stock',figsize=(10,8))\nplt.legend()\nplt.title('Cumulative Return')\n\nText(0.5, 1.0, 'Cumulative Return')"
  },
  {
    "objectID": "posts/2021-02-08-capital-asset-pricing-model.html#get-daily-return",
    "href": "posts/2021-02-08-capital-asset-pricing-model.html#get-daily-return",
    "title": "Daily and Cumulative Returns, CAPM",
    "section": "Get Daily Return",
    "text": "Get Daily Return\n\nFXAIX_stock['Daily Return'] = FXAIX_stock['Close'].pct_change(1)\nVRTTX_stock['Daily Return'] = VRTTX_stock['Close'].pct_change(1)\n\n\nFXAIX_stock['Daily Return']\n\nDate\n2020-01-02         NaN\n2020-01-03   -0.006992\n2020-01-06    0.003565\n2020-01-07   -0.002664\n2020-01-08    0.004898\n                ...   \n2021-02-08    0.007414\n2021-02-09   -0.000957\n2021-02-10   -0.000368\n2021-02-11    0.001916\n2021-02-12    0.004707\nName: Daily Return, Length: 282, dtype: float64\n\n\n\nplt.scatter(FXAIX_stock['Daily Return'],VRTTX_stock['Daily Return'],alpha=0.3)\n\n<matplotlib.collections.PathCollection at 0x7fe6c8aef400>\n\n\n\n\n\n\nVRTTX_stock['Daily Return'].hist(bins=100)\n\n<AxesSubplot:>\n\n\n\n\n\n\nFXAIX_stock['Daily Return'].hist(bins=100)\n\n<AxesSubplot:>\n\n\n\n\n\n\nbeta,alpha,r_value,p_value,std_err = stats.linregress(FXAIX_stock['Daily Return'].iloc[1:],VRTTX_stock['Daily Return'].iloc[1:])\n\n\nbeta\n\n1.0124150273238781\n\n\n\nalpha\n\n0.00012460564758023734\n\n\n\nr_value\n\n0.9973781994620393\n\n\n\nFXAIX_stock['Daily Return'].head()\n\nDate\n2020-01-02         NaN\n2020-01-03   -0.006992\n2020-01-06    0.003565\n2020-01-07   -0.002664\n2020-01-08    0.004898\nName: Daily Return, dtype: float64\n\n\n\nimport numpy as np\n\n\nnoise = np.random.normal(0,0.001,len(FXAIX_stock['Daily Return'].iloc[1:]))\n\n\n#noise\n\n\nFXAIX_stock['Daily Return'].iloc[1:] + noise\n\nDate\n2020-01-03   -0.006673\n2020-01-06    0.002797\n2020-01-07   -0.002088\n2020-01-08    0.005773\n2020-01-09    0.007326\n                ...   \n2021-02-08    0.006484\n2021-02-09   -0.000506\n2021-02-10   -0.000078\n2021-02-11    0.001277\n2021-02-12    0.005154\nName: Daily Return, Length: 281, dtype: float64\n\n\n\nbeta,alpha,r_value,p_value,std_err = stats.linregress(FXAIX_stock['Daily Return'].iloc[1:]+noise,FXAIX_stock['Daily Return'].iloc[1:])\n\n\nbeta\n\n0.9959066537237841\n\n\n\nalpha\n\n0.00011917125339144037"
  },
  {
    "objectID": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#t-test-comparisons-uses-the-means-counts-and-standard-deviations-of-a-treatment-and-control-in-comparison-to-an-idealized-normal-distribution-to-calculate-a-p-value-which-by-intuition-is-the-likelihood-of-seeing-a-mean-difference-of-the-same-or-more-extreme-magnitude-between-treatment-and-control-as-a-result-of-chance.-this-is-done-through-a-comparison-to-an-idealized-normal-distribution-through-the-calculation-of-a-t-statistic.-while-the-test-statistic-is-assumed-to-follow-an-idealized-normal-distribution-if-the-scaling-term-but-where-the-scaling-term-is-unknown-and-it-is-instead-estimated-based-on-the-data-which-is-assumed-to-follow-the-students-t-distribution.this-process-can-be-thought-of-trying-to-disentangle-the-signal-mean-difference-and-counts-from-the-noise-variability.-here-the-mean-difference-is-the-direction-of-the-signal-and-the-counts-are-the-strength-of-the-signal.",
    "href": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#t-test-comparisons-uses-the-means-counts-and-standard-deviations-of-a-treatment-and-control-in-comparison-to-an-idealized-normal-distribution-to-calculate-a-p-value-which-by-intuition-is-the-likelihood-of-seeing-a-mean-difference-of-the-same-or-more-extreme-magnitude-between-treatment-and-control-as-a-result-of-chance.-this-is-done-through-a-comparison-to-an-idealized-normal-distribution-through-the-calculation-of-a-t-statistic.-while-the-test-statistic-is-assumed-to-follow-an-idealized-normal-distribution-if-the-scaling-term-but-where-the-scaling-term-is-unknown-and-it-is-instead-estimated-based-on-the-data-which-is-assumed-to-follow-the-students-t-distribution.this-process-can-be-thought-of-trying-to-disentangle-the-signal-mean-difference-and-counts-from-the-noise-variability.-here-the-mean-difference-is-the-direction-of-the-signal-and-the-counts-are-the-strength-of-the-signal.",
    "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
    "section": "T test comparisons uses the means, counts and standard deviations of a treatment and control in comparison to an idealized normal distribution to calculate a p value, which by intuition is the likelihood of seeing a mean difference of the same or more extreme magnitude between treatment and control as a result of chance. This is done through a comparison to an idealized normal distribution, through the calculation of a t-statistic. While the test statistic is assumed to follow an idealized normal distribution if the scaling term, but where the scaling term is unknown and it is instead estimated based on the data, which is assumed to follow the students t distribution.This process can be thought of trying to disentangle the signal (mean difference and counts) from the noise variability. Here the mean difference is the direction of the signal and the counts are the strength of the signal.",
    "text": "T test comparisons uses the means, counts and standard deviations of a treatment and control in comparison to an idealized normal distribution to calculate a p value, which by intuition is the likelihood of seeing a mean difference of the same or more extreme magnitude between treatment and control as a result of chance. This is done through a comparison to an idealized normal distribution, through the calculation of a t-statistic. While the test statistic is assumed to follow an idealized normal distribution if the scaling term, but where the scaling term is unknown and it is instead estimated based on the data, which is assumed to follow the students t distribution.This process can be thought of trying to disentangle the signal (mean difference and counts) from the noise variability. Here the mean difference is the direction of the signal and the counts are the strength of the signal.\n\nimport numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\nimport seaborn as sns\n\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.stats.power import NormalIndPower, TTestIndPower\nfrom scipy.stats import ttest_ind_from_stats\nimport numpy as np\nimport scipy\n\n/home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n  import pandas.util.testing as tm\n\n\n\ndf = pd.read_csv('df_panel_fix.csv')\n\n\ndf_subset = df[[\"year\", \"reg\", \"province\", \"gdp\", \"fdi\", 'it',\"specific\"]]\ndf_subset.columns = [\"year\", \"region\", \"province\", \"gdp\", \"fdi\", 'it',\"specific\"]\n\n\ndf=df_subset\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.30\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.70\n      668128\n      3162299\n      656175.0\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      18753.73\n      1036576\n      2939778\n      844647.0\n    \n  \n\n360 rows  7 columns\n\n\n\n\n# Add distributions by region\nimport matplotlib.pyplot as plt\n#fig, axes = plt.subplots(nrows=3, ncols=3)\n\ntest_cells = ['East China', 'North China']\nmetrics = ['gdp', 'fdi', 'it']\n\nfor test_cell in test_cells:\n    for metric in metrics:\n        df.loc[df[\"region\"] == test_cell].hist(column=[metric], bins=60)\n        print(test_cell)\n        print(metric)\n\n        \n\nEast China\ngdp\nEast China\nfdi\nEast China\nit\nNorth China\ngdp\nNorth China\nfdi\nNorth China\nit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf.hist(column=['fdi'], bins=60)\n\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec7cbdd8>]],\n      dtype=object)"
  },
  {
    "objectID": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#distributions-of-dependant-variables",
    "href": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#distributions-of-dependant-variables",
    "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
    "section": "Distributions of Dependant Variables",
    "text": "Distributions of Dependant Variables\n\nRight skew\n\ndf.hist(column=['fdi'], bins=60)\n\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec6e00f0>]],\n      dtype=object)\n\n\n\n\n\n\nsns.distplot(df['gdp'])\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec5d6a90>\n\n\n\n\n\n\nsns.distplot(df['fdi'])\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec4a4d30>\n\n\n\n\n\n\nsns.distplot(df['it'])\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec4df278>\n\n\n\n\n\n\nsns.distplot(df['specific'].dropna())\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec3e09e8>\n\n\n\n\n\n\ndf.hist(column=['fdi'], bins=60)\n\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec31ccc0>]],\n      dtype=object)"
  },
  {
    "objectID": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "href": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
    "section": "Removal of GDP value outliers more than 3 standard deviations away from the mean",
    "text": "Removal of GDP value outliers more than 3 standard deviations away from the mean"
  },
  {
    "objectID": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "href": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
    "section": "outlier removal of rows with GDP values that are > 3 standard deviations away form the mean",
    "text": "outlier removal of rows with GDP values that are > 3 standard deviations away form the mean\n\nimport scipy.stats as stats\n\n\ndf['gdp_zscore'] = stats.zscore(df['gdp'])"
  },
  {
    "objectID": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "href": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
    "section": "these are the observations more then > 3 SDs away from the mean of gdp that will be dropped",
    "text": "these are the observations more then > 3 SDs away from the mean of gdp that will be dropped\n\ndf[abs(df['gdp_zscore'])>3].hist(column = ['gdp'])\n\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec873208>]],\n      dtype=object)\n\n\n\n\n\n\ndf_no_gdp_outliers=df[abs(df['gdp_zscore'])<3]\n\n\ndf_no_gdp_outliers\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n      gdp_zscore\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.30\n      50661\n      631930\n      147002.0\n      -0.521466\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n      -0.464746\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n      -0.421061\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n      -0.383239\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n      -0.340870\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      354\n      2002\n      East China\n      Zhejiang\n      8003.67\n      307610\n      1962633\n      365437.0\n      0.798274\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n      1.178172\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.70\n      668128\n      3162299\n      656175.0\n      1.612181\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n      2.007180\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n      2.520929\n    \n  \n\n350 rows  8 columns\n\n\n\n\ndf_no_gdp_outliers.hist(column=['gdp'], bins=60)\n\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec95e4e0>]],\n      dtype=object)\n\n\n\n\n\n\ncounts_fiscal=df.groupby('region').count()\ncounts_fiscal\n\n\n\n\n\n  \n    \n      \n      year\n      province\n      gdp\n      fdi\n      it\n      specific\n      gdp_zscore\n    \n    \n      region\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      East China\n      84\n      84\n      84\n      84\n      84\n      84\n      84\n    \n    \n      North China\n      48\n      48\n      48\n      48\n      48\n      47\n      48\n    \n    \n      Northeast China\n      36\n      36\n      36\n      36\n      36\n      36\n      36\n    \n    \n      Northwest China\n      60\n      60\n      60\n      60\n      60\n      60\n      60\n    \n    \n      South Central China\n      72\n      72\n      72\n      72\n      72\n      72\n      72\n    \n    \n      Southwest China\n      60\n      60\n      60\n      60\n      60\n      57\n      60\n    \n  \n\n\n\n\n\ncounts_fiscal=df.groupby('province').count()\ncounts_fiscal\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      gdp\n      fdi\n      it\n      specific\n      gdp_zscore\n    \n    \n      province\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Anhui\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Beijing\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Chongqing\n      12\n      12\n      12\n      12\n      12\n      9\n      12\n    \n    \n      Fujian\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Gansu\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Guangdong\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Guangxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Guizhou\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hainan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hebei\n      12\n      12\n      12\n      12\n      12\n      11\n      12\n    \n    \n      Heilongjiang\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Henan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hubei\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hunan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Jiangsu\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Jiangxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Jilin\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Liaoning\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Ningxia\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Qinghai\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shaanxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shandong\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shanghai\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shanxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Sichuan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Tianjin\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Tibet\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Xinjiang\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Yunnan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Zhejiang\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n  \n\n\n\n\n\n#df_no_gdp_outliers.pivot_table(index='grouping column 1', columns='grouping column 2', values='aggregating column', aggfunc='sum')\n\n\n#pd.crosstab(df_no_gdp_outliers, 'year')\n\n\ndf_no_gdp_outliers_subset = df_no_gdp_outliers[['region', 'gdp', 'fdi', 'it']]\ndf_no_gdp_outliers_subset\n\n\n\n\n\n  \n    \n      \n      region\n      gdp\n      fdi\n      it\n    \n  \n  \n    \n      0\n      East China\n      2093.30\n      50661\n      631930\n    \n    \n      1\n      East China\n      2347.32\n      43443\n      657860\n    \n    \n      2\n      East China\n      2542.96\n      27673\n      889463\n    \n    \n      3\n      East China\n      2712.34\n      26131\n      1227364\n    \n    \n      4\n      East China\n      2902.09\n      31847\n      1499110\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      354\n      East China\n      8003.67\n      307610\n      1962633\n    \n    \n      355\n      East China\n      9705.02\n      498055\n      2261631\n    \n    \n      356\n      East China\n      11648.70\n      668128\n      3162299\n    \n    \n      357\n      East China\n      13417.68\n      772000\n      2370200\n    \n    \n      358\n      East China\n      15718.47\n      888935\n      2553268\n    \n  \n\n350 rows  4 columns\n\n\n\n\ndef aggregate_and_ttest(dataset, groupby_feature='region', alpha=.05, test_cells = [0, 1]):\n    #Imports\n    from tqdm import tqdm\n    from scipy.stats import ttest_ind_from_stats\n\n    \n    metrics = ['gdp', 'fdi', 'it']\n    \n    feature_size = 'size'\n    feature_mean = 'mean'\n    feature_std = 'std'    \n\n    for metric in tqdm(metrics):\n        \n        #print(metric)\n        crosstab = dataset.groupby(groupby_feature, as_index=False)[metric].agg(['size', 'mean', 'std'])\n        print(crosstab)\n        \n        treatment = crosstab.index[test_cells[0]]\n        control = crosstab.index[test_cells[1]]\n        \n        counts_control = crosstab.loc[control, feature_size]\n        counts_treatment = crosstab.loc[treatment, feature_size]\n\n        mean_control = crosstab.loc[control, feature_mean]\n        mean_treatment = crosstab.loc[treatment, feature_mean]\n\n        standard_deviation_control = crosstab.loc[control, feature_std]\n        standard_deviation_treatment = crosstab.loc[treatment, feature_std]\n        \n        t_statistic, p_value = ttest_ind_from_stats(mean1=mean_treatment, std1=standard_deviation_treatment, nobs1=counts_treatment,mean2=mean_control,std2=standard_deviation_control,nobs2=counts_control)\n        \n        #fstring to print the p value and t statistic\n        print(f\"The t statistic of the comparison of the treatment test cell of {treatment} compared to the control test cell of {control} is {t_statistic} and the p value is {p_value}.\")\n        \n        #f string to say of the comparison is significant at a given alpha level\n\n        if p_value < alpha: \n            print(f'The comparison between {treatment} and {control} is statistically significant at the threshold of {alpha}') \n        else: \n            print(f'The comparison between {treatment} and {control} is not statistically significant at the threshold of {alpha}')\n\n\naggregate_and_ttest(df_no_gdp_outliers_subset, test_cells = [0,1])\n\n100%|| 3/3 [00:00<00:00, 115.78it/s]\n\n\n                     size         mean          std\nregion                                             \nEast China             78  6070.604231  3500.372702\nNorth China            48  4239.038542  2866.705149\nNortheast China        36  3849.076944  1948.531835\nNorthwest China        60  1340.026167  1174.399739\nSouth Central China    68  4835.540882  3697.129915\nSouthwest China        60  2410.398833  2144.589994\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 3.0488753833171947 and the p value is 0.002808541335921234.\nThe comparison between East China and North China is statistically significant at the threshold of 0.05\n                     size           mean            std\nregion                                                 \nEast China             78  355577.897436  275635.866746\nNorth China            48  169600.583333  127011.475909\nNortheast China        36  136623.750000  142734.495232\nNorthwest China        60   15111.133333   22954.193559\nSouth Central China    68  218931.426471  339981.399823\nSouthwest China        60   25405.083333   31171.373876\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 4.391461461316698 and the p value is 2.3859390186769955e-05.\nThe comparison between East China and North China is statistically significant at the threshold of 0.05\n                     size          mean           std\nregion                                               \nEast China             78  1.775615e+06  1.153030e+06\nNorth China            48  1.733719e+06  1.548794e+06\nNortheast China        36  2.665148e+06  1.768442e+06\nNorthwest China        60  1.703538e+06  1.446408e+06\nSouth Central China    68  2.500962e+06  2.196436e+06\nSouthwest China        60  2.424971e+06  2.002198e+06\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 0.17339716493934587 and the p value is 0.862621991978372.\nThe comparison between East China and North China is not statistically significant at the threshold of 0.05\n\n\n\n\n\n\nfrom tqdm import tqdm\nfor i in tqdm(range(10000)):\n    ...\n\n100%|| 10000/10000 [00:00<00:00, 2169617.21it/s]\n\n\n\nEastvNorth=pd.DataFrame()\nEastvNorth= aggregate_and_ttest(df_no_gdp_outliers_subset, test_cells = [0,1])\nEastvNorth\n\n100%|| 3/3 [00:00<00:00, 135.00it/s]\n\n\n                     size         mean          std\nregion                                             \nEast China             78  6070.604231  3500.372702\nNorth China            48  4239.038542  2866.705149\nNortheast China        36  3849.076944  1948.531835\nNorthwest China        60  1340.026167  1174.399739\nSouth Central China    68  4835.540882  3697.129915\nSouthwest China        60  2410.398833  2144.589994\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 3.0488753833171947 and the p value is 0.002808541335921234.\nThe comparison between East China and North China is statistically significant at the threshold of 0.05\n                     size           mean            std\nregion                                                 \nEast China             78  355577.897436  275635.866746\nNorth China            48  169600.583333  127011.475909\nNortheast China        36  136623.750000  142734.495232\nNorthwest China        60   15111.133333   22954.193559\nSouth Central China    68  218931.426471  339981.399823\nSouthwest China        60   25405.083333   31171.373876\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 4.391461461316698 and the p value is 2.3859390186769955e-05.\nThe comparison between East China and North China is statistically significant at the threshold of 0.05\n                     size          mean           std\nregion                                               \nEast China             78  1.775615e+06  1.153030e+06\nNorth China            48  1.733719e+06  1.548794e+06\nNortheast China        36  2.665148e+06  1.768442e+06\nNorthwest China        60  1.703538e+06  1.446408e+06\nSouth Central China    68  2.500962e+06  2.196436e+06\nSouthwest China        60  2.424971e+06  2.002198e+06\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 0.17339716493934587 and the p value is 0.862621991978372.\nThe comparison between East China and North China is not statistically significant at the threshold of 0.05"
  },
  {
    "objectID": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#genearate-an-experimental_crosstab-to-be-used-in-statistical-tests",
    "href": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#genearate-an-experimental_crosstab-to-be-used-in-statistical-tests",
    "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
    "section": "Genearate an experimental_crosstab to be used in statistical tests",
    "text": "Genearate an experimental_crosstab to be used in statistical tests\n\nexperimental_crosstab = df_no_gdp_outliers_subset.groupby('region').agg(['size', 'mean', 'std'])\n\n\nexperimental_crosstab.index\n\nIndex(['East China', 'North China', 'Northeast China', 'Northwest China',\n       'South Central China', 'Southwest China'],\n      dtype='object', name='region')\n\n\n\ndf = experimental_crosstab.T\ndf\n\n\n\n\n\n  \n    \n      \n      region\n      East China\n      North China\n      Northeast China\n      Northwest China\n      South Central China\n      Southwest China\n    \n  \n  \n    \n      gdp\n      size\n      7.800000e+01\n      4.800000e+01\n      3.600000e+01\n      6.000000e+01\n      6.800000e+01\n      6.000000e+01\n    \n    \n      mean\n      6.070604e+03\n      4.239039e+03\n      3.849077e+03\n      1.340026e+03\n      4.835541e+03\n      2.410399e+03\n    \n    \n      std\n      3.500373e+03\n      2.866705e+03\n      1.948532e+03\n      1.174400e+03\n      3.697130e+03\n      2.144590e+03\n    \n    \n      fdi\n      size\n      7.800000e+01\n      4.800000e+01\n      3.600000e+01\n      6.000000e+01\n      6.800000e+01\n      6.000000e+01\n    \n    \n      mean\n      3.555779e+05\n      1.696006e+05\n      1.366238e+05\n      1.511113e+04\n      2.189314e+05\n      2.540508e+04\n    \n    \n      std\n      2.756359e+05\n      1.270115e+05\n      1.427345e+05\n      2.295419e+04\n      3.399814e+05\n      3.117137e+04\n    \n    \n      it\n      size\n      7.800000e+01\n      4.800000e+01\n      3.600000e+01\n      6.000000e+01\n      6.800000e+01\n      6.000000e+01\n    \n    \n      mean\n      1.775615e+06\n      1.733719e+06\n      2.665148e+06\n      1.703538e+06\n      2.500962e+06\n      2.424971e+06\n    \n    \n      std\n      1.153030e+06\n      1.548794e+06\n      1.768442e+06\n      1.446408e+06\n      2.196436e+06\n      2.002198e+06\n    \n  \n\n\n\n\n\n#experimental_crosstab.reset_index().unstack()\n\n\nexperimental_crosstab.iloc[0,1]\n\n6070.604230769231\n\n\n\nexperimental_crosstab.index\n\nIndex(['East China', 'North China', 'Northeast China', 'Northwest China',\n       'South Central China', 'Southwest China'],\n      dtype='object', name='region')\n\n\n\nexperimental_crosstab\n\n\n\n\n\n  \n    \n      \n      gdp\n      fdi\n      it\n    \n    \n      \n      size\n      mean\n      std\n      size\n      mean\n      std\n      size\n      mean\n      std\n    \n    \n      region\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      East China\n      78\n      6070.604231\n      3500.372702\n      78\n      355577.897436\n      275635.866746\n      78\n      1.775615e+06\n      1.153030e+06\n    \n    \n      North China\n      48\n      4239.038542\n      2866.705149\n      48\n      169600.583333\n      127011.475909\n      48\n      1.733719e+06\n      1.548794e+06\n    \n    \n      Northeast China\n      36\n      3849.076944\n      1948.531835\n      36\n      136623.750000\n      142734.495232\n      36\n      2.665148e+06\n      1.768442e+06\n    \n    \n      Northwest China\n      60\n      1340.026167\n      1174.399739\n      60\n      15111.133333\n      22954.193559\n      60\n      1.703538e+06\n      1.446408e+06\n    \n    \n      South Central China\n      68\n      4835.540882\n      3697.129915\n      68\n      218931.426471\n      339981.399823\n      68\n      2.500962e+06\n      2.196436e+06\n    \n    \n      Southwest China\n      60\n      2410.398833\n      2144.589994\n      60\n      25405.083333\n      31171.373876\n      60\n      2.424971e+06\n      2.002198e+06\n    \n  \n\n\n\n\n\nexperimental_crosstab.columns = ['_'.join(col) for col in experimental_crosstab.columns.values]\n\n\nexperimental_crosstab\n\n\n\n\n\n  \n    \n      \n      gdp_size\n      gdp_mean\n      gdp_std\n      fdi_size\n      fdi_mean\n      fdi_std\n      it_size\n      it_mean\n      it_std\n    \n    \n      region\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      East China\n      78\n      6070.604231\n      3500.372702\n      78\n      355577.897436\n      275635.866746\n      78\n      1.775615e+06\n      1.153030e+06\n    \n    \n      North China\n      48\n      4239.038542\n      2866.705149\n      48\n      169600.583333\n      127011.475909\n      48\n      1.733719e+06\n      1.548794e+06\n    \n    \n      Northeast China\n      36\n      3849.076944\n      1948.531835\n      36\n      136623.750000\n      142734.495232\n      36\n      2.665148e+06\n      1.768442e+06\n    \n    \n      Northwest China\n      60\n      1340.026167\n      1174.399739\n      60\n      15111.133333\n      22954.193559\n      60\n      1.703538e+06\n      1.446408e+06\n    \n    \n      South Central China\n      68\n      4835.540882\n      3697.129915\n      68\n      218931.426471\n      339981.399823\n      68\n      2.500962e+06\n      2.196436e+06\n    \n    \n      Southwest China\n      60\n      2410.398833\n      2144.589994\n      60\n      25405.083333\n      31171.373876\n      60\n      2.424971e+06\n      2.002198e+06\n    \n  \n\n\n\n\n\nexperimental_crosstab.loc['East China', 'gdp_size']\n\n78\n\n\n\nexperimental_crosstab.to_csv('fiscal_experimental_crosstab.csv')"
  },
  {
    "objectID": "posts/2021-05-31-eda_sklearn_examples.html",
    "href": "posts/2021-05-31-eda_sklearn_examples.html",
    "title": "EDA with Sklearn examples",
    "section": "",
    "text": "code adapted from: https://github.com/thomasjpfan/ml-workshop-intro"
  },
  {
    "objectID": "posts/2021-05-31-eda_sklearn_examples.html#generated-datasets",
    "href": "posts/2021-05-31-eda_sklearn_examples.html#generated-datasets",
    "title": "EDA with Sklearn examples",
    "section": "Generated datasets",
    "text": "Generated datasets\n\n## Reading the dataset using pandas\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/CTG.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n  \n    \n      \n      FileName\n      Date\n      SegFile\n      b\n      e\n      LBE\n      LB\n      AC\n      FM\n      UC\n      ...\n      C\n      D\n      E\n      AD\n      DE\n      LD\n      FS\n      SUSP\n      CLASS\n      NSP\n    \n  \n  \n    \n      0\n      Variab10.txt\n      12/1/1996\n      CTG0001.txt\n      240.0\n      357.0\n      120.0\n      120.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      9.0\n      2.0\n    \n    \n      1\n      Fmcs_1.txt\n      5/3/1996\n      CTG0002.txt\n      5.0\n      632.0\n      132.0\n      132.0\n      4.0\n      0.0\n      4.0\n      ...\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      6.0\n      1.0\n    \n    \n      2\n      Fmcs_1.txt\n      5/3/1996\n      CTG0003.txt\n      177.0\n      779.0\n      133.0\n      133.0\n      2.0\n      0.0\n      5.0\n      ...\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      6.0\n      1.0\n    \n    \n      3\n      Fmcs_1.txt\n      5/3/1996\n      CTG0004.txt\n      411.0\n      1192.0\n      134.0\n      134.0\n      2.0\n      0.0\n      6.0\n      ...\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      6.0\n      1.0\n    \n    \n      4\n      Fmcs_1.txt\n      5/3/1996\n      CTG0005.txt\n      533.0\n      1147.0\n      132.0\n      132.0\n      4.0\n      0.0\n      5.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2.0\n      1.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2124\n      S8001045.dsp\n      6/6/1998\n      CTG2127.txt\n      1576.0\n      3049.0\n      140.0\n      140.0\n      1.0\n      0.0\n      9.0\n      ...\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      5.0\n      2.0\n    \n    \n      2125\n      S8001045.dsp\n      6/6/1998\n      CTG2128.txt\n      2796.0\n      3415.0\n      142.0\n      142.0\n      1.0\n      1.0\n      5.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n    \n    \n      2126\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2127\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2128\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      564.0\n      23.0\n      ...\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n2129 rows  40 columns\n\n\n\n\n## Dropping the columns which we don't need\ndf=df.drop([\"FileName\",\"Date\",\"SegFile\",\"b\",\"e\"],axis=1)\ndf.head()\n\n\n\n\n\n  \n    \n      \n      LBE\n      LB\n      AC\n      FM\n      UC\n      ASTV\n      MSTV\n      ALTV\n      MLTV\n      DL\n      ...\n      C\n      D\n      E\n      AD\n      DE\n      LD\n      FS\n      SUSP\n      CLASS\n      NSP\n    \n  \n  \n    \n      0\n      120.0\n      120.0\n      0.0\n      0.0\n      0.0\n      73.0\n      0.5\n      43.0\n      2.4\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      9.0\n      2.0\n    \n    \n      1\n      132.0\n      132.0\n      4.0\n      0.0\n      4.0\n      17.0\n      2.1\n      0.0\n      10.4\n      2.0\n      ...\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      6.0\n      1.0\n    \n    \n      2\n      133.0\n      133.0\n      2.0\n      0.0\n      5.0\n      16.0\n      2.1\n      0.0\n      13.4\n      2.0\n      ...\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      6.0\n      1.0\n    \n    \n      3\n      134.0\n      134.0\n      2.0\n      0.0\n      6.0\n      16.0\n      2.4\n      0.0\n      23.0\n      2.0\n      ...\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      6.0\n      1.0\n    \n    \n      4\n      132.0\n      132.0\n      4.0\n      0.0\n      5.0\n      16.0\n      2.4\n      0.0\n      19.9\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2.0\n      1.0\n    \n  \n\n5 rows  35 columns\n\n\n\n\ndf['C']\n\n0       0.0\n1       0.0\n2       0.0\n3       0.0\n4       0.0\n       ... \n2124    0.0\n2125    0.0\n2126    NaN\n2127    NaN\n2128    NaN\nName: C, Length: 2129, dtype: float64\n\n\n\nX = df['C']\nX\n\n0       0.0\n1       0.0\n2       0.0\n3       0.0\n4       0.0\n       ... \n2124    0.0\n2125    0.0\n2126    NaN\n2127    NaN\n2128    NaN\nName: C, Length: 2129, dtype: float64\n\n\n\ny = df['NSP']\ny\n\n0       2.0\n1       1.0\n2       1.0\n3       1.0\n4       1.0\n       ... \n2124    2.0\n2125    1.0\n2126    NaN\n2127    NaN\n2128    NaN\nName: NSP, Length: 2129, dtype: float64\n\n\n\nimport seaborn as sns\nsns.set_theme(font_scale=1.5)\n\n\ndf.columns\n\nIndex(['LBE', 'LB', 'AC', 'FM', 'UC', 'ASTV', 'MSTV', 'ALTV', 'MLTV', 'DL',\n       'DS', 'DP', 'DR', 'Width', 'Min', 'Max', 'Nmax', 'Nzeros', 'Mode',\n       'Mean', 'Median', 'Variance', 'Tendency', 'A', 'B', 'C', 'D', 'E', 'AD',\n       'DE', 'LD', 'FS', 'SUSP', 'CLASS', 'NSP'],\n      dtype='object')\n\n\n\nsns.relplot(data=df, x='B', y='NSP', height=6);\n\n\n\n\n\nsns.displot(data=df, x='B', hue='NSP', kind='kde', aspect=2);\n\n/home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\nsns.jointplot(data=df, x=\"B\", y=\"C\", height=10, hue='NSP');\n\n/home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n  warnings.warn(msg, UserWarning)\n/home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\nsns.jointplot(x=\"C\", y=\"B\", data=df, height=10, hue='NSP')\n\n/home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n  warnings.warn(msg, UserWarning)\n/home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n  warnings.warn(msg, UserWarning)\n\n\n<seaborn.axisgrid.JointGrid at 0x7f72c8970460>"
  },
  {
    "objectID": "posts/2021-01-23-voila-datasette-blog-post.html",
    "href": "posts/2021-01-23-voila-datasette-blog-post.html",
    "title": "didactic",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport pandas_datareader.data as web\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport datetime\nimport sqlite3\nimport pandas as pd\nfrom sqlalchemy.sql.schema import Column\n\n\nstart = pd.to_datetime('2018-01-01')\nend = pd.to_datetime('today')\n\nFXAIX_stock = web.DataReader('FXAIX', 'yahoo', start, end)\nFXAIX_stock.head()\n\nSP500_stock = web.DataReader('^GSPC', 'yahoo', start, end)\nSP500_stock.head()\n\nRussell_2000_stock = web.DataReader('^RUT', 'yahoo', start, end)\nRussell_2000_stock.head()\n\n\nstocks = pd.concat([SP500_stock['Open'], Russell_2000_stock['Open'], FXAIX_stock['Open']], axis = 1)\n\nstocks.columns = ['SP500_stock','Russell_2000_stock', 'FXAIX_stock']\n\nstocks.reset_index(level=0, inplace=True)\nstocks\n\nengine = db.create_engine('sqlite:///stocks.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nstocks_table = db.Table('index_stocks_table', metadata, \n    db.Column('Date',db.Date, nullable=True, index=False),\n    db.Column('SP500_stock',db.Integer, nullable=True),\n    db.Column('Russell_2000_stock',db.Integer, nullable=True),\n    db.Column('FXAIX_stock', db.Numeric, nullable=True)\n)\n\nmetadata.create_all(engine) \n\nstocks.to_sql('index_stocks_table', con=engine, if_exists='replace', index=False)\n\nsql = \"SELECT * FROM index_stocks_table LIMIT 10\"\ncnxn = connection\ndf = pd.read_sql(sql, cnxn)\nprint(df.head(10))\n\n                         Date  SP500_stock  Russell_2000_stock  FXAIX_stock\n0  2018-01-02 00:00:00.000000  2683.729980         1536.119995    94.230003\n1  2018-01-03 00:00:00.000000  2697.850098         1550.280029    94.830002\n2  2018-01-04 00:00:00.000000  2719.310059         1552.979980    95.230003\n3  2018-01-05 00:00:00.000000  2731.330078         1555.869995    95.900002\n4  2018-01-08 00:00:00.000000  2742.669922         1559.800049    96.059998\n5  2018-01-09 00:00:00.000000  2751.149902         1562.219971    96.220001\n6  2018-01-10 00:00:00.000000  2745.550049         1559.010010    96.110001\n7  2018-01-11 00:00:00.000000  2752.969971         1560.219971    96.790001\n8  2018-01-12 00:00:00.000000  2770.179932         1587.119995    97.449997\n9  2018-01-16 00:00:00.000000  2798.959961         1592.910034    97.110001\n\n\n\n#!/bin/bash\nsource ~/anaconda3/etc/profile.d/conda.sh\nconda activate base\npython stocks_fiax.py\ndatasette publish heroku -n stocks-snp-500 stocks.db --install=datasette-vega\n\n\n#snp_list_tables.columns = ['Symbol','Security', 'SEC filings', 'GICS Sector', 'GICS Sub-Industry', ' Headquarters Location', 'Date first added', 'CIK', 'Founded']\n#snp_list_tables.head(10)\n#\n#\n#snp_list_table = db.Table('snp_list_tables', metadata, \n#    db.Column('Symbol',db.String, nullable=True, index=False),\n#    db.Column('Security',db.String, nullable=True),\n#    db.Column('SEC filings',db.String, nullable=True),\n#    db.Column('GICS Sub-Industry',db.String, nullable=True),\n#    db.Column('Headquarters Location',db.String, nullable=True)\n#    , db.Column('Date first added', db.Date, nullable=True)\n#    , db.Column('CIK', db.String, nullable=True)\n#    , db.Column('Founded', db.Date, nullable=True)\n#)\n#\n#snp_list_table.to_sql('snp_list', con=engine, if_exists='replace', index=False)\n#\n#sql = \"SELECT * FROM snp_list LIMIT 10\"\n#cnxn = connection\n#df = pd.read_sql(sql, cnxn)\n#print(df.head(30))"
  },
  {
    "objectID": "posts/2020-10-11-dask-xgboost-fiscal-data.html",
    "href": "posts/2020-10-11-dask-xgboost-fiscal-data.html",
    "title": "Moving Fiscal Data from a sqlite db to a dask dataframe",
    "section": "",
    "text": "import numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///fiscal_data.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nengine.execute(\"SELECT * FROM fiscal_data LIMIT 1\").fetchall()\n\n[(1996, 'East China', 'Anhui', '2093.3', 50661, 631930, 147002)]\n\n\n\nsql = \"\"\"\nSELECT year\n, region\n, province\n, gdp\n, fdi\n, it\n, specific\nFROM fiscal_data\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.3\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.7\n      668128\n      3162299\n      656175.0\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      18753.73\n      1036576\n      2939778\n      844647.0\n    \n  \n\n360 rows  7 columns\n\n\n\n\ndf.columns\n\nIndex(['year', 'region', 'province', 'gdp', 'fdi', 'it', 'specific'], dtype='object')\n\n\n\ndf.gdp.hist()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f41255eacc0>\n\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n\nfrom dask import dataframe as dd \n\n\nddf = dd.from_pandas(df, npartitions=5)\n\n\nprint(ddf)\n\n\nddf.npartitions\n\n\nddf.npartitions\n\n\nlen(ddf)\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n\nddf.describe().compute()\n\n\nddf.columns\n\n\nfeat_list = [\"year\", \"fdi\"]\ncat_feat_list = [\"region\", \"province\"]\ntarget = [\"gdp\"]\n\n\nddf[\"year\"] = ddf[\"year\"].astype(float)\nddf[\"fdi\"] = ddf[\"fdi\"].astype(float)\n#ddf[\"province\"] = ddf[\"province\"].astype(float)\n#ddf[\"region\"] = ddf[\"region\"].astype(float)\nddf[\"gdp\"] = ddf[\"gdp\"].astype(float)\nddf[\"it\"] = ddf[\"it\"].astype(float)\n\n\ntype(target)\n\n\nx=ddf[feat_list].persist()\ny=ddf[target].persist()\n\n\nx\n\n\ny.compute()\n\n\nprint(x.shape,y.shape)\n\n\nx.count().compute()\n\n\nfrom dask_ml.xgboost import XGBRegressor\n\n\nXGBR = XGBRegressor()\n\n\n%%time\nXGBR_model = XGBR.fit(x,y)\n\n\nXGBR_model\n\n\nclient.close()"
  },
  {
    "objectID": "posts/2020-09-28-databases_sqllite_sqlalchemy_27.html",
    "href": "posts/2020-09-28-databases_sqllite_sqlalchemy_27.html",
    "title": "Creating and accessing SQL databases with python using sqlalchemy and sqlite3",
    "section": "",
    "text": "import sqlalchemy as db\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///music.sqlite')\n\n\nconnection = engine.connect()\nmetadata = db.MetaData()\n\nmusic = db.Table('music', metadata,\n              db.Column('Id', db.Integer()),\n              db.Column('song', db.String(255), nullable=False),\n              db.Column('album', db.String(255), nullable=False),\n              db.Column('artist', db.String(255), nullable=False)\n              )\n\nmetadata.create_all(engine) \n\n\n#Inserting one record\nquery = db.insert(music).values(Id=1, song='song3', album='album3', artist='artist3') \nResultProxy = connection.execute(query)\n\n\n#Inserting many records\nquery = db.insert(music) \nvalues_list = [{'Id':'2', 'song':'song1', 'album':'album1', 'artist':'artist1'},\n               {'Id':'3', 'song':'song2', 'album':'album2', 'artist':'artist2'}]\n\nResultProxy = connection.execute(query,values_list)\n\nresults = connection.execute(db.select([music])).fetchall()\ndf = pd.DataFrame(results)\ndf.columns = results[0].keys()\ndf.head(10)\n\n\n\n\n\n  \n    \n      \n      Id\n      song\n      album\n      artist\n    \n  \n  \n    \n      0\n      1\n      song3\n      album3\n      artist3\n    \n    \n      1\n      2\n      song1\n      album1\n      artist1\n    \n    \n      2\n      3\n      song2\n      album2\n      artist2\n    \n    \n      3\n      2\n      song1\n      album1\n      artist1\n    \n    \n      4\n      3\n      song2\n      album2\n      artist2\n    \n  \n\n\n\n\n\nresults = connection.execute(db.select([music])).fetchall()\ndf = pd.DataFrame(results)\ndf.columns = results[0].keys()\ndf.head(4)\n\n\nquery = db.select([music]).where(db.and_(music.columns.song == 'song3', music.columns.artist == 'artist3'))\nresult = connection.execute(query).fetchall()\nresult[:3]\n\n[(1, 'song3', 'album3', 'artist3')]\n\n\n\nconn = sqlite3.connect('music.sqlite')\n\n\nc = conn.cursor()\n\n# Create table\nc.execute('''CREATE TABLE stockmarket\n             (date text, trans text, symbol text, qty real, price real)''')\n\n# Insert a row of data\nc.execute(\"INSERT INTO stockmarket VALUES ('2006-01-05','BUY','RHAT',100,35.14)\")\n\n# Save (commit) the changes\nconn.commit()\n\n# We can also close the connection if we are done with it.\n# Just be sure any changes have been committed or they will be lost.\nconn.close()\n\n\nconn = sqlite3.connect('music.sqlite')\nc = conn.cursor()\n\n\nsymbol = 'RHAT'\nc.execute(\"SELECT * FROM stockmarket WHERE symbol = '%s'\" % symbol)\n\n<sqlite3.Cursor at 0x7f0098ae0180>\n\n\n\nt = ('RHAT',)\nc.execute('SELECT * FROM stockmarket WHERE symbol=?', t)\nprint(c.fetchone())\n\n('2006-01-05', 'BUY', 'RHAT', 100.0, 35.14)\n\n\n\n# Larger example that inserts many records at a time\npurchases = [('2006-03-28', 'BUY', 'IBM', 1000, 45.00),\n             ('2006-04-05', 'BUY', 'MSFT', 1000, 72.00),\n             ('2006-04-06', 'SELL', 'IBM', 500, 53.00),\n            ]\nc.executemany('INSERT INTO stockmarket VALUES (?,?,?,?,?)', purchases)\n\n<sqlite3.Cursor at 0x7f0098ae0180>\n\n\n\nfor row in c.execute('SELECT * FROM stockmarket ORDER BY price'):\n        print(row)\n\n('2006-01-05', 'BUY', 'RHAT', 100.0, 35.14)\n('2006-03-28', 'BUY', 'IBM', 1000.0, 45.0)\n('2006-04-06', 'SELL', 'IBM', 500.0, 53.0)\n('2006-04-05', 'BUY', 'MSFT', 1000.0, 72.0)\n\n\n\n# Use dbeaver to examine"
  },
  {
    "objectID": "posts/2020-08-19-pyspark-filtering.html",
    "href": "posts/2020-08-19-pyspark-filtering.html",
    "title": "Dataframe Filitering and Operations with Pyspark",
    "section": "",
    "text": "from pyspark.sql import SparkSession\n\n# Load data from a CSV\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").option(\"inferSchema\", True).option(\"header\", True).load(file_location)\ndisplay(df.take(5))\n\n\n_c0provincespecificgeneralyeargdpfdirnrrrifrregit0Anhui147002.0null19962093.3506610.00.00.01128873East China6319301Anhui151981.0null19972347.32434430.00.00.01356287East China6578602Anhui174930.0null19982542.96276730.00.00.01518236East China8894633Anhui285324.0null19992712.3426131nullnullnull1646891East China12273644Anhui195580.032100.020002902.09318470.00.00.01601508East China1499110"
  },
  {
    "objectID": "posts/2020-08-19-pyspark-filtering.html#filtering-on-values-in-a-column",
    "href": "posts/2020-08-19-pyspark-filtering.html#filtering-on-values-in-a-column",
    "title": "Dataframe Filitering and Operations with Pyspark",
    "section": "Filtering on values in a column",
    "text": "Filtering on values in a column\n\ndf.filter(\"specific<10000\").show()\n\n\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n_c0|province|specific|general|year|    gdp|   fdi|rnr| rr|   i|     fr|       reg|     it|\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n268|Shanghai|  8964.0|   null|2000|4771.17|316014|0.0|0.0|0.44|2224124|East China|1212473|\n269|Shanghai|  9834.0|   null|2001|5210.12|429159|0.0|0.0|0.44|2947285|East China|1053917|\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n\n\n\n\n\ndf.filter(\"specific<10000\").select('province').show()\n\n\n+--------+\nprovince|\n+--------+\nShanghai|\nShanghai|\n+--------+\n\n\n\n\n\ndf.filter(\"specific<10000\").select(['province','year']).show()\n\n\n+--------+----+\nprovince|year|\n+--------+----+\nShanghai|2000|\nShanghai|2001|\n+--------+----+\n\n\n\n\n\ndf.filter(df[\"specific\"] < 10000).show()\n\n\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n_c0|province|specific|general|year|    gdp|   fdi|rnr| rr|   i|     fr|       reg|     it|\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n268|Shanghai|  8964.0|   null|2000|4771.17|316014|0.0|0.0|0.44|2224124|East China|1212473|\n269|Shanghai|  9834.0|   null|2001|5210.12|429159|0.0|0.0|0.44|2947285|East China|1053917|\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+"
  },
  {
    "objectID": "posts/2020-08-19-pyspark-filtering.html#filtering-on-values-in-2-columns",
    "href": "posts/2020-08-19-pyspark-filtering.html#filtering-on-values-in-2-columns",
    "title": "Dataframe Filitering and Operations with Pyspark",
    "section": "Filtering on values in 2+ columns",
    "text": "Filtering on values in 2+ columns\n\ndf.filter((df[\"specific\"] < 55000) & (df['gdp'] > 200) ).show()\n\n\n+---+--------+--------+-------+----+--------+------+----+----+----+-------+-------------------+-------+\n_c0|province|specific|general|year|     gdp|   fdi| rnr|  rr|   i|     fr|                reg|     it|\n+---+--------+--------+-------+----+--------+------+----+----+----+-------+-------------------+-------+\n 98|  Hainan| 54462.0|   null|1998|  442.13| 71715|null|null|null| 236461|South Central China| 177748|\n216| Ningxia| 32088.0|   null|1996|   202.9|  2826|null|null|null|  90805|    Northwest China| 178668|\n217| Ningxia| 44267.0|   null|1997|  224.59|   671|null|null|null| 102083|    Northwest China| 195295|\n268|Shanghai|  8964.0|   null|2000| 4771.17|316014| 0.0| 0.0|0.44|2224124|         East China|1212473|\n269|Shanghai|  9834.0|   null|2001| 5210.12|429159| 0.0| 0.0|0.44|2947285|         East China|1053917|\n270|Shanghai| 19985.0|   null|2002| 5741.03|427229| 0.0| 0.0|0.44|3380397|         East China|1572208|\n271|Shanghai| 23547.0|   null|2003| 6694.23|546849| 0.0|0.53| 0.0|4461153|         East China|2031496|\n272|Shanghai| 29943.0|   null|2004| 8072.83|654100| 0.0|0.53| 0.0|   null|         East China|2703643|\n273|Shanghai| 29943.0|   null|2005| 9247.66|685000| 0.0|0.53| 0.0|   null|         East China|2140461|\n274|Shanghai| 42928.0|   null|2006|10572.24|710700| 0.0|0.53| 0.0|8175966|         East China|2239987|\n302| Tianjin| 39364.0|   null|1998|  1374.6|211361|null|null|null| 540178|        North China| 361723|\n303| Tianjin| 45463.0|   null|1999| 1500.95|176399| 0.0| 0.0| 0.0| 605662|        North China| 422522|\n304| Tianjin| 51821.0|   null|2000| 1701.88|116601| 0.0| 0.0| 0.0| 757464|        North China| 547120|\n305| Tianjin| 35084.0|   null|2001| 1919.09|213348| 0.0| 0.0| 0.0| 942763|        North China| 688810|\n+---+--------+--------+-------+----+--------+------+----+----+----+-------+-------------------+-------+\n\n\n\n\n\ndf.filter((df[\"specific\"] < 55000) | (df['gdp'] > 20000) ).show()\n\n\n+---+---------+---------+--------+----+--------+-------+--------------------+----+-----------+--------+-------------------+-------+\n_c0| province| specific| general|year|     gdp|    fdi|                 rnr|  rr|          i|      fr|                reg|     it|\n+---+---------+---------+--------+----+--------+-------+--------------------+----+-----------+--------+-------------------+-------+\n 69|Guangdong|1491588.0|    null|2005|22557.37|1236400|0.027027027000000002| 0.0|        0.0|    null|South Central China|4327217|\n 70|Guangdong|1897575.0|498913.0|2006|26587.76|1451065|0.027027027000000002| 0.0|        0.0|16804703|South Central China|4559252|\n 71|Guangdong| 859482.0|     0.0|2007|31777.01|1712603|0.027027027000000002| 0.0|        0.0|27858007|South Central China|4947824|\n 98|   Hainan|  54462.0|    null|1998|  442.13|  71715|                null|null|       null|  236461|South Central China| 177748|\n179|  Jiangsu|1188989.0|     0.0|2007|21742.05|1743140|                 0.0| 0.0|0.275862069|22377276|         East China|3557071|\n216|  Ningxia|  32088.0|    null|1996|   202.9|   2826|                null|null|       null|   90805|    Northwest China| 178668|\n217|  Ningxia|  44267.0|    null|1997|  224.59|    671|                null|null|       null|  102083|    Northwest China| 195295|\n228|  Qinghai|  37976.0|    null|1996|  184.17|    576|                null|null|       null|   73260|    Northwest China| 218361|\n262| Shandong|1204547.0|112137.0|2006|21900.19|1000069|                 0.0| 0.0|        0.0|11673659|         East China|5304833|\n263| Shandong|2121243.0|581800.0|2007|25776.91|1101159|                 0.0| 0.0|        0.0|16753980|         East China|6357869|\n268| Shanghai|   8964.0|    null|2000| 4771.17| 316014|                 0.0| 0.0|       0.44| 2224124|         East China|1212473|\n269| Shanghai|   9834.0|    null|2001| 5210.12| 429159|                 0.0| 0.0|       0.44| 2947285|         East China|1053917|\n270| Shanghai|  19985.0|    null|2002| 5741.03| 427229|                 0.0| 0.0|       0.44| 3380397|         East China|1572208|\n271| Shanghai|  23547.0|    null|2003| 6694.23| 546849|                 0.0|0.53|        0.0| 4461153|         East China|2031496|\n272| Shanghai|  29943.0|    null|2004| 8072.83| 654100|                 0.0|0.53|        0.0|    null|         East China|2703643|\n273| Shanghai|  29943.0|    null|2005| 9247.66| 685000|                 0.0|0.53|        0.0|    null|         East China|2140461|\n274| Shanghai|  42928.0|    null|2006|10572.24| 710700|                 0.0|0.53|        0.0| 8175966|         East China|2239987|\n302|  Tianjin|  39364.0|    null|1998|  1374.6| 211361|                null|null|       null|  540178|        North China| 361723|\n303|  Tianjin|  45463.0|    null|1999| 1500.95| 176399|                 0.0| 0.0|        0.0|  605662|        North China| 422522|\n304|  Tianjin|  51821.0|    null|2000| 1701.88| 116601|                 0.0| 0.0|        0.0|  757464|        North China| 547120|\n+---+---------+---------+--------+----+--------+-------+--------------------+----+-----------+--------+-------------------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.filter((df[\"specific\"] < 55000) & ~(df['gdp'] > 20000) ).show()\n\n\n+---+--------+--------+-------+----+--------+------+-----------+----+----+-------+-------------------+-------+\n_c0|province|specific|general|year|     gdp|   fdi|        rnr|  rr|   i|     fr|                reg|     it|\n+---+--------+--------+-------+----+--------+------+-----------+----+----+-------+-------------------+-------+\n 98|  Hainan| 54462.0|   null|1998|  442.13| 71715|       null|null|null| 236461|South Central China| 177748|\n216| Ningxia| 32088.0|   null|1996|   202.9|  2826|       null|null|null|  90805|    Northwest China| 178668|\n217| Ningxia| 44267.0|   null|1997|  224.59|   671|       null|null|null| 102083|    Northwest China| 195295|\n228| Qinghai| 37976.0|   null|1996|  184.17|   576|       null|null|null|  73260|    Northwest China| 218361|\n268|Shanghai|  8964.0|   null|2000| 4771.17|316014|        0.0| 0.0|0.44|2224124|         East China|1212473|\n269|Shanghai|  9834.0|   null|2001| 5210.12|429159|        0.0| 0.0|0.44|2947285|         East China|1053917|\n270|Shanghai| 19985.0|   null|2002| 5741.03|427229|        0.0| 0.0|0.44|3380397|         East China|1572208|\n271|Shanghai| 23547.0|   null|2003| 6694.23|546849|        0.0|0.53| 0.0|4461153|         East China|2031496|\n272|Shanghai| 29943.0|   null|2004| 8072.83|654100|        0.0|0.53| 0.0|   null|         East China|2703643|\n273|Shanghai| 29943.0|   null|2005| 9247.66|685000|        0.0|0.53| 0.0|   null|         East China|2140461|\n274|Shanghai| 42928.0|   null|2006|10572.24|710700|        0.0|0.53| 0.0|8175966|         East China|2239987|\n302| Tianjin| 39364.0|   null|1998|  1374.6|211361|       null|null|null| 540178|        North China| 361723|\n303| Tianjin| 45463.0|   null|1999| 1500.95|176399|        0.0| 0.0| 0.0| 605662|        North China| 422522|\n304| Tianjin| 51821.0|   null|2000| 1701.88|116601|        0.0| 0.0| 0.0| 757464|        North China| 547120|\n305| Tianjin| 35084.0|   null|2001| 1919.09|213348|        0.0| 0.0| 0.0| 942763|        North China| 688810|\n312|   Tibet| 18829.0|   null|1996|   64.98|   679|0.181818182| 0.0| 0.0|  27801|    Southwest China| 306114|\n313|   Tibet| 25185.0|   null|1997|   77.24|    63|0.181818182| 0.0| 0.0|  33787|    Southwest China| 346368|\n314|   Tibet| 48197.0|   null|1998|    91.5|   481|        0.0|0.24| 0.0|   3810|    Southwest China| 415547|\n+---+--------+--------+-------+----+--------+------+-----------+----+----+-------+-------------------+-------+\n\n\n\n\n\ndf.filter(df[\"specific\"] == 8964.0).show()\n\n\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n_c0|province|specific|general|year|    gdp|   fdi|rnr| rr|   i|     fr|       reg|     it|\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n268|Shanghai|  8964.0|   null|2000|4771.17|316014|0.0|0.0|0.44|2224124|East China|1212473|\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n\n\n\n\n\ndf.filter(df[\"province\"] == \"Zhejiang\").show()\n\n\n+---+--------+---------+--------+----+--------+-------+-----------+-----------+-----------+--------+----------+-------+\n_c0|province| specific| general|year|     gdp|    fdi|        rnr|         rr|          i|      fr|       reg|     it|\n+---+--------+---------+--------+----+--------+-------+-----------+-----------+-----------+--------+----------+-------+\n348|Zhejiang| 273253.0|    null|1996| 4188.53| 152021|        0.0|        0.0|        0.0| 1291252|East China| 740327|\n349|Zhejiang| 330558.0|    null|1997| 4686.11| 150345|        0.0|        0.0|        0.0| 1432453|East China| 814253|\n350|Zhejiang| 426756.0|    null|1998| 5052.62| 131802|        0.0|        0.0|        0.0| 1761084|East China| 923455|\n351|Zhejiang| 586457.0|    null|1999| 5443.92| 123262|        0.0|        0.0|        0.0| 2146200|East China|1001703|\n352|Zhejiang| 408151.0|    null|2000| 6141.03| 161266|        0.0|        0.0|        0.0| 2955508|East China|1135215|\n353|Zhejiang| 358714.0|    null|2001| 6898.34| 221162|        0.0|        0.0|        0.0| 4436868|East China|1203372|\n354|Zhejiang| 365437.0|321686.0|2002| 8003.67| 307610|        0.0|        0.0|        0.0| 4958329|East China|1962633|\n355|Zhejiang| 391292.0|260313.0|2003| 9705.02| 498055|1.214285714|0.035714286|0.035714286| 6217715|East China|2261631|\n356|Zhejiang| 656175.0|276652.0|2004| 11648.7| 668128|1.214285714|0.035714286|0.035714286|    null|East China|3162299|\n357|Zhejiang| 656175.0|    null|2005|13417.68| 772000|1.214285714|0.035714286|0.035714286|    null|East China|2370200|\n358|Zhejiang|1017303.0|394795.0|2006|15718.47| 888935|1.214285714|0.035714286|0.035714286|11537149|East China|2553268|\n359|Zhejiang| 844647.0|     0.0|2007|18753.73|1036576|0.047619048|        0.0|        0.0|16494981|East China|2939778|\n+---+--------+---------+--------+----+--------+-------+-----------+-----------+-----------+--------+----------+-------+\n\n\n\n\n\ndf.filter(df[\"specific\"] == 8964.0).collect()\n\n\nOut[15]: [Row(_c0=268, province='Shanghai', specific=8964.0, general=None, year=2000, gdp=4771.17, fdi=316014, rnr=0.0, rr=0.0, i=0.44, fr='2224124', reg='East China', it=1212473)]\n\n\n\nresult = df.filter(df[\"specific\"] == 8964.0).collect()\n\n\n\n\n\n\ntype(result[0])\n\n\nOut[17]: pyspark.sql.types.Row\n\n\n\nrow = result[0]\n\n\n\n\n\n\nrow.asDict()\n\n\nOut[19]: {'_c0': 268,\n 'province': 'Shanghai',\n 'specific': 8964.0,\n 'general': None,\n 'year': 2000,\n 'gdp': 4771.17,\n 'fdi': 316014,\n 'rnr': 0.0,\n 'rr': 0.0,\n 'i': 0.44,\n 'fr': '2224124',\n 'reg': 'East China',\n 'it': 1212473}\n\n\n\nfor item in result[0]:\n    print(item)\n\n\n268\nShanghai\n8964.0\nNone\n2000\n4771.17\n316014\n0.0\n0.0\n0.44\n2224124\nEast China\n1212473\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "",
    "text": "import numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\nimport seaborn as sns"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html#distributions-of-dependant-variables",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html#distributions-of-dependant-variables",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "Distributions of Dependant Variables",
    "text": "Distributions of Dependant Variables\n\nRight skew\n\nsns.distplot(df['gdp'])\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f1043c20588>\n\n\n\n\n\n\nsns.distplot(df['fdi'])\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f10437f74a8>\n\n\n\n\n\n\nsns.distplot(df['it'])\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f1043a09ef0>\n\n\n\n\n\n\nsns.distplot(df['specific'].dropna())\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f10439b7a20>\n\n\n\n\n\n\ndf.hist(column=['fdi'], bins=60)\n\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f10439a19e8>]],\n      dtype=object)"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "Removal of GDP value outliers more than 3 standard deviations away from the mean",
    "text": "Removal of GDP value outliers more than 3 standard deviations away from the mean"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "outlier removal of rows with GDP values that are > 3 standard deviations away form the mean",
    "text": "outlier removal of rows with GDP values that are > 3 standard deviations away form the mean\n\nimport scipy.stats as stats\n\n\ndf['gdp_zscore'] = stats.zscore(df['gdp'])"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "these are the observations more then > 3 SDs away from the mean of gdp that will be dropped",
    "text": "these are the observations more then > 3 SDs away from the mean of gdp that will be dropped\n\ndf[abs(df['gdp_zscore'])>3].hist(column = ['gdp'])\n\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f104364e0f0>]],\n      dtype=object)\n\n\n\n\n\n\ndf_no_gdp_outliers=df[abs(df['gdp_zscore'])<3]\n\n\ndf_no_gdp_outliers\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n      gdp_zscore\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.30\n      50661\n      631930\n      147002.0\n      -0.521466\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n      -0.464746\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n      -0.421061\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n      -0.383239\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n      -0.340870\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      354\n      2002\n      East China\n      Zhejiang\n      8003.67\n      307610\n      1962633\n      365437.0\n      0.798274\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n      1.178172\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.70\n      668128\n      3162299\n      656175.0\n      1.612181\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n      2.007180\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n      2.520929\n    \n  \n\n350 rows  8 columns\n\n\n\n\ndf_no_gdp_outliers.hist(column=['gdp'], bins=60)\n\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f10429f5ba8>]],\n      dtype=object)\n\n\n\n\n\n\ncounts_fiscal=df.groupby('region').count()\ncounts_fiscal\n\n\n\n\n\n  \n    \n      \n      year\n      province\n      gdp\n      fdi\n      it\n      specific\n      gdp_zscore\n    \n    \n      region\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      East China\n      84\n      84\n      84\n      84\n      84\n      84\n      84\n    \n    \n      North China\n      48\n      48\n      48\n      48\n      48\n      47\n      48\n    \n    \n      Northeast China\n      36\n      36\n      36\n      36\n      36\n      36\n      36\n    \n    \n      Northwest China\n      60\n      60\n      60\n      60\n      60\n      60\n      60\n    \n    \n      South Central China\n      72\n      72\n      72\n      72\n      72\n      72\n      72\n    \n    \n      Southwest China\n      60\n      60\n      60\n      60\n      60\n      57\n      60\n    \n  \n\n\n\n\n\ncounts_fiscal=df.groupby('province').count()\ncounts_fiscal\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      gdp\n      fdi\n      it\n      specific\n      gdp_zscore\n    \n    \n      province\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Anhui\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Beijing\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Chongqing\n      12\n      12\n      12\n      12\n      12\n      9\n      12\n    \n    \n      Fujian\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Gansu\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Guangdong\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Guangxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Guizhou\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hainan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hebei\n      12\n      12\n      12\n      12\n      12\n      11\n      12\n    \n    \n      Heilongjiang\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Henan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hubei\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hunan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Jiangsu\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Jiangxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Jilin\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Liaoning\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Ningxia\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Qinghai\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shaanxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shandong\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shanghai\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shanxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Sichuan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Tianjin\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Tibet\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Xinjiang\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Yunnan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Zhejiang\n      12\n      12\n      12\n      12\n      12\n      12\n      12"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html#subset-by-needed-columns",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html#subset-by-needed-columns",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "Subset by needed columns",
    "text": "Subset by needed columns\n\ndf_no_gdp_outliers.columns\n\nIndex(['year', 'region', 'province', 'gdp', 'fdi', 'it', 'specific',\n       'gdp_zscore'],\n      dtype='object')\n\n\n\ndf_no_gdp_outliers_subset = df_no_gdp_outliers[['region', 'gdp', 'fdi', 'it']]\ndf_no_gdp_outliers_subset\n\n\n\n\n\n  \n    \n      \n      region\n      gdp\n      fdi\n      it\n    \n  \n  \n    \n      0\n      East China\n      2093.30\n      50661\n      631930\n    \n    \n      1\n      East China\n      2347.32\n      43443\n      657860\n    \n    \n      2\n      East China\n      2542.96\n      27673\n      889463\n    \n    \n      3\n      East China\n      2712.34\n      26131\n      1227364\n    \n    \n      4\n      East China\n      2902.09\n      31847\n      1499110\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      354\n      East China\n      8003.67\n      307610\n      1962633\n    \n    \n      355\n      East China\n      9705.02\n      498055\n      2261631\n    \n    \n      356\n      East China\n      11648.70\n      668128\n      3162299\n    \n    \n      357\n      East China\n      13417.68\n      772000\n      2370200\n    \n    \n      358\n      East China\n      15718.47\n      888935\n      2553268\n    \n  \n\n350 rows  4 columns"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html#genearate-an-experimental_crosstab-to-be-used-in-statistical-tests",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html#genearate-an-experimental_crosstab-to-be-used-in-statistical-tests",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "Genearate an experimental_crosstab to be used in statistical tests",
    "text": "Genearate an experimental_crosstab to be used in statistical tests\n\nexperimental_crosstab = df_no_gdp_outliers_subset.groupby('region').agg(['size', 'mean', 'std'])\n\n\nexperimental_crosstab.index\n\nIndex(['East China', 'North China', 'Northeast China', 'Northwest China',\n       'South Central China', 'Southwest China'],\n      dtype='object', name='region')\n\n\n\nexperimental_crosstab = experimental_crosstab.reset_index()\n\n\nexperimental_crosstab\n\n\n\n\n\n  \n    \n      \n      region\n      gdp\n      fdi\n      it\n    \n    \n      \n      \n      size\n      mean\n      std\n      size\n      mean\n      std\n      size\n      mean\n      std\n    \n  \n  \n    \n      0\n      East China\n      78\n      6070.604231\n      3500.372702\n      78\n      355577.897436\n      275635.866746\n      78\n      1.775615e+06\n      1.153030e+06\n    \n    \n      1\n      North China\n      48\n      4239.038542\n      2866.705149\n      48\n      169600.583333\n      127011.475909\n      48\n      1.733719e+06\n      1.548794e+06\n    \n    \n      2\n      Northeast China\n      36\n      3849.076944\n      1948.531835\n      36\n      136623.750000\n      142734.495232\n      36\n      2.665148e+06\n      1.768442e+06\n    \n    \n      3\n      Northwest China\n      60\n      1340.026167\n      1174.399739\n      60\n      15111.133333\n      22954.193559\n      60\n      1.703538e+06\n      1.446408e+06\n    \n    \n      4\n      South Central China\n      68\n      4835.540882\n      3697.129915\n      68\n      218931.426471\n      339981.399823\n      68\n      2.500962e+06\n      2.196436e+06\n    \n    \n      5\n      Southwest China\n      60\n      2410.398833\n      2144.589994\n      60\n      25405.083333\n      31171.373876\n      60\n      2.424971e+06\n      2.002198e+06\n    \n  \n\n\n\n\n\nexperimental_crosstab.to_csv('fiscal_experimental_crosstab.csv')"
  },
  {
    "objectID": "posts/2020-10-25-arima-adf-forecasting.html",
    "href": "posts/2020-10-25-arima-adf-forecasting.html",
    "title": "Stock Market Analysis of the S&P 500 Index using ARIMA and Seasonal ARIMA for forecasting",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport pandas_datareader\nimport datetime\nimport pandas_datareader.data as web\n\nimport statsmodels.api as sm\nimport quandl"
  },
  {
    "objectID": "posts/2020-10-25-arima-adf-forecasting.html#step-2-visualize-the-data",
    "href": "posts/2020-10-25-arima-adf-forecasting.html#step-2-visualize-the-data",
    "title": "Stock Market Analysis of the S&P 500 Index using ARIMA and Seasonal ARIMA for forecasting",
    "section": "Step 2: Visualize the Data",
    "text": "Step 2: Visualize the Data\nLets visualize this data with a few methods.\n\ndf.plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8873251b38>\n\n\n\n\n\n\ntimeseries = df['Value']\n\n\ntimeseries.rolling(12).mean().plot(label='12 Month Rolling Mean')\ntimeseries.rolling(12).std().plot(label='12 Month Rolling Std')\ntimeseries.plot()\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f88731e22e8>\n\n\n\n\n\n\ntimeseries.rolling(12).mean().plot(label='12 Month Rolling Mean')\ntimeseries.plot()\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f887315f630>"
  },
  {
    "objectID": "posts/2020-10-25-arima-adf-forecasting.html#decomposition",
    "href": "posts/2020-10-25-arima-adf-forecasting.html#decomposition",
    "title": "Stock Market Analysis of the S&P 500 Index using ARIMA and Seasonal ARIMA for forecasting",
    "section": "Decomposition",
    "text": "Decomposition\nETS decomposition allows us to see the individual parts!\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(df['Value'], freq=12)  \nfig = plt.figure()  \nfig = decomposition.plot()  \nfig.set_size_inches(15, 8)\n\n<Figure size 432x288 with 0 Axes>"
  },
  {
    "objectID": "posts/2020-10-25-arima-adf-forecasting.html#testing-for-stationarity",
    "href": "posts/2020-10-25-arima-adf-forecasting.html#testing-for-stationarity",
    "title": "Stock Market Analysis of the S&P 500 Index using ARIMA and Seasonal ARIMA for forecasting",
    "section": "Testing for Stationarity",
    "text": "Testing for Stationarity\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      1960-01-01\n      58.03\n    \n    \n      1960-02-01\n      55.78\n    \n    \n      1960-03-01\n      55.02\n    \n    \n      1960-04-01\n      55.73\n    \n    \n      1960-05-01\n      55.22\n    \n  \n\n\n\n\n\nfrom statsmodels.tsa.stattools import adfuller\n\n\nresult = adfuller(df['Value'])\n\n\nprint('Augmented Dickey-Fuller Test:')\nlabels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n\nfor value,label in zip(result,labels):\n    print(label+' : '+str(value) )\n    \nif result[1] <= 0.05:\n    print(\"strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\")\nelse:\n    print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : 1.7247353245135\np-value : 0.9981874531215522\n#Lags Used : 20\nNumber of Observations Used : 719\nweak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \n\n\n\ndef adf_check(time_series):\n    \"\"\"\n    Pass in a time series, returns ADF report\n    \"\"\"\n    result = adfuller(time_series)\n    print('Augmented Dickey-Fuller Test:')\n    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n\n    for value,label in zip(result,labels):\n        print(label+' : '+str(value) )\n    \n    if result[1] <= 0.05:\n        print(\"strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\")\n    else:\n        print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n\n** First Difference **\n\ndf['Value First Difference'] = df['Value'] - df['Value'].shift(1)\n\n\nadf_check(df['Value First Difference'].dropna())\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : -4.267790128581322\np-value : 0.0005048563860225925\n#Lags Used : 20\nNumber of Observations Used : 718\nstrong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\n\n\n\ndf['Value First Difference'].plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f886d63feb8>\n\n\n\n\n\n** Second Difference **\n\ndf['Value Second Difference'] = df['Value First Difference'] - df['Value First Difference'].shift(1)\n\n\nadf_check(df['Value Second Difference'].dropna())\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : -12.29955077642857\np-value : 7.504260735615441e-23\n#Lags Used : 18\nNumber of Observations Used : 719\nstrong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\n\n\n\ndf['Value Second Difference'].plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8877f726d8>\n\n\n\n\n\n** Seasonal Difference **\n\ndf['Seasonal Difference'] = df['Value'] - df['Value'].shift(12)\ndf['Seasonal Difference'].plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8877fb79e8>\n\n\n\n\n\n\nadf_check(df['Seasonal Difference'].dropna())\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : -5.239903673260254\np-value : 7.284266188346342e-06\n#Lags Used : 20\nNumber of Observations Used : 707\nstrong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\n\n\n** Seasonal First Difference **\n\ndf['Seasonal First Difference'] = df['Value First Difference'] - df['Value First Difference'].shift(12)\ndf['Seasonal First Difference'].plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f88acb0c2b0>\n\n\n\n\n\n\nadf_check(df['Seasonal First Difference'].dropna())\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : -6.196739887980032\np-value : 5.940155101037563e-08\n#Lags Used : 20\nNumber of Observations Used : 706\nstrong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n\n\n# Check out: https://stackoverflow.com/questions/21788593/statsmodels-duplicate-charts\n# https://github.com/statsmodels/statsmodels/issues/1265\nfig_first = plot_acf(df[\"Value First Difference\"].dropna())\n\n\n\n\n\nfig_seasonal_first = plot_acf(df[\"Seasonal First Difference\"].dropna())\n\n\n\n\nPandas also has this functionality built in, but only for ACF, not PACF. So I recommend using statsmodels, as ACF and PACF is more core to its functionality than it is to pandas functionality.\n\nfrom pandas.plotting import autocorrelation_plot\nautocorrelation_plot(df['Seasonal First Difference'].dropna())\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f886d660358>\n\n\n\n\n\nWe can then plot this relationship:\n\nresult = plot_pacf(df[\"Seasonal First Difference\"].dropna())\n\n\n\n\n\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(df['Seasonal First Difference'].iloc[13:], lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(df['Seasonal First Difference'].iloc[13:], lags=40, ax=ax2)\n\n\n\n\n\nfrom statsmodels.tsa.arima_model import ARIMA\n\n\nhelp(ARIMA)\n\nHelp on class ARIMA in module statsmodels.tsa.arima_model:\n\nclass ARIMA(ARMA)\n |  ARIMA(endog, order, exog=None, dates=None, freq=None, missing='none')\n |  \n |  Autoregressive Integrated Moving Average ARIMA(p,d,q) Model\n |  \n |  Parameters\n |  ----------\n |  endog : array-like\n |      The endogenous variable.\n |  order : iterable\n |      The (p,d,q) order of the model for the number of AR parameters,\n |      differences, and MA parameters to use.\n |  exog : array-like, optional\n |      An optional array of exogenous variables. This should *not* include a\n |      constant or trend. You can specify this in the `fit` method.\n |  dates : array-like of datetime, optional\n |      An array-like object of datetime objects. If a pandas object is given\n |      for endog or exog, it is assumed to have a DateIndex.\n |  freq : str, optional\n |      The frequency of the time-series. A Pandas offset or 'B', 'D', 'W',\n |      'M', 'A', or 'Q'. This is optional if dates are given.\n |  \n |  \n |  Notes\n |  -----\n |  If exogenous variables are given, then the model that is fit is\n |  \n |  .. math::\n |  \n |     \\phi(L)(y_t - X_t\\beta) = \\theta(L)\\epsilon_t\n |  \n |  where :math:`\\phi` and :math:`\\theta` are polynomials in the lag\n |  operator, :math:`L`. This is the regression model with ARMA errors,\n |  or ARMAX model. This specification is used, whether or not the model\n |  is fit using conditional sum of square or maximum-likelihood, using\n |  the `method` argument in\n |  :meth:`statsmodels.tsa.arima_model.ARIMA.fit`. Therefore, for\n |  now, `css` and `mle` refer to estimation methods only. This may\n |  change for the case of the `css` model in future versions.\n |  \n |  Method resolution order:\n |      ARIMA\n |      ARMA\n |      statsmodels.tsa.base.tsa_model.TimeSeriesModel\n |      statsmodels.base.model.LikelihoodModel\n |      statsmodels.base.model.Model\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __getnewargs__(self)\n |  \n |  __init__(self, endog, order, exog=None, dates=None, freq=None, missing='none')\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  fit(self, start_params=None, trend='c', method='css-mle', transparams=True, solver='lbfgs', maxiter=500, full_output=1, disp=5, callback=None, start_ar_lags=None, **kwargs)\n |      Fits ARIMA(p,d,q) model by exact maximum likelihood via Kalman filter.\n |      \n |      Parameters\n |      ----------\n |      start_params : array-like, optional\n |          Starting parameters for ARMA(p,q).  If None, the default is given\n |          by ARMA._fit_start_params.  See there for more information.\n |      transparams : bool, optional\n |          Whehter or not to transform the parameters to ensure stationarity.\n |          Uses the transformation suggested in Jones (1980).  If False,\n |          no checking for stationarity or invertibility is done.\n |      method : str {'css-mle','mle','css'}\n |          This is the loglikelihood to maximize.  If \"css-mle\", the\n |          conditional sum of squares likelihood is maximized and its values\n |          are used as starting values for the computation of the exact\n |          likelihood via the Kalman filter.  If \"mle\", the exact likelihood\n |          is maximized via the Kalman Filter.  If \"css\" the conditional sum\n |          of squares likelihood is maximized.  All three methods use\n |          `start_params` as starting parameters.  See above for more\n |          information.\n |      trend : str {'c','nc'}\n |          Whether to include a constant or not.  'c' includes constant,\n |          'nc' no constant.\n |      solver : str or None, optional\n |          Solver to be used.  The default is 'lbfgs' (limited memory\n |          Broyden-Fletcher-Goldfarb-Shanno).  Other choices are 'bfgs',\n |          'newton' (Newton-Raphson), 'nm' (Nelder-Mead), 'cg' -\n |          (conjugate gradient), 'ncg' (non-conjugate gradient), and\n |          'powell'. By default, the limited memory BFGS uses m=12 to\n |          approximate the Hessian, projected gradient tolerance of 1e-8 and\n |          factr = 1e2. You can change these by using kwargs.\n |      maxiter : int, optional\n |          The maximum number of function evaluations. Default is 500.\n |      tol : float\n |          The convergence tolerance.  Default is 1e-08.\n |      full_output : bool, optional\n |          If True, all output from solver will be available in\n |          the Results object's mle_retvals attribute.  Output is dependent\n |          on the solver.  See Notes for more information.\n |      disp : int, optional\n |          If True, convergence information is printed.  For the default\n |          l_bfgs_b solver, disp controls the frequency of the output during\n |          the iterations. disp < 0 means no output in this case.\n |      callback : function, optional\n |          Called after each iteration as callback(xk) where xk is the current\n |          parameter vector.\n |      start_ar_lags : int, optional\n |          Parameter for fitting start_params. When fitting start_params,\n |          residuals are obtained from an AR fit, then an ARMA(p,q) model is\n |          fit via OLS using these residuals. If start_ar_lags is None, fit\n |          an AR process according to best BIC. If start_ar_lags is not None,\n |          fits an AR process with a lag length equal to start_ar_lags.\n |          See ARMA._fit_start_params_hr for more information.\n |      kwargs\n |          See Notes for keyword arguments that can be passed to fit.\n |      \n |      Returns\n |      -------\n |      `statsmodels.tsa.arima.ARIMAResults` class\n |      \n |      See Also\n |      --------\n |      statsmodels.base.model.LikelihoodModel.fit : for more information\n |          on using the solvers.\n |      ARIMAResults : results class returned by fit\n |      \n |      Notes\n |      -----\n |      If fit by 'mle', it is assumed for the Kalman Filter that the initial\n |      unknown state is zero, and that the initial variance is\n |      P = dot(inv(identity(m**2)-kron(T,T)),dot(R,R.T).ravel('F')).reshape(r,\n |      r, order = 'F')\n |  \n |  predict(self, params, start=None, end=None, exog=None, typ='linear', dynamic=False)\n |      ARIMA model in-sample and out-of-sample prediction\n |      \n |      Parameters\n |      ----------\n |      params : array-like\n |          The fitted parameters of the model.\n |      start : int, str, or datetime\n |          Zero-indexed observation number at which to start forecasting, ie.,\n |          the first forecast is start. Can also be a date string to\n |          parse or a datetime type.\n |      end : int, str, or datetime\n |          Zero-indexed observation number at which to end forecasting, ie.,\n |          the first forecast is start. Can also be a date string to\n |          parse or a datetime type. However, if the dates index does not\n |          have a fixed frequency, end must be an integer index if you\n |          want out of sample prediction.\n |      exog : array-like, optional\n |          If the model is an ARMAX and out-of-sample forecasting is\n |          requested, exog must be given. Note that you'll need to pass\n |          `k_ar` additional lags for any exogenous variables. E.g., if you\n |          fit an ARMAX(2, q) model and want to predict 5 steps, you need 7\n |          observations to do this.\n |      dynamic : bool, optional\n |          The `dynamic` keyword affects in-sample prediction. If dynamic\n |          is False, then the in-sample lagged values are used for\n |          prediction. If `dynamic` is True, then in-sample forecasts are\n |          used in place of lagged dependent variables. The first forecasted\n |          value is `start`.\n |      typ : str {'linear', 'levels'}\n |      \n |          - 'linear' : Linear prediction in terms of the differenced\n |            endogenous variables.\n |          - 'levels' : Predict the levels of the original endogenous\n |            variables.\n |      \n |      \n |      Returns\n |      -------\n |      predict : array\n |          The predicted values.\n |      \n |      \n |      \n |      Notes\n |      -----\n |      Use the results predict method instead.\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  __new__(cls, endog, order, exog=None, dates=None, freq=None, missing='none')\n |      Create and return a new object.  See help(type) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from ARMA:\n |  \n |  geterrors(self, params)\n |      Get the errors of the ARMA process.\n |      \n |      Parameters\n |      ----------\n |      params : array-like\n |          The fitted ARMA parameters\n |      order : array-like\n |          3 item iterable, with the number of AR, MA, and exogenous\n |          parameters, including the trend\n |  \n |  hessian(self, params)\n |      Compute the Hessian at params,\n |      \n |      Notes\n |      -----\n |      This is a numerical approximation.\n |  \n |  loglike(self, params, set_sigma2=True)\n |      Compute the log-likelihood for ARMA(p,q) model\n |      \n |      Notes\n |      -----\n |      Likelihood used depends on the method set in fit\n |  \n |  loglike_css(self, params, set_sigma2=True)\n |      Conditional Sum of Squares likelihood function.\n |  \n |  loglike_kalman(self, params, set_sigma2=True)\n |      Compute exact loglikelihood for ARMA(p,q) model by the Kalman Filter.\n |  \n |  score(self, params)\n |      Compute the score function at params.\n |      \n |      Notes\n |      -----\n |      This is a numerical approximation.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from ARMA:\n |  \n |  from_formula(formula, data, subset=None, drop_cols=None, *args, **kwargs) from builtins.type\n |      Create a Model from a formula and dataframe.\n |      \n |      Parameters\n |      ----------\n |      formula : str or generic Formula object\n |          The formula specifying the model\n |      data : array-like\n |          The data for the model. See Notes.\n |      subset : array-like\n |          An array-like object of booleans, integers, or index values that\n |          indicate the subset of df to use in the model. Assumes df is a\n |          `pandas.DataFrame`\n |      drop_cols : array-like\n |          Columns to drop from the design matrix.  Cannot be used to\n |          drop terms involving categoricals.\n |      args : extra arguments\n |          These are passed to the model\n |      kwargs : extra keyword arguments\n |          These are passed to the model with one exception. The\n |          ``eval_env`` keyword is passed to patsy. It can be either a\n |          :class:`patsy:patsy.EvalEnvironment` object or an integer\n |          indicating the depth of the namespace to use. For example, the\n |          default ``eval_env=0`` uses the calling namespace. If you wish\n |          to use a \"clean\" environment set ``eval_env=-1``.\n |      \n |      Returns\n |      -------\n |      model : Model instance\n |      \n |      Notes\n |      -----\n |      data must define __getitem__ with the keys in the formula terms\n |      args and kwargs are passed on to the model instantiation. E.g.,\n |      a numpy structured or rec array, a dictionary, or a pandas DataFrame.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from statsmodels.tsa.base.tsa_model.TimeSeriesModel:\n |  \n |  exog_names\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from statsmodels.base.model.LikelihoodModel:\n |  \n |  information(self, params)\n |      Fisher information matrix of model\n |      \n |      Returns -Hessian of loglike evaluated at params.\n |  \n |  initialize(self)\n |      Initialize (possibly re-initialize) a Model instance. For\n |      instance, the design matrix of a linear model may change\n |      and some things must be recomputed.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from statsmodels.base.model.Model:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  endog_names\n |      Names of endogenous variables\n\n\n\n\nmodel = sm.tsa.statespace.SARIMAX(df['Value'],order=(0,1,0), seasonal_order=(1,1,1,12))\nresults = model.fit()\nprint(results.summary())\n\n/home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:219: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  ' ignored when e.g. forecasting.', ValueWarning)\n\n\n                                 Statespace Model Results                                 \n==========================================================================================\nDep. Variable:                              Value   No. Observations:                  740\nModel:             SARIMAX(0, 1, 0)x(1, 1, 1, 12)   Log Likelihood               -3719.108\nDate:                            Fri, 23 Oct 2020   AIC                           7444.215\nTime:                                    09:03:22   BIC                           7457.982\nSample:                                         0   HQIC                          7449.528\n                                            - 740                                         \nCovariance Type:                              opg                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.S.L12       0.0043      0.021      0.201      0.840      -0.037       0.046\nma.S.L12      -0.9513      0.018    -53.297      0.000      -0.986      -0.916\nsigma2      1563.8216     30.663     51.001      0.000    1503.724    1623.919\n===================================================================================\nLjung-Box (Q):                      116.83   Jarque-Bera (JB):              9251.13\nProb(Q):                              0.00   Prob(JB):                         0.00\nHeteroskedasticity (H):             410.23   Skew:                            -1.86\nProb(H) (two-sided):                  0.00   Kurtosis:                        20.08\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\nresults.resid.plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8872daf940>\n\n\n\n\n\n\nresults.resid.plot(kind='kde')\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8872d863c8>"
  },
  {
    "objectID": "posts/2020-10-25-arima-adf-forecasting.html#prediction-of-future-values",
    "href": "posts/2020-10-25-arima-adf-forecasting.html#prediction-of-future-values",
    "title": "Stock Market Analysis of the S&P 500 Index using ARIMA and Seasonal ARIMA for forecasting",
    "section": "Prediction of Future Values",
    "text": "Prediction of Future Values\nFirts we can get an idea of how well our model performs by just predicting for values that we actually already know:\n\ndf['forecast'] = results.predict(start = 1, end= 720, dynamic= True)  \ndf[['Value','forecast']].plot(figsize=(12,8))\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f88ace0d7b8>\n\n\n\n\n\n\nForecasting\n\ndf.tail()\n\n\n\n\n\n  \n    \n      \n      Value\n      Value First Difference\n      Value Second Difference\n      Seasonal Difference\n      Seasonal First Difference\n      forecast\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-08-01\n      3391.71\n      120.59\n      57.09\n      166.67\n      173.75\n      NaN\n    \n    \n      2020-08-31\n      3500.31\n      108.60\n      -11.99\n      223.00\n      56.33\n      NaN\n    \n    \n      2020-09-01\n      3526.65\n      26.34\n      -82.26\n      571.84\n      348.84\n      NaN\n    \n    \n      2020-09-30\n      3363.00\n      -163.65\n      -189.99\n      710.61\n      138.77\n      NaN\n    \n    \n      2020-10-01\n      3380.80\n      17.80\n      181.45\n      796.21\n      85.60\n      NaN\n    \n  \n\n\n\n\n\nfrom pandas.tseries.offsets import DateOffset\n\n\nfuture_dates = [df.index[-1] + DateOffset(months=x) for x in range(0,24) ]\n\n\nfuture_dates\n\n[Timestamp('2020-10-01 00:00:00'),\n Timestamp('2020-11-01 00:00:00'),\n Timestamp('2020-12-01 00:00:00'),\n Timestamp('2021-01-01 00:00:00'),\n Timestamp('2021-02-01 00:00:00'),\n Timestamp('2021-03-01 00:00:00'),\n Timestamp('2021-04-01 00:00:00'),\n Timestamp('2021-05-01 00:00:00'),\n Timestamp('2021-06-01 00:00:00'),\n Timestamp('2021-07-01 00:00:00'),\n Timestamp('2021-08-01 00:00:00'),\n Timestamp('2021-09-01 00:00:00'),\n Timestamp('2021-10-01 00:00:00'),\n Timestamp('2021-11-01 00:00:00'),\n Timestamp('2021-12-01 00:00:00'),\n Timestamp('2022-01-01 00:00:00'),\n Timestamp('2022-02-01 00:00:00'),\n Timestamp('2022-03-01 00:00:00'),\n Timestamp('2022-04-01 00:00:00'),\n Timestamp('2022-05-01 00:00:00'),\n Timestamp('2022-06-01 00:00:00'),\n Timestamp('2022-07-01 00:00:00'),\n Timestamp('2022-08-01 00:00:00'),\n Timestamp('2022-09-01 00:00:00')]\n\n\n\nfuture_dates_df = pd.DataFrame(index=future_dates[1:],columns=df.columns)\n\n\nfuture_df = pd.concat([df,future_dates_df])\n\n\nfuture_df.head()\n\n\n\n\n\n  \n    \n      \n      Value\n      Value First Difference\n      Value Second Difference\n      Seasonal Difference\n      Seasonal First Difference\n      forecast\n    \n  \n  \n    \n      1960-01-01\n      58.03\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1960-02-01\n      55.78\n      -2.25\n      NaN\n      NaN\n      NaN\n      58.03\n    \n    \n      1960-03-01\n      55.02\n      -0.76\n      1.49\n      NaN\n      NaN\n      58.03\n    \n    \n      1960-04-01\n      55.73\n      0.71\n      1.47\n      NaN\n      NaN\n      58.03\n    \n    \n      1960-05-01\n      55.22\n      -0.51\n      -1.22\n      NaN\n      NaN\n      58.03\n    \n  \n\n\n\n\n\nfuture_df.tail()\n\n\n\n\n\n  \n    \n      \n      Value\n      Value First Difference\n      Value Second Difference\n      Seasonal Difference\n      Seasonal First Difference\n      forecast\n    \n  \n  \n    \n      2022-05-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2022-06-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2022-07-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2022-08-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2022-09-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nfuture_df['forecast'] = results.predict(start = 1, end = 720, dynamic= True)  \nfuture_df[['Value', 'forecast']].plot(figsize=(12, 8)) \n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8872997fd0>"
  },
  {
    "objectID": "posts/2020-09-23-analyzingusinflation.html",
    "href": "posts/2020-09-23-analyzingusinflation.html",
    "title": "Analyzing US Inflation From 1959 - 2009 with statsmodels",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n%matplotlib inline\n\ndf = sm.datasets.macrodata.load_pandas().data\n\nprint(sm.datasets.macrodata.NOTE)\n\n::\n    Number of Observations - 203\n\n    Number of Variables - 14\n\n    Variable name definitions::\n\n        year      - 1959q1 - 2009q3\n        quarter   - 1-4\n        realgdp   - Real gross domestic product (Bil. of chained 2005 US$,\n                    seasonally adjusted annual rate)\n        realcons  - Real personal consumption expenditures (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realinv   - Real gross private domestic investment (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realgovt  - Real federal consumption expenditures & gross investment\n                    (Bil. of chained 2005 US$, seasonally adjusted annual rate)\n        realdpi   - Real private disposable income (Bil. of chained 2005\n                    US$, seasonally adjusted annual rate)\n        cpi       - End of the quarter consumer price index for all urban\n                    consumers: all items (1982-84 = 100, seasonally adjusted).\n        m1        - End of the quarter M1 nominal money stock (Seasonally\n                    adjusted)\n        tbilrate  - Quarterly monthly average of the monthly 3-month\n                    treasury bill: secondary market rate\n        unemp     - Seasonally adjusted unemployment rate (%)\n        pop       - End of the quarter total population: all ages incl. armed\n                    forces over seas\n        infl      - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400)\n        realint   - Real interest rate (tbilrate - infl)\n\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      quarter\n      realgdp\n      realcons\n      realinv\n      realgovt\n      realdpi\n      cpi\n      m1\n      tbilrate\n      unemp\n      pop\n      infl\n      realint\n    \n  \n  \n    \n      0\n      1959.0\n      1.0\n      2710.349\n      1707.4\n      286.898\n      470.045\n      1886.9\n      28.98\n      139.7\n      2.82\n      5.8\n      177.146\n      0.00\n      0.00\n    \n    \n      1\n      1959.0\n      2.0\n      2778.801\n      1733.7\n      310.859\n      481.301\n      1919.7\n      29.15\n      141.7\n      3.08\n      5.1\n      177.830\n      2.34\n      0.74\n    \n    \n      2\n      1959.0\n      3.0\n      2775.488\n      1751.8\n      289.226\n      491.260\n      1916.4\n      29.35\n      140.5\n      3.82\n      5.3\n      178.657\n      2.74\n      1.09\n    \n    \n      3\n      1959.0\n      4.0\n      2785.204\n      1753.7\n      299.356\n      484.052\n      1931.3\n      29.37\n      140.0\n      4.33\n      5.6\n      179.386\n      0.27\n      4.06\n    \n    \n      4\n      1960.0\n      1.0\n      2847.699\n      1770.5\n      331.722\n      462.199\n      1955.5\n      29.54\n      139.6\n      3.50\n      5.2\n      180.007\n      2.31\n      1.19\n    \n  \n\n\n\n\n\nindex = pd.Index(sm.tsa.datetools.dates_from_range('1959Q1', '2009Q3'))\ndf.index = index\ndf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      quarter\n      realgdp\n      realcons\n      realinv\n      realgovt\n      realdpi\n      cpi\n      m1\n      tbilrate\n      unemp\n      pop\n      infl\n      realint\n    \n  \n  \n    \n      1959-03-31\n      1959.0\n      1.0\n      2710.349\n      1707.4\n      286.898\n      470.045\n      1886.9\n      28.98\n      139.7\n      2.82\n      5.8\n      177.146\n      0.00\n      0.00\n    \n    \n      1959-06-30\n      1959.0\n      2.0\n      2778.801\n      1733.7\n      310.859\n      481.301\n      1919.7\n      29.15\n      141.7\n      3.08\n      5.1\n      177.830\n      2.34\n      0.74\n    \n    \n      1959-09-30\n      1959.0\n      3.0\n      2775.488\n      1751.8\n      289.226\n      491.260\n      1916.4\n      29.35\n      140.5\n      3.82\n      5.3\n      178.657\n      2.74\n      1.09\n    \n    \n      1959-12-31\n      1959.0\n      4.0\n      2785.204\n      1753.7\n      299.356\n      484.052\n      1931.3\n      29.37\n      140.0\n      4.33\n      5.6\n      179.386\n      0.27\n      4.06\n    \n    \n      1960-03-31\n      1960.0\n      1.0\n      2847.699\n      1770.5\n      331.722\n      462.199\n      1955.5\n      29.54\n      139.6\n      3.50\n      5.2\n      180.007\n      2.31\n      1.19\n    \n  \n\n\n\n\n\ndf['infl'].plot()\nplt.ylabel(\"infl\")\n\nText(0, 0.5, 'infl')\n\n\n\n\n\n\\(\\min_{\\\\{ \\tau_{t}\\\\} }\\sum_{t}^{T}\\zeta_{t}^{2}+\\lambda\\sum_{t=1}^{T}\\left[\\left(\\tau_{t}-\\tau_{t-1}\\right)-\\left(\\tau_{t-1}-\\tau_{t-2}\\right)\\right]^{2}\\)\n\n# unpacking\ninfl_cycle, infl_trend = sm.tsa.filters.hpfilter(df.infl)\n\ninfl_cycle\n\n1959-03-31    -1.206811\n1959-06-30     1.141499\n1959-09-30     1.550564\n1959-12-31    -0.909577\n1960-03-31     1.140149\n                ...    \n2008-09-30    -5.064733\n2008-12-31   -10.550048\n2009-03-31    -0.681429\n2009-06-30     1.883255\n2009-09-30     2.206560\nName: infl, Length: 203, dtype: float64\n\n\n\ntype(infl_cycle)\n\npandas.core.series.Series\n\n\n\ndf[\"trend\"] = infl_trend\n\ndf[['trend','infl']].plot(figsize = (12, 8))\n\ndf[['trend','infl']][\"2000-03-31\":].plot(figsize = (12, 8))\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7fec38616be0>"
  },
  {
    "objectID": "posts/2020-10-16-dask-nlp-gutenberg-books.html",
    "href": "posts/2020-10-16-dask-nlp-gutenberg-books.html",
    "title": "Using Dask with dask.bag and regex to parse The Brothers Karamazov from project gutenberg",
    "section": "",
    "text": "import dask.bag as db\nimport re\n\n\nbook_bag = db.from_url('https://www.gutenberg.org/files/28054/28054-0.txt')\n\n\nbook_bag.take(5)\n\n(b'\\xef\\xbb\\xbfThe Project Gutenberg EBook of The Brothers Karamazov by Fyodor\\r\\n',\n b'Dostoyevsky\\r\\n',\n b'\\r\\n',\n b'\\r\\n',\n b'\\r\\n')\n\n\n\nremove_spaces = book_bag.map(lambda x:x.strip())\n\n\nremove_spaces.take(10)\n\n(b'\\xef\\xbb\\xbfThe Project Gutenberg EBook of The Brothers Karamazov by Fyodor',\n b'Dostoyevsky',\n b'',\n b'',\n b'',\n b'This ebook is for the use of anyone anywhere in the United States and most',\n b'other parts of the world at no cost and with almost no restrictions',\n b'whatsoever. You may copy it, give it away or re\\xe2\\x80\\x90use it under the terms of',\n b'the Project Gutenberg License included with this eBook or online at',\n b'http://www.gutenberg.org/license. If you are not located in the United')\n\n\n\ndef decode_to_ascii(x):\n    return x.decode(\"ascii\",\"ignore\") \n\n\nascii_text = remove_spaces.map(decode_to_ascii)\n\n\nascii_text.take(10)\n\n('The Project Gutenberg EBook of The Brothers Karamazov by Fyodor',\n 'Dostoyevsky',\n '',\n '',\n '',\n 'This ebook is for the use of anyone anywhere in the United States and most',\n 'other parts of the world at no cost and with almost no restrictions',\n 'whatsoever. You may copy it, give it away or reuse it under the terms of',\n 'the Project Gutenberg License included with this eBook or online at',\n 'http://www.gutenberg.org/license. If you are not located in the United')\n\n\n\ndef remove_punctuation(x):\n    return re.sub(r'[^\\w\\s]','',x)\n\n\nremove_punctuation = ascii_text.map(remove_punctuation)\n\n\nremove_punctuation.take(10)\n\n('The Project Gutenberg EBook of The Brothers Karamazov by Fyodor',\n 'Dostoyevsky',\n '',\n '',\n '',\n 'This ebook is for the use of anyone anywhere in the United States and most',\n 'other parts of the world at no cost and with almost no restrictions',\n 'whatsoever You may copy it give it away or reuse it under the terms of',\n 'the Project Gutenberg License included with this eBook or online at',\n 'httpwwwgutenbergorglicense If you are not located in the United')\n\n\n\nlower_text = remove_punctuation.map(str.lower)\n\n\nlower_text.take(10)\n\n('the project gutenberg ebook of the brothers karamazov by fyodor',\n 'dostoyevsky',\n '',\n '',\n '',\n 'this ebook is for the use of anyone anywhere in the united states and most',\n 'other parts of the world at no cost and with almost no restrictions',\n 'whatsoever you may copy it give it away or reuse it under the terms of',\n 'the project gutenberg license included with this ebook or online at',\n 'httpwwwgutenbergorglicense if you are not located in the united')\n\n\n\nsplit_word_list = lower_text.map(lambda x: x.split(' '))\n\n\nsplit_word_list.take(10)\n\n(['the',\n  'project',\n  'gutenberg',\n  'ebook',\n  'of',\n  'the',\n  'brothers',\n  'karamazov',\n  'by',\n  'fyodor'],\n ['dostoyevsky'],\n [''],\n [''],\n [''],\n ['this',\n  'ebook',\n  'is',\n  'for',\n  'the',\n  'use',\n  'of',\n  'anyone',\n  'anywhere',\n  'in',\n  'the',\n  'united',\n  'states',\n  'and',\n  'most'],\n ['other',\n  'parts',\n  'of',\n  'the',\n  'world',\n  'at',\n  'no',\n  'cost',\n  'and',\n  'with',\n  'almost',\n  'no',\n  'restrictions'],\n ['whatsoever',\n  'you',\n  'may',\n  'copy',\n  'it',\n  'give',\n  'it',\n  'away',\n  'or',\n  'reuse',\n  'it',\n  'under',\n  'the',\n  'terms',\n  'of'],\n ['the',\n  'project',\n  'gutenberg',\n  'license',\n  'included',\n  'with',\n  'this',\n  'ebook',\n  'or',\n  'online',\n  'at'],\n ['httpwwwgutenbergorglicense',\n  'if',\n  'you',\n  'are',\n  'not',\n  'located',\n  'in',\n  'the',\n  'united'])\n\n\n\ndef remove_empty_words(word_list):\n    return list(filter(lambda a: a != '', word_list))\n\nnon_empty_words = split_word_list.filter(remove_empty_words)\n\n\nnon_empty_words.take(10)\n\n(['the',\n  'project',\n  'gutenberg',\n  'ebook',\n  'of',\n  'the',\n  'brothers',\n  'karamazov',\n  'by',\n  'fyodor'],\n ['dostoyevsky'],\n ['this',\n  'ebook',\n  'is',\n  'for',\n  'the',\n  'use',\n  'of',\n  'anyone',\n  'anywhere',\n  'in',\n  'the',\n  'united',\n  'states',\n  'and',\n  'most'],\n ['other',\n  'parts',\n  'of',\n  'the',\n  'world',\n  'at',\n  'no',\n  'cost',\n  'and',\n  'with',\n  'almost',\n  'no',\n  'restrictions'],\n ['whatsoever',\n  'you',\n  'may',\n  'copy',\n  'it',\n  'give',\n  'it',\n  'away',\n  'or',\n  'reuse',\n  'it',\n  'under',\n  'the',\n  'terms',\n  'of'],\n ['the',\n  'project',\n  'gutenberg',\n  'license',\n  'included',\n  'with',\n  'this',\n  'ebook',\n  'or',\n  'online',\n  'at'],\n ['httpwwwgutenbergorglicense',\n  'if',\n  'you',\n  'are',\n  'not',\n  'located',\n  'in',\n  'the',\n  'united'],\n ['states',\n  'youll',\n  'have',\n  'to',\n  'check',\n  'the',\n  'laws',\n  'of',\n  'the',\n  'country',\n  'where',\n  'you',\n  'are',\n  'located'],\n ['before', 'using', 'this', 'ebook'],\n ['title', 'the', 'brothers', 'karamazov'])\n\n\n\nall_words = non_empty_words.flatten()\n\n\ntype(all_words)\n\ndask.bag.core.Bag\n\n\n\nall_words.take(30)\n\n('the',\n 'project',\n 'gutenberg',\n 'ebook',\n 'of',\n 'the',\n 'brothers',\n 'karamazov',\n 'by',\n 'fyodor',\n 'dostoyevsky',\n 'this',\n 'ebook',\n 'is',\n 'for',\n 'the',\n 'use',\n 'of',\n 'anyone',\n 'anywhere',\n 'in',\n 'the',\n 'united',\n 'states',\n 'and',\n 'most',\n 'other',\n 'parts',\n 'of',\n 'the')\n\n\n\nchange_to_key_value = all_words.map(lambda x: (x, 1))\n\n\nchange_to_key_value.take(4)\n\n(('the', 1), ('project', 1), ('gutenberg', 1), ('ebook', 1))\n\n\n\ngrouped_words = all_words.groupby(lambda x:x)\n\n\ngrouped_words.take(1)\n\n(('the',\n  ['the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   ...]),)\n\n\n\nword_count = grouped_words.map(lambda x: (x[0], len(x[1])))\n\n\nword_count.take(10)\n\n(('the', 15379),\n ('project', 89),\n ('gutenberg', 88),\n ('ebook', 14),\n ('of', 7410),\n ('brothers', 82),\n ('karamazov', 170),\n ('by', 1165),\n ('fyodor', 303),\n ('dostoyevsky', 3))\n\n\n\nchange_to_key_value.take(10)\n\n(('the', 1),\n ('project', 1),\n ('gutenberg', 1),\n ('ebook', 1),\n ('of', 1),\n ('the', 1),\n ('brothers', 1),\n ('karamazov', 1),\n ('by', 1),\n ('fyodor', 1))\n\n\n\n# Take a running count of a word\n# In this case, the default value of \n# count needs to be provided\ndef add_bin_op(count, x):\n    return count + x[1]\n\n# Take the output from multiple bin_op(s)\n# and add them to get the total count of\n# a word\ndef add_combine_op(x, y):\n    return x + y\n\nword_count = change_to_key_value.foldby(lambda x: x[0],\n                                       add_bin_op, 0,\n                                       add_combine_op)\n\n\nword_count.take(10)\n\n(('the', 15379),\n ('project', 89),\n ('gutenberg', 88),\n ('ebook', 14),\n ('of', 7410),\n ('brothers', 82),\n ('karamazov', 170),\n ('by', 1165),\n ('fyodor', 303),\n ('dostoyevsky', 3))\n\n\n\nmuch_easier = all_words.frequencies()\n\n\nmuch_easier.take(10)\n\n(('the', 15379),\n ('project', 89),\n ('gutenberg', 88),\n ('ebook', 14),\n ('of', 7410),\n ('brothers', 82),\n ('karamazov', 170),\n ('by', 1165),\n ('fyodor', 303),\n ('dostoyevsky', 3))\n\n\n\nRemoving stop words in top word frequency counts\n\nfrom spacy.lang.en import STOP_WORDS\n\n\nwithout_stopwords = all_words.filter(lambda x: x not in STOP_WORDS)\n\n\nnew_freq = without_stopwords.frequencies()\n\n\nnew_freq.take(20)\n\n(('project', 89),\n ('gutenberg', 88),\n ('ebook', 14),\n ('brothers', 82),\n ('karamazov', 170),\n ('fyodor', 303),\n ('dostoyevsky', 3),\n ('use', 77),\n ('united', 24),\n ('states', 21),\n ('parts', 19),\n ('world', 182),\n ('cost', 12),\n ('restrictions', 2),\n ('whatsoever', 5),\n ('copy', 16),\n ('away', 445),\n ('reuse', 2),\n ('terms', 33),\n ('license', 14))\n\n\n\nnew_freq.topk(10)\n\ndask.bag<topk-aggregate, npartitions=1>\n\n\n\nnew_freq.topk(10, key=lambda x: x[1]).compute()\n\n[('alyosha', 1176),\n ('said', 993),\n ('know', 843),\n ('man', 842),\n ('mitya', 814),\n ('dont', 784),\n ('come', 772),\n ('father', 721),\n ('ivan', 677),\n ('time', 669)]"
  },
  {
    "objectID": "posts/2021-04-02-poisson-regression.html",
    "href": "posts/2021-04-02-poisson-regression.html",
    "title": "Poisson regression",
    "section": "",
    "text": "#code from https://github.com/thomasjpfan/ml-workshop-advanced"
  },
  {
    "objectID": "posts/2021-04-02-poisson-regression.html#load-london-bike-data",
    "href": "posts/2021-04-02-poisson-regression.html#load-london-bike-data",
    "title": "Poisson regression",
    "section": "Load London Bike Data",
    "text": "Load London Bike Data\n\nfrom pathlib import Path\n\ndata_path = Path(\"data\")\nbikes_path = data_path / \"london_bikes.csv\"\n\n\ntimestamp - timestamp field for grouping the data\ncnt - the count of a new bike shares\nt1 - real temperature in C\nt2 - temperature in C feels like\nhum - humidity in percentage\nwindspeed - wind speed in km/h\nweathercode - category of the weather\nisholiday - boolean field - 1 holiday / 0 non holiday\nisweekend - boolean field - 1 if the day is weekend\nseason - category field meteorological seasons: 0-spring ; 1-summer; 2-fall; 3-winter.\n\n\nbikes = pd.read_csv(bikes_path, parse_dates=['timestamp'])\n\n\nbikes.head()\n\n\n\n\n\n  \n    \n      \n      timestamp\n      cnt\n      t1\n      t2\n      hum\n      wind_speed\n      weather_code\n      is_holiday\n      is_weekend\n      season\n    \n  \n  \n    \n      0\n      2015-01-04 00:00:00\n      182\n      3.0\n      2.0\n      93.0\n      6.0\n      3.0\n      0.0\n      1.0\n      3.0\n    \n    \n      1\n      2015-01-04 01:00:00\n      138\n      3.0\n      2.5\n      93.0\n      5.0\n      1.0\n      0.0\n      1.0\n      3.0\n    \n    \n      2\n      2015-01-04 02:00:00\n      134\n      2.5\n      2.5\n      96.5\n      0.0\n      1.0\n      0.0\n      1.0\n      3.0\n    \n    \n      3\n      2015-01-04 03:00:00\n      72\n      2.0\n      2.0\n      100.0\n      0.0\n      1.0\n      0.0\n      1.0\n      3.0\n    \n    \n      4\n      2015-01-04 04:00:00\n      47\n      2.0\n      0.0\n      93.0\n      6.5\n      1.0\n      0.0\n      1.0\n      3.0\n    \n  \n\n\n\n\n\nbikes['timestamp'].head()\n\n0   2015-01-04 00:00:00\n1   2015-01-04 01:00:00\n2   2015-01-04 02:00:00\n3   2015-01-04 03:00:00\n4   2015-01-04 04:00:00\nName: timestamp, dtype: datetime64[ns]\n\n\n\nbikes['hr'] = bikes['timestamp'].dt.hour\n\n\nbikes['weather_code'].unique()\n\narray([ 3.,  1.,  4.,  7.,  2., 26., 10.])\n\n\n\nbikes['season'].unique()\n\narray([3., 0., 1., 2.])\n\n\n\nX = bikes[['t1', 't2', 'hum', 'wind_speed', 'weather_code', 'is_holiday', 'is_weekend', 'season', 'hr']]\ny = bikes['cnt']\n\n\nSplit data\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n\n\nColumn Transformer\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nnumerical_featurse = ['t1', 't2', 'hum', 'wind_speed', 'is_holiday', 'is_weekend', 'hr']\ncat_features = ['weather_code', 'season']\n\nct = ColumnTransformer([\n    ('numerical', 'passthrough', numerical_featurse),\n    ('categorical', OneHotEncoder(sparse=False, handle_unknown='ignore'), cat_features)\n    \n])\n\n\n\nPipeline\n\nfrom sklearn.linear_model import PoissonRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\npois_reg = Pipeline([\n    ('prep', ct),\n    ('scaler', StandardScaler()),\n    ('reg', PoissonRegressor())\n])\n\npois_reg\n\nPipelinePipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('numerical', 'passthrough',\n                                                  ['t1', 't2', 'hum',\n                                                   'wind_speed', 'is_holiday',\n                                                   'is_weekend', 'hr']),\n                                                 ('categorical',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False),\n                                                  ['weather_code',\n                                                   'season'])])),\n                ('scaler', StandardScaler()), ('reg', PoissonRegressor())])prep: ColumnTransformerColumnTransformer(transformers=[('numerical', 'passthrough',\n                                 ['t1', 't2', 'hum', 'wind_speed', 'is_holiday',\n                                  'is_weekend', 'hr']),\n                                ('categorical',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               sparse=False),\n                                 ['weather_code', 'season'])])numerical['t1', 't2', 'hum', 'wind_speed', 'is_holiday', 'is_weekend', 'hr']passthroughpassthroughcategorical['weather_code', 'season']OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)StandardScalerStandardScaler()PoissonRegressorPoissonRegressor()\n\n\n\npois_reg.fit(X_train, y_train);\n\n\npois_reg.score(X_test, y_test)\n\n0.331281167666172\n\n\n\n\nThis seems low what can we change?\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nnumerical_featurse = ['t1', 't2', 'hum', 'wind_speed', 'is_holiday', 'is_weekend']\ncat_features = ['weather_code', 'season', 'hr']\n\nct = ColumnTransformer([\n    ('numerical', 'passthrough', numerical_featurse),\n    ('categorical', OneHotEncoder(sparse=False, handle_unknown='ignore'), cat_features)\n    \n])\n\npois_reg = Pipeline([\n    ('prep', ct),\n    ('scalar', StandardScaler()),\n    ('reg', PoissonRegressor())\n])\n\npois_reg.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('numerical', 'passthrough',\n                                                  ['t1', 't2', 'hum',\n                                                   'wind_speed', 'is_holiday',\n                                                   'is_weekend']),\n                                                 ('categorical',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False),\n                                                  ['weather_code', 'season',\n                                                   'hr'])])),\n                ('scalar', StandardScaler()), ('reg', PoissonRegressor())])prep: ColumnTransformerColumnTransformer(transformers=[('numerical', 'passthrough',\n                                 ['t1', 't2', 'hum', 'wind_speed', 'is_holiday',\n                                  'is_weekend']),\n                                ('categorical',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               sparse=False),\n                                 ['weather_code', 'season', 'hr'])])numerical['t1', 't2', 'hum', 'wind_speed', 'is_holiday', 'is_weekend']passthroughpassthroughcategorical['weather_code', 'season', 'hr']OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)StandardScalerStandardScaler()PoissonRegressorPoissonRegressor()\n\n\n\npois_reg.score(X_test, y_test)\n\n0.8156604730601431\n\n\n\n\nWhat about ridge?\n\nfrom sklearn.linear_model import Ridge\n\nct = ColumnTransformer([\n    ('numerical', 'passthrough', numerical_featurse),\n    ('categorical', OneHotEncoder(sparse=False, handle_unknown='ignore'), cat_features)\n    \n])\n\nridge = Pipeline([\n    ('prep', ct),\n    ('scalar', StandardScaler()),\n    ('reg', Ridge(random_state=42))\n])\n\nridge.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('numerical', 'passthrough',\n                                                  ['t1', 't2', 'hum',\n                                                   'wind_speed', 'is_holiday',\n                                                   'is_weekend']),\n                                                 ('categorical',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False),\n                                                  ['weather_code', 'season',\n                                                   'hr'])])),\n                ('scalar', StandardScaler()), ('reg', Ridge(random_state=42))])prep: ColumnTransformerColumnTransformer(transformers=[('numerical', 'passthrough',\n                                 ['t1', 't2', 'hum', 'wind_speed', 'is_holiday',\n                                  'is_weekend']),\n                                ('categorical',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               sparse=False),\n                                 ['weather_code', 'season', 'hr'])])numerical['t1', 't2', 'hum', 'wind_speed', 'is_holiday', 'is_weekend']passthroughpassthroughcategorical['weather_code', 'season', 'hr']OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)StandardScalerStandardScaler()RidgeRidge(random_state=42)\n\n\n\nridge.score(X_test, y_test)\n\n0.7264287681524049\n\n\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_poisson_deviance\n\ndef compute_metrics(y_true, y_pred, sample_weight=None):\n    \n    mask = y_pred > 0\n    if (~mask).any():\n        n_masked, n_samples = (~mask).sum(), mask.shape[0]\n        print(f\"WARNING: Estimator yields invalid, non-positive predictions \"\n              f\" for {n_masked} samples out of {n_samples}. These predictions \"\n              f\"are ignored when computing the Poisson deviance.\")\n        \n        y_true = y_true[mask]\n        y_pred = y_pred[mask]\n        if sample_weight is not None:\n            sample_weight = sample_weight[mask]\n   \n    return {\n        'mse': mean_squared_error(y_true, y_pred, sample_weight=sample_weight),\n        'mean poisson deviance': mean_poisson_deviance(y_true, y_pred, sample_weight=sample_weight)\n    }\n\n\n\nCompute metrics\n\nridge_pred = ridge.predict(X_test)\ncompute_metrics(y_test, ridge_pred)\n\nWARNING: Estimator yields invalid, non-positive predictions  for 403 samples out of 4354. These predictions are ignored when computing the Poisson deviance.\n\n\n{'mse': 347993.3878355729, 'mean poisson deviance': 254.31140084177204}\n\n\n\npoisson_pred = pois_reg.predict(X_test)\ncompute_metrics(y_test, poisson_pred)\n\n{'mse': 273873.6171595357, 'mean poisson deviance': 181.15334073868877}"
  },
  {
    "objectID": "posts/2021-04-02-poisson-regression.html#plotting-the-prediction-distrubutions",
    "href": "posts/2021-04-02-poisson-regression.html#plotting-the-prediction-distrubutions",
    "title": "Poisson regression",
    "section": "Plotting the prediction distrubutions",
    "text": "Plotting the prediction distrubutions\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\nax1.hist(y_test, bins=30, alpha=0.5)\nax1.set_title(\"Test data\")\nax2.hist(poisson_pred, bins=30, alpha=0.5)\nax2.set_title(\"Poisson predictions\")\nax3.hist(ridge_pred, bins=30, alpha=0.5)\nax3.set_title(\"Ridge predictions\")\n\nText(0.5, 1.0, 'Ridge predictions')\n\n\n\n\n\n\n# %load solutions/03-ex01-solutions.py\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\nhist = HistGradientBoostingRegressor(random_state=42)\nhist.fit(X_train, y_train)\nhist_pred = hist.predict(X_test)\n\ncompute_metrics(y_test, hist_pred)\n\nhist_poisson = HistGradientBoostingRegressor(loss='poisson', random_state=42)\nhist_poisson.fit(X_train, y_train)\n\nhist_poisson_pred = hist_poisson.predict(X_test)\n\ncompute_metrics(y_test, hist_poisson_pred)\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\nax1.hist(y_test, bins=30, alpha=0.5)\nax1.set_title(\"Test data\")\nax2.hist(hist_pred, bins=30, alpha=0.5)\nax2.set_title(\"Default Hist\")\nax3.hist(hist_poisson_pred, bins=30, alpha=0.5)\nax3.set_title(\"Poisson Hist\");\n\nWARNING: Estimator yields invalid, non-positive predictions  for 22 samples out of 4354. These predictions are ignored when computing the Poisson deviance."
  },
  {
    "objectID": "posts/2021-04-02-poisson-regression.html#calibration",
    "href": "posts/2021-04-02-poisson-regression.html#calibration",
    "title": "Poisson regression",
    "section": "Calibration",
    "text": "Calibration\n\nfrom sklearn.utils import gen_even_slices\n\ndef _calibration_curve_weighted(y_true, y_pred, n_bins=10, sample_weight=None):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    \n    idx_sort = np.argsort(y_pred)\n    y_pred_bin = np.zeros(n_bins)\n    y_true_bin = np.zeros(n_bins)\n    \n    if sample_weight is not None:\n        sample_weight = np.asarray(sample_weight)\n    \n    for i, sl in enumerate(gen_even_slices(len(y_true), n_bins)):\n        if sample_weight is None:\n            y_pred_bin[i] = np.average(y_pred[idx_sort][sl])\n            y_true_bin[i] = np.average(y_true[idx_sort][sl])\n        else:\n            weights = sample_weight[idx_sort][sl]\n            y_pred_bin[i] = np.average(y_pred[idx_sort][sl], weights=weights)\n            y_true_bin[i] = np.average(y_true[idx_sort][sl], weights=weights)\n    return y_pred_bin, y_true_bin\n\ndef plot_calibration_curve_weights(y_true, y_pred, n_bins=10, ax=None, title=\"\", sample_weight=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n    y_pred_bin, y_true_bin = _calibration_curve_weighted(y_test, y_pred, sample_weight=sample_weight)\n    \n    bin_centers = np.arange(1, len(y_pred_bin) + 1)\n    ax.plot(bin_centers, y_pred_bin, marker='x', linestyle=\"--\", label=\"predictions\")\n    ax.plot(bin_centers, y_true_bin, marker='o', linestyle=\"--\", label=\"observations\")\n    ax.set(xlabel=\"Bin number\", xticks=bin_centers, title=title)\n    ax.legend()\n    return ax\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 8))\nplot_calibration_curve_weights(y_test, ridge_pred, ax=ax1, title=\"Ridge\")\nplot_calibration_curve_weights(y_test, poisson_pred, ax=ax2, title=\"Poisson Regression\")\nplot_calibration_curve_weights(y_test, hist_poisson_pred, ax=ax3, title=\"Hist Poisson\");"
  },
  {
    "objectID": "posts/2021-04-02-poisson-regression.html#claims-dataset",
    "href": "posts/2021-04-02-poisson-regression.html#claims-dataset",
    "title": "Poisson regression",
    "section": "Claims dataset",
    "text": "Claims dataset\n\nclaims_path = data_path / \"claims.csv\"\nclaims = pd.read_csv(claims_path)\n\n\nClaimNb: number of claims on the given policy;\nExposure: total exposure in yearly units;\nArea: area code (categorical, ordinal);\nVehPower: power of the car (categorical, ordinal);\nVehAge: age of the car in years;\nDrivAge: age of the (most common) driver in years;\nBonusMalus: bonus-malus level between 50 and 230 (with reference level 100);\nVehBrand: car brand (categorical, nominal);\nVehGas: diesel or regular fuel car (binary);\nDensity: density of inhabitants per km2 in the city of the living place of the driver;\nRegion: regions in France (prior to 2016)\n\n\nclaims.head()\n\n\n\n\n\n  \n    \n      \n      ClaimNb\n      Exposure\n      Area\n      VehPower\n      VehAge\n      DrivAge\n      BonusMalus\n      VehBrand\n      VehGas\n      Density\n      Region\n    \n  \n  \n    \n      0\n      0.0\n      1.00\n      D\n      4.0\n      11.0\n      42.0\n      64.0\n      B2\n      Regular\n      856.0\n      R24\n    \n    \n      1\n      0.0\n      0.18\n      E\n      10.0\n      12.0\n      35.0\n      100.0\n      B1\n      Regular\n      4762.0\n      R93\n    \n    \n      2\n      0.0\n      0.08\n      E\n      6.0\n      4.0\n      53.0\n      50.0\n      B1\n      Diesel\n      3317.0\n      R93\n    \n    \n      3\n      0.0\n      0.36\n      A\n      5.0\n      2.0\n      44.0\n      50.0\n      B2\n      Diesel\n      35.0\n      R52\n    \n    \n      4\n      0.0\n      0.60\n      C\n      4.0\n      0.0\n      32.0\n      85.0\n      B12\n      Diesel\n      200.0\n      R73\n    \n  \n\n\n\n\n\nexposure = claims['Exposure']\n\n\ny = claims[\"ClaimNb\"] / exposure\n\n\nX = claims.drop([\"Exposure\", \"ClaimNb\"], axis=\"columns\")\n\n\nX.head()\n\n\n\n\n\n  \n    \n      \n      Area\n      VehPower\n      VehAge\n      DrivAge\n      BonusMalus\n      VehBrand\n      VehGas\n      Density\n      Region\n    \n  \n  \n    \n      0\n      D\n      4.0\n      11.0\n      42.0\n      64.0\n      B2\n      Regular\n      856.0\n      R24\n    \n    \n      1\n      E\n      10.0\n      12.0\n      35.0\n      100.0\n      B1\n      Regular\n      4762.0\n      R93\n    \n    \n      2\n      E\n      6.0\n      4.0\n      53.0\n      50.0\n      B1\n      Diesel\n      3317.0\n      R93\n    \n    \n      3\n      A\n      5.0\n      2.0\n      44.0\n      50.0\n      B2\n      Diesel\n      35.0\n      R52\n    \n    \n      4\n      C\n      4.0\n      0.0\n      32.0\n      85.0\n      B12\n      Diesel\n      200.0\n      R73\n    \n  \n\n\n\n\n\nSplit data\n\nX_train, X_test, y_train, y_test, exposure_train, exposure_test = train_test_split(\n    X, y, exposure, random_state=42)\n\n\n\nTrain simple dummy regresor\n\nfrom sklearn.dummy import DummyRegressor\n\ndummy = DummyRegressor()\ndummy.fit(X_train, y_train, sample_weight=exposure_train)\n\nDummyRegressorDummyRegressor()\n\n\n\ndummy_pred = dummy.predict(X_test)\ncompute_metrics(y_test, dummy_pred, sample_weight=exposure_test)\n\n{'mse': 0.5307356741415867, 'mean poisson deviance': 0.6308647534621802}\n\n\n\ndummy_pred[:10]\n\narray([0.10038206, 0.10038206, 0.10038206, 0.10038206, 0.10038206,\n       0.10038206, 0.10038206, 0.10038206, 0.10038206, 0.10038206])"
  },
  {
    "objectID": "posts/2021-04-02-poisson-regression.html#ridge",
    "href": "posts/2021-04-02-poisson-regression.html#ridge",
    "title": "Poisson regression",
    "section": "Ridge",
    "text": "Ridge\n\nX['Density'].hist(bins=20);\n\n\n\n\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.compose import ColumnTransformer\n\nlinear_model_preprocessor = ColumnTransformer(\n    [\n        (\"passthrough_numeric\", \"passthrough\",\n            [\"BonusMalus\"]),\n        (\"binned_numeric\", KBinsDiscretizer(n_bins=10),\n            [\"VehAge\", \"DrivAge\"]),\n        (\"log_scaled_numeric\", FunctionTransformer(np.log, validate=False),\n            [\"Density\"]),\n        (\"onehot_categorical\", OneHotEncoder(handle_unknown='ignore'),\n            [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"]),\n    ],\n)\n\n\nlinear_model_preprocessor.fit_transform(X_train)\n\n<254254x75 sparse matrix of type '<class 'numpy.float64'>'\n    with 2288282 stored elements in Compressed Sparse Row format>\n\n\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import MaxAbsScaler\n\nridge = Pipeline([\n    (\"preprocessor\", linear_model_preprocessor),\n    (\"scaler\", MaxAbsScaler()),\n    (\"reg\", Ridge(alpha=1e-6))])\nridge.fit(X_train, y_train, reg__sample_weight=exposure_train)\n\nPipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('passthrough_numeric',\n                                                  'passthrough',\n                                                  ['BonusMalus']),\n                                                 ('binned_numeric',\n                                                  KBinsDiscretizer(n_bins=10),\n                                                  ['VehAge', 'DrivAge']),\n                                                 ('log_scaled_numeric',\n                                                  FunctionTransformer(func=),\n                                                  ['Density']),\n                                                 ('onehot_categorical',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['VehBrand', 'VehPower',\n                                                   'VehGas', 'Region',\n                                                   'Area'])])),\n                ('scaler', MaxAbsScaler()), ('reg', Ridge(alpha=1e-06))])preprocessor: ColumnTransformerColumnTransformer(transformers=[('passthrough_numeric', 'passthrough',\n                                 ['BonusMalus']),\n                                ('binned_numeric', KBinsDiscretizer(n_bins=10),\n                                 ['VehAge', 'DrivAge']),\n                                ('log_scaled_numeric',\n                                 FunctionTransformer(func=),\n                                 ['Density']),\n                                ('onehot_categorical',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 ['VehBrand', 'VehPower', 'VehGas', 'Region',\n                                  'Area'])])passthrough_numeric['BonusMalus']passthroughpassthroughbinned_numeric['VehAge', 'DrivAge']KBinsDiscretizerKBinsDiscretizer(n_bins=10)log_scaled_numeric['Density']FunctionTransformerFunctionTransformer(func=)onehot_categorical['VehBrand', 'VehPower', 'VehGas', 'Region', 'Area']OneHotEncoderOneHotEncoder(handle_unknown='ignore')MaxAbsScalerMaxAbsScaler()RidgeRidge(alpha=1e-06)\n\n\n\nridge_pred = ridge.predict(X_test)\ncompute_metrics(y_test, ridge_pred, sample_weight=exposure_test)\n\nWARNING: Estimator yields invalid, non-positive predictions  for 866 samples out of 84752. These predictions are ignored when computing the Poisson deviance.\n\n\n{'mse': 0.5306782259705353, 'mean poisson deviance': 0.6051468688331222}\n\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplot_calibration_curve_weights(y_test, ridge_pred, ax=ax, title=\"Ridge\", sample_weight=exposure_test);\n\n\n\n\n\n# %load solutions/03-ex02-solutions.py\npoission_reg = Pipeline([\n    (\"preprocessor\", linear_model_preprocessor),\n    (\"scaler\", MaxAbsScaler()),\n    (\"reg\", PoissonRegressor(alpha=1e-12))])\n\npoission_reg.fit(X_train, y_train, reg__sample_weight=exposure_train)\n\npoisson_pred = poission_reg.predict(X_test)\ncompute_metrics(y_test, poisson_pred, sample_weight=exposure_test)\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplot_calibration_curve_weights(y_test, poisson_pred, ax=ax, title=\"Poisson\", sample_weight=exposure_test)\n\n<AxesSubplot:title={'center':'Poisson'}, xlabel='Bin number'>\n\n\n\n\n\n\n# %load solutions/03-ex03-solutions.py\nfrom sklearn.preprocessing import OrdinalEncoder\n\ntree_preprocessor = ColumnTransformer(\n    [\n        (\"categorical\", OrdinalEncoder(),\n            [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"]),\n        (\"numeric\", \"passthrough\",\n            [\"VehAge\", \"DrivAge\", \"BonusMalus\", \"Density\"]),\n    ]\n)\n\nhist_poisson = Pipeline([\n    (\"preprocessor\", tree_preprocessor),\n    (\"reg\", HistGradientBoostingRegressor(loss=\"poisson\", random_state=0))\n])\nhist_poisson.fit(X_train, y_train, reg__sample_weight=exposure_train)\n\nhist_poisson_pred = hist_poisson.predict(X_test)\ncompute_metrics(y_test, hist_poisson_pred, sample_weight=exposure_test)\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplot_calibration_curve_weights(y_test, hist_poisson_pred, ax=ax, title=\"Hist Poisson\", sample_weight=exposure_test);"
  },
  {
    "objectID": "posts/2020-10-12-dask-xgboost-fiscal-data..html",
    "href": "posts/2020-10-12-dask-xgboost-fiscal-data..html",
    "title": "Visualizing Operations with Dask Dataframes on Fiscal Data",
    "section": "",
    "text": "import numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine(\"sqlite:///fiscal_data.db\")\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nengine.execute(\"SELECT * FROM fiscal_data LIMIT 1\").fetchall()\n\n[(1996, 'East China', 'Anhui', '2093.3', 50661, 631930, 147002)]\n\n\n\nsql = \"\"\"\nSELECT year\n, region\n, province\n, gdp\n, fdi\n, it\n, specific\nFROM fiscal_data\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.3\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.7\n      668128\n      3162299\n      656175.0\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      18753.73\n      1036576\n      2939778\n      844647.0\n    \n  \n\n360 rows  7 columns\n\n\n\n\ndf.columns\n\nIndex(['year', 'region', 'province', 'gdp', 'fdi', 'it', 'specific'], dtype='object')\n\n\n\ndf.gdp.hist()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f0272396c50>\n\n\n\n\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n\n\n\n\nClient\n\n  Scheduler: inproc://192.168.1.71/5995/1\n  Dashboard: http://192.168.1.71:8787/status\n\n\n\nCluster\n\n  Workers: 3\n  Cores: 6\n  Memory: 12.00 GB\n\n\n\n\n\n\n\nfrom dask import dataframe as dd \n\n\nddf = dd.from_pandas(df, npartitions=5)\n\n\nprint(ddf)\n\nDask DataFrame Structure:\n                year  region province     gdp    fdi     it specific\nnpartitions=5                                                       \n0              int64  object   object  object  int64  int64  float64\n72               ...     ...      ...     ...    ...    ...      ...\n...              ...     ...      ...     ...    ...    ...      ...\n288              ...     ...      ...     ...    ...    ...      ...\n359              ...     ...      ...     ...    ...    ...      ...\nDask Name: from_pandas, 5 tasks\n\n\n\nddf.describe().visualize(filename='describe.png')\n\n\n\n\n\nddf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.3\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n  \n\n\n\n\n\nmax_gdp_per_region = ddf.groupby('region')['gdp'].max()\n\n\nmax_gdp_per_region.visualize()\n\n\n\n\n\nmax_gdp_per_region.compute()\n\nregion\nEast China             9705.02\nNorth China            9846.81\nNorthwest China         956.32\nSouth Central China     9439.6\nSouthwest China          937.5\nNortheast China        9304.52\nName: gdp, dtype: object\n\n\n\nddf\n\n\nDask DataFrame Structure:\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n    \n      npartitions=5\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      int64\n      object\n      object\n      object\n      int64\n      int64\n      float64\n    \n    \n      72\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      288\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      359\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n  \n\n\nDask Name: from_pandas, 5 tasks\n\n\n\nddf.npartitions\n\n5\n\n\n\nddf.npartitions\n\n5\n\n\n\nlen(ddf)\n\n360\n\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=\"4GB\")\nclient\n\n/home/gao/anaconda3/lib/python3.7/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use.\nPerhaps you already have a cluster running?\nHosting the HTTP server on port 39701 instead\n  http_address[\"port\"], self.http_server.port\n\n\n\n\n\n\nClient\n\n  Scheduler: inproc://192.168.1.71/5995/20\n  Dashboard: http://192.168.1.71:39701/status\n\n\n\nCluster\n\n  Workers: 3\n  Cores: 6\n  Memory: 12.00 GB\n\n\n\n\n\n\n\nclient.id\n\n'Client-9f9a71c2-0c90-11eb-976b-cff3b7a8059e'\n\n\n\nddf.describe().compute()\n\n\n\n\n\n  \n    \n      \n      year\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      count\n      360.000000\n      3.600000e+02\n      3.600000e+02\n      3.560000e+02\n    \n    \n      mean\n      2001.500000\n      1.961394e+05\n      2.165819e+06\n      5.834707e+05\n    \n    \n      std\n      3.456857\n      3.030440e+05\n      1.769294e+06\n      6.540553e+05\n    \n    \n      min\n      1996.000000\n      2.000000e+00\n      1.478970e+05\n      8.964000e+03\n    \n    \n      25%\n      1998.750000\n      3.309900e+04\n      1.077466e+06\n      2.237530e+05\n    \n    \n      50%\n      2001.500000\n      1.411025e+05\n      2.020634e+06\n      4.243700e+05\n    \n    \n      75%\n      2004.250000\n      4.065125e+05\n      3.375492e+06\n      1.011846e+06\n    \n    \n      max\n      2007.000000\n      1.743140e+06\n      1.053331e+07\n      3.937966e+06\n    \n  \n\n\n\n\n\nddf.columns\n\nIndex(['year', 'region', 'province', 'gdp', 'fdi', 'it', 'specific'], dtype='object')\n\n\n\nddf[\"year\"] = ddf[\"year\"].astype(int)\nddf[\"fdi\"] = ddf[\"fdi\"].astype(float)\n# ddf[\"province\"] = ddf[\"province\"].astype(float)\n# ddf[\"region\"] = ddf[\"region\"].astype(float)\nddf[\"gdp\"] = ddf[\"gdp\"].astype(float)\nddf[\"it\"] = ddf[\"it\"].astype(float)\n\n\nddf\n\n\nDask DataFrame Structure:\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n    \n      npartitions=5\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      int64\n      object\n      object\n      float64\n      float64\n      float64\n      float64\n    \n    \n      72\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      288\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      359\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n  \n\n\nDask Name: assign, 65 tasks\n\n\n\nddf.nlargest(20, 'gdp').compute()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      71\n      2007\n      South Central China\n      Guangdong\n      31777.01\n      1712603.0\n      4947824.0\n      859482.0\n    \n    \n      70\n      2006\n      South Central China\n      Guangdong\n      26587.76\n      1451065.0\n      4559252.0\n      1897575.0\n    \n    \n      263\n      2007\n      East China\n      Shandong\n      25776.91\n      1101159.0\n      6357869.0\n      2121243.0\n    \n    \n      69\n      2005\n      South Central China\n      Guangdong\n      22557.37\n      1236400.0\n      4327217.0\n      1491588.0\n    \n    \n      262\n      2006\n      East China\n      Shandong\n      21900.19\n      1000069.0\n      5304833.0\n      1204547.0\n    \n    \n      179\n      2007\n      East China\n      Jiangsu\n      21742.05\n      1743140.0\n      3557071.0\n      1188989.0\n    \n    \n      68\n      2004\n      South Central China\n      Guangdong\n      18864.62\n      1001158.0\n      5193902.0\n      1491588.0\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      18753.73\n      1036576.0\n      2939778.0\n      844647.0\n    \n    \n      178\n      2006\n      East China\n      Jiangsu\n      18598.69\n      1318339.0\n      2926542.0\n      1388043.0\n    \n    \n      261\n      2005\n      East China\n      Shandong\n      18366.87\n      897000.0\n      4142859.0\n      1011203.0\n    \n    \n      67\n      2003\n      South Central China\n      Guangdong\n      15844.64\n      782294.0\n      4073606.0\n      1550764.0\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935.0\n      2553268.0\n      1017303.0\n    \n    \n      260\n      2004\n      East China\n      Shandong\n      15021.84\n      870064.0\n      3732990.0\n      1011203.0\n    \n    \n      143\n      2007\n      South Central China\n      Henan\n      15012.46\n      306162.0\n      10533312.0\n      3860764.0\n    \n    \n      177\n      2005\n      East China\n      Jiangsu\n      15003.60\n      1213800.0\n      3479548.0\n      1483371.0\n    \n    \n      119\n      2007\n      North China\n      Hebei\n      13607.32\n      241621.0\n      7537692.0\n      2981235.0\n    \n    \n      66\n      2002\n      South Central China\n      Guangdong\n      13502.42\n      1133400.0\n      3545004.0\n      1235386.0\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000.0\n      2370200.0\n      656175.0\n    \n    \n      275\n      2007\n      East China\n      Shanghai\n      12494.01\n      792000.0\n      2386339.0\n      272744.0\n    \n    \n      176\n      2004\n      East China\n      Jiangsu\n      12442.87\n      1056365.0\n      2410257.0\n      1483371.0\n    \n  \n\n\n\n\n\nwithout_ec = ddf[ddf.region !='East China']\n\n\nwithout_ec.nlargest(20, 'gdp').compute()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      71\n      2007\n      South Central China\n      Guangdong\n      31777.01\n      1712603.0\n      4947824.0\n      859482.0\n    \n    \n      70\n      2006\n      South Central China\n      Guangdong\n      26587.76\n      1451065.0\n      4559252.0\n      1897575.0\n    \n    \n      69\n      2005\n      South Central China\n      Guangdong\n      22557.37\n      1236400.0\n      4327217.0\n      1491588.0\n    \n    \n      68\n      2004\n      South Central China\n      Guangdong\n      18864.62\n      1001158.0\n      5193902.0\n      1491588.0\n    \n    \n      67\n      2003\n      South Central China\n      Guangdong\n      15844.64\n      782294.0\n      4073606.0\n      1550764.0\n    \n    \n      143\n      2007\n      South Central China\n      Henan\n      15012.46\n      306162.0\n      10533312.0\n      3860764.0\n    \n    \n      119\n      2007\n      North China\n      Hebei\n      13607.32\n      241621.0\n      7537692.0\n      2981235.0\n    \n    \n      66\n      2002\n      South Central China\n      Guangdong\n      13502.42\n      1133400.0\n      3545004.0\n      1235386.0\n    \n    \n      142\n      2006\n      South Central China\n      Henan\n      12362.79\n      184526.0\n      7601825.0\n      2018158.0\n    \n    \n      65\n      2001\n      South Central China\n      Guangdong\n      12039.25\n      1193203.0\n      2152243.0\n      1257232.0\n    \n    \n      118\n      2006\n      North China\n      Hebei\n      11467.60\n      201434.0\n      5831974.0\n      1253141.0\n    \n    \n      64\n      2000\n      South Central China\n      Guangdong\n      10741.25\n      1128091.0\n      1927102.0\n      714572.0\n    \n    \n      141\n      2005\n      South Central China\n      Henan\n      10587.42\n      123000.0\n      5676863.0\n      1171796.0\n    \n    \n      299\n      2007\n      Southwest China\n      Sichuan\n      10562.39\n      149322.0\n      10384846.0\n      3937966.0\n    \n    \n      117\n      2005\n      North China\n      Hebei\n      10012.11\n      191000.0\n      4503640.0\n      859056.0\n    \n    \n      23\n      2007\n      North China\n      Beijing\n      9846.81\n      506572.0\n      1962192.0\n      752279.0\n    \n    \n      167\n      2007\n      South Central China\n      Hunan\n      9439.60\n      327051.0\n      8340692.0\n      3156087.0\n    \n    \n      155\n      2007\n      South Central China\n      Hubei\n      9333.40\n      276622.0\n      7666512.0\n      2922784.0\n    \n    \n      215\n      2007\n      Northeast China\n      Liaoning\n      9304.52\n      598554.0\n      5502192.0\n      3396397.0\n    \n    \n      63\n      1999\n      South Central China\n      Guangdong\n      9250.68\n      1165750.0\n      1789235.0\n      988521.0\n    \n  \n\n\n\n\n\nddf['province'].compute()\n\n0         Anhui\n1         Anhui\n2         Anhui\n3         Anhui\n4         Anhui\n         ...   \n355    Zhejiang\n356    Zhejiang\n357    Zhejiang\n358    Zhejiang\n359    Zhejiang\nName: province, Length: 360, dtype: object\n\n\n\nddf.where(ddf['province']=='Zhejiang').compute()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003.0\n      East China\n      Zhejiang\n      9705.02\n      498055.0\n      2261631.0\n      391292.0\n    \n    \n      356\n      2004.0\n      East China\n      Zhejiang\n      11648.70\n      668128.0\n      3162299.0\n      656175.0\n    \n    \n      357\n      2005.0\n      East China\n      Zhejiang\n      13417.68\n      772000.0\n      2370200.0\n      656175.0\n    \n    \n      358\n      2006.0\n      East China\n      Zhejiang\n      15718.47\n      888935.0\n      2553268.0\n      1017303.0\n    \n    \n      359\n      2007.0\n      East China\n      Zhejiang\n      18753.73\n      1036576.0\n      2939778.0\n      844647.0\n    \n  \n\n360 rows  7 columns\n\n\n\n\nmask_after_2010 = ddf.where(ddf['year']>2000)\n\n\nmask_after_2010.compute()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003.0\n      East China\n      Zhejiang\n      9705.02\n      498055.0\n      2261631.0\n      391292.0\n    \n    \n      356\n      2004.0\n      East China\n      Zhejiang\n      11648.70\n      668128.0\n      3162299.0\n      656175.0\n    \n    \n      357\n      2005.0\n      East China\n      Zhejiang\n      13417.68\n      772000.0\n      2370200.0\n      656175.0\n    \n    \n      358\n      2006.0\n      East China\n      Zhejiang\n      15718.47\n      888935.0\n      2553268.0\n      1017303.0\n    \n    \n      359\n      2007.0\n      East China\n      Zhejiang\n      18753.73\n      1036576.0\n      2939778.0\n      844647.0\n    \n  \n\n360 rows  7 columns\n\n\n\n\ndef add_some_text(cname, *args, **kwargs):\n    return \"Region name is \" + cname\n\ndummy_values = ddf['region'].apply(add_some_text, axis=1)\n\n/home/gao/anaconda3/lib/python3.7/site-packages/dask/dataframe/core.py:3208: UserWarning: \nYou did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\nTo provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n  Before: .apply(func)\n  After:  .apply(func, meta=('region', 'object'))\n\n  warnings.warn(meta_warning(meta))\n\n\n\ndummy_values\n\nDask Series Structure:\nnpartitions=5\n0      object\n72        ...\n        ...  \n288       ...\n359       ...\nName: region, dtype: object\nDask Name: apply, 75 tasks\n\n\n\ndummy_values.visualize()\n\n\n\n\n\ndummy_values.compute()\n\n0      Region name is East China\n1      Region name is East China\n2      Region name is East China\n3      Region name is East China\n4      Region name is East China\n                 ...            \n355    Region name is East China\n356    Region name is East China\n357    Region name is East China\n358    Region name is East China\n359    Region name is East China\nName: region, Length: 360, dtype: object\n\n\n\nmax_per_region_yr = ddf.groupby('region').apply(lambda x: x.loc[x['gdp'].idxmax(), 'year'])\n\n/home/gao/.local/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n  Before: .apply(func)\n  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n  or:     .apply(func, meta=('x', 'f8'))            for series result\n  \"\"\"Entry point for launching an IPython kernel.\n\n\n\nmax_per_region_yr.visualize()\n\n\n\n\n\nmax_per_region_yr.compute()\n\nregion\nNorth China            2007\nNortheast China        2007\nNorthwest China        2007\nSouth Central China    2007\nEast China             2007\nSouthwest China        2007\ndtype: int64"
  },
  {
    "objectID": "posts/2020-10-15-dask-xgboost-linear-regresion-copy1.html",
    "href": "posts/2020-10-15-dask-xgboost-linear-regresion-copy1.html",
    "title": "Linear Regression using Dask Data Frames",
    "section": "",
    "text": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples."
  },
  {
    "objectID": "posts/2020-10-15-dask-xgboost-linear-regresion-copy1.html#selecting-features-and-target",
    "href": "posts/2020-10-15-dask-xgboost-linear-regresion-copy1.html#selecting-features-and-target",
    "title": "Linear Regression using Dask Data Frames",
    "section": "Selecting Features and Target",
    "text": "Selecting Features and Target\n\nfeat_list = [\"year\", \"fdi\"]\ncat_feat_list = [\"region\", \"province\"]\ntarget = [\"gdp\"]\n\n\nddf[\"year\"] = ddf[\"year\"].astype(int)\nddf[\"fdi\"] = ddf[\"fdi\"].astype(float)\nddf[\"gdp\"] = ddf[\"gdp\"].astype(float)\nddf[\"it\"] = ddf[\"it\"].astype(float)\n\n\n#OHE\nfrom dask_ml.preprocessing import OneHotEncoder\n\n\nddf = ddf.categorize(cat_feat_list)\n\n\nohe = OneHotEncoder(sparse=False)\n\n\nohe_ddf = ohe.fit_transform(ddf[cat_feat_list])\n\n\nfeat_list = feat_list + ohe_ddf.columns.tolist()\nfeat_list = [f for f in feat_list if f not in cat_feat_list]\n\n\nddf_processed = (dd.concat([ddf,ohe_ddf], axis=1) [feat_list + target])\n\n\nddf_processed.compute()\n\n\n\n\n\n  \n    \n      \n      year\n      fdi\n      region_East China\n      region_North China\n      region_Southwest China\n      region_Northwest China\n      region_South Central China\n      region_Northeast China\n      province_Anhui\n      province_Beijing\n      ...\n      province_Shandong\n      province_Shanghai\n      province_Shanxi\n      province_Sichuan\n      province_Tianjin\n      province_Tibet\n      province_Xinjiang\n      province_Yunnan\n      province_Zhejiang\n      gdp\n    \n  \n  \n    \n      0\n      1996\n      50661.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2093.30\n    \n    \n      1\n      1997\n      43443.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2347.32\n    \n    \n      2\n      1998\n      27673.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2542.96\n    \n    \n      3\n      1999\n      26131.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2712.34\n    \n    \n      4\n      2000\n      31847.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2902.09\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      498055.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      9705.02\n    \n    \n      356\n      2004\n      668128.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      11648.70\n    \n    \n      357\n      2005\n      772000.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      13417.68\n    \n    \n      358\n      2006\n      888935.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      15718.47\n    \n    \n      359\n      2007\n      1036576.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      18753.73\n    \n  \n\n360 rows  75 columns\n\n\n\n\nfeat_list\n\n['year',\n 'fdi',\n 'region_East China',\n 'region_North China',\n 'region_Southwest China',\n 'region_Northwest China',\n 'region_South Central China',\n 'region_Northeast China',\n 'province_Anhui',\n 'province_Beijing',\n 'province_Chongqing',\n 'province_Fujian',\n 'province_Gansu',\n 'province_Guangdong',\n 'province_Guangxi',\n 'province_Guizhou',\n 'province_Hainan',\n 'province_Hebei',\n 'province_Heilongjiang',\n 'province_Henan',\n 'province_Hubei',\n 'province_Hunan',\n 'province_Jiangsu',\n 'province_Jiangxi',\n 'province_Jilin',\n 'province_Liaoning',\n 'province_Ningxia',\n 'province_Qinghai',\n 'province_Shaanxi',\n 'province_Shandong',\n 'province_Shanghai',\n 'province_Shanxi',\n 'province_Sichuan',\n 'province_Tianjin',\n 'province_Tibet',\n 'province_Xinjiang',\n 'province_Yunnan',\n 'province_Zhejiang',\n 'region_East China',\n 'region_North China',\n 'region_Southwest China',\n 'region_Northwest China',\n 'region_South Central China',\n 'region_Northeast China',\n 'province_Anhui',\n 'province_Beijing',\n 'province_Chongqing',\n 'province_Fujian',\n 'province_Gansu',\n 'province_Guangdong',\n 'province_Guangxi',\n 'province_Guizhou',\n 'province_Hainan',\n 'province_Hebei',\n 'province_Heilongjiang',\n 'province_Henan',\n 'province_Hubei',\n 'province_Hunan',\n 'province_Jiangsu',\n 'province_Jiangxi',\n 'province_Jilin',\n 'province_Liaoning',\n 'province_Ningxia',\n 'province_Qinghai',\n 'province_Shaanxi',\n 'province_Shandong',\n 'province_Shanghai',\n 'province_Shanxi',\n 'province_Sichuan',\n 'province_Tianjin',\n 'province_Tibet',\n 'province_Xinjiang',\n 'province_Yunnan',\n 'province_Zhejiang']"
  },
  {
    "objectID": "posts/2020-10-15-dask-xgboost-linear-regresion-copy1.html#dask-linear-regression",
    "href": "posts/2020-10-15-dask-xgboost-linear-regresion-copy1.html#dask-linear-regression",
    "title": "Linear Regression using Dask Data Frames",
    "section": "Dask Linear Regression",
    "text": "Dask Linear Regression\n\nX=ddf_processed[feat_list].persist()\ny=ddf_processed[target].persist()\n\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression, Ridge\n\n\nX\n\n\nDask DataFrame Structure:\n\n\n\n  \n    \n      \n      year\n      fdi\n      region_East China\n      region_East China\n      region_North China\n      region_North China\n      region_Southwest China\n      region_Southwest China\n      region_Northwest China\n      region_Northwest China\n      region_South Central China\n      region_South Central China\n      region_Northeast China\n      region_Northeast China\n      province_Anhui\n      province_Anhui\n      province_Beijing\n      province_Beijing\n      province_Chongqing\n      province_Chongqing\n      province_Fujian\n      province_Fujian\n      province_Gansu\n      province_Gansu\n      province_Guangdong\n      province_Guangdong\n      province_Guangxi\n      province_Guangxi\n      province_Guizhou\n      province_Guizhou\n      province_Hainan\n      province_Hainan\n      province_Hebei\n      province_Hebei\n      province_Heilongjiang\n      province_Heilongjiang\n      province_Henan\n      province_Henan\n      province_Hubei\n      province_Hubei\n      province_Hunan\n      province_Hunan\n      province_Jiangsu\n      province_Jiangsu\n      province_Jiangxi\n      province_Jiangxi\n      province_Jilin\n      province_Jilin\n      province_Liaoning\n      province_Liaoning\n      province_Ningxia\n      province_Ningxia\n      province_Qinghai\n      province_Qinghai\n      province_Shaanxi\n      province_Shaanxi\n      province_Shandong\n      province_Shandong\n      province_Shanghai\n      province_Shanghai\n      province_Shanxi\n      province_Shanxi\n      province_Sichuan\n      province_Sichuan\n      province_Tianjin\n      province_Tianjin\n      province_Tibet\n      province_Tibet\n      province_Xinjiang\n      province_Xinjiang\n      province_Yunnan\n      province_Yunnan\n      province_Zhejiang\n      province_Zhejiang\n      region_East China\n      region_East China\n      region_North China\n      region_North China\n      region_Southwest China\n      region_Southwest China\n      region_Northwest China\n      region_Northwest China\n      region_South Central China\n      region_South Central China\n      region_Northeast China\n      region_Northeast China\n      province_Anhui\n      province_Anhui\n      province_Beijing\n      province_Beijing\n      province_Chongqing\n      province_Chongqing\n      province_Fujian\n      province_Fujian\n      province_Gansu\n      province_Gansu\n      province_Guangdong\n      province_Guangdong\n      province_Guangxi\n      province_Guangxi\n      province_Guizhou\n      province_Guizhou\n      province_Hainan\n      province_Hainan\n      province_Hebei\n      province_Hebei\n      province_Heilongjiang\n      province_Heilongjiang\n      province_Henan\n      province_Henan\n      province_Hubei\n      province_Hubei\n      province_Hunan\n      province_Hunan\n      province_Jiangsu\n      province_Jiangsu\n      province_Jiangxi\n      province_Jiangxi\n      province_Jilin\n      province_Jilin\n      province_Liaoning\n      province_Liaoning\n      province_Ningxia\n      province_Ningxia\n      province_Qinghai\n      province_Qinghai\n      province_Shaanxi\n      province_Shaanxi\n      province_Shandong\n      province_Shandong\n      province_Shanghai\n      province_Shanghai\n      province_Shanxi\n      province_Shanxi\n      province_Sichuan\n      province_Sichuan\n      province_Tianjin\n      province_Tianjin\n      province_Tibet\n      province_Tibet\n      province_Xinjiang\n      province_Xinjiang\n      province_Yunnan\n      province_Yunnan\n      province_Zhejiang\n      province_Zhejiang\n    \n    \n      npartitions=5\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      int64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n    \n    \n      72\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      288\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      359\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n  \n\n\nDask Name: getitem, 5 tasks\n\n\n\nX.compute()\n\n\n\n\n\n  \n    \n      \n      year\n      fdi\n      region_East China\n      region_East China\n      region_North China\n      region_North China\n      region_Southwest China\n      region_Southwest China\n      region_Northwest China\n      region_Northwest China\n      ...\n      province_Tianjin\n      province_Tianjin\n      province_Tibet\n      province_Tibet\n      province_Xinjiang\n      province_Xinjiang\n      province_Yunnan\n      province_Yunnan\n      province_Zhejiang\n      province_Zhejiang\n    \n  \n  \n    \n      0\n      1996\n      50661.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      1\n      1997\n      43443.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      2\n      1998\n      27673.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      3\n      1999\n      26131.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      4\n      2000\n      31847.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      498055.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n    \n    \n      356\n      2004\n      668128.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n    \n    \n      357\n      2005\n      772000.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n    \n    \n      358\n      2006\n      888935.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n    \n    \n      359\n      2007\n      1036576.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n    \n  \n\n360 rows  146 columns\n\n\n\n\ny\n\n\nDask DataFrame Structure:\n\n\n\n  \n    \n      \n      gdp\n    \n    \n      npartitions=5\n      \n    \n  \n  \n    \n      0\n      float64\n    \n    \n      72\n      ...\n    \n    \n      ...\n      ...\n    \n    \n      288\n      ...\n    \n    \n      359\n      ...\n    \n  \n\n\nDask Name: getitem, 5 tasks\n\n\n\nLinReg = LinearRegression()\n\n\nLinReg.fit(X, y)\n\nLinearRegression()\n\n\n\nRidgeReg = Ridge()\nRidgeReg.fit(x, y)\n\nRidge()\n\n\n\nLinReg.predict(x)[:5]\n\narray([[1830.87851079],\n       [2076.99855135],\n       [2220.28956053],\n       [2534.65768132],\n       [2936.29581027]])\n\n\n\nRidgeReg.predict(x)[:5]\n\narray([[1804.41754025],\n       [2053.19939587],\n       [2200.05297844],\n       [2516.48507702],\n       [2919.42271884]])\n\n\n\nclient.restart()\n\n\n\n\n\nClient\n\n  Scheduler: inproc://192.168.1.71/9672/30\n  Dashboard: http://192.168.1.71:46451/status\n\n\n\nCluster\n\n  Workers: 3\n  Cores: 6\n  Memory: 12.00 GB\n\n\n\n\n\n\n\nclient.close()"
  },
  {
    "objectID": "posts/2020-09-22-analyzingusunemployment.html",
    "href": "posts/2020-09-22-analyzingusunemployment.html",
    "title": "Analyzing US Unemployment From 1959 - 2009 with statsmodels",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n%matplotlib inline\n\ndf = sm.datasets.macrodata.load_pandas().data\n\nprint(sm.datasets.macrodata.NOTE)\n\n::\n    Number of Observations - 203\n\n    Number of Variables - 14\n\n    Variable name definitions::\n\n        year      - 1959q1 - 2009q3\n        quarter   - 1-4\n        realgdp   - Real gross domestic product (Bil. of chained 2005 US$,\n                    seasonally adjusted annual rate)\n        realcons  - Real personal consumption expenditures (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realinv   - Real gross private domestic investment (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realgovt  - Real federal consumption expenditures & gross investment\n                    (Bil. of chained 2005 US$, seasonally adjusted annual rate)\n        realdpi   - Real private disposable income (Bil. of chained 2005\n                    US$, seasonally adjusted annual rate)\n        cpi       - End of the quarter consumer price index for all urban\n                    consumers: all items (1982-84 = 100, seasonally adjusted).\n        m1        - End of the quarter M1 nominal money stock (Seasonally\n                    adjusted)\n        tbilrate  - Quarterly monthly average of the monthly 3-month\n                    treasury bill: secondary market rate\n        unemp     - Seasonally adjusted unemployment rate (%)\n        pop       - End of the quarter total population: all ages incl. armed\n                    forces over seas\n        infl      - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400)\n        realint   - Real interest rate (tbilrate - infl)\n\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      quarter\n      realgdp\n      realcons\n      realinv\n      realgovt\n      realdpi\n      cpi\n      m1\n      tbilrate\n      unemp\n      pop\n      infl\n      realint\n    \n  \n  \n    \n      0\n      1959.0\n      1.0\n      2710.349\n      1707.4\n      286.898\n      470.045\n      1886.9\n      28.98\n      139.7\n      2.82\n      5.8\n      177.146\n      0.00\n      0.00\n    \n    \n      1\n      1959.0\n      2.0\n      2778.801\n      1733.7\n      310.859\n      481.301\n      1919.7\n      29.15\n      141.7\n      3.08\n      5.1\n      177.830\n      2.34\n      0.74\n    \n    \n      2\n      1959.0\n      3.0\n      2775.488\n      1751.8\n      289.226\n      491.260\n      1916.4\n      29.35\n      140.5\n      3.82\n      5.3\n      178.657\n      2.74\n      1.09\n    \n    \n      3\n      1959.0\n      4.0\n      2785.204\n      1753.7\n      299.356\n      484.052\n      1931.3\n      29.37\n      140.0\n      4.33\n      5.6\n      179.386\n      0.27\n      4.06\n    \n    \n      4\n      1960.0\n      1.0\n      2847.699\n      1770.5\n      331.722\n      462.199\n      1955.5\n      29.54\n      139.6\n      3.50\n      5.2\n      180.007\n      2.31\n      1.19\n    \n  \n\n\n\n\n\nindex = pd.Index(sm.tsa.datetools.dates_from_range('1959Q1', '2009Q3'))\ndf.index = index\ndf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      quarter\n      realgdp\n      realcons\n      realinv\n      realgovt\n      realdpi\n      cpi\n      m1\n      tbilrate\n      unemp\n      pop\n      infl\n      realint\n    \n  \n  \n    \n      1959-03-31\n      1959.0\n      1.0\n      2710.349\n      1707.4\n      286.898\n      470.045\n      1886.9\n      28.98\n      139.7\n      2.82\n      5.8\n      177.146\n      0.00\n      0.00\n    \n    \n      1959-06-30\n      1959.0\n      2.0\n      2778.801\n      1733.7\n      310.859\n      481.301\n      1919.7\n      29.15\n      141.7\n      3.08\n      5.1\n      177.830\n      2.34\n      0.74\n    \n    \n      1959-09-30\n      1959.0\n      3.0\n      2775.488\n      1751.8\n      289.226\n      491.260\n      1916.4\n      29.35\n      140.5\n      3.82\n      5.3\n      178.657\n      2.74\n      1.09\n    \n    \n      1959-12-31\n      1959.0\n      4.0\n      2785.204\n      1753.7\n      299.356\n      484.052\n      1931.3\n      29.37\n      140.0\n      4.33\n      5.6\n      179.386\n      0.27\n      4.06\n    \n    \n      1960-03-31\n      1960.0\n      1.0\n      2847.699\n      1770.5\n      331.722\n      462.199\n      1955.5\n      29.54\n      139.6\n      3.50\n      5.2\n      180.007\n      2.31\n      1.19\n    \n  \n\n\n\n\n\ndf['unemp'].plot()\nplt.ylabel(\"unemp\")\n\nText(0, 0.5, 'unemp')\n\n\n\n\n\n\\(\\min_{\\\\{ \\tau_{t}\\\\} }\\sum_{t}^{T}\\zeta_{t}^{2}+\\lambda\\sum_{t=1}^{T}\\left[\\left(\\tau_{t}-\\tau_{t-1}\\right)-\\left(\\tau_{t-1}-\\tau_{t-2}\\right)\\right]^{2}\\)\n\n# unpacking\nunemp_cycle, unemp_trend = sm.tsa.filters.hpfilter(df.unemp)\n\nunemp_cycle\n\n1959-03-31    0.011338\n1959-06-30   -0.702548\n1959-09-30   -0.516441\n1959-12-31   -0.229910\n1960-03-31   -0.642198\n                ...   \n2008-09-30   -0.481666\n2008-12-31    0.198598\n2009-03-31    1.171440\n2009-06-30    2.040247\n2009-09-30    2.207674\nName: unemp, Length: 203, dtype: float64\n\n\n\ntype(unemp_cycle)\n\npandas.core.series.Series\n\n\n\ndf[\"trend\"] = unemp_trend\n\ndf[['trend','unemp']].plot(figsize = (12, 8))\n\ndf[['trend','unemp']][\"2000-03-31\":].plot(figsize = (12, 8))\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7fdb5c2ecdd8>"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html",
    "href": "posts/2021-05-31-missing-values.html",
    "title": "Missing values in scikit-learn",
    "section": "",
    "text": "#code adapted from https://github.com/thomasjpfan/ml-workshop-intermediate-1-of-2"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html#simpleimputer",
    "href": "posts/2021-05-31-missing-values.html#simpleimputer",
    "title": "Missing values in scikit-learn",
    "section": "SimpleImputer",
    "text": "SimpleImputer\n\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nimport sklearn\nsklearn.set_config(display='diagram')\n\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\n\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['it', 'Unnamed: 0'], axis = 1), df['it']\n\n\nX = X.select_dtypes(include='number')\nX\n\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\n\n\n\n\ndf.isnull().sum()\n\nUnnamed: 0      0\nprovince        0\nspecific        4\ngeneral       191\nyear            0\ngdp             0\nfdi             0\nrnr            66\nrr             64\ni              73\nfr             65\nreg             0\nit              0\ndtype: int64\n\n\n\nDefault uses mean\n\nimputer = SimpleImputer()\nimputer.fit_transform(X)\n\narray([[1.47002000e+05, 3.09127538e+05, 1.99600000e+03, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.51981000e+05, 3.09127538e+05, 1.99700000e+03, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.74930000e+05, 3.09127538e+05, 1.99800000e+03, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       ...,\n       [6.56175000e+05, 3.09127538e+05, 2.00500000e+03, ...,\n        1.21428571e+00, 3.57142860e-02, 3.57142860e-02],\n       [1.01730300e+06, 3.94795000e+05, 2.00600000e+03, ...,\n        1.21428571e+00, 3.57142860e-02, 3.57142860e-02],\n       [8.44647000e+05, 0.00000000e+00, 2.00700000e+03, ...,\n        4.76190480e-02, 0.00000000e+00, 0.00000000e+00]])\n\n\n\ndf.isnull().sum()\n\nUnnamed: 0      0\nprovince        0\nspecific        4\ngeneral       191\nyear            0\ngdp             0\nfdi             0\nrnr            66\nrr             64\ni              73\nfr             65\nreg             0\nit              0\ndtype: int64\n\n\n\n\nAdd indicator!\n\nimputer = SimpleImputer(add_indicator=True)\nimputer.fit_transform(X)\n\narray([[ 147002.        ,  309127.53846154,    1996.        , ...,\n              0.        ,       0.        ,       0.        ],\n       [ 151981.        ,  309127.53846154,    1997.        , ...,\n              0.        ,       0.        ,       0.        ],\n       [ 174930.        ,  309127.53846154,    1998.        , ...,\n              0.        ,       0.        ,       0.        ],\n       ...,\n       [ 656175.        ,  309127.53846154,    2005.        , ...,\n              0.        ,       0.        ,       0.        ],\n       [1017303.        ,  394795.        ,    2006.        , ...,\n              0.        ,       0.        ,       0.        ],\n       [ 844647.        ,       0.        ,    2007.        , ...,\n              0.        ,       0.        ,       0.        ]])\n\n\n\ndf.isnull().sum()\n\nUnnamed: 0      0\nprovince        0\nspecific        4\ngeneral       191\nyear            0\ngdp             0\nfdi             0\nrnr            66\nrr             64\ni              73\nfr             65\nreg             0\nit              0\ndtype: int64\n\n\n\n\nOther strategies\n\nimputer = SimpleImputer(strategy='median')\nimputer.fit_transform(X)\n\narray([[1.47002000e+05, 1.53640000e+05, 1.99600000e+03, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.51981000e+05, 1.53640000e+05, 1.99700000e+03, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.74930000e+05, 1.53640000e+05, 1.99800000e+03, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       ...,\n       [6.56175000e+05, 1.53640000e+05, 2.00500000e+03, ...,\n        1.21428571e+00, 3.57142860e-02, 3.57142860e-02],\n       [1.01730300e+06, 3.94795000e+05, 2.00600000e+03, ...,\n        1.21428571e+00, 3.57142860e-02, 3.57142860e-02],\n       [8.44647000e+05, 0.00000000e+00, 2.00700000e+03, ...,\n        4.76190480e-02, 0.00000000e+00, 0.00000000e+00]])\n\n\n\nimputer = SimpleImputer(strategy='most_frequent')\nimputer.fit_transform(X)\n\narray([[1.47002000e+05, 0.00000000e+00, 1.99600000e+03, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.51981000e+05, 0.00000000e+00, 1.99700000e+03, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.74930000e+05, 0.00000000e+00, 1.99800000e+03, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       ...,\n       [6.56175000e+05, 0.00000000e+00, 2.00500000e+03, ...,\n        1.21428571e+00, 3.57142860e-02, 3.57142860e-02],\n       [1.01730300e+06, 3.94795000e+05, 2.00600000e+03, ...,\n        1.21428571e+00, 3.57142860e-02, 3.57142860e-02],\n       [8.44647000e+05, 0.00000000e+00, 2.00700000e+03, ...,\n        4.76190480e-02, 0.00000000e+00, 0.00000000e+00]])"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html#categorical-data",
    "href": "posts/2021-05-31-missing-values.html#categorical-data",
    "title": "Missing values in scikit-learn",
    "section": "Categorical data",
    "text": "Categorical data\n\nimport pandas as pd\n\n\nimputer = SimpleImputer(strategy='constant', fill_value='sk_missing')\nimputer.fit_transform(df)\n\narray([[0, 'Anhui', 147002.0, ..., '1128873', 'East China', 631930],\n       [1, 'Anhui', 151981.0, ..., '1356287', 'East China', 657860],\n       [2, 'Anhui', 174930.0, ..., '1518236', 'East China', 889463],\n       ...,\n       [357, 'Zhejiang', 656175.0, ..., 'sk_missing', 'East China',\n        2370200],\n       [358, 'Zhejiang', 1017303.0, ..., '11537149', 'East China',\n        2553268],\n       [359, 'Zhejiang', 844647.0, ..., '16494981', 'East China',\n        2939778]], dtype=object)"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html#pandas-categorical",
    "href": "posts/2021-05-31-missing-values.html#pandas-categorical",
    "title": "Missing values in scikit-learn",
    "section": "pandas categorical",
    "text": "pandas categorical\n\ndf['a'] = df['a'].astype('category')\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      province\n      specific\n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      reg\n      it\n    \n  \n  \n    \n      0\n      0\n      Anhui\n      147002.0\n      NaN\n      1996\n      2093.30\n      50661\n      0.000000\n      0.000000\n      0.000000\n      1128873\n      East China\n      631930\n    \n    \n      1\n      1\n      Anhui\n      151981.0\n      NaN\n      1997\n      2347.32\n      43443\n      0.000000\n      0.000000\n      0.000000\n      1356287\n      East China\n      657860\n    \n    \n      2\n      2\n      Anhui\n      174930.0\n      NaN\n      1998\n      2542.96\n      27673\n      0.000000\n      0.000000\n      0.000000\n      1518236\n      East China\n      889463\n    \n    \n      3\n      3\n      Anhui\n      285324.0\n      NaN\n      1999\n      2712.34\n      26131\n      NaN\n      NaN\n      NaN\n      1646891\n      East China\n      1227364\n    \n    \n      4\n      4\n      Anhui\n      195580.0\n      32100.0\n      2000\n      2902.09\n      31847\n      0.000000\n      0.000000\n      0.000000\n      1601508\n      East China\n      1499110\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      355\n      Zhejiang\n      391292.0\n      260313.0\n      2003\n      9705.02\n      498055\n      1.214286\n      0.035714\n      0.035714\n      6217715\n      East China\n      2261631\n    \n    \n      356\n      356\n      Zhejiang\n      656175.0\n      276652.0\n      2004\n      11648.70\n      668128\n      1.214286\n      0.035714\n      0.035714\n      NaN\n      East China\n      3162299\n    \n    \n      357\n      357\n      Zhejiang\n      656175.0\n      NaN\n      2005\n      13417.68\n      772000\n      1.214286\n      0.035714\n      0.035714\n      NaN\n      East China\n      2370200\n    \n    \n      358\n      358\n      Zhejiang\n      1017303.0\n      394795.0\n      2006\n      15718.47\n      888935\n      1.214286\n      0.035714\n      0.035714\n      11537149\n      East China\n      2553268\n    \n    \n      359\n      359\n      Zhejiang\n      844647.0\n      0.0\n      2007\n      18753.73\n      1036576\n      0.047619\n      0.000000\n      0.000000\n      16494981\n      East China\n      2939778\n    \n  \n\n360 rows  13 columns\n\n\n\n\ndf.dtypes\n\nUnnamed: 0      int64\nprovince       object\nspecific      float64\ngeneral       float64\nyear            int64\ngdp           float64\nfdi             int64\nrnr           float64\nrr            float64\ni             float64\nfr             object\nreg            object\nit              int64\ndtype: object\n\n\n\nimputer.fit_transform(df)\n\narray([[0, 'Anhui', 147002.0, ..., '1128873', 'East China', 631930],\n       [1, 'Anhui', 151981.0, ..., '1356287', 'East China', 657860],\n       [2, 'Anhui', 174930.0, ..., '1518236', 'East China', 889463],\n       ...,\n       [357, 'Zhejiang', 656175.0, ..., 'sk_missing', 'East China',\n        2370200],\n       [358, 'Zhejiang', 1017303.0, ..., '11537149', 'East China',\n        2553268],\n       [359, 'Zhejiang', 844647.0, ..., '16494981', 'East China',\n        2939778]], dtype=object)\n\n\n\n# %load solutions/03-ex01-solutions.py\nfrom sklearn.datasets import fetch_openml\n\ncancer = fetch_openml(data_id=15, as_frame=True)\n\nX, y = cancer.data, cancer.target\n\nX.shape\n\nX.isna().sum()\n\nimputer = SimpleImputer(add_indicator=True)\nX_imputed = imputer.fit_transform(X)\n\nX_imputed.shape\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=42, stratify=y\n)\n\nlog_reg = make_pipeline(\n    SimpleImputer(add_indicator=True),\n    StandardScaler(),\n    LogisticRegression(random_state=0)\n)\n\nlog_reg.fit(X_train, y_train)\n\nlog_reg.score(X_test, y_test)\n\n0.96"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html#histgradientboosting-native-support-for-missing-values",
    "href": "posts/2021-05-31-missing-values.html#histgradientboosting-native-support-for-missing-values",
    "title": "Missing values in scikit-learn",
    "section": "HistGradientBoosting Native support for missing values",
    "text": "HistGradientBoosting Native support for missing values\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\n\nhist = HistGradientBoostingClassifier(random_state=42)\nhist.fit(X_train, y_train)\n\nHistGradientBoostingClassifierHistGradientBoostingClassifier(random_state=42)\n\n\n\nhist.score(X_test, y_test)\n\n0.9485714285714286"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html#grid-searching-the-imputer",
    "href": "posts/2021-05-31-missing-values.html#grid-searching-the-imputer",
    "title": "Missing values in scikit-learn",
    "section": "Grid searching the imputer",
    "text": "Grid searching the imputer\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\n\niris = pd.read_csv('data/iris_w_missing.csv')\n\n\niris.head()\n\n\n\n\n\n  \n    \n      \n      sepal length (cm)\n      sepal width (cm)\n      petal length (cm)\n      petal width (cm)\n      target\n    \n  \n  \n    \n      0\n      6.4\n      2.9\n      4.3\n      1.3\n      1\n    \n    \n      1\n      5.7\n      2.8\n      4.1\n      1.3\n      1\n    \n    \n      2\n      6.8\n      2.8\n      NaN\n      1.4\n      1\n    \n    \n      3\n      6.7\n      3.3\n      5.7\n      2.1\n      2\n    \n    \n      4\n      4.8\n      3.4\n      1.6\n      0.2\n      0\n    \n  \n\n\n\n\n\nX = iris.drop('target', axis='columns')\ny = iris['target']\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=0, stratify=y\n)\n\n\npipe = Pipeline([\n    ('imputer', SimpleImputer(add_indicator=True)),\n    ('rf', RandomForestClassifier(random_state=42))\n])"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html#scikit-learn-uses-get_params-to-find-names",
    "href": "posts/2021-05-31-missing-values.html#scikit-learn-uses-get_params-to-find-names",
    "title": "Missing values in scikit-learn",
    "section": "scikit-learn uses get_params to find names",
    "text": "scikit-learn uses get_params to find names\n\npipe.get_params()\n\n{'memory': None,\n 'steps': [('imputer', SimpleImputer(add_indicator=True)),\n  ('rf', RandomForestClassifier(random_state=42))],\n 'verbose': False,\n 'imputer': SimpleImputer(add_indicator=True),\n 'rf': RandomForestClassifier(random_state=42),\n 'imputer__add_indicator': True,\n 'imputer__copy': True,\n 'imputer__fill_value': None,\n 'imputer__missing_values': nan,\n 'imputer__strategy': 'mean',\n 'imputer__verbose': 0,\n 'rf__bootstrap': True,\n 'rf__ccp_alpha': 0.0,\n 'rf__class_weight': None,\n 'rf__criterion': 'gini',\n 'rf__max_depth': None,\n 'rf__max_features': 'auto',\n 'rf__max_leaf_nodes': None,\n 'rf__max_samples': None,\n 'rf__min_impurity_decrease': 0.0,\n 'rf__min_impurity_split': None,\n 'rf__min_samples_leaf': 1,\n 'rf__min_samples_split': 2,\n 'rf__min_weight_fraction_leaf': 0.0,\n 'rf__n_estimators': 100,\n 'rf__n_jobs': None,\n 'rf__oob_score': False,\n 'rf__random_state': 42,\n 'rf__verbose': 0,\n 'rf__warm_start': False}"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html#is-it-better-to-add-the-indicator",
    "href": "posts/2021-05-31-missing-values.html#is-it-better-to-add-the-indicator",
    "title": "Missing values in scikit-learn",
    "section": "Is it better to add the indicator?",
    "text": "Is it better to add the indicator?\n\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {\n    'imputer__add_indicator': [True, False]\n}\n\ngrid_search = GridSearchCV(pipe, param_grid=params, verbose=1)\n\n\ngrid_search.fit(X_train, y_train)\n\nFitting 5 folds for each of 2 candidates, totalling 10 fits\n\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.2s finished\n\n\nGridSearchCVGridSearchCV(estimator=Pipeline(steps=[('imputer',\n                                        SimpleImputer(add_indicator=True)),\n                                       ('rf',\n                                        RandomForestClassifier(random_state=42))]),\n             param_grid={'imputer__add_indicator': [True, False]}, verbose=1)SimpleImputerSimpleImputer(add_indicator=True)RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\ngrid_search.best_params_\n\n{'imputer__add_indicator': True}\n\n\n\ngrid_search.best_score_\n\n0.8837944664031621\n\n\n\ngrid_search.score(X_test, y_test)\n\n0.9473684210526315"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html#compare-to-make_pipeline",
    "href": "posts/2021-05-31-missing-values.html#compare-to-make_pipeline",
    "title": "Missing values in scikit-learn",
    "section": "Compare to make_pipeline",
    "text": "Compare to make_pipeline\n\nfrom sklearn.pipeline import make_pipeline\n\npipe2 = make_pipeline(SimpleImputer(add_indicator=True),\n                     RandomForestClassifier(random_state=42))\n\n\npipe2.get_params()\n\n{'memory': None,\n 'steps': [('simpleimputer', SimpleImputer(add_indicator=True)),\n  ('randomforestclassifier', RandomForestClassifier(random_state=42))],\n 'verbose': False,\n 'simpleimputer': SimpleImputer(add_indicator=True),\n 'randomforestclassifier': RandomForestClassifier(random_state=42),\n 'simpleimputer__add_indicator': True,\n 'simpleimputer__copy': True,\n 'simpleimputer__fill_value': None,\n 'simpleimputer__missing_values': nan,\n 'simpleimputer__strategy': 'mean',\n 'simpleimputer__verbose': 0,\n 'randomforestclassifier__bootstrap': True,\n 'randomforestclassifier__ccp_alpha': 0.0,\n 'randomforestclassifier__class_weight': None,\n 'randomforestclassifier__criterion': 'gini',\n 'randomforestclassifier__max_depth': None,\n 'randomforestclassifier__max_features': 'auto',\n 'randomforestclassifier__max_leaf_nodes': None,\n 'randomforestclassifier__max_samples': None,\n 'randomforestclassifier__min_impurity_decrease': 0.0,\n 'randomforestclassifier__min_impurity_split': None,\n 'randomforestclassifier__min_samples_leaf': 1,\n 'randomforestclassifier__min_samples_split': 2,\n 'randomforestclassifier__min_weight_fraction_leaf': 0.0,\n 'randomforestclassifier__n_estimators': 100,\n 'randomforestclassifier__n_jobs': None,\n 'randomforestclassifier__oob_score': False,\n 'randomforestclassifier__random_state': 42,\n 'randomforestclassifier__verbose': 0,\n 'randomforestclassifier__warm_start': False}"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html#which-imputer-to-use",
    "href": "posts/2021-05-31-missing-values.html#which-imputer-to-use",
    "title": "Missing values in scikit-learn",
    "section": "Which imputer to use?",
    "text": "Which imputer to use?\n\nfrom sklearn.impute import KNNImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nparams = {\n    'imputer': [\n        SimpleImputer(strategy='median', add_indicator=True),\n        SimpleImputer(strategy='mean', add_indicator=True),\n        KNNImputer(add_indicator=True),\n        IterativeImputer(estimator=RandomForestRegressor(random_state=42),\n                         random_state=42, add_indicator=True)]\n}\n\nsearch_cv = GridSearchCV(pipe, param_grid=params, verbose=1, n_jobs=-1)\n\n\nsearch_cv.fit(X_train, y_train)\n\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n\n\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    7.8s finished\n\n\nGridSearchCVGridSearchCV(estimator=Pipeline(steps=[('imputer',\n                                        SimpleImputer(add_indicator=True)),\n                                       ('rf',\n                                        RandomForestClassifier(random_state=42))]),\n             n_jobs=-1,\n             param_grid={'imputer': [SimpleImputer(add_indicator=True,\n                                                   strategy='median'),\n                                     SimpleImputer(add_indicator=True),\n                                     KNNImputer(add_indicator=True),\n                                     IterativeImputer(add_indicator=True,\n                                                      estimator=RandomForestRegressor(random_state=42),\n                                                      random_state=42)]},\n             verbose=1)SimpleImputerSimpleImputer(add_indicator=True)RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\nsearch_cv.best_params_\n\n{'imputer': KNNImputer(add_indicator=True)}\n\n\n\nsearch_cv.best_score_\n\n0.9102766798418973\n\n\n\nsearch_cv.score(X_test, y_test)\n\n0.9736842105263158"
  },
  {
    "objectID": "posts/2020-10-22-stock-market-returns.html",
    "href": "posts/2020-10-22-stock-market-returns.html",
    "title": "Stock Market Analysis of Microsoft, Zoom, and Snowflake",
    "section": "",
    "text": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nimport pandas_datareader\nimport datetime\n\n\nimport pandas_datareader.data as web\n\n\nstart = datetime.datetime(2019, 1, 1)\nend = datetime.datetime(2021, 1, 1)\n\n#start = datetime.datetime(2012, 1, 1)\n#end = datetime.datetime(2017, 1, 1)\n#tesla = web.DataReader(\"TSLA\", 'yahoo', start, end)\n\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nTSLA_stock = web.DataReader('TSLA', 'yahoo', start, end)\nTSLA_stock.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2019-01-02\n      63.026001\n      59.759998\n      61.220001\n      62.023998\n      58293000.0\n      62.023998\n    \n    \n      2019-01-03\n      61.880001\n      59.476002\n      61.400002\n      60.071999\n      34826000.0\n      60.071999\n    \n    \n      2019-01-04\n      63.599998\n      60.546001\n      61.200001\n      63.537998\n      36970500.0\n      63.537998\n    \n    \n      2019-01-07\n      67.348000\n      63.549999\n      64.344002\n      66.991997\n      37756000.0\n      66.991997\n    \n    \n      2019-01-08\n      68.802002\n      65.403999\n      68.391998\n      67.070000\n      35042500.0\n      67.070000\n    \n  \n\n\n\n\n\nMSFT_stock['Open'].plot(label='MSFT_stock',figsize=(16,8),title='Open Price')\nZOOM_stock['Open'].plot(label='ZOOM_stock')\nTSLA_stock['Open'].plot(label='TSLA_stock')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7fabf2074c90>\n\n\n\n\n\n\n\nMSFT_stock['Volume'].plot(label='MSFT_stock',figsize=(16,8),title='Volume Traded')\nZOOM_stock['Volume'].plot(label='ZOOM_stock')\nTSLA_stock['Volume'].plot(label='TSLA_stock')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7fabf1feab90>\n\n\n\n\n\n\nMSFT_stock['Volume'].argmax()\n\n291\n\n\n\nMSFT_stock['Total Traded'] = MSFT_stock['Open']*MSFT_stock['Volume']\nTSLA_stock['Total Traded'] = TSLA_stock['Open']*TSLA_stock['Volume']\nZOOM_stock['Total Traded'] = ZOOM_stock['Open']*ZOOM_stock['Volume']\n\n\nMSFT_stock['Total Traded'].plot(label='MSFT_stock',figsize=(16,8))\nZOOM_stock['Total Traded'].plot(label='ZOOM_stock')\nTSLA_stock['Total Traded'].plot(label='TSLA_stock')\nplt.legend()\nplt.ylabel('Total Traded')\n\nText(0, 0.5, 'Total Traded')\n\n\n\n\n\n\nZOOM_stock['Total Traded'].argmax()\n\n346\n\n\nMA (Moving Averages)\n\nZOOM_stock['MA50'] = ZOOM_stock['Open'].rolling(50).mean()\nZOOM_stock['MA200'] = ZOOM_stock['Open'].rolling(200).mean()\nZOOM_stock[['Open','MA50','MA200']].plot(label='ZOOM_stock',figsize=(16,8))\n\n<AxesSubplot:xlabel='Date'>\n\n\n\n\n\n\nfrom pandas.plotting import scatter_matrix\n\ncar_comp = pd.concat([MSFT_stock['Open'],ZOOM_stock['Open'],SNOW_stock['Open']],axis=1)\n\ncar_comp.columns = ['MSFT_stock Open','ZOOM_stock Open','SNOW_stock Open']\n\n\n# You can use a semi-colon to remove the axes print outs\nscatter_matrix(car_comp,figsize=(8,8),alpha=0.2,hist_kwds={'bins':50});\n\n\n\n\n\nfrom mpl_finance import candlestick_ohlc\n\nfrom matplotlib.dates import DateFormatter, date2num, WeekdayLocator, DayLocator, MONDAY\n\n\n# Rest the index to get a column of January Dates\nMSFT_stock_reset = MSFT_stock.loc['2019-01':'2019-01'].reset_index()\n\n\n# Create a new column of numerical \"date\" values for matplotlib to use\nMSFT_stock_reset['date_ax'] = MSFT_stock_reset['Date'].apply(lambda date: date2num(date))\nMSFT_stock_values = [tuple(vals) for vals in MSFT_stock_reset[['date_ax', 'Open', 'High', 'Low', 'Close']].values]\n\n\nmondays = WeekdayLocator(MONDAY)        # major ticks on the mondays\nalldays = DayLocator()              # minor ticks on the days\nweekFormatter = DateFormatter('%b %d')  # e.g., Jan 12\ndayFormatter = DateFormatter('%d')      # e.g., 12\n\n\nfig, ax = plt.subplots()\nfig.subplots_adjust(bottom=0.2)\nax.xaxis.set_major_locator(mondays)\nax.xaxis.set_minor_locator(alldays)\nax.xaxis.set_major_formatter(weekFormatter)\n\ncandlestick_ohlc(ax, MSFT_stock_values, width=0.6, colorup='g',colordown='r');\n\n\n\n\n\n# Method 1: Using shift\nMSFT_stock['returns'] = (MSFT_stock['Close'] / MSFT_stock['Close'].shift(1) ) - 1\n\nMSFT_stock.head()\n\nMSFT_stock['returns'] = MSFT_stock['Close'].pct_change(1)\n\nMSFT_stock.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n      Total Traded\n      returns\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2019-01-02\n      101.750000\n      98.940002\n      99.550003\n      101.120003\n      35329300.0\n      98.860214\n      3.517032e+09\n      NaN\n    \n    \n      2019-01-03\n      100.190002\n      97.199997\n      100.099998\n      97.400002\n      42579100.0\n      95.223351\n      4.262168e+09\n      -0.036788\n    \n    \n      2019-01-04\n      102.510002\n      98.930000\n      99.720001\n      101.930000\n      44060600.0\n      99.652115\n      4.393723e+09\n      0.046509\n    \n    \n      2019-01-07\n      103.269997\n      100.980003\n      101.639999\n      102.059998\n      35656100.0\n      99.779205\n      3.624086e+09\n      0.001275\n    \n    \n      2019-01-08\n      103.970001\n      101.709999\n      103.040001\n      102.800003\n      31514400.0\n      100.502670\n      3.247244e+09\n      0.007251\n    \n  \n\n\n\n\n\nTSLA_stock['returns'] = TSLA_stock['Close'].pct_change(1)\nZOOM_stock['returns'] = ZOOM_stock['Close'].pct_change(1)\n\n\nTSLA_stock.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n      Total Traded\n      returns\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2019-01-02\n      63.026001\n      59.759998\n      61.220001\n      62.023998\n      58293000.0\n      62.023998\n      3.568698e+09\n      NaN\n    \n    \n      2019-01-03\n      61.880001\n      59.476002\n      61.400002\n      60.071999\n      34826000.0\n      60.071999\n      2.138316e+09\n      -0.031472\n    \n    \n      2019-01-04\n      63.599998\n      60.546001\n      61.200001\n      63.537998\n      36970500.0\n      63.537998\n      2.262595e+09\n      0.057697\n    \n    \n      2019-01-07\n      67.348000\n      63.549999\n      64.344002\n      66.991997\n      37756000.0\n      66.991997\n      2.429372e+09\n      0.054361\n    \n    \n      2019-01-08\n      68.802002\n      65.403999\n      68.391998\n      67.070000\n      35042500.0\n      67.070000\n      2.396627e+09\n      0.001164\n    \n  \n\n\n\n\n\nZOOM_stock.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n      Total Traded\n      MA50\n      MA200\n      returns\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2019-04-18\n      66.000000\n      60.320999\n      65.000000\n      62.000000\n      25764700\n      62.000000\n      1.674706e+09\n      NaN\n      NaN\n      NaN\n    \n    \n      2019-04-22\n      68.900002\n      59.939999\n      61.000000\n      65.699997\n      9949700\n      65.699997\n      6.069317e+08\n      NaN\n      NaN\n      0.059677\n    \n    \n      2019-04-23\n      74.168999\n      65.550003\n      66.870003\n      69.000000\n      6786500\n      69.000000\n      4.538133e+08\n      NaN\n      NaN\n      0.050228\n    \n    \n      2019-04-24\n      71.500000\n      63.160000\n      71.400002\n      63.200001\n      4973500\n      63.200001\n      3.551079e+08\n      NaN\n      NaN\n      -0.084058\n    \n    \n      2019-04-25\n      66.849998\n      62.599998\n      64.739998\n      65.000000\n      3863300\n      65.000000\n      2.501100e+08\n      NaN\n      NaN\n      0.028481\n    \n  \n\n\n\n\n\nTSLA_stock['returns'].hist(bins=50)\n\n<AxesSubplot:>\n\n\n\n\n\n\nMSFT_stock['returns'].hist(bins=50)\n\n<AxesSubplot:>\n\n\n\n\n\n\nZOOM_stock['returns'].hist(bins=50)\n\n<AxesSubplot:>\n\n\n\n\n\n\nMSFT_stock['returns'].hist(bins=100,label='MSFT_stock',figsize=(10,8),alpha=0.5)\nZOOM_stock['returns'].hist(bins=100,label='ZOOM_stock',alpha=0.5)\nTSLA_stock['returns'].hist(bins=100,label='TSLA_stock',alpha=0.5)\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7fabf1902d90>\n\n\n\n\n\n\nMSFT_stock['returns'].plot(kind='kde',label='MSFT_stock',figsize=(12,6))\nZOOM_stock['returns'].plot(kind='kde',label='ZOOM_stock')\nTSLA_stock['returns'].plot(kind='kde',label='TSLA_stock')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7fabf11a3e50>\n\n\n\n\n\n\nbox_df = pd.concat([MSFT_stock['returns'],ZOOM_stock['returns'],TSLA_stock['returns']],axis=1)\nbox_df.columns = ['MSFT_stock Returns',' ZOOM_stock Returns','TSLA_stock Returns']\nbox_df.plot(kind='box',figsize=(8,11),colormap='jet')\n\n<AxesSubplot:>\n\n\n\n\n\n\nscatter_matrix(box_df,figsize=(8,8),alpha=0.2,hist_kwds={'bins':50});\n\n\n\n\n\nscatter_matrix(box_df,figsize=(8,8),alpha=0.2,hist_kwds={'bins':50});\n\n\n\n\n\nbox_df.plot(kind='scatter',x=' ZOOM_stock Returns',y='MSFT_stock Returns',alpha=0.4,figsize=(10,8))\n\n<AxesSubplot:xlabel=' ZOOM_stock Returns', ylabel='MSFT_stock Returns'>"
  },
  {
    "objectID": "posts/2020-10-22-stock-market-returns.html#daily-return-and-cumulative-return",
    "href": "posts/2020-10-22-stock-market-returns.html#daily-return-and-cumulative-return",
    "title": "Stock Market Analysis of Microsoft, Zoom, and Snowflake",
    "section": "Daily Return and Cumulative Return",
    "text": "Daily Return and Cumulative Return\n\nMSFT_stock['Cumulative Return'] = (1 + MSFT_stock['returns']).cumprod()\n\nMSFT_stock.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n      Total Traded\n      returns\n      Cumulative Return\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2019-01-02\n      101.750000\n      98.940002\n      99.550003\n      101.120003\n      35329300.0\n      98.860214\n      3.517032e+09\n      NaN\n      NaN\n    \n    \n      2019-01-03\n      100.190002\n      97.199997\n      100.099998\n      97.400002\n      42579100.0\n      95.223351\n      4.262168e+09\n      -0.036788\n      0.963212\n    \n    \n      2019-01-04\n      102.510002\n      98.930000\n      99.720001\n      101.930000\n      44060600.0\n      99.652115\n      4.393723e+09\n      0.046509\n      1.008010\n    \n    \n      2019-01-07\n      103.269997\n      100.980003\n      101.639999\n      102.059998\n      35656100.0\n      99.779205\n      3.624086e+09\n      0.001275\n      1.009296\n    \n    \n      2019-01-08\n      103.970001\n      101.709999\n      103.040001\n      102.800003\n      31514400.0\n      100.502670\n      3.247244e+09\n      0.007251\n      1.016614\n    \n  \n\n\n\n\n\nTSLA_stock['Cumulative Return'] = (1 + TSLA_stock['returns']).cumprod()\nZOOM_stock['Cumulative Return'] = (1 + ZOOM_stock['returns']).cumprod()\n\n\nMSFT_stock['Cumulative Return'].plot(label='MSFT_stock',figsize=(16,8),title='Cumulative Return')\nTSLA_stock['Cumulative Return'].plot(label='TSLA_stock')\nZOOM_stock['Cumulative Return'].plot(label='ZOOM_stock')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7fabf0c54650>\n\n\n\n\n\n\nfrom fbprophet import Prophet\n\n\nMSFT_stock['ds'] = MSFT_stock.index\nMSFT_stock['y'] = MSFT_stock.Open\n\nm = Prophet()\nm.fit(MSFT_stock)\n\nINFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n<fbprophet.forecaster.Prophet at 0x7fabf0b5e7d0>\n\n\n\nfuture = m.make_future_dataframe(periods=365)\nfuture.tail()\n\n\n\n\n\n  \n    \n      \n      ds\n    \n  \n  \n    \n      828\n      2021-11-02\n    \n    \n      829\n      2021-11-03\n    \n    \n      830\n      2021-11-04\n    \n    \n      831\n      2021-11-05\n    \n    \n      832\n      2021-11-06\n    \n  \n\n\n\n\n\nforecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n\n\n\n\n\n  \n    \n      \n      ds\n      yhat\n      yhat_lower\n      yhat_upper\n    \n  \n  \n    \n      828\n      2021-11-02\n      264.480553\n      180.297752\n      351.490936\n    \n    \n      829\n      2021-11-03\n      264.454392\n      181.887667\n      348.682850\n    \n    \n      830\n      2021-11-04\n      264.470521\n      183.308874\n      351.080864\n    \n    \n      831\n      2021-11-05\n      264.680107\n      182.084709\n      351.905134\n    \n    \n      832\n      2021-11-06\n      263.754254\n      180.125064\n      351.617704\n    \n  \n\n\n\n\n\nfig1 = m.plot(forecast)\n\n\n\n\n\nfig2 = m.plot_components(forecast)\n\n\n\n\n\nMSFT_stock['Open'].plot()\n\n<AxesSubplot:xlabel='Date'>\n\n\n\n\n\n\nimport statsmodels.api as sm\n\n\nMSFT_stock_cycle, MSFT_stock_trend = sm.tsa.filters.hpfilter(MSFT_stock['Open'])\n\n\nMSFT_stock_cycle\n\nDate\n2019-01-02   -1.311918\n2019-01-03   -1.097494\n2019-01-04   -1.812243\n2019-01-07   -0.224671\n2019-01-08    0.847869\n                ...   \n2020-11-02   -8.431408\n2020-11-03   -8.845880\n2020-11-04    1.239842\n2020-11-05    9.198111\n2020-11-06    9.350548\nName: Open_cycle, Length: 468, dtype: float64\n\n\n\nMSFT_stock['trend'] = MSFT_stock_trend\n\n\nMSFT_stock['trend']\n\nDate\n2019-01-02    100.861921\n2019-01-03    101.197493\n2019-01-04    101.532244\n2019-01-07    101.864670\n2019-01-08    102.192132\n                 ...    \n2020-11-02    212.721402\n2020-11-03    212.735880\n2020-11-04    212.780163\n2020-11-05    212.841883\n2020-11-06    212.909447\nName: trend, Length: 468, dtype: float64\n\n\n\nMSFT_stock[[\"trend\", \"Open\"]].plot(figsize=(16,6))\n\n<AxesSubplot:xlabel='Date'>\n\n\n\n\n\n\nTSLA_stock['ds'] = TSLA_stock.index\nTSLA_stock['y'] = TSLA_stock.Open\n\nm = Prophet()\nm.fit(TSLA_stock)\n\nINFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n<fbprophet.forecaster.Prophet at 0x7fabeaebca10>\n\n\n\n# Python\nTSLA_stock['cap'] = 8.5\nm = Prophet(growth='logistic')\nm.fit(TSLA_stock)\n\nINFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n<fbprophet.forecaster.Prophet at 0x7fabf081d190>\n\n\n\n# future = m.make_future_dataframe(periods=1826)\n# future['cap'] = 600\n# fcst = m.predict(future)\n# fig = m.plot(fcst)\n\n\nfuture = m.make_future_dataframe(periods=365)\nfuture.tail()\n\n\n\n\n\n  \n    \n      \n      ds\n    \n  \n  \n    \n      828\n      2021-11-02\n    \n    \n      829\n      2021-11-03\n    \n    \n      830\n      2021-11-04\n    \n    \n      831\n      2021-11-05\n    \n    \n      832\n      2021-11-06\n    \n  \n\n\n\n\n\nforecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n\n\n\n\n\n  \n    \n      \n      ds\n      yhat\n      yhat_lower\n      yhat_upper\n    \n  \n  \n    \n      828\n      2021-11-02\n      1136.958087\n      1029.660624\n      1241.408457\n    \n    \n      829\n      2021-11-03\n      1138.386277\n      1029.960322\n      1245.559028\n    \n    \n      830\n      2021-11-04\n      1139.054611\n      1027.658532\n      1245.938576\n    \n    \n      831\n      2021-11-05\n      1140.980461\n      1031.883548\n      1250.009090\n    \n    \n      832\n      2021-11-06\n      1142.201501\n      1037.157758\n      1245.925892\n    \n  \n\n\n\n\n\nfig1 = m.plot(forecast)\n\n\n\n\n\nfig2 = m.plot_components(forecast)\n\n\n\n\n\nZOOM_stock['ds'] = ZOOM_stock.index\nZOOM_stock['y'] = ZOOM_stock.Open\n\nm = Prophet()\nm.fit(ZOOM_stock)\n\nINFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n<fbprophet.forecaster.Prophet at 0x7fabf0b64790>\n\n\n\nfuture = m.make_future_dataframe(periods=365)\nfuture.tail()\n\n\n\n\n\n  \n    \n      \n      ds\n    \n  \n  \n    \n      754\n      2021-11-02\n    \n    \n      755\n      2021-11-03\n    \n    \n      756\n      2021-11-04\n    \n    \n      757\n      2021-11-05\n    \n    \n      758\n      2021-11-06\n    \n  \n\n\n\n\n\nforecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n\n\n\n\n\n  \n    \n      \n      ds\n      yhat\n      yhat_lower\n      yhat_upper\n    \n  \n  \n    \n      754\n      2021-11-02\n      1498.560150\n      1318.618163\n      1677.021462\n    \n    \n      755\n      2021-11-03\n      1500.430463\n      1324.946607\n      1679.133120\n    \n    \n      756\n      2021-11-04\n      1502.503460\n      1323.082402\n      1683.102389\n    \n    \n      757\n      2021-11-05\n      1505.218307\n      1323.789633\n      1683.789366\n    \n    \n      758\n      2021-11-06\n      1499.986193\n      1323.049103\n      1680.953619\n    \n  \n\n\n\n\n\nfig1 = m.plot(forecast)\n\n\n\n\n\nfig2 = m.plot_components(forecast)"
  },
  {
    "objectID": "posts/2020-08-20-pyspark-dataframes-data-types.html",
    "href": "posts/2020-08-20-pyspark-dataframes-data-types.html",
    "title": "Dataframes, Formatting, Casting Data Type and Correlation with Pyspark",
    "section": "",
    "text": "from pyspark.sql import SparkSession\n\n# Load data from a CSV\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").option(\"inferSchema\", True).option(\"header\", True).load(file_location)\ndisplay(df.take(5))\n\n\n_c0provincespecificgeneralyeargdpfdirnrrrifrregit0Anhui147002.0null19962093.3506610.00.00.01128873East China6319301Anhui151981.0null19972347.32434430.00.00.01356287East China6578602Anhui174930.0null19982542.96276730.00.00.01518236East China8894633Anhui285324.0null19992712.3426131nullnullnull1646891East China12273644Anhui195580.032100.020002902.09318470.00.00.01601508East China1499110"
  },
  {
    "objectID": "posts/2020-08-20-pyspark-dataframes-data-types.html#casting-data-types-and-formatting-significant-digits",
    "href": "posts/2020-08-20-pyspark-dataframes-data-types.html#casting-data-types-and-formatting-significant-digits",
    "title": "Dataframes, Formatting, Casting Data Type and Correlation with Pyspark",
    "section": "Casting Data Types and Formatting Significant Digits",
    "text": "Casting Data Types and Formatting Significant Digits\n\nfrom pyspark.sql.functions import format_number\n\n\n\n\n\n\nresult = df.describe()\nresult.select(result['province']\n,format_number(result['specific'].cast('float'),2).alias('specific')\n,format_number(result['general'].cast('float'),2).alias('general')\n,format_number(result['year'].cast('int'),2).alias('year'),format_number(result['gdp'].cast('float'),2).alias('gdp')\n,format_number(result['rnr'].cast('int'),2).alias('rnr'),format_number(result['rr'].cast('float'),2).alias('rr')\n,format_number(result['fdi'].cast('int'),2).alias('fdi'),format_number(result['it'].cast('float'),2).alias('it')\n,result['reg'].cast('string').alias('reg')\n             ).show()\n\n\n+--------+------------+------------+--------+---------+------+------+------------+-------------+---------------+\nprovince|    specific|     general|    year|      gdp|   rnr|    rr|         fdi|           it|            reg|\n+--------+------------+------------+--------+---------+------+------+------------+-------------+---------------+\n     360|      356.00|      169.00|  360.00|   360.00|294.00|296.00|      360.00|       360.00|            360|\n    null|  583,470.75|  309,127.53|2,001.00| 4,428.65|  0.00|  0.06|  196,139.00| 2,165,819.25|           null|\n    null|  654,055.31|  355,423.56|    3.00| 4,484.67|  0.00|  0.16|  303,043.00| 1,769,294.25|           null|\n   Anhui|    8,964.00|        0.00|1,996.00|    64.98|  0.00|  0.00|        2.00|   147,897.00|     East China|\nZhejiang|3,937,966.00|1,737,800.00|2,007.00|31,777.01|  1.00|  0.84|1,743,140.00|10,533,312.00|Southwest China|\n+--------+------------+------------+--------+---------+------+------+------------+-------------+---------------+"
  },
  {
    "objectID": "posts/2020-08-20-pyspark-dataframes-data-types.html#new-columns-generated-from-extant-columns-using-withcolumn",
    "href": "posts/2020-08-20-pyspark-dataframes-data-types.html#new-columns-generated-from-extant-columns-using-withcolumn",
    "title": "Dataframes, Formatting, Casting Data Type and Correlation with Pyspark",
    "section": "New Columns generated from extant columns using withColumn",
    "text": "New Columns generated from extant columns using withColumn\n\ndf2 = df.withColumn(\"specific_gdp_ratio\",df[\"specific\"]/(df[\"gdp\"]*100))#.show()\n\n\n\n\n\n\ndf2.select('specific_gdp_ratio').show()\n\n\n+------------------+\nspecific_gdp_ratio|\n+------------------+\n0.7022500358285959|\n0.6474660463848132|\n0.6878991411583352|\n1.0519477646607727|\n 0.673928100093381|\n0.7727761333780966|\n 1.233475958314866|\n1.5783421826051272|\n1.8877587040110941|\n1.6792756118029895|\n2.3850666666666664|\n3.0077639751552794|\n0.9275486250838364|\n0.7989880072601573|\n1.0314658544998698|\n 1.448708759827088|\n0.8912058855158366|\n1.1918224576316896|\n1.2944820393974508|\n 1.283311464867661|\n+------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.orderBy(df[\"specific\"].asc()).head(1)[0][0]\n\n\nOut[94]: 24"
  },
  {
    "objectID": "posts/2020-08-20-pyspark-dataframes-data-types.html#finding-the-mean-max-and-min",
    "href": "posts/2020-08-20-pyspark-dataframes-data-types.html#finding-the-mean-max-and-min",
    "title": "Dataframes, Formatting, Casting Data Type and Correlation with Pyspark",
    "section": "Finding the Mean, Max, and Min",
    "text": "Finding the Mean, Max, and Min\n\nfrom pyspark.sql.functions import mean\ndf.select(mean(\"specific\")).show()\n\n\n+-----------------+\n    avg(specific)|\n+-----------------+\n583470.7303370787|\n+-----------------+\n\n\n\n\n\nfrom pyspark.sql.functions import max,min\n\n\n\n\n\n\ndf.select(max(\"specific\"),min(\"specific\")).show()\n\n\n+-------------+-------------+\nmax(specific)|min(specific)|\n+-------------+-------------+\n    3937966.0|       8964.0|\n+-------------+-------------+\n\n\n\n\n\ndf.filter(\"specific < 60000\").count()\n\n\nOut[98]: 23\n\n\n\ndf.filter(df['specific'] < 60000).count()\n\n\nOut[99]: 23\n\n\n\nfrom pyspark.sql.functions import count\nresult = df.filter(df['specific'] < 60000)\nresult.select(count('specific')).show()\n\n\n+---------------+\ncount(specific)|\n+---------------+\n             23|\n+---------------+\n\n\n\n\n\n(df.filter(df[\"gdp\"]>8000).count()*1.0/df.count())*100\n\n\nOut[101]: 14.444444444444443\n\n\n\nfrom pyspark.sql.functions import corr\ndf.select(corr(\"gdp\",\"fdi\")).show()\n\n\n+------------------+\n    corr(gdp, fdi)|\n+------------------+\n0.8366328478935896|\n+------------------+"
  },
  {
    "objectID": "posts/2020-08-20-pyspark-dataframes-data-types.html#finding-the-max-value-by-year",
    "href": "posts/2020-08-20-pyspark-dataframes-data-types.html#finding-the-max-value-by-year",
    "title": "Dataframes, Formatting, Casting Data Type and Correlation with Pyspark",
    "section": "Finding the max value by Year",
    "text": "Finding the max value by Year\n\nfrom pyspark.sql.functions import year\n#yeardf = df.withColumn(\"Year\",year(df[\"year\"]))\n\n\n\n\n\n\nmax_df = df.groupBy('year').max()\n\n\n\n\n\n\nmax_df.select('year','max(gdp)').show()\n\n\n+----+--------+\nyear|max(gdp)|\n+----+--------+\n2003|15844.64|\n2007|31777.01|\n2006|26587.76|\n1997| 7774.53|\n2004|18864.62|\n1996| 6834.97|\n1998| 8530.88|\n2001|12039.25|\n2005|22557.37|\n2000|10741.25|\n1999| 9250.68|\n2002|13502.42|\n+----+--------+\n\n\n\n\n\nfrom pyspark.sql.functions import month\n\n\n\n\n\n\n#df.select(\"year\",\"avg(gdp)\").orderBy('year').show()\n\n\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-09-25-analyzingsizeofarmedforces.html",
    "href": "posts/2020-09-25-analyzingsizeofarmedforces.html",
    "title": "Analyzing Size of Armed Forces From 1947 - 1963 with statsmodels",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n%matplotlib inline\n\n\ndf = sm.datasets.longley.load_pandas().data\n\n#print(sm.datasets.longley.NOTE)\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      TOTEMP\n      GNPDEFL\n      GNP\n      UNEMP\n      ARMED\n      POP\n      YEAR\n    \n  \n  \n    \n      0\n      60323.0\n      83.0\n      234289.0\n      2356.0\n      1590.0\n      107608.0\n      1947.0\n    \n    \n      1\n      61122.0\n      88.5\n      259426.0\n      2325.0\n      1456.0\n      108632.0\n      1948.0\n    \n    \n      2\n      60171.0\n      88.2\n      258054.0\n      3682.0\n      1616.0\n      109773.0\n      1949.0\n    \n    \n      3\n      61187.0\n      89.5\n      284599.0\n      3351.0\n      1650.0\n      110929.0\n      1950.0\n    \n    \n      4\n      63221.0\n      96.2\n      328975.0\n      2099.0\n      3099.0\n      112075.0\n      1951.0\n    \n  \n\n\n\n\n\nindex = pd.Index(sm.tsa.datetools.dates_from_range('1947', '1962'))\ndf.index = index\ndf.head()\n\n\n\n\n\n  \n    \n      \n      TOTEMP\n      GNPDEFL\n      GNP\n      UNEMP\n      ARMED\n      POP\n      YEAR\n    \n  \n  \n    \n      1947-12-31\n      60323.0\n      83.0\n      234289.0\n      2356.0\n      1590.0\n      107608.0\n      1947.0\n    \n    \n      1948-12-31\n      61122.0\n      88.5\n      259426.0\n      2325.0\n      1456.0\n      108632.0\n      1948.0\n    \n    \n      1949-12-31\n      60171.0\n      88.2\n      258054.0\n      3682.0\n      1616.0\n      109773.0\n      1949.0\n    \n    \n      1950-12-31\n      61187.0\n      89.5\n      284599.0\n      3351.0\n      1650.0\n      110929.0\n      1950.0\n    \n    \n      1951-12-31\n      63221.0\n      96.2\n      328975.0\n      2099.0\n      3099.0\n      112075.0\n      1951.0\n    \n  \n\n\n\n\n\ndf['ARMED'].plot()\nplt.ylabel(\"ARMED\")\n\nText(0, 0.5, 'ARMED')\n\n\n\n\n\n\n# unpacking\ncycle, trend = sm.tsa.filters.hpfilter(df.ARMED)\n\ncycle\n\n1947-12-31    -497.642333\n1948-12-31    -713.661033\n1949-12-31    -635.368706\n1950-12-31    -682.008289\n1951-12-31     688.574390\n1952-12-31    1108.959755\n1953-12-31     992.297873\n1954-12-31     731.045710\n1955-12-31     370.040046\n1956-12-31     124.660757\n1957-12-31      15.056446\n1958-12-31    -193.702199\n1959-12-31    -324.553899\n1960-12-31    -407.316313\n1961-12-31    -393.604252\n1962-12-31    -182.777954\nName: ARMED, dtype: float64\n\n\n\ntype(cycle)\n\npandas.core.series.Series\n\n\n\ndf[\"trend\"] = trend\n\ndf[['trend','ARMED']].plot(figsize = (12, 8))\n\ndf[['trend','ARMED']][\"1950-01-01\":\"1955-01-01\"].plot(figsize = (12, 8))\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f2d5af13940>"
  },
  {
    "objectID": "posts/2021-05-31-review-sklearn.html",
    "href": "posts/2021-05-31-review-sklearn.html",
    "title": "Review of scikit-learn",
    "section": "",
    "text": "import seaborn as sns\nimport sklearn\nsns.set_theme(context=\"notebook\", font_scale=1.2,\n              rc={\"figure.figsize\": [10, 6]})\nsklearn.set_config(display=\"diagram\")\n\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      province\n      specific\n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      reg\n      it\n    \n  \n  \n    \n      0\n      0\n      Anhui\n      147002.0\n      NaN\n      1996\n      2093.30\n      50661\n      0.000000\n      0.000000\n      0.000000\n      1128873\n      East China\n      631930\n    \n    \n      1\n      1\n      Anhui\n      151981.0\n      NaN\n      1997\n      2347.32\n      43443\n      0.000000\n      0.000000\n      0.000000\n      1356287\n      East China\n      657860\n    \n    \n      2\n      2\n      Anhui\n      174930.0\n      NaN\n      1998\n      2542.96\n      27673\n      0.000000\n      0.000000\n      0.000000\n      1518236\n      East China\n      889463\n    \n    \n      3\n      3\n      Anhui\n      285324.0\n      NaN\n      1999\n      2712.34\n      26131\n      NaN\n      NaN\n      NaN\n      1646891\n      East China\n      1227364\n    \n    \n      4\n      4\n      Anhui\n      195580.0\n      32100.0\n      2000\n      2902.09\n      31847\n      0.000000\n      0.000000\n      0.000000\n      1601508\n      East China\n      1499110\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      355\n      Zhejiang\n      391292.0\n      260313.0\n      2003\n      9705.02\n      498055\n      1.214286\n      0.035714\n      0.035714\n      6217715\n      East China\n      2261631\n    \n    \n      356\n      356\n      Zhejiang\n      656175.0\n      276652.0\n      2004\n      11648.70\n      668128\n      1.214286\n      0.035714\n      0.035714\n      NaN\n      East China\n      3162299\n    \n    \n      357\n      357\n      Zhejiang\n      656175.0\n      NaN\n      2005\n      13417.68\n      772000\n      1.214286\n      0.035714\n      0.035714\n      NaN\n      East China\n      2370200\n    \n    \n      358\n      358\n      Zhejiang\n      1017303.0\n      394795.0\n      2006\n      15718.47\n      888935\n      1.214286\n      0.035714\n      0.035714\n      11537149\n      East China\n      2553268\n    \n    \n      359\n      359\n      Zhejiang\n      844647.0\n      0.0\n      2007\n      18753.73\n      1036576\n      0.047619\n      0.000000\n      0.000000\n      16494981\n      East China\n      2939778\n    \n  \n\n360 rows  13 columns\n\n\n\n\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['it', 'Unnamed: 0'], axis = 1), df['it']\n\n\nX = X.select_dtypes(include='number')\nX\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\n\n\n\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n\n\nTrain DummyClassifer\n\nfrom sklearn.dummy import DummyClassifier\n\ndc = DummyClassifier(strategy='prior').fit(X_train, y_train)\ndc.score(X_test, y_test)\n\n0.9811502476609797\n\n\n\n\nTrain KNN based model\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknc = make_pipeline(\n    StandardScaler(),\n    KNeighborsClassifier()\n)\nknc.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('kneighborsclassifier', KNeighborsClassifier())])StandardScalerStandardScaler()KNeighborsClassifierKNeighborsClassifier()\n\n\n\nknc.score(X_test, y_test)\n\n0.9801871216290589\n\n\n\n# %load solutions/00-ex01-solutions.py\nfrom sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer(as_frame=True)\n\nX, y = cancer.data, cancer.target\n\ny.value_counts()\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=42, stratify=y)\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = make_pipeline(\n    StandardScaler(),\n    LogisticRegression()\n)\n\nlog_reg.fit(X_train, y_train)\n\nlog_reg.score(X_test, y_test)\n\nfrom sklearn.metrics import f1_score\ny_pred = log_reg.predict(X_test)\n\ny_pred\n\nf1_score(y_test, y_pred)\n\n0.9888888888888889"
  },
  {
    "objectID": "posts/2021-05-31-sklearn-pipelines-example.html",
    "href": "posts/2021-05-31-sklearn-pipelines-example.html",
    "title": "sklearn-pipelines-example",
    "section": "",
    "text": "# code adapted from https://github.com/thomasjpfan/ml-workshop-intro"
  },
  {
    "objectID": "posts/2021-05-31-sklearn-pipelines-example.html#make-pipeline",
    "href": "posts/2021-05-31-sklearn-pipelines-example.html#make-pipeline",
    "title": "sklearn-pipelines-example",
    "section": "Make pipeline!",
    "text": "Make pipeline!\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\n\nknr = make_pipeline(\n    StandardScaler(), KNeighborsRegressor()\n)\nknr.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('kneighborsregressor', KNeighborsRegressor())])StandardScalerStandardScaler()KNeighborsRegressorKNeighborsRegressor()\n\n\n\nknr.score(X_test, y_test)\n\n-0.16833369709565682\n\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\nknr_select = make_pipeline(\n    StandardScaler(),\n    PolynomialFeatures(),\n    KNeighborsRegressor()\n)\nknr_select.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('polynomialfeatures', PolynomialFeatures()),\n                ('kneighborsregressor', KNeighborsRegressor())])StandardScalerStandardScaler()PolynomialFeaturesPolynomialFeatures()KNeighborsRegressorKNeighborsRegressor()\n\n\n\nknr_select.score(X_test, y_test)\n\n-0.1635984889204516\n\n\n\nfrom sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer(as_frame=True)\n\nX, y = cancer.data, cancer.target\n\ny.value_counts()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n                                                   random_state=0)\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = make_pipeline(\n    StandardScaler(),\n    LogisticRegression(random_state=0)\n)\n\nlog_reg.fit(X_train, y_train)\n\nlog_reg.score(X_test, y_test)\n\nlog_reg_poly = make_pipeline(\n    StandardScaler(),\n    PolynomialFeatures(),\n    LogisticRegression(random_state=0)\n)\n\nlog_reg_poly.fit(X_train, y_train)\n\nlog_reg_poly.score(X_test, y_test)\n\n0.965034965034965"
  },
  {
    "objectID": "posts/2021-05-30-classification example 2 using health data with pycaret.html",
    "href": "posts/2021-05-30-classification example 2 using health data with pycaret.html",
    "title": "Classification example 2 using Health Data with PyCaret",
    "section": "",
    "text": "# check version\nfrom pycaret.utils import version\nversion()\n\n'2.3.1'\n\n\n\n1. Data Repository\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n  \n    \n      \n      id\n      gender\n      age\n      hypertension\n      heart_disease\n      ever_married\n      work_type\n      Residence_type\n      avg_glucose_level\n      bmi\n      smoking_status\n      stroke\n    \n  \n  \n    \n      0\n      30669\n      Male\n      3.0\n      0\n      0\n      No\n      children\n      Rural\n      95.12\n      18.0\n      NaN\n      0\n    \n    \n      1\n      30468\n      Male\n      58.0\n      1\n      0\n      Yes\n      Private\n      Urban\n      87.96\n      39.2\n      never smoked\n      0\n    \n    \n      2\n      16523\n      Female\n      8.0\n      0\n      0\n      No\n      Private\n      Urban\n      110.89\n      17.6\n      NaN\n      0\n    \n    \n      3\n      56543\n      Female\n      70.0\n      0\n      0\n      Yes\n      Private\n      Rural\n      69.04\n      35.9\n      formerly smoked\n      0\n    \n    \n      4\n      46136\n      Male\n      14.0\n      0\n      0\n      No\n      Never_worked\n      Rural\n      161.28\n      19.1\n      NaN\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      43395\n      56196\n      Female\n      10.0\n      0\n      0\n      No\n      children\n      Urban\n      58.64\n      20.4\n      never smoked\n      0\n    \n    \n      43396\n      5450\n      Female\n      56.0\n      0\n      0\n      Yes\n      Govt_job\n      Urban\n      213.61\n      55.4\n      formerly smoked\n      0\n    \n    \n      43397\n      28375\n      Female\n      82.0\n      1\n      0\n      Yes\n      Private\n      Urban\n      91.94\n      28.9\n      formerly smoked\n      0\n    \n    \n      43398\n      27973\n      Male\n      40.0\n      0\n      0\n      Yes\n      Private\n      Urban\n      99.16\n      33.2\n      never smoked\n      0\n    \n    \n      43399\n      36271\n      Female\n      82.0\n      0\n      0\n      Yes\n      Private\n      Urban\n      79.48\n      20.6\n      never smoked\n      0\n    \n  \n\n43400 rows  12 columns\n\n\n\n\ndata=df\n\n\n\n2. Initialize Setup\n\nfrom pycaret.classification import *\nclf1 = setup(df, target = 'stroke', session_id=123, log_experiment=True, experiment_name='health2')\n\n\n                    Description        Value    \n                \n                        0\n                        session_id\n                        123\n            \n            \n                        1\n                        Target\n                        stroke\n            \n            \n                        2\n                        Target Type\n                        Binary\n            \n            \n                        3\n                        Label Encoded\n                        0: 0, 1: 1\n            \n            \n                        4\n                        Original Data\n                        (43400, 12)\n            \n            \n                        5\n                        Missing Values\n                        True\n            \n            \n                        6\n                        Numeric Features\n                        4\n            \n            \n                        7\n                        Categorical Features\n                        7\n            \n            \n                        8\n                        Ordinal Features\n                        False\n            \n            \n                        9\n                        High Cardinality Features\n                        False\n            \n            \n                        10\n                        High Cardinality Method\n                        None\n            \n            \n                        11\n                        Transformed Train Set\n                        (30379, 19)\n            \n            \n                        12\n                        Transformed Test Set\n                        (13021, 19)\n            \n            \n                        13\n                        Shuffle Train-Test\n                        True\n            \n            \n                        14\n                        Stratify Train-Test\n                        False\n            \n            \n                        15\n                        Fold Generator\n                        StratifiedKFold\n            \n            \n                        16\n                        Fold Number\n                        10\n            \n            \n                        17\n                        CPU Jobs\n                        -1\n            \n            \n                        18\n                        Use GPU\n                        False\n            \n            \n                        19\n                        Log Experiment\n                        True\n            \n            \n                        20\n                        Experiment Name\n                        health2\n            \n            \n                        21\n                        USI\n                        eaf8\n            \n            \n                        22\n                        Imputation Type\n                        simple\n            \n            \n                        23\n                        Iterative Imputation Iteration\n                        None\n            \n            \n                        24\n                        Numeric Imputer\n                        mean\n            \n            \n                        25\n                        Iterative Imputation Numeric Model\n                        None\n            \n            \n                        26\n                        Categorical Imputer\n                        constant\n            \n            \n                        27\n                        Iterative Imputation Categorical Model\n                        None\n            \n            \n                        28\n                        Unknown Categoricals Handling\n                        least_frequent\n            \n            \n                        29\n                        Normalize\n                        False\n            \n            \n                        30\n                        Normalize Method\n                        None\n            \n            \n                        31\n                        Transformation\n                        False\n            \n            \n                        32\n                        Transformation Method\n                        None\n            \n            \n                        33\n                        PCA\n                        False\n            \n            \n                        34\n                        PCA Method\n                        None\n            \n            \n                        35\n                        PCA Components\n                        None\n            \n            \n                        36\n                        Ignore Low Variance\n                        False\n            \n            \n                        37\n                        Combine Rare Levels\n                        False\n            \n            \n                        38\n                        Rare Level Threshold\n                        None\n            \n            \n                        39\n                        Numeric Binning\n                        False\n            \n            \n                        40\n                        Remove Outliers\n                        False\n            \n            \n                        41\n                        Outliers Threshold\n                        None\n            \n            \n                        42\n                        Remove Multicollinearity\n                        False\n            \n            \n                        43\n                        Multicollinearity Threshold\n                        None\n            \n            \n                        44\n                        Clustering\n                        False\n            \n            \n                        45\n                        Clustering Iteration\n                        None\n            \n            \n                        46\n                        Polynomial Features\n                        False\n            \n            \n                        47\n                        Polynomial Degree\n                        None\n            \n            \n                        48\n                        Trignometry Features\n                        False\n            \n            \n                        49\n                        Polynomial Threshold\n                        None\n            \n            \n                        50\n                        Group Features\n                        False\n            \n            \n                        51\n                        Feature Selection\n                        False\n            \n            \n                        52\n                        Feature Selection Method\n                        classic\n            \n            \n                        53\n                        Features Selection Threshold\n                        None\n            \n            \n                        54\n                        Feature Interaction\n                        False\n            \n            \n                        55\n                        Feature Ratio\n                        False\n            \n            \n                        56\n                        Interaction Threshold\n                        None\n            \n            \n                        57\n                        Fix Imbalance\n                        False\n            \n            \n                        58\n                        Fix Imbalance Method\n                        SMOTE\n            \n    \n\n\n\n\n3. Compare Baseline\n\nbest_model = compare_models()\n\n\n                    Model        Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC        TT (Sec)    \n                \n                        ridge\n                        Ridge Classifier\n                        0.9822\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0210\n            \n            \n                        rf\n                        Random Forest Classifier\n                        0.9822\n                        0.7962\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.7070\n            \n            \n                        lr\n                        Logistic Regression\n                        0.9821\n                        0.6887\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0002\n                        -0.0007\n                        0.4090\n            \n            \n                        ada\n                        Ada Boost Classifier\n                        0.9821\n                        0.8441\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0001\n                        -0.0003\n                        0.3870\n            \n            \n                        knn\n                        K Neighbors Classifier\n                        0.9819\n                        0.5417\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0005\n                        -0.0015\n                        0.0940\n            \n            \n                        lightgbm\n                        Light Gradient Boosting Machine\n                        0.9818\n                        0.8354\n                        0.0111\n                        0.2450\n                        0.0210\n                        0.0195\n                        0.0470\n                        0.1320\n            \n            \n                        et\n                        Extra Trees Classifier\n                        0.9817\n                        0.7437\n                        0.0037\n                        0.1500\n                        0.0072\n                        0.0060\n                        0.0202\n                        0.4670\n            \n            \n                        gbc\n                        Gradient Boosting Classifier\n                        0.9816\n                        0.8480\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0012\n                        -0.0027\n                        1.2880\n            \n            \n                        lda\n                        Linear Discriminant Analysis\n                        0.9795\n                        0.8395\n                        0.0259\n                        0.1347\n                        0.0433\n                        0.0376\n                        0.0515\n                        0.0560\n            \n            \n                        svm\n                        SVM - Linear Kernel\n                        0.9794\n                        0.0000\n                        0.0111\n                        0.0534\n                        0.0178\n                        0.0127\n                        0.0167\n                        0.2770\n            \n            \n                        nb\n                        Naive Bayes\n                        0.9657\n                        0.8367\n                        0.1181\n                        0.1022\n                        0.1092\n                        0.0919\n                        0.0923\n                        0.0230\n            \n            \n                        dt\n                        Decision Tree Classifier\n                        0.9621\n                        0.5334\n                        0.0889\n                        0.0675\n                        0.0766\n                        0.0576\n                        0.0583\n                        0.0520\n            \n            \n                        qda\n                        Quadratic Discriminant Analysis\n                        0.6119\n                        0.5457\n                        0.4771\n                        0.0222\n                        0.0414\n                        0.0085\n                        0.0266\n                        0.0570\n            \n    \n\n\n\n\n4. Create Model\n\nlr = create_model('lr')\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9819\n                        0.6751\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0006\n                        -0.0024\n            \n            \n                        1\n                        0.9822\n                        0.6980\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        2\n                        0.9822\n                        0.6669\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        3\n                        0.9822\n                        0.6529\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        4\n                        0.9822\n                        0.7013\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        5\n                        0.9819\n                        0.6957\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0006\n                        -0.0024\n            \n            \n                        6\n                        0.9822\n                        0.7857\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        7\n                        0.9819\n                        0.6158\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0006\n                        -0.0024\n            \n            \n                        8\n                        0.9819\n                        0.6596\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        9\n                        0.9822\n                        0.7355\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        Mean\n                        0.9821\n                        0.6887\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0002\n                        -0.0007\n            \n            \n                        SD\n                        0.0002\n                        0.0447\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0003\n                        0.0011\n            \n    \n\n\n\ndt = create_model('dt')\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9618\n                        0.5351\n                        0.0926\n                        0.0694\n                        0.0794\n                        0.0603\n                        0.0609\n            \n            \n                        1\n                        0.9608\n                        0.5073\n                        0.0370\n                        0.0290\n                        0.0325\n                        0.0128\n                        0.0129\n            \n            \n                        2\n                        0.9631\n                        0.5539\n                        0.1296\n                        0.0972\n                        0.1111\n                        0.0927\n                        0.0937\n            \n            \n                        3\n                        0.9625\n                        0.5172\n                        0.0556\n                        0.0455\n                        0.0500\n                        0.0311\n                        0.0312\n            \n            \n                        4\n                        0.9638\n                        0.5997\n                        0.2222\n                        0.1500\n                        0.1791\n                        0.1613\n                        0.1646\n            \n            \n                        5\n                        0.9628\n                        0.5265\n                        0.0741\n                        0.0597\n                        0.0661\n                        0.0474\n                        0.0477\n            \n            \n                        6\n                        0.9645\n                        0.5455\n                        0.1111\n                        0.0909\n                        0.1000\n                        0.0821\n                        0.0825\n            \n            \n                        7\n                        0.9628\n                        0.5265\n                        0.0741\n                        0.0597\n                        0.0661\n                        0.0474\n                        0.0477\n            \n            \n                        8\n                        0.9562\n                        0.4958\n                        0.0182\n                        0.0125\n                        0.0148\n                        -0.0068\n                        -0.0069\n            \n            \n                        9\n                        0.9631\n                        0.5266\n                        0.0741\n                        0.0606\n                        0.0667\n                        0.0480\n                        0.0483\n            \n            \n                        Mean\n                        0.9621\n                        0.5334\n                        0.0889\n                        0.0675\n                        0.0766\n                        0.0576\n                        0.0583\n            \n            \n                        SD\n                        0.0022\n                        0.0273\n                        0.0542\n                        0.0366\n                        0.0436\n                        0.0444\n                        0.0453\n            \n    \n\n\n\nrf = create_model('rf', fold = 5)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9822\n                        0.8053\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        1\n                        0.9822\n                        0.7562\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        2\n                        0.9822\n                        0.8108\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        3\n                        0.9821\n                        0.7705\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        4\n                        0.9822\n                        0.8142\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        Mean\n                        0.9822\n                        0.7914\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        SD\n                        0.0001\n                        0.0235\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n    \n\n\n\nmodels()\n\n\n\n\n\n  \n    \n      \n      Name\n      Reference\n      Turbo\n    \n    \n      ID\n      \n      \n      \n    \n  \n  \n    \n      lr\n      Logistic Regression\n      sklearn.linear_model._logistic.LogisticRegression\n      True\n    \n    \n      knn\n      K Neighbors Classifier\n      sklearn.neighbors._classification.KNeighborsCl...\n      True\n    \n    \n      nb\n      Naive Bayes\n      sklearn.naive_bayes.GaussianNB\n      True\n    \n    \n      dt\n      Decision Tree Classifier\n      sklearn.tree._classes.DecisionTreeClassifier\n      True\n    \n    \n      svm\n      SVM - Linear Kernel\n      sklearn.linear_model._stochastic_gradient.SGDC...\n      True\n    \n    \n      rbfsvm\n      SVM - Radial Kernel\n      sklearn.svm._classes.SVC\n      False\n    \n    \n      gpc\n      Gaussian Process Classifier\n      sklearn.gaussian_process._gpc.GaussianProcessC...\n      False\n    \n    \n      mlp\n      MLP Classifier\n      sklearn.neural_network._multilayer_perceptron....\n      False\n    \n    \n      ridge\n      Ridge Classifier\n      sklearn.linear_model._ridge.RidgeClassifier\n      True\n    \n    \n      rf\n      Random Forest Classifier\n      sklearn.ensemble._forest.RandomForestClassifier\n      True\n    \n    \n      qda\n      Quadratic Discriminant Analysis\n      sklearn.discriminant_analysis.QuadraticDiscrim...\n      True\n    \n    \n      ada\n      Ada Boost Classifier\n      sklearn.ensemble._weight_boosting.AdaBoostClas...\n      True\n    \n    \n      gbc\n      Gradient Boosting Classifier\n      sklearn.ensemble._gb.GradientBoostingClassifier\n      True\n    \n    \n      lda\n      Linear Discriminant Analysis\n      sklearn.discriminant_analysis.LinearDiscrimina...\n      True\n    \n    \n      et\n      Extra Trees Classifier\n      sklearn.ensemble._forest.ExtraTreesClassifier\n      True\n    \n    \n      lightgbm\n      Light Gradient Boosting Machine\n      lightgbm.sklearn.LGBMClassifier\n      True\n    \n  \n\n\n\n\n\nmodels(type='ensemble').index.tolist()\n\n['rf', 'ada', 'gbc', 'et', 'lightgbm']\n\n\n\n#ensembled_models = compare_models(whitelist = models(type='ensemble').index.tolist(), fold = 3)\n\n\n\n5. Tune Hyperparameters\n\ntuned_lr = tune_model(lr)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9819\n                        0.6751\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0006\n                        -0.0024\n            \n            \n                        1\n                        0.9822\n                        0.8103\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        2\n                        0.9822\n                        0.6669\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        3\n                        0.9822\n                        0.6529\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        4\n                        0.9822\n                        0.7013\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        5\n                        0.9819\n                        0.6957\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0006\n                        -0.0024\n            \n            \n                        6\n                        0.9822\n                        0.6268\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        7\n                        0.9819\n                        0.6158\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0006\n                        -0.0024\n            \n            \n                        8\n                        0.9819\n                        0.6596\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        9\n                        0.9822\n                        0.7355\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        Mean\n                        0.9821\n                        0.6840\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0002\n                        -0.0007\n            \n            \n                        SD\n                        0.0002\n                        0.0537\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0003\n                        0.0011\n            \n    \n\n\n\ntuned_rf = tune_model(rf)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9822\n                        0.5000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        1\n                        0.9822\n                        0.5000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        2\n                        0.9822\n                        0.5000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        3\n                        0.9822\n                        0.5000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        4\n                        0.9822\n                        0.5000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        5\n                        0.9822\n                        0.5000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        6\n                        0.9822\n                        0.5000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        7\n                        0.9822\n                        0.5000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        8\n                        0.9819\n                        0.5000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        9\n                        0.9822\n                        0.5000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        Mean\n                        0.9822\n                        0.5000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        SD\n                        0.0001\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n    \n\n\n\n\n6. Ensemble Model\n\nbagged_dt = ensemble_model(dt)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9819\n                        0.6107\n                        0.0185\n                        0.3333\n                        0.0351\n                        0.0333\n                        0.0751\n            \n            \n                        1\n                        0.9819\n                        0.6763\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0006\n                        -0.0024\n            \n            \n                        2\n                        0.9812\n                        0.6402\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0019\n                        -0.0042\n            \n            \n                        3\n                        0.9816\n                        0.5961\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0013\n                        -0.0035\n            \n            \n                        4\n                        0.9819\n                        0.6676\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0006\n                        -0.0024\n            \n            \n                        5\n                        0.9822\n                        0.6122\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        6\n                        0.9822\n                        0.6289\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        7\n                        0.9809\n                        0.6649\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0025\n                        -0.0049\n            \n            \n                        8\n                        0.9819\n                        0.6254\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        9\n                        0.9816\n                        0.6504\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0013\n                        -0.0035\n            \n            \n                        Mean\n                        0.9817\n                        0.6373\n                        0.0019\n                        0.0333\n                        0.0035\n                        0.0025\n                        0.0054\n            \n            \n                        SD\n                        0.0004\n                        0.0258\n                        0.0056\n                        0.1000\n                        0.0105\n                        0.0103\n                        0.0233\n            \n    \n\n\n\nboosted_dt = ensemble_model(dt, method = 'Boosting')\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9615\n                        0.5349\n                        0.0926\n                        0.0685\n                        0.0787\n                        0.0595\n                        0.0602\n            \n            \n                        1\n                        0.9608\n                        0.5073\n                        0.0370\n                        0.0290\n                        0.0325\n                        0.0128\n                        0.0129\n            \n            \n                        2\n                        0.9598\n                        0.5341\n                        0.0926\n                        0.0641\n                        0.0758\n                        0.0559\n                        0.0569\n            \n            \n                        3\n                        0.9602\n                        0.5160\n                        0.0556\n                        0.0411\n                        0.0472\n                        0.0274\n                        0.0277\n            \n            \n                        4\n                        0.9641\n                        0.5908\n                        0.2037\n                        0.1429\n                        0.1679\n                        0.1502\n                        0.1527\n            \n            \n                        5\n                        0.9658\n                        0.5462\n                        0.1111\n                        0.0968\n                        0.1034\n                        0.0861\n                        0.0863\n            \n            \n                        6\n                        0.9638\n                        0.5452\n                        0.1111\n                        0.0882\n                        0.0984\n                        0.0801\n                        0.0807\n            \n            \n                        7\n                        0.9598\n                        0.5250\n                        0.0741\n                        0.0526\n                        0.0615\n                        0.0416\n                        0.0423\n            \n            \n                        8\n                        0.9575\n                        0.5054\n                        0.0364\n                        0.0256\n                        0.0301\n                        0.0090\n                        0.0092\n            \n            \n                        9\n                        0.9648\n                        0.5275\n                        0.0741\n                        0.0656\n                        0.0696\n                        0.0517\n                        0.0518\n            \n            \n                        Mean\n                        0.9618\n                        0.5332\n                        0.0888\n                        0.0674\n                        0.0765\n                        0.0574\n                        0.0581\n            \n            \n                        SD\n                        0.0025\n                        0.0234\n                        0.0460\n                        0.0334\n                        0.0385\n                        0.0393\n                        0.0399\n            \n    \n\n\n\n\n7. Blend Models\n\nblender = blend_models(estimator_list = [boosted_dt, bagged_dt, tuned_rf], method = 'soft')\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9819\n                        0.6065\n                        0.0185\n                        0.3333\n                        0.0351\n                        0.0333\n                        0.0751\n            \n            \n                        1\n                        0.9809\n                        0.6705\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0025\n                        -0.0049\n            \n            \n                        2\n                        0.9816\n                        0.6368\n                        0.0185\n                        0.2500\n                        0.0345\n                        0.0321\n                        0.0638\n            \n            \n                        3\n                        0.9806\n                        0.5950\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0030\n                        -0.0055\n            \n            \n                        4\n                        0.9826\n                        0.6973\n                        0.0185\n                        1.0000\n                        0.0364\n                        0.0357\n                        0.1349\n            \n            \n                        5\n                        0.9819\n                        0.6485\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0006\n                        -0.0024\n            \n            \n                        6\n                        0.9816\n                        0.6384\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0013\n                        -0.0035\n            \n            \n                        7\n                        0.9816\n                        0.6687\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0013\n                        -0.0035\n            \n            \n                        8\n                        0.9809\n                        0.6266\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0019\n                        -0.0043\n            \n            \n                        9\n                        0.9809\n                        0.6580\n                        0.0000\n                        0.0000\n                        0.0000\n                        -0.0025\n                        -0.0049\n            \n            \n                        Mean\n                        0.9814\n                        0.6446\n                        0.0056\n                        0.1583\n                        0.0106\n                        0.0088\n                        0.0245\n            \n            \n                        SD\n                        0.0006\n                        0.0293\n                        0.0085\n                        0.3038\n                        0.0162\n                        0.0163\n                        0.0469\n            \n    \n\n\n\n\n8. Stack Models\n\nstacker = stack_models(estimator_list = [boosted_dt,bagged_dt,tuned_rf], meta_model=rf)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9822\n                        0.7220\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        1\n                        0.9822\n                        0.8276\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        2\n                        0.9822\n                        0.7600\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        3\n                        0.9822\n                        0.8048\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        4\n                        0.9822\n                        0.8862\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        5\n                        0.9822\n                        0.8140\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        6\n                        0.9822\n                        0.7350\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        7\n                        0.9822\n                        0.8074\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        8\n                        0.9819\n                        0.7859\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        9\n                        0.9822\n                        0.7983\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        Mean\n                        0.9822\n                        0.7941\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n            \n                        SD\n                        0.0001\n                        0.0450\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n                        0.0000\n            \n    \n\n\n\n\n9. Analyze Model\n\nplot_model(rf)\n\n\n\n\n\nplot_model(rf, plot = 'confusion_matrix')\n\n\n\n\n\nplot_model(rf, plot = 'boundary')\n\n\n\n\n\nplot_model(rf, plot = 'feature')\n\n\n\n\n\nplot_model(rf, plot = 'pr')\n\n\n\n\n\nplot_model(rf, plot = 'class_report')\n\n\n\n\n\nevaluate_model(rf)\n\n\n\n\n\n  \n    \n      \n      Parameters\n    \n  \n  \n    \n      bootstrap\n      True\n    \n    \n      ccp_alpha\n      0.0\n    \n    \n      class_weight\n      None\n    \n    \n      criterion\n      gini\n    \n    \n      max_depth\n      None\n    \n    \n      max_features\n      auto\n    \n    \n      max_leaf_nodes\n      None\n    \n    \n      max_samples\n      None\n    \n    \n      min_impurity_decrease\n      0.0\n    \n    \n      min_impurity_split\n      None\n    \n    \n      min_samples_leaf\n      1\n    \n    \n      min_samples_split\n      2\n    \n    \n      min_weight_fraction_leaf\n      0.0\n    \n    \n      n_estimators\n      100\n    \n    \n      n_jobs\n      -1\n    \n    \n      oob_score\n      False\n    \n    \n      random_state\n      123\n    \n    \n      verbose\n      0\n    \n    \n      warm_start\n      False\n    \n  \n\n\n\n\n\n\n10. Interpret Model\n\ncatboost = create_model('rf', cross_validation=False)\n\n\ninterpret_model(catboost)\n\n\ninterpret_model(catboost, plot = 'correlation')\n\n\ninterpret_model(catboost, plot = 'reason', observation = 12)\n\n\n\n11. AutoML()\n\nbest = automl(optimize = 'Recall')\nbest\n\n\n\n12. Predict Model\n\npred_holdouts = predict_model(lr)\npred_holdouts.head()\n\n\nnew_data = data.copy()\nnew_data.drop(['Purchase'], axis=1, inplace=True)\npredict_new = predict_model(best, data=new_data)\npredict_new.head()\n\n\n\n13. Save / Load Model\n\nsave_model(best, model_name='best-model')\n\n\nloaded_bestmodel = load_model('best-model')\nprint(loaded_bestmodel)\n\n\nfrom sklearn import set_config\nset_config(display='diagram')\nloaded_bestmodel[0]\n\n\nfrom sklearn import set_config\nset_config(display='text')\n\n\n\n14. Deploy Model\n\ndeploy_model(best, model_name = 'best-aws', authentication = {'bucket' : 'pycaret-test'})\n\n\n\n15. Get Config / Set Config\n\nX_train = get_config('X_train')\nX_train.head()\n\n\nget_config('seed')\n\n\nfrom pycaret.classification import set_config\nset_config('seed', 999)\n\n\nget_config('seed')\n\n\n\n16. MLFlow UI\n\n# !mlflow ui"
  },
  {
    "objectID": "posts/2021-02-05-emails-python.html",
    "href": "posts/2021-02-05-emails-python.html",
    "title": "Python Email Example",
    "section": "",
    "text": "import csv, smtplib, ssl\nfrom datetime import date\nimport pandas as pd\nimport email, smtplib, ssl\nfrom email import encoders\nfrom email.mime.base import MIMEBase\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\n\n\ntoday = date.today()\nprint(\"Today's date is\", today)\nprint(\"Date components\", today.year, today.month, today.day)\n\n\ndf = pd.read_csv(\"~/csvs/example.csv\")\n\n\nexample_filter_cols = df[\n    [\n        \"example\",\n        \"example\"\n    ]\n]\n\nexample_filter_cols\n\nfilter1 = df[\"month\"] == today.month\nfilter2 = df[\"day\"] == today.day\n\nexample_filter = filter1 & filter2\n\nexample_final_df = example_filter_cols[example_filter]\nexample_final_df\n\n\nexample_final_df_cols = example_final_df[\n    [\"full_name\", \"name\", \"birthday\", \"email\", \"phone\", \"gift_ideas\", \"address\"]\n]\n\n\nexample_final_df_cols.to_csv(\"example_final_df.csv\", index=False)\n\n\n\n# Email:\n\nsubject = \"Example\"\nbody = \"Example\"\n\n\n\nsender_email = \"email@gmail.com\"\nreceiver_email = \"email@gmail.com\"\n\n\n# Create a multipart message and set headers\nmessage = MIMEMultipart()\nmessage[\"From\"] = sender_email\nmessage[\"To\"] = receiver_email\nmessage[\"Subject\"] = subject\nmessage[\"Bcc\"] = receiver_email  # Recommended for mass emails\n\n# Add body to email\nmessage.attach(MIMEText(body, \"plain\"))\n\nfilename = \"example_final_df.csv\"  # In same directory as script\n\n# Open PDF file in binary mode\nwith open(filename, \"rb\") as attachment:\n    # Add file as application/octet-stream\n    # Email client can usually download this automatically as attachment\n    part = MIMEBase(\"application\", \"octet-stream\")\n    part.set_payload(attachment.read())\n\n# Encode file in ASCII characters to send by email\nencoders.encode_base64(part)\n\n# Add header as key/value pair to attachment part\npart.add_header(\n    \"Content-Disposition\",\n    f\"attachment; filename= {filename}\",\n)\n\n# Add attachment to message and convert message to string\nmessage.attach(part)\ntext = message.as_string()\n\n# Log in to server using secure context and send email\ncontext = ssl.create_default_context()\nwith smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context=context) as server:\n    server.login(sender_email, password)\n    server.sendmail(sender_email, receiver_email, text)"
  },
  {
    "objectID": "posts/2021-06-05-causalml-tree-based-models.html",
    "href": "posts/2021-06-05-causalml-tree-based-models.html",
    "title": "CausalML uplift with tree-based algorithms",
    "section": "",
    "text": "# Code from https://github.com/uber/causalml/tree/master/examples"
  },
  {
    "objectID": "posts/2021-06-05-causalml-tree-based-models.html#create-a-synthetic-population",
    "href": "posts/2021-06-05-causalml-tree-based-models.html#create-a-synthetic-population",
    "title": "CausalML uplift with tree-based algorithms",
    "section": "Create a synthetic population",
    "text": "Create a synthetic population\nThe uplift curve is calculated on a synthetic population that consists of those that were in the control group and those who happened to be in the treatment group recommended by the model. We use the synthetic population to calculate the actual treatment effect within predicted treatment effect quantiles. Because the data is randomized, we have a roughly equal number of treatment and control observations in the predicted quantiles and there is no self selection to treatment groups.\n\n# If all deltas are negative, assing to control; otherwise assign to the treatment\n# with the highest delta\nbest_treatment = np.where((result < 0).all(axis=1),\n                           'control',\n                           result.idxmax(axis=1))\n\n# Create indicator variables for whether a unit happened to have the\n# recommended treatment or was in the control group\nactual_is_best = np.where(df_test['treatment_group_key'] == best_treatment, 1, 0)\nactual_is_control = np.where(df_test['treatment_group_key'] == 'control', 1, 0)\n\n\nsynthetic = (actual_is_best == 1) | (actual_is_control == 1)\nsynth = result[synthetic]"
  },
  {
    "objectID": "posts/2021-06-05-causalml-tree-based-models.html#calculate-the-observed-treatment-effect-per-predicted-treatment-effect-quantile",
    "href": "posts/2021-06-05-causalml-tree-based-models.html#calculate-the-observed-treatment-effect-per-predicted-treatment-effect-quantile",
    "title": "CausalML uplift with tree-based algorithms",
    "section": "Calculate the observed treatment effect per predicted treatment effect quantile",
    "text": "Calculate the observed treatment effect per predicted treatment effect quantile\nWe use the observed treatment effect to calculate the uplift curve, which answers the question: how much of the total cumulative uplift could we have captured by targeting a subset of the population sorted according to the predicted uplift, from highest to lowest?\nCausalML has the plot_gain() function which calculates the uplift curve given a DataFrame containing the treatment assignment, observed outcome and the predicted treatment effect.\n\nauuc_metrics = (synth.assign(is_treated = 1 - actual_is_control[synthetic],\n                             conversion = df_test.loc[synthetic, 'conversion'].values,\n                             uplift_tree = synth.max(axis=1))\n                     .drop(columns=list(uplift_model.classes_)))\n\n\nplot_gain(auuc_metrics, outcome_col='conversion', treatment_col='is_treated')"
  },
  {
    "objectID": "posts/2020-09-24-analyzingusrealinterestrate.html",
    "href": "posts/2020-09-24-analyzingusrealinterestrate.html",
    "title": "Analyzing US Real Interest Rate From 1959 - 2009 with statsmodels",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n%matplotlib inline\n\ndf = sm.datasets.macrodata.load_pandas().data\n\nprint(sm.datasets.macrodata.NOTE)\n\n/home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n  import pandas.util.testing as tm\n\n\n::\n    Number of Observations - 203\n\n    Number of Variables - 14\n\n    Variable name definitions::\n\n        year      - 1959q1 - 2009q3\n        quarter   - 1-4\n        realgdp   - Real gross domestic product (Bil. of chained 2005 US$,\n                    seasonally adjusted annual rate)\n        realcons  - Real personal consumption expenditures (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realinv   - Real gross private domestic investment (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realgovt  - Real federal consumption expenditures & gross investment\n                    (Bil. of chained 2005 US$, seasonally adjusted annual rate)\n        realdpi   - Real private disposable income (Bil. of chained 2005\n                    US$, seasonally adjusted annual rate)\n        cpi       - End of the quarter consumer price index for all urban\n                    consumers: all items (1982-84 = 100, seasonally adjusted).\n        m1        - End of the quarter M1 nominal money stock (Seasonally\n                    adjusted)\n        tbilrate  - Quarterly monthly average of the monthly 3-month\n                    treasury bill: secondary market rate\n        unemp     - Seasonally adjusted unemployment rate (%)\n        pop       - End of the quarter total population: all ages incl. armed\n                    forces over seas\n        infl      - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400)\n        realint   - Real interest rate (tbilrate - infl)\n\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      quarter\n      realgdp\n      realcons\n      realinv\n      realgovt\n      realdpi\n      cpi\n      m1\n      tbilrate\n      unemp\n      pop\n      infl\n      realint\n    \n  \n  \n    \n      0\n      1959.0\n      1.0\n      2710.349\n      1707.4\n      286.898\n      470.045\n      1886.9\n      28.98\n      139.7\n      2.82\n      5.8\n      177.146\n      0.00\n      0.00\n    \n    \n      1\n      1959.0\n      2.0\n      2778.801\n      1733.7\n      310.859\n      481.301\n      1919.7\n      29.15\n      141.7\n      3.08\n      5.1\n      177.830\n      2.34\n      0.74\n    \n    \n      2\n      1959.0\n      3.0\n      2775.488\n      1751.8\n      289.226\n      491.260\n      1916.4\n      29.35\n      140.5\n      3.82\n      5.3\n      178.657\n      2.74\n      1.09\n    \n    \n      3\n      1959.0\n      4.0\n      2785.204\n      1753.7\n      299.356\n      484.052\n      1931.3\n      29.37\n      140.0\n      4.33\n      5.6\n      179.386\n      0.27\n      4.06\n    \n    \n      4\n      1960.0\n      1.0\n      2847.699\n      1770.5\n      331.722\n      462.199\n      1955.5\n      29.54\n      139.6\n      3.50\n      5.2\n      180.007\n      2.31\n      1.19\n    \n  \n\n\n\n\n\nindex = pd.Index(sm.tsa.datetools.dates_from_range('1959Q1', '2009Q3'))\ndf.index = index\ndf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      quarter\n      realgdp\n      realcons\n      realinv\n      realgovt\n      realdpi\n      cpi\n      m1\n      tbilrate\n      unemp\n      pop\n      infl\n      realint\n    \n  \n  \n    \n      1959-03-31\n      1959.0\n      1.0\n      2710.349\n      1707.4\n      286.898\n      470.045\n      1886.9\n      28.98\n      139.7\n      2.82\n      5.8\n      177.146\n      0.00\n      0.00\n    \n    \n      1959-06-30\n      1959.0\n      2.0\n      2778.801\n      1733.7\n      310.859\n      481.301\n      1919.7\n      29.15\n      141.7\n      3.08\n      5.1\n      177.830\n      2.34\n      0.74\n    \n    \n      1959-09-30\n      1959.0\n      3.0\n      2775.488\n      1751.8\n      289.226\n      491.260\n      1916.4\n      29.35\n      140.5\n      3.82\n      5.3\n      178.657\n      2.74\n      1.09\n    \n    \n      1959-12-31\n      1959.0\n      4.0\n      2785.204\n      1753.7\n      299.356\n      484.052\n      1931.3\n      29.37\n      140.0\n      4.33\n      5.6\n      179.386\n      0.27\n      4.06\n    \n    \n      1960-03-31\n      1960.0\n      1.0\n      2847.699\n      1770.5\n      331.722\n      462.199\n      1955.5\n      29.54\n      139.6\n      3.50\n      5.2\n      180.007\n      2.31\n      1.19\n    \n  \n\n\n\n\n\ndf['realint'].plot()\nplt.ylabel(\"realint\")\n\nText(0, 0.5, 'realint')\n\n\n\n\n\n\\(\\min_{\\\\{ \\tau_{t}\\\\} }\\sum_{t}^{T}\\zeta_{t}^{2}+\\lambda\\sum_{t=1}^{T}\\left[\\left(\\tau_{t}-\\tau_{t-1}\\right)-\\left(\\tau_{t-1}-\\tau_{t-2}\\right)\\right]^{2}\\)\n\n# unpacking\ncycle, trend = sm.tsa.filters.hpfilter(df.realint)\n\ncycle\n\n1959-03-31   -1.195751\n1959-06-30   -0.505792\n1959-09-30   -0.205086\n1959-12-31    2.717430\n1960-03-31   -0.197051\n                ...   \n2008-09-30    4.330269\n2008-12-31    8.961987\n2009-03-31   -0.596183\n2009-06-30   -3.008487\n2009-09-30   -3.188797\nName: realint, Length: 203, dtype: float64\n\n\n\ntype(cycle)\n\npandas.core.series.Series\n\n\n\ndf[\"trend\"] = trend\n\ndf[['trend','realint']].plot(figsize = (12, 8))\n\ndf[['trend','realint']][\"2000-03-31\":].plot(figsize = (12, 8))\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd2a8100438>"
  },
  {
    "objectID": "posts/2020-09-21-stockmarketportfolioanaylsis2.html",
    "href": "posts/2020-09-21-stockmarketportfolioanaylsis2.html",
    "title": "Stock Market and Optimal Portfolio Anaylsis scipy and quandl",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport quandl\n%matplotlib inline\n\n\nstart = pd.to_datetime('2010-01-01')\nend = pd.to_datetime('today')\n\n\n# Grabbing a bunch of tech stocks for our portfolio\nCOST = quandl.get('WIKI/COST.11',\n                  start_date = start,\n                  end_date = end)\nNLSN = quandl.get('WIKI/NLSN.11',\n                   start_date = start,\n                   end_date = end)\nNKE = quandl.get('WIKI/NKE.11',\n                 start_date = start,\n                 end_date = end)\nDIS = quandl.get('WIKI/DIS.11',\n                  start_date = start,\n                  end_date = end)\n\n\nstocks = pd.concat([COST, NLSN, NKE, DIS],\n                   axis = 1)\nstocks.columns = ['COST','NLSN','NKE','DIS']\n\n\nstocks\n\n\n\n\n\n  \n    \n      \n      COST\n      NLSN\n      NKE\n      DIS\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2010-01-04\n      49.085078\n      NaN\n      14.751122\n      28.960651\n    \n    \n      2010-01-05\n      48.936361\n      NaN\n      14.809811\n      28.888407\n    \n    \n      2010-01-06\n      49.572542\n      NaN\n      14.719521\n      28.734890\n    \n    \n      2010-01-07\n      49.332941\n      NaN\n      14.863985\n      28.743920\n    \n    \n      2010-01-08\n      48.977671\n      NaN\n      14.834641\n      28.789072\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2018-03-21\n      186.070000\n      32.44\n      66.350000\n      101.820000\n    \n    \n      2018-03-22\n      182.640000\n      31.82\n      64.420000\n      100.600000\n    \n    \n      2018-03-23\n      180.840000\n      31.51\n      64.630000\n      98.540000\n    \n    \n      2018-03-26\n      187.220000\n      32.03\n      65.900000\n      100.650000\n    \n    \n      2018-03-27\n      183.150000\n      32.09\n      66.170000\n      99.360000\n    \n  \n\n2071 rows  4 columns\n\n\n\n\nmean_daily_ret = stocks.pct_change(1).mean()\nmean_daily_ret\n\nCOST    0.000699\nNLSN    0.000312\nNKE     0.000833\nDIS     0.000683\ndtype: float64\n\n\n\nstocks.pct_change(1).corr()\n\n\n\n\n\n  \n    \n      \n      COST\n      NLSN\n      NKE\n      DIS\n    \n  \n  \n    \n      COST\n      1.000000\n      0.265003\n      0.370978\n      0.415377\n    \n    \n      NLSN\n      0.265003\n      1.000000\n      0.312192\n      0.392808\n    \n    \n      NKE\n      0.370978\n      0.312192\n      1.000000\n      0.446150\n    \n    \n      DIS\n      0.415377\n      0.392808\n      0.446150\n      1.000000\n    \n  \n\n\n\n\n\nstocks.head()\n\n\n\n\n\n  \n    \n      \n      COST\n      NLSN\n      NKE\n      DIS\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2010-01-04\n      49.085078\n      NaN\n      14.751122\n      28.960651\n    \n    \n      2010-01-05\n      48.936361\n      NaN\n      14.809811\n      28.888407\n    \n    \n      2010-01-06\n      49.572542\n      NaN\n      14.719521\n      28.734890\n    \n    \n      2010-01-07\n      49.332941\n      NaN\n      14.863985\n      28.743920\n    \n    \n      2010-01-08\n      48.977671\n      NaN\n      14.834641\n      28.789072\n    \n  \n\n\n\n\n\nstock_normed = stocks/stocks.iloc[0]\nstock_normed.plot()\n\n<AxesSubplot:xlabel='Date'>\n\n\n\n\n\n\nstock_daily_ret = stocks.pct_change(1)\nstock_daily_ret.head()\n\n\n\n\n\n  \n    \n      \n      COST\n      NLSN\n      NKE\n      DIS\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2010-01-04\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2010-01-05\n      -0.003030\n      NaN\n      0.003979\n      -0.002495\n    \n    \n      2010-01-06\n      0.013000\n      NaN\n      -0.006097\n      -0.005314\n    \n    \n      2010-01-07\n      -0.004833\n      NaN\n      0.009814\n      0.000314\n    \n    \n      2010-01-08\n      -0.007201\n      NaN\n      -0.001974\n      0.001571\n    \n  \n\n\n\n\n\nlog_ret = np.log(stocks / stocks.shift(1))\nlog_ret.head()\n\n\n\n\n\n  \n    \n      \n      COST\n      NLSN\n      NKE\n      DIS\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2010-01-04\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2010-01-05\n      -0.003034\n      NaN\n      0.003971\n      -0.002498\n    \n    \n      2010-01-06\n      0.012916\n      NaN\n      -0.006115\n      -0.005328\n    \n    \n      2010-01-07\n      -0.004845\n      NaN\n      0.009767\n      0.000314\n    \n    \n      2010-01-08\n      -0.007228\n      NaN\n      -0.001976\n      0.001570\n    \n  \n\n\n\n\n\nlog_ret.hist(bins = 100,\n             figsize = (12, 6));\nplt.tight_layout()\n\n\n\n\n\nlog_ret.describe().transpose()\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n  \n  \n    \n      COST\n      2068.0\n      0.000633\n      0.011172\n      -0.083110\n      -0.005293\n      0.000413\n      0.006618\n      0.060996\n    \n    \n      NLSN\n      1801.0\n      0.000198\n      0.015121\n      -0.185056\n      -0.007131\n      0.000000\n      0.008051\n      0.095201\n    \n    \n      NKE\n      2070.0\n      0.000725\n      0.014682\n      -0.098743\n      -0.006602\n      0.000656\n      0.008155\n      0.115342\n    \n    \n      DIS\n      2070.0\n      0.000596\n      0.013220\n      -0.096190\n      -0.005710\n      0.000776\n      0.007453\n      0.073531\n    \n  \n\n\n\n\n\nlog_ret.mean() * 252\n\nCOST    0.159439\nNLSN    0.049979\nNKE     0.182719\nDIS     0.150081\ndtype: float64\n\n\n\n# Compute pairwise covariance of columns\nlog_ret.cov()\n\n\n\n\n\n  \n    \n      \n      COST\n      NLSN\n      NKE\n      DIS\n    \n  \n  \n    \n      COST\n      0.000125\n      0.000045\n      0.000061\n      0.000061\n    \n    \n      NLSN\n      0.000045\n      0.000229\n      0.000070\n      0.000077\n    \n    \n      NKE\n      0.000061\n      0.000070\n      0.000216\n      0.000087\n    \n    \n      DIS\n      0.000061\n      0.000077\n      0.000087\n      0.000175\n    \n  \n\n\n\n\n\n# Set seed (optional)\nnp.random.seed(101)\n\n# Stock Columns\nprint('Stocks')\nprint(stocks.columns)\nprint('\\n')\n\n# Create Random Weights\nprint('Creating Random Weights')\nweights = np.array(np.random.random(4))\nprint(weights)\nprint('\\n')\n\n# Rebalance Weights\nprint('Rebalance to sum to 1.0')\nweights = weights / np.sum(weights)\nprint(weights)\nprint('\\n')\n\n# Expected Return\nprint('Expected Portfolio Return')\nexp_ret = np.sum(log_ret.mean() * weights) *252\nprint(exp_ret)\nprint('\\n')\n\n# Expected Variance\nprint('Expected Volatility')\nexp_vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\nprint(exp_vol)\nprint('\\n')\n\n# Sharpe Ratio\nSR = exp_ret/exp_vol\nprint('Sharpe Ratio')\nprint(SR)\n\nStocks\nIndex(['COST', 'NLSN', 'NKE', 'DIS'], dtype='object')\n\n\nCreating Random Weights\n[0.51639863 0.57066759 0.02847423 0.17152166]\n\n\nRebalance to sum to 1.0\n[0.40122278 0.44338777 0.02212343 0.13326603]\n\n\nExpected Portfolio Return\n0.11017373023155777\n\n\nExpected Volatility\n0.16110487214223854\n\n\nSharpe Ratio\n0.6838634286260817\n\n\n\nnum_ports = 15000\n\nall_weights = np.zeros((num_ports, len(stocks.columns)))\nret_arr = np.zeros(num_ports)\nvol_arr = np.zeros(num_ports)\nsharpe_arr = np.zeros(num_ports)\n\nfor ind in range(num_ports):\n\n    # Create Random Weights\n    weights = np.array(np.random.random(4))\n\n    # Rebalance Weights\n    weights = weights / np.sum(weights)\n    \n    # Save Weights\n    all_weights[ind,:] = weights\n\n    # Expected Return\n    ret_arr[ind] = np.sum((log_ret.mean() * weights) *252)\n\n    # Expected Variance\n    vol_arr[ind] = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n\n    # Sharpe Ratio\n    sharpe_arr[ind] = ret_arr[ind] / vol_arr[ind]\n\n\nsharpe_arr.max()\n\n1.042687299617254\n\n\n\nsharpe_arr.argmax()\n\n10619\n\n\n\nall_weights[10619,:]\n\narray([5.06395348e-01, 4.67772019e-04, 2.64242193e-01, 2.28894687e-01])\n\n\n\nmax_sr_ret = ret_arr[1419]\nmax_sr_vol = vol_arr[1419]\n\n\nplt.figure(figsize = (12, 8))\nplt.scatter(vol_arr,\n            ret_arr,\n            c = sharpe_arr,\n            cmap = 'plasma')\nplt.colorbar(label = 'Sharpe Ratio')\nplt.xlabel('Volatility')\nplt.ylabel('Return')\n\n# Add red dot for max SR\nplt.scatter(max_sr_vol,\n            max_sr_ret,\n            c = 'red',\n            s = 50,\n            edgecolors = 'black')\n\n<matplotlib.collections.PathCollection at 0x7f703b26fd00>\n\n\n\n\n\n\ndef get_ret_vol_sr(weights):\n    \"\"\"\n    Takes in weights, returns array or return,volatility, sharpe ratio\n    \"\"\"\n    weights = np.array(weights)\n    ret = np.sum(log_ret.mean() * weights) * 252\n    vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n    sr = ret/vol\n    return np.array([ret, vol, sr])\n\nfrom scipy.optimize import minimize\nimport numpy as np\n\ndef neg_sharpe(weights):\n    return  get_ret_vol_sr(weights)[2] * -1\n\n# Contraints\ndef check_sum(weights):\n    '''\n    Returns 0 if sum of weights is 1.0\n    '''\n    return np.sum(weights) - 1\n\n# By convention of minimize function it should be a function that returns zero for conditions\ncons = ({'type' : 'eq', 'fun': check_sum})\n\n# 0-1 bounds for each weight\nbounds = ((0, 1), (0, 1), (0, 1), (0, 1))\n\n# Initial Guess (equal distribution)\ninit_guess = [0.25, 0.25, 0.25, 0.25]\n\n# Sequential Least Squares \nopt_results = minimize(neg_sharpe,\n                       init_guess,\n                       method = 'SLSQP',\n                       bounds = bounds,\n                       constraints = cons)\n\nopt_results\n\n     fun: -1.0442236428192482\n     jac: array([-1.85623765e-04,  3.00063133e-01,  3.43203545e-04,  1.72853470e-05])\n message: 'Optimization terminated successfully'\n    nfev: 20\n     nit: 4\n    njev: 4\n  status: 0\n success: True\n       x: array([0.53438392, 0.        , 0.27969302, 0.18592306])\n\n\n\nopt_results.x\n\nget_ret_vol_sr(opt_results.x)\n\narray([0.16421049, 0.15725605, 1.04422364])\n\n\n\nfrontier_y = np.linspace(0, 0.3, 100)\n\n\ndef minimize_volatility(weights):\n    return  get_ret_vol_sr(weights)[1] \n\nfrontier_volatility = []\n\nfor possible_return in frontier_y:\n    # function for return\n    cons = ({'type':'eq','fun': check_sum},\n            {'type':'eq','fun': lambda w: get_ret_vol_sr(w)[0] - possible_return})\n    \n    result = minimize(minimize_volatility,\n                      init_guess,\n                      method = 'SLSQP',\n                      bounds = bounds,\n                      constraints = cons)\n    \n    frontier_volatility.append(result['fun'])\n\n\nplt.figure(figsize = (12, 8))\nplt.scatter(vol_arr,\n            ret_arr,\n            c = sharpe_arr,\n            cmap = 'plasma')\nplt.colorbar(label = 'Sharpe Ratio')\nplt.xlabel('Volatility')\nplt.ylabel('Return')\n\n\n\n# Add frontier line\nplt.plot(frontier_volatility,\n         frontier_y,\n         'g--',\n         linewidth = 3)"
  },
  {
    "objectID": "posts/2022-10-01-quant_data_datareader_quandl.html",
    "href": "posts/2022-10-01-quant_data_datareader_quandl.html",
    "title": "Using the Quandl API and Pandas Datareader API to call stocks and other finance data",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport pandas_datareader.data as web\nimport datetime\n\n\nstart = datetime.datetime(2018, 1, 1)\nend = pd.to_datetime('today')\n\n\nAAPL_stock = web.DataReader('MTCH', 'yahoo', start, end)\nAAPL_stock.head()\n\nMSFT_stock = web.DataReader('META', 'yahoo', start, end)\nMSFT_stock.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2018-01-02\n      181.580002\n      177.550003\n      177.679993\n      181.419998\n      18151900\n      181.419998\n    \n    \n      2018-01-03\n      184.779999\n      181.330002\n      181.880005\n      184.669998\n      16886600\n      184.669998\n    \n    \n      2018-01-04\n      186.210007\n      184.100006\n      184.899994\n      184.330002\n      13880900\n      184.330002\n    \n    \n      2018-01-05\n      186.899994\n      184.929993\n      185.589996\n      186.850006\n      13574500\n      186.850006\n    \n    \n      2018-01-08\n      188.899994\n      186.330002\n      187.199997\n      188.279999\n      17994700\n      188.279999\n    \n  \n\n\n\n\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Open')\n\nMSFT_stock['Open'].plot(label='Meta')\nAAPL_stock['Open'].plot(label='Match')\nplt.legend()\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Volume')\n\nMSFT_stock['Volume'].plot(label='Meta')\nAAPL_stock['Volume'].plot(label='Match')\n\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7feb12375160>\n\n\n\n\n\n\n\n\n\nimport pandas_datareader.data as web\n\nimport datetime\n\n\n\ngdp = web.DataReader(\"GDP\", \"fred\", start, end)\n\n\ngdp.head()\n\n\n\n\n\n  \n    \n      \n      GDP\n    \n    \n      DATE\n      \n    \n  \n  \n    \n      2020-01-01\n      21538.032\n    \n    \n      2020-04-01\n      19636.731\n    \n    \n      2020-07-01\n      21362.428\n    \n    \n      2020-10-01\n      21704.706\n    \n    \n      2021-01-01\n      22313.850\n    \n  \n\n\n\n\n\nimport quandl\n\n\n#quandl.ApiConfig.api_key = ''\n\n\n#mydata = quandl.get(\"EIA/PET_RWTC_D\")\n\n#mydata.head()\n\n\nmydata.plot(figsize=(12,6))\n\n\n#mydata = quandl.get(\"EIA/PET_RWTC_D\", returns=\"numpy\",start_date=start,end_date=end)\n\n\nmydata = quandl.get(\"FRED/GDP\",start_date=start,end_date=end)\n\n\nmydata.head()\n\n\nmydata = quandl.get([\"NSE/OIL.1\", \"WIKI/AAPL.4\"],start_date=start,end_date=end)\n\n\nmydata.head()\n\n\nmydata = quandl.get(\"FRED/GDP\")\n\n\nmydata = quandl.get('WIKI/FB',start_date=start,end_date=end)\n\n\nmydata.head()\n\n\nmydata = quandl.get('WIKI/FB.1',start_date=start,end_date=end)\n\nmydata.head()\n\n\nmydata = quandl.get('WIKI/FB.7',start_date=start,end_date=end)\nmydata.head()\n\n\n# Homes\n\n\nhouses = quandl.get('ZILLOW/M11_ZRIAH',start_date=start,end_date=end)\n\n\nhouses.head()\n\n\nhouses.plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f900d58fef0>"
  },
  {
    "objectID": "posts/2020-10-05-stockmarketportfolioanaylsis_snp_pandas_datareader_sqlite-copy1.html",
    "href": "posts/2020-10-05-stockmarketportfolioanaylsis_snp_pandas_datareader_sqlite-copy1.html",
    "title": "Using sqlalchemy and pandas to read and write from and to a local sqlite database for Stock Market and Portfolio Anaylsis Tech Stocks and the S&P 500 in 2020",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport pandas_datareader.data as web\nimport datetime\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndf = pd.read_csv('https://stocks-snp-500.herokuapp.com/stocks/stocks_table.csv?_size=max')\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      rowid\n      Date\n      MSFT_stock\n      ZOOM_stock\n      SNOW_stock\n      FXAIX_stock\n    \n  \n  \n    \n      0\n      1\n      2020-01-02 00:00:00.000000\n      158.779999\n      68.800003\n      NaN\n      112.980003\n    \n    \n      1\n      2\n      2020-01-03 00:00:00.000000\n      158.320007\n      67.620003\n      NaN\n      112.190002\n    \n    \n      2\n      3\n      2020-01-06 00:00:00.000000\n      157.080002\n      66.629997\n      NaN\n      112.589996\n    \n    \n      3\n      4\n      2020-01-07 00:00:00.000000\n      159.320007\n      70.290001\n      NaN\n      112.290001\n    \n    \n      4\n      5\n      2020-01-08 00:00:00.000000\n      158.929993\n      71.809998\n      NaN\n      112.839996\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      258\n      259\n      2021-01-11 00:00:00.000000\n      218.470001\n      344.980011\n      295.000000\n      131.750000\n    \n    \n      259\n      260\n      2021-01-12 00:00:00.000000\n      216.500000\n      333.200012\n      298.000000\n      131.800003\n    \n    \n      260\n      261\n      2021-01-13 00:00:00.000000\n      214.020004\n      360.000000\n      295.000000\n      132.100006\n    \n    \n      261\n      262\n      2021-01-14 00:00:00.000000\n      215.910004\n      371.000000\n      305.000000\n      131.619995\n    \n    \n      262\n      263\n      2021-01-15 00:00:00.000000\n      213.520004\n      397.709991\n      306.820007\n      130.679993\n    \n  \n\n263 rows  6 columns\n\n\n\n\nstart = pd.to_datetime('2020-01-01')\nend = pd.to_datetime('today')\n\n\nFXAIX_stock = web.DataReader('FXAIX', 'yahoo', start, end)\nFXAIX_stock.head()\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nSNOW_stock = web.DataReader('SNOW', 'yahoo', start, end)\nSNOW_stock.head()\n\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Open')\n\nMSFT_stock['Open'].plot(label='Microsoft')\nZOOM_stock['Open'].plot(label='Zoom')\nSNOW_stock['Open'].plot(label='Snowflake')\nFXAIX_stock['Open'].plot(label='SNP_500')\nplt.legend()\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Volume')\n\nMSFT_stock['Volume'].plot(label='Microsoft')\nZOOM_stock['Volume'].plot(label='Zoom')\nSNOW_stock['Volume'].plot(label='Snowflake')\nFXAIX_stock['Volume'].plot(label='SNP_500')\n\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f7f95b65f40>\n\n\n\n\n\n\n\n\n\nFXAIX_stock = web.DataReader('FXAIX', 'yahoo', start, end)\nFXAIX_stock.head()\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nSNOW_stock = web.DataReader('SNOW', 'yahoo', start, end)\nSNOW_stock.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-09-16\n      319.0\n      231.110001\n      245.000000\n      253.929993\n      36099700\n      253.929993\n    \n    \n      2020-09-17\n      241.5\n      215.240005\n      230.759995\n      227.539993\n      11907500\n      227.539993\n    \n    \n      2020-09-18\n      249.0\n      218.589996\n      235.000000\n      240.000000\n      7475400\n      240.000000\n    \n    \n      2020-09-21\n      241.5\n      218.600006\n      230.000000\n      228.850006\n      5524900\n      228.850006\n    \n    \n      2020-09-22\n      239.0\n      225.149994\n      238.500000\n      235.160004\n      3889100\n      235.160004\n    \n  \n\n\n\n\n\nstocks = pd.concat([MSFT_stock['Open'], ZOOM_stock['Open'], SNOW_stock['Open'], FXAIX_stock['Open']],\n                   axis = 1)\n\n\nstocks.reset_index(level=0, inplace=True)\n\n\nstocks\n\n\n\n\n\n  \n    \n      \n      Date\n      Open\n      Open\n      Open\n      Open\n    \n  \n  \n    \n      0\n      2020-01-02\n      158.779999\n      68.800003\n      NaN\n      112.980003\n    \n    \n      1\n      2020-01-03\n      158.320007\n      67.620003\n      NaN\n      112.190002\n    \n    \n      2\n      2020-01-06\n      157.080002\n      66.629997\n      NaN\n      112.589996\n    \n    \n      3\n      2020-01-07\n      159.320007\n      70.290001\n      NaN\n      112.290001\n    \n    \n      4\n      2020-01-08\n      158.929993\n      71.809998\n      NaN\n      112.839996\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      258\n      2021-01-11\n      218.470001\n      344.980011\n      295.000000\n      131.750000\n    \n    \n      259\n      2021-01-12\n      216.500000\n      333.200012\n      298.000000\n      131.800003\n    \n    \n      260\n      2021-01-13\n      214.020004\n      360.000000\n      295.000000\n      132.100006\n    \n    \n      261\n      2021-01-14\n      215.910004\n      371.000000\n      305.000000\n      131.619995\n    \n    \n      262\n      2021-01-15\n      213.520004\n      397.709991\n      306.820007\n      130.679993\n    \n  \n\n263 rows  5 columns\n\n\n\n\nstocks.columns = ['Date','MSFT_stock','ZOOM_stock','SNOW_stock','FXAIX_stock']\n\n\nstocks\n\n\n\n\n\n  \n    \n      \n      Date\n      MSFT_stock\n      ZOOM_stock\n      SNOW_stock\n      FXAIX_stock\n    \n  \n  \n    \n      0\n      2020-01-02\n      158.779999\n      68.800003\n      NaN\n      112.980003\n    \n    \n      1\n      2020-01-03\n      158.320007\n      67.620003\n      NaN\n      112.190002\n    \n    \n      2\n      2020-01-06\n      157.080002\n      66.629997\n      NaN\n      112.589996\n    \n    \n      3\n      2020-01-07\n      159.320007\n      70.290001\n      NaN\n      112.290001\n    \n    \n      4\n      2020-01-08\n      158.929993\n      71.809998\n      NaN\n      112.839996\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      258\n      2021-01-11\n      218.470001\n      344.980011\n      295.000000\n      131.750000\n    \n    \n      259\n      2021-01-12\n      216.500000\n      333.200012\n      298.000000\n      131.800003\n    \n    \n      260\n      2021-01-13\n      214.020004\n      360.000000\n      295.000000\n      132.100006\n    \n    \n      261\n      2021-01-14\n      215.910004\n      371.000000\n      305.000000\n      131.619995\n    \n    \n      262\n      2021-01-15\n      213.520004\n      397.709991\n      306.820007\n      130.679993\n    \n  \n\n263 rows  5 columns\n\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///stocks.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nstocks_table = db.Table('stocks_table', metadata, \n    db.Column('Date',db.Integer, nullable=True, index=False),\n    db.Column('MSFT_stock',db.Integer, nullable=True),\n    db.Column('ZOOM_stock',db.Integer, nullable=True),\n    db.Column('SNOW_stock',db.Integer, nullable=True),\n    db.Column('FXAIX_stock', db.Numeric, nullable=True)\n)\n\n\nmetadata.create_all(engine) #Creates the table\n\n\nstocks_table\n\nTable('stocks_table', MetaData(bind=None), Column('Date', Integer(), table=<stocks_table>), Column('MSFT_stock', Integer(), table=<stocks_table>), Column('ZOOM_stock', Integer(), table=<stocks_table>), Column('SNOW_stock', Integer(), table=<stocks_table>), Column('FXAIX_stock', Numeric(), table=<stocks_table>), schema=None)\n\n\n\nstocks.to_sql('stocks_table', con=engine, if_exists='append', index=False)\n\n\nengine.execute(\"SELECT * FROM stocks_table LIMIT 10\").fetchall()\n\n[('2020-01-02 00:00:00.000000', 158.77999877929688, 68.80000305175781, None, 112.9800033569336),\n ('2020-01-03 00:00:00.000000', 158.32000732421875, 67.62000274658203, None, 112.19000244140625),\n ('2020-01-06 00:00:00.000000', 157.0800018310547, 66.62999725341797, None, 112.58999633789062),\n ('2020-01-07 00:00:00.000000', 159.32000732421875, 70.29000091552734, None, 112.29000091552734),\n ('2020-01-08 00:00:00.000000', 158.92999267578125, 71.80999755859375, None, 112.83999633789062),\n ('2020-01-09 00:00:00.000000', 161.83999633789062, 73.98999786376953, None, 113.62000274658203),\n ('2020-01-10 00:00:00.000000', 162.82000732421875, 73.08000183105469, None, 113.30000305175781),\n ('2020-01-13 00:00:00.000000', 161.75999450683594, 73.88999938964844, None, 114.08999633789062),\n ('2020-01-14 00:00:00.000000', 163.38999938964844, 74.31999969482422, None, 113.93000030517578),\n ('2020-01-15 00:00:00.000000', 162.6199951171875, 73.27999877929688, None, 114.13999938964844)]\n\n\n\nsql = \"\"\"\nSELECT\n  DATE(date) AS DATE\n, FXAIX_stock\n, MSFT_stock\n, SNOW_stock\n, row_number() OVER (PARTITION BY Date, MSFT_stock ORDER BY Date) AS REC_NBR\n, COUNT(*) OVER (PARTITION BY Date, MSFT_stock ORDER BY Date) AS REC_CNT\n, CASE WHEN FXAIX_stock >= 120 THEN 'SNP_High' ELSE 'SNP_low' END AS SNP_HIGH_LOW\nFROM stocks_table\n--WHERE FXAIX_stock >= 120\n\"\"\"\n\ncnxn = connection\n\n\nstocks = pd.read_sql(sql, cnxn)\n\n\nstocks.tail(30)\n\n\n\n\n\n  \n    \n      \n      DATE\n      FXAIX_stock\n      MSFT_stock\n      SNOW_stock\n      REC_NBR\n      REC_CNT\n      SNP_HIGH_LOW\n    \n  \n  \n    \n      233\n      2020-12-03\n      127.540001\n      214.610001\n      290.540009\n      1\n      1\n      SNP_High\n    \n    \n      234\n      2020-12-04\n      128.669998\n      214.220001\n      335.399994\n      1\n      1\n      SNP_High\n    \n    \n      235\n      2020-12-07\n      128.419998\n      214.369995\n      393.500000\n      1\n      1\n      SNP_High\n    \n    \n      236\n      2020-12-08\n      128.789993\n      213.970001\n      388.799988\n      1\n      1\n      SNP_High\n    \n    \n      237\n      2020-12-09\n      127.769997\n      215.160004\n      393.399994\n      1\n      1\n      SNP_High\n    \n    \n      238\n      2020-12-10\n      127.610001\n      211.770004\n      362.000000\n      1\n      1\n      SNP_High\n    \n    \n      239\n      2020-12-11\n      126.870003\n      210.050003\n      360.399994\n      1\n      1\n      SNP_High\n    \n    \n      240\n      2020-12-14\n      126.339996\n      213.100006\n      352.489990\n      1\n      1\n      SNP_High\n    \n    \n      241\n      2020-12-15\n      127.970001\n      215.169998\n      308.980011\n      1\n      1\n      SNP_High\n    \n    \n      242\n      2020-12-16\n      128.199997\n      214.750000\n      328.429993\n      1\n      1\n      SNP_High\n    \n    \n      243\n      2020-12-17\n      128.940002\n      219.869995\n      333.820007\n      1\n      1\n      SNP_High\n    \n    \n      244\n      2020-12-18\n      128.500000\n      218.589996\n      332.769989\n      1\n      1\n      SNP_High\n    \n    \n      245\n      2020-12-21\n      128.000000\n      217.550003\n      329.000000\n      1\n      1\n      SNP_High\n    \n    \n      246\n      2020-12-22\n      127.750000\n      222.690002\n      349.890015\n      1\n      1\n      SNP_High\n    \n    \n      247\n      2020-12-23\n      127.839996\n      223.110001\n      341.160004\n      1\n      1\n      SNP_High\n    \n    \n      248\n      2020-12-24\n      128.309998\n      221.419998\n      334.100006\n      1\n      1\n      SNP_High\n    \n    \n      249\n      2020-12-28\n      129.429993\n      224.449997\n      324.869995\n      1\n      1\n      SNP_High\n    \n    \n      250\n      2020-12-29\n      129.139999\n      226.309998\n      305.250000\n      1\n      1\n      SNP_High\n    \n    \n      251\n      2020-12-30\n      129.330002\n      225.229996\n      304.000000\n      1\n      1\n      SNP_High\n    \n    \n      252\n      2020-12-31\n      130.169998\n      221.699997\n      299.700012\n      1\n      1\n      SNP_High\n    \n    \n      253\n      2021-01-04\n      128.259995\n      222.529999\n      285.410004\n      1\n      1\n      SNP_High\n    \n    \n      254\n      2021-01-05\n      129.179993\n      217.259995\n      280.619995\n      1\n      1\n      SNP_High\n    \n    \n      255\n      2021-01-06\n      129.919998\n      212.169998\n      279.989990\n      1\n      1\n      SNP_High\n    \n    \n      256\n      2021-01-07\n      131.880005\n      214.039993\n      272.589996\n      1\n      1\n      SNP_High\n    \n    \n      257\n      2021-01-08\n      132.619995\n      218.679993\n      315.000000\n      1\n      1\n      SNP_High\n    \n    \n      258\n      2021-01-11\n      131.750000\n      218.470001\n      295.000000\n      1\n      1\n      SNP_High\n    \n    \n      259\n      2021-01-12\n      131.800003\n      216.500000\n      298.000000\n      1\n      1\n      SNP_High\n    \n    \n      260\n      2021-01-13\n      132.100006\n      214.020004\n      295.000000\n      1\n      1\n      SNP_High\n    \n    \n      261\n      2021-01-14\n      131.619995\n      215.910004\n      305.000000\n      1\n      1\n      SNP_High\n    \n    \n      262\n      2021-01-15\n      130.679993\n      213.520004\n      306.820007\n      1\n      1\n      SNP_High\n    \n  \n\n\n\n\n\nstocks['FXAIX_stock'].plot(figsize = (12, 8))\nplt.title('Total S&P 500 in 2020 Value')\n\nText(0.5, 1.0, 'Total S&P 500 in 2020 Value')"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html",
    "href": "posts/2021-06-03-model-inspection.html",
    "title": "Model Inspection",
    "section": "",
    "text": "#code adapted from https://github.com/thomasjpfan/ml-workshop-intermediate-1-of-2"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#load-the-dataset",
    "href": "posts/2021-06-03-model-inspection.html#load-the-dataset",
    "title": "Model Inspection",
    "section": "Load the dataset",
    "text": "Load the dataset\n\n# %load solutions/regression_example.py\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\n\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\n\n\nX, y = df.drop(['specific', 'Unnamed: 0'], axis = 1), df['specific']\n\n\nX = X.select_dtypes(include='number')\nX\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\n\n\n\n\nX.head()\n\n\n\n\n\n  \n    \n      \n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      it\n    \n  \n  \n    \n      4\n      32100.0\n      2000\n      2902.09\n      31847\n      0.0\n      0.0\n      0.000000\n      1499110\n    \n    \n      6\n      66529.0\n      2002\n      3519.72\n      38375\n      0.0\n      0.0\n      0.000000\n      2404936\n    \n    \n      7\n      52108.0\n      2003\n      3923.11\n      36720\n      0.0\n      0.0\n      0.000000\n      2815820\n    \n    \n      10\n      279052.0\n      2006\n      6112.50\n      139354\n      0.0\n      0.0\n      0.324324\n      5167300\n    \n    \n      11\n      178705.0\n      2007\n      7360.92\n      299892\n      0.0\n      0.0\n      0.324324\n      7040099\n    \n  \n\n\n\n\n\ny.head()\n\n4      195580.0\n6      434149.0\n7      619201.0\n10    1457872.0\n11    2213991.0\nName: specific, dtype: float64\n\n\n\nInsert random data for demonstration\n\nimport numpy as np\n\nX = X.assign(ran_num=np.arange(0, X.shape[0]))\n\n\n\nSplit dataset\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=42)"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#train-linear-model",
    "href": "posts/2021-06-03-model-inspection.html#train-linear-model",
    "title": "Model Inspection",
    "section": "Train linear model",
    "text": "Train linear model\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import Pipeline\n\nridge = Pipeline([\n    ('scale', StandardScaler()),\n    ('reg', Ridge())\n])\nridge.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())])StandardScalerStandardScaler()RidgeRidge()\n\n\n\nridge.score(X_train, y_train)\n\n0.8843443502191103\n\n\n\nridge.score(X_test, y_test)\n\n0.7491370703502245"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#plot-coefficients",
    "href": "posts/2021-06-03-model-inspection.html#plot-coefficients",
    "title": "Model Inspection",
    "section": "Plot coefficients",
    "text": "Plot coefficients\nCoefficients represent the relationship between a feature and the target assuming that all other features remain constant.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_linear_coef(coefs, names, ax=None, sorted=False):\n    if ax is None:\n        fig, ax = plt.subplots()\n    coefs = pd.DataFrame(\n       coefs, columns=['Coefficients'],\n       index=names\n    )\n    \n    if sorted:\n        coefs = coefs.sort_values(by='Coefficients')\n\n    coefs.plot(kind='barh', ax=ax)\n    ax.axvline(x=0, color='.5')\n    return ax\n\nplot_linear_coef(ridge['reg'].coef_, names=X_train.columns, sorted=True);"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#coefficient-variability",
    "href": "posts/2021-06-03-model-inspection.html#coefficient-variability",
    "title": "Model Inspection",
    "section": "Coefficient variability",
    "text": "Coefficient variability\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import RepeatedKFold\n\n\nridges_cv = cross_validate(\n    ridge, X_train, y_train, cv=RepeatedKFold(n_splits=5, n_repeats=5),\n    return_estimator=True)\n\n\nridges_cv\n\n{'fit_time': array([0.00852752, 0.00879049, 0.00563025, 0.00589609, 0.00541282,\n        0.00482273, 0.00472617, 0.00440693, 0.00431228, 0.00409317,\n        0.00431299, 0.003896  , 0.00612736, 0.09311175, 0.00695705,\n        0.00576901, 0.00550413, 0.00539637, 0.00509334, 0.00491738,\n        0.00479674, 0.00459194, 0.00439835, 0.00426984, 0.00396895]),\n 'score_time': array([0.00384283, 0.00219274, 0.00248122, 0.00215101, 0.00219202,\n        0.00196052, 0.00188565, 0.00182557, 0.00175428, 0.00166225,\n        0.0016768 , 0.00160766, 0.00199437, 0.00305367, 0.00251174,\n        0.00237942, 0.00228405, 0.00211263, 0.00204182, 0.00194716,\n        0.00198007, 0.00189042, 0.00182319, 0.00166941, 0.00160313]),\n 'estimator': (Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())])),\n 'test_score': array([0.78102813, 0.6260654 , 0.78600362, 0.74219093, 0.97805921,\n        0.63885999, 0.90069639, 0.86957882, 0.89608878, 0.97272764,\n        0.83761009, 0.93328631, 0.85460586, 0.47428742, 0.78822307,\n        0.8933257 , 0.80865875, 0.78604436, 0.66129305, 0.93062503,\n        0.81909785, 0.86437887, 0.65233286, 0.79389227, 0.96456357])}\n\n\n\nridge_coefs = pd.DataFrame(\n   [model['reg'].coef_ for model in ridges_cv['estimator']],\n   columns=X.columns\n)\n\n\nridge_coefs.head()\n\n\n\n\n\n  \n    \n      \n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      it\n      ran_num\n    \n  \n  \n    \n      0\n      366170.413056\n      -53199.923796\n      29027.500401\n      103838.720536\n      -38871.206032\n      21826.179010\n      18730.354007\n      432388.097936\n      15395.947631\n    \n    \n      1\n      278344.791911\n      31229.815166\n      105656.219289\n      12157.657371\n      -42445.111433\n      33836.845100\n      10283.384018\n      490405.339232\n      138.790062\n    \n    \n      2\n      370195.639332\n      4058.337195\n      180433.806468\n      -20521.007552\n      -46413.645489\n      51399.811179\n      -20177.524103\n      315977.584466\n      -43552.653962\n    \n    \n      3\n      344530.343098\n      -33517.706292\n      154739.154793\n      -33164.713671\n      -22366.886793\n      -8720.784251\n      37077.197240\n      464446.608528\n      -6371.954113\n    \n    \n      4\n      296527.214315\n      -41401.308214\n      78119.685147\n      21683.387951\n      -41251.611341\n      41388.300696\n      26437.090097\n      418431.007311\n      11901.614954\n    \n  \n\n\n\n\n\nPlotting the variability of the cofficients\n\nfig, ax = plt.subplots()\n_ = ax.boxplot(ridge_coefs, vert=False, labels=ridge_coefs.columns)\n\n\n\n\n\nfrom sklearn.linear_model import Lasso\nlasso = Pipeline([\n    ('scale', StandardScaler()),\n    ('reg', Lasso(alpha=0.06))\n])\n\n\nlasso.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('scale', StandardScaler()), ('reg', Lasso(alpha=0.06))])StandardScalerStandardScaler()LassoLasso(alpha=0.06)\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 9))\nplot_linear_coef(lasso['reg'].coef_, names=X_train.columns, sorted=True, ax=ax1);\nplot_linear_coef(ridge['reg'].coef_, names=X_train.columns, sorted=True, ax=ax2);\n\n\n\n\n\nlasso_cvs = cross_validate(\n    lasso, X_train, y_train, return_estimator=True, cv=RepeatedKFold(n_splits=5, n_repeats=5)\n)\n\n\nlasso_coefs = pd.DataFrame(\n   [model['reg'].coef_ for model in lasso_cvs['estimator']],\n   columns=X.columns\n)\n\n\nfig, ax = plt.subplots()\n_ = ax.boxplot(lasso_coefs, vert=False, labels=ridge_coefs.columns)\n\n\n\n\n\n# %load solutions/03-ex01-solutions.py\nfrom sklearn.linear_model import Lasso\n\nlasso = Pipeline([\n    ('scale', StandardScaler()),\n    ('reg', Lasso(random_state=42, alpha=0.04))\n])\nlasso.fit(X_train, y_train)\n\nlasso.score(X_test, y_test)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\nplot_linear_coef(ridge['reg'].coef_, X_train.columns, ax=ax1)\nplot_linear_coef(lasso['reg'].coef_, X_train.columns, ax=ax2)\n\n<AxesSubplot:>"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#random-forest",
    "href": "posts/2021-06-03-model-inspection.html#random-forest",
    "title": "Model Inspection",
    "section": "Random Forest",
    "text": "Random Forest\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(random_state=42)\nrf.fit(X_train, y_train)\n\nRandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\nrf.score(X_train, y_train)\n\n0.9711219647906122\n\n\n\nrf.score(X_test, y_test)\n\n0.7873539531176165\n\n\n\ndef plot_importances(importances, names, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n    indices = np.argsort(importances)\n    ax.barh(range(len(importances)), importances[indices])\n    ax.set(yticks=range(len(importances)),\n           yticklabels=np.array(names)[indices]);\n\n\nimportances = rf.feature_importances_\nplot_importances(importances, X_train.columns);\n\n\n\n\nPay attention to ran_num!"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#permutation-feature-importance",
    "href": "posts/2021-06-03-model-inspection.html#permutation-feature-importance",
    "title": "Model Inspection",
    "section": "Permutation Feature Importance",
    "text": "Permutation Feature Importance\n\nCan be used on the test data!\n\nfrom sklearn.inspection import permutation_importance\n\nrf_perm_results = permutation_importance(rf, X_test, y_test,\n                                        n_repeats=10, n_jobs=-1)\n\n\ndef plot_permutation_importance(perm_results, names, ax=None):\n    perm_sorted_idx = perm_results.importances_mean.argsort()\n    if ax is None:\n        fig, ax = plt.subplots()\n    _ = ax.boxplot(perm_results.importances[perm_sorted_idx].T, vert=False,\n                   labels=np.array(names)[perm_sorted_idx])\n    return ax\n\n\n_ = plot_permutation_importance(rf_perm_results, X_test.columns)\n\n\n\n\n\n\nLoad cancer dataset\n\n# %load solutions/classifier_example.py\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\nrf.score(X_test, y_test)\n\ny_pred = rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7131\n           1       0.00      0.00      0.00       137\n\n    accuracy                           0.98      7268\n   macro avg       0.49      0.50      0.50      7268\nweighted avg       0.96      0.98      0.97      7268\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\nRandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\nrf.score(X_test, y_test)\n\n0.9808750687947165\n\n\n\n\nPermutation importance with random forest\n\nfrom sklearn.inspection import permutation_importance\n\nrf_result = permutation_importance(rf, X_train, y_train,\n                                   n_repeats=10, n_jobs=-1)\n\n\n\nTraining data\n\n_ = plot_permutation_importance(rf_result, X)\n\n/home/david/anaconda3/lib/python3.8/site-packages/matplotlib/text.py:1163: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  if s != self._text:\n\n\n\n\n\n\nfrom scipy.stats import spearmanr\nfrom scipy.cluster import hierarchy\n\ncorr = spearmanr(X_train).correlation\ncorr_linkage = hierarchy.ward(corr)\ncorr_linkage\n\narray([[0.        , 1.        , 1.05218646, 2.        ],\n       [2.        , 5.        , 1.20582869, 3.        ],\n       [3.        , 4.        , 1.24728653, 2.        ],\n       [6.        , 7.        , 1.38884045, 5.        ]])\n\n\n\nfrom collections import defaultdict\n\ncluster_ids = hierarchy.fcluster(corr_linkage, 1, criterion='distance')\ncluster_id_to_feature_ids = defaultdict(list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nselected_features\n\n[0, 1, 2, 3, 4]\n\n\n\n\nFeature importance with selected features\n\nfrom sklearn.inspection import permutation_importance\n\nrf_sel_result = permutation_importance(\n    rf_sel, X_test, y_test, n_repeats=10, n_jobs=-1)\n\n\nfeatures_sel = data.feature_names[selected_features]\n_ = plot_permutation_importance(rf_sel_result, features_sel)"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#partial-dependence",
    "href": "posts/2021-06-03-model-inspection.html#partial-dependence",
    "title": "Model Inspection",
    "section": "Partial Dependence",
    "text": "Partial Dependence\n\nTrain a HistGradientBostingClassifer\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier \n\n\nhist = HistGradientBoostingClassifier(random_state=0)\nhist.fit(X_train, y_train)\n\nHistGradientBoostingClassifierHistGradientBoostingClassifier(random_state=0)\n\n\n\n# %load solutions/03-ex03-solutions.py\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\n\nX, y = boston.data, boston.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=0)\n\nfrom sklearn.ensemble import GradientBoostingRegressor\ngb = GradientBoostingRegressor(random_state=0)\n\ngb.fit(X_train, y_train)\n\ngb.score(X_train, y_train)\n\nplot_importances(gb.feature_importances_, boston.feature_names)\n\ngb_perm_results = permutation_importance(gb, X_test, y_test, n_repeats=10, n_jobs=-1)\n\nplot_permutation_importance(gb_perm_results, boston.feature_names)\n\nplot_partial_dependence(gb, X_test, features=[\"LSTAT\", \"RM\", \"DIS\", \"CRIM\"],\n                        feature_names=boston.feature_names, n_cols=2)\n\nplot_partial_dependence(gb, X_test, features=[('LSTAT', 'RM')],\n                        feature_names=boston.feature_names)\n\n<sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay at 0x7f0e9d226dc0>"
  },
  {
    "objectID": "posts/2020-08-15-pyspark-fiscal-data-regression.html",
    "href": "posts/2020-08-15-pyspark-fiscal-data-regression.html",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "",
    "text": "A minimal example of using Pyspark for Linear Regression"
  },
  {
    "objectID": "posts/2020-08-15-pyspark-fiscal-data-regression.html#bring-in-needed-imports",
    "href": "posts/2020-08-15-pyspark-fiscal-data-regression.html#bring-in-needed-imports",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "Bring in needed imports",
    "text": "Bring in needed imports\n\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import StringType,BooleanType,DateType,IntegerType\nfrom pyspark.sql.functions import *"
  },
  {
    "objectID": "posts/2020-08-15-pyspark-fiscal-data-regression.html#load-data-from-csv",
    "href": "posts/2020-08-15-pyspark-fiscal-data-regression.html#load-data-from-csv",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "Load data from CSV",
    "text": "Load data from CSV\n\n\nCode\n# Load data from a CSV\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").option(\"inferSchema\", True).option(\"header\", True).load(file_location)\ndisplay(df.take(5))\n\n\n\n_c0provincespecificgeneralyeargdpfdirnrrrifrregit0Anhui147002.0null19962093.3506610.00.00.01128873East China6319301Anhui151981.0null19972347.32434430.00.00.01356287East China6578602Anhui174930.0null19982542.96276730.00.00.01518236East China8894633Anhui285324.0null19992712.3426131nullnullnull1646891East China12273644Anhui195580.032100.020002902.09318470.00.00.01601508East China1499110\n\n\n\ndf.createOrReplaceTempView(\"fiscal_stats\")\n\nsums = spark.sql(\"\"\"\nselect year, sum(it) as total_yearly_it, sum(fr) as total_yearly_fr\nfrom fiscal_stats\ngroup by 1\norder by year asc\n\"\"\")\n\nsums.show()\n\n\n+----+---------------+---------------+\nyear|total_yearly_it|total_yearly_fr|\n+----+---------------+---------------+\n1996|       19825341|    2.9579215E7|\n1997|       21391321|    2.9110765E7|\n1998|       25511453|    3.8154711E7|\n1999|       31922107|    4.2128627E7|\n2000|       38721293|    4.8288092E7|\n2001|       50754944|    5.8910649E7|\n2002|       62375881|    6.2071474E7|\n2003|       69316709|    7.2479293E7|\n2004|       88626786|           null|\n2005|       98263665|           null|\n2006|      119517822|    1.3349148E8|\n2007|      153467611|   2.27385701E8|\n+----+---------------+---------------+"
  },
  {
    "objectID": "posts/2020-08-15-pyspark-fiscal-data-regression.html#describing-the-data",
    "href": "posts/2020-08-15-pyspark-fiscal-data-regression.html#describing-the-data",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "Describing the Data",
    "text": "Describing the Data\n\ndf.describe().toPandas().transpose()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n    \n  \n  \n    \n      summary\n      count\n      mean\n      stddev\n      min\n      max\n    \n    \n      _c0\n      360\n      179.5\n      104.06728592598157\n      0\n      359\n    \n    \n      province\n      360\n      None\n      None\n      Anhui\n      Zhejiang\n    \n    \n      specific\n      356\n      583470.7303370787\n      654055.3290782663\n      8964.0\n      3937966.0\n    \n    \n      general\n      169\n      309127.53846153844\n      355423.5760674793\n      0.0\n      1737800.0\n    \n    \n      year\n      360\n      2001.5\n      3.4568570586927794\n      1996\n      2007\n    \n    \n      gdp\n      360\n      4428.653416666667\n      4484.668659976412\n      64.98\n      31777.01\n    \n    \n      fdi\n      360\n      196139.38333333333\n      303043.97011891654\n      2\n      1743140\n    \n    \n      rnr\n      294\n      0.0355944252244898\n      0.16061503029299648\n      0.0\n      1.214285714\n    \n    \n      rr\n      296\n      0.059688621057432424\n      0.15673351824073453\n      0.0\n      0.84\n    \n    \n      i\n      287\n      0.08376351662369343\n      0.1838933104683607\n      0.0\n      1.05\n    \n    \n      fr\n      295\n      2522449.0034013605\n      3491329.8613106664\n      #REF!\n      9898522\n    \n    \n      reg\n      360\n      None\n      None\n      East China\n      Southwest China\n    \n    \n      it\n      360\n      2165819.2583333333\n      1769294.2935487411\n      147897\n      10533312"
  },
  {
    "objectID": "posts/2020-08-15-pyspark-fiscal-data-regression.html#cast-data-type",
    "href": "posts/2020-08-15-pyspark-fiscal-data-regression.html#cast-data-type",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "Cast Data Type",
    "text": "Cast Data Type\n\ndf2 = df.withColumn(\"gdp\",col(\"gdp\").cast(IntegerType())) \\\n.withColumn(\"specific\",col(\"specific\").cast(IntegerType())) \\\n.withColumn(\"general\",col(\"general\").cast(IntegerType())) \\\n.withColumn(\"year\",col(\"year\").cast(IntegerType())) \\\n.withColumn(\"fdi\",col(\"fdi\").cast(IntegerType())) \\\n.withColumn(\"rnr\",col(\"rnr\").cast(IntegerType())) \\\n.withColumn(\"rr\",col(\"rr\").cast(IntegerType())) \\\n.withColumn(\"i\",col(\"i\").cast(IntegerType())) \\\n.withColumn(\"fr\",col(\"fr\").cast(IntegerType()))"
  },
  {
    "objectID": "posts/2020-08-15-pyspark-fiscal-data-regression.html#printschema",
    "href": "posts/2020-08-15-pyspark-fiscal-data-regression.html#printschema",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "printSchema",
    "text": "printSchema\n\ndf2.printSchema()\n\n\nroot\n-- _c0: integer (nullable = true)\n-- province: string (nullable = true)\n-- specific: integer (nullable = true)\n-- general: integer (nullable = true)\n-- year: integer (nullable = true)\n-- gdp: integer (nullable = true)\n-- fdi: integer (nullable = true)\n-- rnr: integer (nullable = true)\n-- rr: integer (nullable = true)\n-- i: integer (nullable = true)\n-- fr: integer (nullable = true)\n-- reg: string (nullable = true)\n-- it: integer (nullable = true)\n\n\n\n\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nassembler = VectorAssembler(inputCols=['gdp', 'fdi'], outputCol=\"features\")\ntrain_df = assembler.transform(df2) \n\n\n\n\n\n\ntrain_df.select(\"specific\", \"year\").show()\n\n\n+--------+----+\nspecific|year|\n+--------+----+\n  147002|1996|\n  151981|1997|\n  174930|1998|\n  285324|1999|\n  195580|2000|\n  250898|2001|\n  434149|2002|\n  619201|2003|\n  898441|2004|\n  898441|2005|\n 1457872|2006|\n 2213991|2007|\n  165957|1996|\n  165957|1997|\n  245198|1998|\n  388083|1999|\n  281769|2000|\n  441923|2001|\n  558569|2002|\n  642581|2003|\n+--------+----+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-15-pyspark-fiscal-data-regression.html#linear-regression-in-pyspark",
    "href": "posts/2020-08-15-pyspark-fiscal-data-regression.html#linear-regression-in-pyspark",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "Linear Regression in Pyspark",
    "text": "Linear Regression in Pyspark\n\nlr = LinearRegression(featuresCol = 'features', labelCol='it')\nlr_model = lr.fit(train_df)\n\ntrainingSummary = lr_model.summary\nprint(\"Coefficients: \" + str(lr_model.coefficients))\nprint(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\nprint(\"R2: %f\" % trainingSummary.r2)\n\n\nCoefficients: [495.05888709337756,-4.968141828763066]\nRMSE: 1234228.673087\nR2: 0.512023\n\n\n\n\nlr_predictions = lr_model.transform(train_df)\nlr_predictions.select(\"prediction\",\"it\",\"features\").show(5)\nfrom pyspark.ml.evaluation import RegressionEvaluator\nlr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n                 labelCol=\"it\",metricName=\"r2\")\n\n\n+------------------+-------+----------------+\n        prediction|     it|        features|\n+------------------+-------+----------------+\n1732528.7382477913| 631930|[2093.0,50661.0]|\n1894133.7432895212| 657860|[2347.0,43443.0]|\n2069017.8229123235| 889463|[2542.0,27673.0]|\n2160838.7084181504|1227364|[2712.0,26131.0]|\n2226501.9982726825|1499110|[2902.0,31847.0]|\n+------------------+-------+----------------+\nonly showing top 5 rows\n\n\n\n\n\nprint(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))\n\n\nR Squared (R2) on test data = 0.512023\n\n\n\n\nprint(\"numIterations: %d\" % trainingSummary.totalIterations)\nprint(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\ntrainingSummary.residuals.show()\n\n\nnumIterations: 1\nobjectiveHistory: [0.0]\n+-------------------+\n          residuals|\n+-------------------+\n-1100598.7382477913|\n-1236273.7432895212|\n-1179554.8229123235|\n -933474.7084181504|\n -727391.9982726825|\n-222546.39659531135|\n -94585.30175113119|\n 108072.63313654158|\n 389732.58121094666|\n  621021.2194867637|\n  1885768.997742407|\n  3938310.059555837|\n  -554084.125169754|\n -615660.3899049093|\n -352195.3468934437|\n-348450.00565795833|\n -918476.5594253046|\n -710059.9133252408|\n-1148661.0062004486|\n  -911572.322055324|\n+-------------------+\nonly showing top 20 rows\n\n\n\n\n\npredictions = lr_model.transform(test_df)\npredictions.select(\"prediction\",\"it\",\"features\").show()\n\n\n+------------------+-------+---------------+\n        prediction|     it|       features|\n+------------------+-------+---------------+\n 976371.9212205639| 306114|   [64.0,679.0]|\n 990722.2032541803| 415547|   [91.0,481.0]|\n1016348.0830204486| 983251|  [139.0,106.0]|\n1036290.7062801318| 218361|  [184.0,576.0]|\n1034023.4471330958| 178668| [202.0,2826.0]|\n1060130.0768520113| 274994| [245.0,1856.0]|\n1023513.0851009073| 546541|[263.0,11020.0]|\n   1053250.6267921| 361358| [264.0,5134.0]|\n1123768.8091592425| 866691| [377.0,2200.0]|\n1128604.8330225947| 948521| [390.0,2522.0]|\n 810587.2575938476| 177748|[442.0,71715.0]|\n 1159703.254297337| 736165| [445.0,1743.0]|\n 1066975.770986663|1260633|[466.0,22500.0]|\n1288507.6625716756|1423771| [725.0,3718.0]|\n 1320055.238474972| 573905| [793.0,4144.0]|\n1188611.0570700848|2347862|[797.0,31000.0]|\n 1321857.482976733| 582711| [805.0,4977.0]|\n1033849.5995896922| 746784|[819.0,64343.0]|\n 1445051.792853667|1216605|[1029.0,2501.0]|\n1437887.1056682135|1258100|[1052.0,6235.0]|\n+------------------+-------+---------------+\nonly showing top 20 rows\n\n\n\n\n\nfrom pyspark.ml.regression import DecisionTreeRegressor\ndt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'it')\ndt_model = dt.fit(train_df)\ndt_predictions = dt_model.transform(train_df)\ndt_evaluator = RegressionEvaluator(\n    labelCol=\"it\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = dt_evaluator.evaluate(dt_predictions)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n\n\nRoot Mean Squared Error (RMSE) on test data = 1.01114e+06\n\n\n\n\nfrom pyspark.ml.regression import GBTRegressor\ngbt = GBTRegressor(featuresCol = 'features', labelCol = 'it', maxIter=10)\ngbt_model = gbt.fit(train_df)\ngbt_predictions = gbt_model.transform(train_df)\ngbt_predictions.select('prediction', 'it', 'features').show(5)\n\n\ngbt_evaluator = RegressionEvaluator(\n    labelCol=\"it\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = gbt_evaluator.evaluate(gbt_predictions)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n\n\n+------------------+-------+----------------+\n        prediction|     it|        features|\n+------------------+-------+----------------+\n 1388898.308543053| 631930|[2093.0,50661.0]|\n 1388898.308543053| 657860|[2347.0,43443.0]|\n1649083.6277172007| 889463|[2542.0,27673.0]|\n1649083.6277172007|1227364|[2712.0,26131.0]|\n1649083.6277172007|1499110|[2902.0,31847.0]|\n+------------------+-------+----------------+\nonly showing top 5 rows\n\nRoot Mean Squared Error (RMSE) on test data = 778728\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-10-21-quant_data_datareader_quandl.html",
    "href": "posts/2020-10-21-quant_data_datareader_quandl.html",
    "title": "Using the Quandl API and Pandas Datareader API to call Microsoft, Apple, Zoom, Snowflake stocks and other finance data",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport pandas_datareader.data as web\nimport datetime\n\n\nstart = datetime.datetime(2020, 1, 1)\nend = pd.to_datetime('today')\n\n\nAAPL_stock = web.DataReader('AAPL', 'yahoo', start, end)\nAAPL_stock.head()\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nSNOW_stock = web.DataReader('SNOW', 'yahoo', start, end)\nSNOW_stock.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-09-16\n      319.0\n      231.110001\n      245.000000\n      253.929993\n      36099700\n      253.929993\n    \n    \n      2020-09-17\n      241.5\n      215.240005\n      230.759995\n      227.539993\n      11907500\n      227.539993\n    \n    \n      2020-09-18\n      249.0\n      218.589996\n      235.000000\n      240.000000\n      7475400\n      240.000000\n    \n    \n      2020-09-21\n      241.5\n      218.600006\n      230.000000\n      228.850006\n      5524900\n      228.850006\n    \n    \n      2020-09-22\n      239.0\n      225.149994\n      238.500000\n      235.160004\n      3889100\n      235.160004\n    \n  \n\n\n\n\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Open')\n\nMSFT_stock['Open'].plot(label='Microsoft')\nZOOM_stock['Open'].plot(label='Zoom')\nSNOW_stock['Open'].plot(label='Snowflake')\nAAPL_stock['Open'].plot(label='Apple')\nplt.legend()\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Volume')\n\nMSFT_stock['Volume'].plot(label='Microsoft')\nZOOM_stock['Volume'].plot(label='Zoom')\nSNOW_stock['Volume'].plot(label='Snowflake')\nAAPL_stock['Volume'].plot(label='Apple')\n\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7fb87c7af550>\n\n\n\n\n\n\n\n\n\nimport pandas_datareader.data as web\n\nimport datetime\n\n\n\ngdp = web.DataReader(\"GDP\", \"fred\", start, end)\n\n\ngdp.head()\n\n\n\n\n\n  \n    \n      \n      GDP\n    \n    \n      DATE\n      \n    \n  \n  \n    \n      2020-01-01\n      21538.032\n    \n    \n      2020-04-01\n      19636.731\n    \n    \n      2020-07-01\n      21362.428\n    \n    \n      2020-10-01\n      21704.706\n    \n    \n      2021-01-01\n      22313.850\n    \n  \n\n\n\n\n\nimport quandl\n\n\n#quandl.ApiConfig.api_key = ''\n\n\n#mydata = quandl.get(\"EIA/PET_RWTC_D\")\n\n#mydata.head()\n\n\nmydata.plot(figsize=(12,6))\n\n\n#mydata = quandl.get(\"EIA/PET_RWTC_D\", returns=\"numpy\",start_date=start,end_date=end)\n\n\nmydata = quandl.get(\"FRED/GDP\",start_date=start,end_date=end)\n\n\nmydata.head()\n\n\nmydata = quandl.get([\"NSE/OIL.1\", \"WIKI/AAPL.4\"],start_date=start,end_date=end)\n\n\nmydata.head()\n\n\nmydata = quandl.get(\"FRED/GDP\")\n\n\nmydata = quandl.get('WIKI/FB',start_date=start,end_date=end)\n\n\nmydata.head()\n\n\nmydata = quandl.get('WIKI/FB.1',start_date=start,end_date=end)\n\nmydata.head()\n\n\nmydata = quandl.get('WIKI/FB.7',start_date=start,end_date=end)\nmydata.head()\n\n\n# Homes\n\n\nhouses = quandl.get('ZILLOW/M11_ZRIAH',start_date=start,end_date=end)\n\n\nhouses.head()\n\n\nhouses.plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f900d58fef0>"
  },
  {
    "objectID": "posts/2020-10-08-dask-day-1.html",
    "href": "posts/2020-10-08-dask-day-1.html",
    "title": "Using Dask for Arrays",
    "section": "",
    "text": "import numpy as np\nimport dask.array as da\n\n\nnp_arr = np.random.randint(20, size=20)\nnp_arr\n\narray([12, 18, 17,  7,  5,  9, 11,  3,  5, 15, 13, 13,  5, 12, 11, 16,  4,\n       10,  9,  7])\n\n\n\ndask_arr = da.random.randint(20, size=20, chunks=5)\n\n\ndask_arr\n\n\n\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  160 B   40 B \n     Shape  (20,)   (5,) \n     Count  4 Tasks  4 Chunks \n     Type  int64  numpy.ndarray \n  \n\n\n\n\n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  20\n  1\n\n\n\n\n\n\n\n## This is simply because Dask does lazy evaluaion.   \n### You need to call `compute()` to start the execution\n\n\ndask_arr.compute()\n\narray([ 3, 17,  5, 11, 19, 14, 14, 11,  9, 18,  9,  7, 10, 13, 10, 10, 11,\n       10,  9,  2])\n\n\n\ndask_arr.chunks\n\n((5, 5, 5, 5),)\n\n\n\ndask_arr_from_np = da.from_array(np_arr, chunks=5)\n\n\ndask_arr_from_np\n\n\n\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  160 B   40 B \n     Shape  (20,)   (5,) \n     Count  5 Tasks  4 Chunks \n     Type  int64  numpy.ndarray \n  \n\n\n\n\n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  20\n  1\n\n\n\n\n\n\n\ndask_arr_from_np.compute()\n\narray([12, 18, 17,  7,  5,  9, 11,  3,  5, 15, 13, 13,  5, 12, 11, 16,  4,\n       10,  9,  7])\n\n\n\n### array operations into a graph to tasks\n#### See : http://docs.dask.org/en/latest/graphviz.html\n\n\ndask_arr_from_np.sum().visualize()\n\n\n\n\n\ndask_arr_from_np.sum().visualize(rankdir=\"LR\")\n\n\n\n\n\n(dask_arr_from_np+1).visualize(rankdir=\"LR\")\n\n\n\n\n\ndask_arr_mean = da.mean(dask_arr_from_np)\ndask_arr_mean.compute()\n\n10.1\n\n\n\ndask_arr_mean.visualize(rankdir=\"LR\")\n\n\n\n\n\nx = da.random.random(10, chunks=2)\ny = da.random.random(10, chunks=2)\n\nsum_x_y = da.add(x, y) #similar to numpy.add\nmean_x_y = da.mean(sum_x_y)\n\n\nsum_x_y.compute()\n\narray([0.96028343, 0.55946179, 1.11161829, 1.28233368, 0.53130934,\n       0.86805782, 0.20173099, 0.77596276, 0.92576765, 1.04750609])\n\n\n\nsum_x_y.visualize()\n\n\n\n\n\nmean_x_y.visualize()\n\n\n\n\n\nda_arr_large = da.random.randint(10000, size=(50000, 50000),\n                                 chunks=(5000, 1000))   \nda_sum_large = da_arr_large.sum()   \n\n\n### Get no. bytes using `nbytes` : http://docs.dask.org/en/latest/array-api.html#dask.array.Array.nbytes\n\n\nda_arr_large.nbytes  \n\n20000000000\n\n\n\n### Convert bytes to GB, 1Gb = 1e+9 bytes\n\n\nda_arr_large.nbytes/1e+9\n\n20.0\n\n\n\nda_sum_large.compute()\n\n12498643590734\n\n\n\n# Dask 2\n\n\nsize_tuple = (500,500)\nchunks_tuple = (10,500)\n\n\nda_arr = da.random.randint(10, size=size_tuple,\n                           chunks=chunks_tuple)\nda_arr2 = da.random.randint(10, size=size_tuple,\n                            chunks=chunks_tuple)\n\n\ndef random_func(x):\n    return np.mean((((x * 2).T)**2),axis=0)\n\n\ngufoo = da.gufunc(random_func, signature=\"(i)->()\",\n                  output_dtypes=float,\n                  vectorize=True)\n\n\nrandom_op_arr = gufoo(da_arr)\nrandom_op_arr.compute()\n\narray([112.056, 107.44 , 111.024, 109.656, 118.832, 109.84 , 117.2  ,\n       111.952, 116.312, 117.368, 128.568, 111.144, 110.656, 112.648,\n       115.24 , 114.624, 113.912, 109.632, 112.864, 113.488, 119.248,\n       121.4  , 108.272, 118.784, 114.968, 115.216, 107.872, 113.6  ,\n       112.456, 112.48 , 114.864, 119.28 , 112.656, 110.208, 109.728,\n       120.576, 119.632, 118.12 , 112.888, 116.384, 113.192, 106.84 ,\n       111.72 , 115.928, 106.08 , 114.568, 121.512, 115.384, 113.864,\n       107.104, 114.32 , 116.176, 117.28 , 116.976, 117.784, 110.088,\n       121.696, 114.2  , 113.864, 116.072, 112.344, 113.808, 113.968,\n       110.472, 119.536, 113.84 , 109.328, 116.552, 119.056, 113.84 ,\n       117.872, 114.928, 116.336, 115.192, 115.808, 106.984, 116.984,\n       114.536, 116.496, 111.968, 115.216, 108.24 , 119.52 , 116.136,\n       111.144, 111.712, 119.224, 114.312, 110.464, 110.216, 111.288,\n       119.6  , 108.264, 114.456, 119.016, 107.032, 114.832, 108.056,\n       105.712, 110.64 , 103.4  , 106.768, 118.216, 112.44 , 113.728,\n       114.6  , 117.832, 108.288, 117.92 , 113.12 , 121.984, 112.776,\n       123.144, 115.968, 112.44 , 115.712, 112.144, 108.448, 114.752,\n       108.376, 101.296, 102.992, 117.872, 114.056, 115.736, 115.528,\n       122.072, 130.168, 106.992, 109.912, 117.872, 112.152, 112.184,\n       113.544, 116.496, 112.832, 108.712, 116.96 , 120.984, 117.808,\n       112.272, 111.816, 118.872, 116.376, 118.992, 112.344, 124.672,\n        97.576, 112.496, 117.92 , 102.392, 109.992, 112.016, 117.92 ,\n       108.352, 112.376, 121.008, 117.808, 113.504, 125.592, 114.936,\n       111.456, 116.488, 104.744, 114.136, 114.   , 107.256, 117.84 ,\n       111.872, 109.152, 118.752, 112.32 , 116.16 , 106.696, 109.472,\n       111.968, 118.264, 115.088, 112.864, 110.016, 111.888, 111.84 ,\n       118.488, 107.952, 121.52 , 126.52 , 112.12 , 110.952, 115.328,\n       110.064, 106.36 , 118.96 , 109.68 , 117.776, 107.112, 111.152,\n       113.888, 113.408, 114.992, 117.632, 116.648, 117.112, 118.2  ,\n       116.36 , 113.104, 113.6  , 112.208, 112.592, 117.192, 102.832,\n       112.08 , 113.744, 116.048, 117.368, 113.96 , 111.24 , 121.824,\n       112.56 , 110.192, 130.776, 111.656, 119.984, 113.592, 113.592,\n       106.664, 125.192, 113.6  , 117.12 , 106.24 , 112.856, 114.544,\n       117.16 , 108.344, 112.208, 109.112, 124.824, 109.824, 106.352,\n       115.568, 112.64 , 112.904, 112.736, 112.52 , 124.808, 120.32 ,\n       114.472, 119.528, 113.456, 112.448, 118.672, 110.016, 116.16 ,\n       122.048, 111.088, 114.56 , 107.448, 115.328, 111.656, 108.688,\n       116.904, 110.8  , 108.896, 112.136, 115.896, 111.848, 108.808,\n       114.504, 124.552, 116.248, 114.576, 110.56 , 112.152, 117.576,\n       125.44 , 110.72 , 108.072, 115.192, 116.048, 107.76 , 111.376,\n       121.608, 115.256, 113.84 , 105.672, 115.024, 115.864, 114.304,\n       123.344, 114.624, 115.696, 113.288, 116.688, 109.048, 125.264,\n       118.8  , 112.2  , 114.312, 109.728, 116.064, 113.808, 106.912,\n       109.288, 117.   , 114.632, 114.456, 110.168, 111.976, 117.816,\n       110.04 , 103.048, 113.656, 112.504, 113.8  , 120.04 , 120.224,\n       110.68 , 110.096, 116.12 , 113.424, 107.408, 111.296, 111.512,\n       117.432, 105.96 , 115.992, 118.44 , 110.024, 119.216, 111.664,\n       119.184, 109.824, 116.736, 116.76 , 107.544, 120.44 , 115.08 ,\n       110.136, 112.144, 113.888, 111.32 , 109.952, 117.096, 111.152,\n       115.728, 110.832, 113.312, 113.664, 112.016, 111.952, 114.896,\n       114.728, 107.848, 108.832, 122.384, 111.824, 107.384, 117.504,\n       117.344, 110.144, 109.568, 101.36 , 111.944, 105.512, 115.792,\n       112.08 , 104.568, 109.008, 108.992, 114.936, 113.008, 120.088,\n       117.328, 117.008, 107.584, 111.688, 115.664, 108.416, 119.48 ,\n       107.336, 120.184, 111.952, 115.824, 113.928, 117.064, 114.296,\n       111.56 , 120.04 , 112.256, 115.368, 109.112, 112.184, 112.128,\n       111.288, 117.856, 109.184, 113.128, 119.888, 110.656, 111.992,\n       116.704, 107.696, 111.608, 121.504, 110.296, 111.008, 112.072,\n       117.072, 115.68 , 108.888, 117.704, 113.112, 101.144, 112.36 ,\n       122.688, 112.016, 111.64 , 113.992, 117.08 , 109.976, 108.048,\n       110.504, 112.936, 111.776, 117.392, 116.568, 106.896, 105.224,\n       115.512, 117.   , 116.192, 113.344, 111.776, 114.312, 113.008,\n       114.768, 121.712, 112.528, 108.976, 106.648, 107.8  , 122.696,\n       104.064, 117.072, 119.064, 111.472, 112.752, 109.52 , 123.712,\n       114.032, 120.888, 109.84 , 123.36 , 111.576, 118.56 , 116.328,\n       113.048, 111.68 , 106.072, 109.752, 112.32 , 114.344, 114.976,\n       114.072, 121.792, 113.024, 109.864, 115.84 , 115.752, 118.648,\n       107.52 , 116.104, 112.464, 123.232, 112.32 , 116.952, 106.32 ,\n       110.992, 111.256, 113.616, 111.344, 115.216, 121.504, 117.504,\n       115.816, 116.6  , 111.08 , 108.776, 110.672, 109.464, 107.096,\n       112.928, 106.8  , 110.4  , 112.576, 114.648, 113.272, 112.504,\n       112.888, 112.84 , 111.496])\n\n\n\nrandom_op_arr.shape\n\n(500,)\n\n\n\n@da.as_gufunc(signature=\"(m,n),(n,j)->(m,j)\", output_dtypes=int, allow_rechunk=True)\ndef random_func(x, y):\n    return np.matmul(x, y)**2\n\n\nda_arr3 = da.random.randint(10, size=(200, 100), chunks=(10, 100))\nda_arr4 = da.random.randint(10, size=(100, 300), chunks=(5,5))\n\n\n# random_matmul = random_func(da_arr3, da_arr4)\n# random_matmul.compute()\n\n\nrandom_matmul.shape\n\n(200, 300)\n\n\n\n# Dask 3\n\n\nmy_arr = da.random.randint(10, size=20, chunks=3)\n\n\nmy_arr.compute()\n\narray([3, 8, 8, 7, 4, 1, 5, 7, 2, 7, 4, 4, 8, 0, 9, 3, 6, 7, 1, 5])\n\n\n\nmy_hundred_arr = my_arr + 100\nmy_hundred_arr.compute()\n\narray([103, 108, 108, 107, 104, 101, 105, 107, 102, 107, 104, 104, 108,\n       100, 109, 103, 106, 107, 101, 105])\n\n\n\n(my_arr * (-1)).compute()\n\narray([-3, -8, -8, -7, -4, -1, -5, -7, -2, -7, -4, -4, -8,  0, -9, -3, -6,\n       -7, -1, -5])\n\n\n\ndask_sum = my_arr.sum()\ndask_sum\n\n\n\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  8 B   8 B \n     Shape  ()   () \n     Count  17 Tasks  1 Chunks \n     Type  int64  numpy.ndarray \n  \n\n\n\n\n\n\n\n\n\n\nmy_arr.compute()\n\narray([3, 8, 8, 7, 4, 1, 5, 7, 2, 7, 4, 4, 8, 0, 9, 3, 6, 7, 1, 5])\n\n\n\ndask_sum.compute()\n\n99\n\n\n\nmy_ones_arr = da.ones((10,10), chunks=2, dtype=int)\n\n\nmy_ones_arr.compute()\n\narray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n\n\n\nmy_ones_arr.mean(axis=0).compute()\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\nmy_custom_array = da.random.randint(10, size=(4,4), chunks=(1,4))\n\n\nmy_custom_array.compute()\n\narray([[0, 1, 7, 6],\n       [0, 1, 2, 4],\n       [6, 3, 5, 3],\n       [3, 2, 2, 6]])\n\n\n\nmy_custom_array.mean(axis=0).compute()\n\narray([2.25, 1.75, 4.  , 4.75])\n\n\n\nmy_custom_array.mean(axis=1).compute()\n\narray([3.5 , 1.75, 4.25, 3.25])\n\n\n\n## Slicing\n\n\nmy_custom_array[1:3, 2:4]\n\n\n\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  32 B   16 B \n     Shape  (2, 2)   (1, 2) \n     Count  6 Tasks  2 Chunks \n     Type  int64  numpy.ndarray \n  \n\n\n\n\n\n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  2\n  2\n\n\n\n\n\n\n\nmy_custom_array[1:3, 2:4].compute()\n\narray([[2, 4],\n       [5, 3]])\n\n\n\n## Broadcasting\n\n\nmy_custom_array.compute()\n\narray([[0, 1, 7, 6],\n       [0, 1, 2, 4],\n       [6, 3, 5, 3],\n       [3, 2, 2, 6]])\n\n\n\nmy_small_arr = da.ones(4, chunks=2)\nmy_small_arr.compute()\n\narray([1., 1., 1., 1.])\n\n\n\nbrd_example1 = da.add(my_custom_array, my_small_arr)\n\n\nbrd_example1.compute()\n\narray([[1., 2., 8., 7.],\n       [1., 2., 3., 5.],\n       [7., 4., 6., 4.],\n       [4., 3., 3., 7.]])\n\n\n\nten_arr = da.full_like(my_small_arr, 10)\n\n\nten_arr.compute()\n\narray([10., 10., 10., 10.])\n\n\n\nbrd_example2 = da.add(my_custom_array, ten_arr)\n\n\nbrd_example2.compute()\n\narray([[10., 11., 17., 16.],\n       [10., 11., 12., 14.],\n       [16., 13., 15., 13.],\n       [13., 12., 12., 16.]])\n\n\n\n## Reshaping\n\n\nmy_custom_array.shape\n\n(4, 4)\n\n\n\ncustom_arr_1d = my_custom_array.reshape(16)\n\n\ncustom_arr_1d\n\n\n\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  128 B   32 B \n     Shape  (16,)   (4,) \n     Count  8 Tasks  4 Chunks \n     Type  int64  numpy.ndarray \n  \n\n\n\n\n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  16\n  1\n\n\n\n\n\n\n\ncustom_arr_1d.compute()\n\narray([0, 1, 7, 6, 0, 1, 2, 4, 6, 3, 5, 3, 3, 2, 2, 6])\n\n\n\n# Stacking\n\n\nstacked_arr = da.stack([brd_example1, brd_example2])\n\n\nstacked_arr.compute()\n\narray([[[ 1.,  2.,  8.,  7.],\n        [ 1.,  2.,  3.,  5.],\n        [ 7.,  4.,  6.,  4.],\n        [ 4.,  3.,  3.,  7.]],\n\n       [[10., 11., 17., 16.],\n        [10., 11., 12., 14.],\n        [16., 13., 15., 13.],\n        [13., 12., 12., 16.]]])\n\n\n\nanother_stacked = da.stack([brd_example1, brd_example2], axis=1)\n\n\nanother_stacked.compute()\n\narray([[[ 1.,  2.,  8.,  7.],\n        [10., 11., 17., 16.]],\n\n       [[ 1.,  2.,  3.,  5.],\n        [10., 11., 12., 14.]],\n\n       [[ 7.,  4.,  6.,  4.],\n        [16., 13., 15., 13.]],\n\n       [[ 4.,  3.,  3.,  7.],\n        [13., 12., 12., 16.]]])\n\n\n\n# Concatenate\n\n\nconcate_arr = da.concatenate([brd_example1, brd_example2])\n\n\nconcate_arr.compute()\n\narray([[ 1.,  2.,  8.,  7.],\n       [ 1.,  2.,  3.,  5.],\n       [ 7.,  4.,  6.,  4.],\n       [ 4.,  3.,  3.,  7.],\n       [10., 11., 17., 16.],\n       [10., 11., 12., 14.],\n       [16., 13., 15., 13.],\n       [13., 12., 12., 16.]])\n\n\n\nanother_concate_arr = da.concatenate([brd_example1, brd_example2],axis=1)\n\n\nanother_concate_arr.compute()\n\narray([[ 1.,  2.,  8.,  7., 10., 11., 17., 16.],\n       [ 1.,  2.,  3.,  5., 10., 11., 12., 14.],\n       [ 7.,  4.,  6.,  4., 16., 13., 15., 13.],\n       [ 4.,  3.,  3.,  7., 13., 12., 12., 16.]])\n\n\n\n# Dask 4\n\n\nimport numpy as np\nimport dask.array as da\n\n\nsize_tuple = (18000,18000)\nnp_arr = np.random.randint(10, size=size_tuple)\nnp_arr2 = np.random.randint(10, size=size_tuple)\n\n\n%time (((np_arr * 2).T)**2 + np_arr2 + 100).sum(axis=1).mean()\n\nMemoryError: \n\n\n\nchunks_tuple = (500, 500)\nda_arr = da.from_array(np_arr, chunks=chunks_tuple)\nda_arr2 = da.from_array(np_arr2, chunks=chunks_tuple)\n\n\n%time (((da_arr * 2).T)**2 + da_arr2 + 100).sum(axis=1).mean().compute()\n\nCPU times: user 10.1 s, sys: 362 ms, total: 10.5 s\nWall time: 2.47 s\n\n\n3933124.5174444444\n\n\n\nsize_tuple = (50000, 50000)\nnp_arr = np.random.randint(10, size=size_tuple)\nnp_arr2 = np.random.randint(10, size=size_tuple)\n\nMemoryError: \n\n\n\nchunks_tuple = (5000, 5000)\nda_arr = da.random.randint(10, size=size_tuple,\n                           chunks=chunks_tuple)\nda_arr2 = da.random.randint(10, size=size_tuple,\n                            chunks=chunks_tuple)\n\n\n%time (((da_arr * 2).T)**2 + da_arr2 + 100).sum(axis=1).mean().compute()\n\nCPU times: user 3min 10s, sys: 10.5 s, total: 3min 20s\nWall time: 28.2 s\n\n\n10925051.41748\n\n\n\nda_arr.nbytes/1e+9\n\n20.0"
  },
  {
    "objectID": "posts/2020-10-09-dask_fiscal-data-sqlite-db.html",
    "href": "posts/2020-10-09-dask_fiscal-data-sqlite-db.html",
    "title": "Moving fiscal data from a pandas dataframe to a sqlite local database",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\n\ndf = pd.read_csv('df_panel_fix.csv')\n\n\ndf.columns\n\nIndex(['Unnamed: 0', 'province', 'specific', 'general', 'year', 'gdp', 'fdi',\n       'rnr', 'rr', 'i', 'fr', 'reg', 'it'],\n      dtype='object')\n\n\n\ndf_subset = df[[\"year\", \"reg\", \"province\", \"gdp\", \"fdi\", 'it',\"specific\"]]\ndf_subset\n\n\n\n\n\n  \n    \n      \n      year\n      reg\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.30\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.70\n      668128\n      3162299\n      656175.0\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      18753.73\n      1036576\n      2939778\n      844647.0\n    \n  \n\n360 rows  7 columns\n\n\n\n\ndf_subset.columns = [\"year\", \"region\", \"province\", \"gdp\", \"fdi\", 'it',\"specific\"]\n\n\ndf_subset\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.30\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.70\n      668128\n      3162299\n      656175.0\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      18753.73\n      1036576\n      2939778\n      844647.0\n    \n  \n\n360 rows  7 columns\n\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///fiscal_data.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nfiscal_data = db.Table('fiscal_data', metadata, \n    db.Column('year',db.Integer, nullable=True, index=False),\n    db.Column('region',db.String, nullable=True),\n    db.Column('province',db.String, nullable=True),\n    db.Column('gdp',db.String, nullable=True),\n    db.Column('fdi',db.Integer, nullable=True),\n    db.Column('it',db.Integer, nullable=True),\n    db.Column('specific', db.Integer, nullable=True)\n)\n\n\nmetadata.create_all(engine) #Creates the table\n\n\nfiscal_data\n\nTable('fiscal_data', MetaData(bind=None), Column('year', Integer(), table=<fiscal_data>), Column('region', String(), table=<fiscal_data>), Column('province', String(), table=<fiscal_data>), Column('gdp', String(), table=<fiscal_data>), Column('fdi', Integer(), table=<fiscal_data>), Column('it', Integer(), table=<fiscal_data>), Column('specific', Integer(), table=<fiscal_data>), schema=None)\n\n\n\ndf_subset.to_sql('fiscal_data', con=engine, if_exists='append', index=False)\n\n\nengine.execute(\"SELECT year, region, province, gdp FROM fiscal_data LIMIT 10\").fetchall()\n\n[(1996, 'East China', 'Anhui', '2093.3'),\n (1997, 'East China', 'Anhui', '2347.32'),\n (1998, 'East China', 'Anhui', '2542.96'),\n (1999, 'East China', 'Anhui', '2712.34'),\n (2000, 'East China', 'Anhui', '2902.09'),\n (2001, 'East China', 'Anhui', '3246.71'),\n (2002, 'East China', 'Anhui', '3519.72'),\n (2003, 'East China', 'Anhui', '3923.11'),\n (2004, 'East China', 'Anhui', '4759.3'),\n (2005, 'East China', 'Anhui', '5350.17')]\n\n\n\nsql = \"\"\"\nSELECT year\n, region\n, province\n, gdp \nFROM fiscal_data\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf.tail(30)\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n    \n  \n  \n    \n      330\n      2002\n      Northwest China\n      Xinjiang\n      1612.65\n    \n    \n      331\n      2003\n      Northwest China\n      Xinjiang\n      1886.35\n    \n    \n      332\n      2004\n      Northwest China\n      Xinjiang\n      2209.09\n    \n    \n      333\n      2005\n      Northwest China\n      Xinjiang\n      2604.19\n    \n    \n      334\n      2006\n      Northwest China\n      Xinjiang\n      3045.26\n    \n    \n      335\n      2007\n      Northwest China\n      Xinjiang\n      3523.16\n    \n    \n      336\n      1996\n      Southwest China\n      Yunnan\n      1517.69\n    \n    \n      337\n      1997\n      Southwest China\n      Yunnan\n      1676.17\n    \n    \n      338\n      1998\n      Southwest China\n      Yunnan\n      1831.33\n    \n    \n      339\n      1999\n      Southwest China\n      Yunnan\n      1899.82\n    \n    \n      340\n      2000\n      Southwest China\n      Yunnan\n      2011.19\n    \n    \n      341\n      2001\n      Southwest China\n      Yunnan\n      2138.31\n    \n    \n      342\n      2002\n      Southwest China\n      Yunnan\n      2312.82\n    \n    \n      343\n      2003\n      Southwest China\n      Yunnan\n      2556.02\n    \n    \n      344\n      2004\n      Southwest China\n      Yunnan\n      3081.91\n    \n    \n      345\n      2005\n      Southwest China\n      Yunnan\n      3462.73\n    \n    \n      346\n      2006\n      Southwest China\n      Yunnan\n      3988.14\n    \n    \n      347\n      2007\n      Southwest China\n      Yunnan\n      4772.52\n    \n    \n      348\n      1996\n      East China\n      Zhejiang\n      4188.53\n    \n    \n      349\n      1997\n      East China\n      Zhejiang\n      4686.11\n    \n    \n      350\n      1998\n      East China\n      Zhejiang\n      5052.62\n    \n    \n      351\n      1999\n      East China\n      Zhejiang\n      5443.92\n    \n    \n      352\n      2000\n      East China\n      Zhejiang\n      6141.03\n    \n    \n      353\n      2001\n      East China\n      Zhejiang\n      6898.34\n    \n    \n      354\n      2002\n      East China\n      Zhejiang\n      8003.67\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.7\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      18753.73\n    \n  \n\n\n\n\n\n#http://manpages.ubuntu.com/manpages/precise/man1/sqlite3.1.html\n\n\n# sqlite3 fiscal_data.db\n\n# create table memos(text, priority INTEGER);\n# insert into memos values('example 1', 10);\n# insert into memos values('example 2', 100);\n# select * from memos;\n\n# sqlite3 -line fiscal_data.db 'select * from memos where priority > 20;'"
  },
  {
    "objectID": "posts/2021-06-02-model-calibration.html",
    "href": "posts/2021-06-02-model-calibration.html",
    "title": "Model Calibration",
    "section": "",
    "text": "#code adapted from https://github.com/thomasjpfan/ml-workshop-intermediate-1-of-2"
  },
  {
    "objectID": "posts/2021-06-02-model-calibration.html#create-dummy-dataset",
    "href": "posts/2021-06-02-model-calibration.html#create-dummy-dataset",
    "title": "Model Calibration",
    "section": "Create dummy dataset",
    "text": "Create dummy dataset\n\n# %load solutions/classifier_example.py\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\nrf.score(X_test, y_test)\n\ny_pred = rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7139\n           1       0.17      0.01      0.01       129\n\n    accuracy                           0.98      7268\n   macro avg       0.57      0.50      0.50      7268\nweighted avg       0.97      0.98      0.97      7268\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n\nTrain linear model\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nlr = make_pipeline(StandardScaler(), LogisticRegression(random_state=42))\nlr.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression(random_state=42))])StandardScalerStandardScaler()LogisticRegressionLogisticRegression(random_state=42)"
  },
  {
    "objectID": "posts/2021-06-02-model-calibration.html#calibration-curve",
    "href": "posts/2021-06-02-model-calibration.html#calibration-curve",
    "title": "Model Calibration",
    "section": "Calibration curve",
    "text": "Calibration curve\n\nfrom sklearn.calibration import calibration_curve\n\n\nlr_proba = lr.predict_proba(X_test)\n\n\nprob_true, prod_pred = calibration_curve(y_test, lr_proba[:, 1], n_bins=5)\n\nprint(prob_true)\nprint(prod_pred)\n\n[0.01750517 0.15384615]\n[0.01906858 0.22387283]\n\n\n\nplot_calibration_curve(y_test, lr_proba[:, 1]);\n\n\n\n\n\nfrom sklearn.metrics import brier_score_loss\n\n\nlr_brier = brier_score_loss(y_test, lr_proba[:, 1])\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 8))\nplot_calibration_curve(y_test, lr_proba[:, 1], n_bins=5, ax=ax1)\nax1.set_title(\"n_bins=5\")\nplot_calibration_curve(y_test, lr_proba[:, 1], n_bins=10, ax=ax2)\nax2.set_title(\"n_bins=10\")\nplot_calibration_curve(y_test, lr_proba[:, 1], n_bins=30, ax=ax3)\nax3.set_title(\"n_bins=30\")\n\nText(0.5, 1.0, 'n_bins=30')\n\n\n\n\n\n\nTrain Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nrf = RandomForestClassifier(random_state=0)\nrf.fit(X_train, y_train)\n\nRandomForestClassifierRandomForestClassifier(random_state=0)\n\n\n\nrf_proba = rf.predict_proba(X_test)\n\n\nrf_brier = brier_score_loss(y_test, rf_proba[:, 1])\nrf_brier\n\n0.0188862960924601\n\n\n\n\nTrain Single Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ntree = DecisionTreeClassifier(random_state=0)\ntree.fit(X_train, y_train)\n\nDecisionTreeClassifierDecisionTreeClassifier(random_state=0)\n\n\n\ntree_proba = tree.predict_proba(X_test)\n\n\ntree_brier = brier_score_loss(y_test, tree_proba[:, 1])\ntree_brier\n\n0.03880022014309301\n\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 8))\nplot_calibration_curve(y_test, lr_proba[:, 1], n_bins=10, ax=ax1)\nax1.set_title(f\"LogisticRegression: {lr_brier:0.4f}\")\nplot_calibration_curve(y_test, tree_proba[:, 1], n_bins=10, ax=ax2)\nax2.set_title(f\"DecisionTreeClassifier: {tree_brier:0.4f}\")\nplot_calibration_curve(y_test, rf_proba[:, 1], n_bins=10, ax=ax3)\nax3.set_title(f\"RandomForestClassifier: {rf_brier:0.4f}\");"
  },
  {
    "objectID": "posts/2021-06-02-model-calibration.html#exercise-1",
    "href": "posts/2021-06-02-model-calibration.html#exercise-1",
    "title": "Model Calibration",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nTrain a sklearn.naive_bayes.GaussianNB on the training set.\nCompute the brier score loss on the test set for the GuassianNB.\nPlot the calibration curve with n_bins=10.\n\n\n# %load solutions/02-ex01-solutions.py\nfrom sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB().fit(X_train, y_train)\n\nnb_proba = nb.predict_proba(X_test)\n\nbrier_score_loss(y_test, nb_proba[:, 1])\n\nplot_calibration_curve(y_test, nb_proba[:, 1], n_bins=10)"
  },
  {
    "objectID": "posts/2021-06-02-model-calibration.html#calibration",
    "href": "posts/2021-06-02-model-calibration.html#calibration",
    "title": "Model Calibration",
    "section": "Calibration",
    "text": "Calibration\n\nfrom sklearn.calibration import CalibratedClassifierCV\n\n\nrf = RandomForestClassifier(random_state=0)\ncal_rf = CalibratedClassifierCV(rf, method=\"isotonic\")\ncal_rf.fit(X_train, y_train)\n\nCalibratedClassifierCVCalibratedClassifierCV(base_estimator=RandomForestClassifier(random_state=0),\n                       method='isotonic')RandomForestClassifierRandomForestClassifier(random_state=0)\n\n\n\ncal_rf_proba = cal_rf.predict_proba(X_test)\n\n\ncal_rf_brier = brier_score_loss(y_test, cal_rf_proba[:, 1])\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nplot_calibration_curve(y_test, rf_proba[:, 1], ax=ax1, n_bins=10)\nax1.set_title(f\"forest no calibration: {rf_brier:0.4f}\")\nplot_calibration_curve(y_test, cal_rf_proba[:, 1], ax=ax2, n_bins=10)\nax2.set_title(f\"calibrated: {cal_rf_brier:0.4f}\");\n\n\n\n\n\nCalibrating the linear model\n\nlr = make_pipeline(StandardScaler(), LogisticRegression(random_state=42))\ncal_lr = CalibratedClassifierCV(lr, method='isotonic')\ncal_lr.fit(X_train, y_train)\n\nCalibratedClassifierCVCalibratedClassifierCV(base_estimator=Pipeline(steps=[('standardscaler',\n                                                       StandardScaler()),\n                                                      ('logisticregression',\n                                                       LogisticRegression(random_state=42))]),\n                       method='isotonic')StandardScalerStandardScaler()LogisticRegressionLogisticRegression(random_state=42)\n\n\n\ncal_lr_proba = cal_lr.predict_proba(X_test)\n\ncal_lr_brier = brier_score_loss(y_test, cal_lr_proba[:, 1])\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nplot_calibration_curve(y_test, lr_proba[:, 1], ax=ax1, n_bins=10)\nax1.set_title(f\"no calibration: {lr_brier:0.4f}\")\nplot_calibration_curve(y_test, cal_lr_proba[:, 1], ax=ax2, n_bins=10)\nax2.set_title(f\"calibrated: {cal_lr_brier:0.4f}\");"
  },
  {
    "objectID": "posts/2021-05-18-groupby_pivot.html",
    "href": "posts/2021-05-18-groupby_pivot.html",
    "title": "Groupby and Pivot Tables in Python",
    "section": "",
    "text": "import pandas as pd"
  },
  {
    "objectID": "posts/2021-05-18-groupby_pivot.html#distributions-of-dependant-variables",
    "href": "posts/2021-05-18-groupby_pivot.html#distributions-of-dependant-variables",
    "title": "Groupby and Pivot Tables in Python",
    "section": "Distributions of Dependant Variables",
    "text": "Distributions of Dependant Variables\n\nRight skew\n\ndf.hist(column=['fdi'], bins=60)\n\n\nsns.histplot(df['fdi'])\n\n\nsns.displot(df['gdp'])\n\n\nsns.displot(df['fdi'])\n\n\nsns.displot(df['it'])\n\n\nsns.displot(df['specific'].dropna())\n\n\ndf.hist(column=['fdi'], bins=60)"
  },
  {
    "objectID": "posts/2021-05-18-groupby_pivot.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "href": "posts/2021-05-18-groupby_pivot.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "title": "Groupby and Pivot Tables in Python",
    "section": "Removal of GDP value outliers more than 3 standard deviations away from the mean",
    "text": "Removal of GDP value outliers more than 3 standard deviations away from the mean"
  },
  {
    "objectID": "posts/2021-05-18-groupby_pivot.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "href": "posts/2021-05-18-groupby_pivot.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "title": "Groupby and Pivot Tables in Python",
    "section": "outlier removal of rows with GDP values that are > 3 standard deviations away form the mean",
    "text": "outlier removal of rows with GDP values that are > 3 standard deviations away form the mean\n\nimport scipy.stats as stats\n\n\ndf['gdp_zscore'] = stats.zscore(df['gdp'])"
  },
  {
    "objectID": "posts/2021-05-18-groupby_pivot.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "href": "posts/2021-05-18-groupby_pivot.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "title": "Groupby and Pivot Tables in Python",
    "section": "these are the observations more then > 3 SDs away from the mean of gdp that will be dropped",
    "text": "these are the observations more then > 3 SDs away from the mean of gdp that will be dropped\n\ndf[abs(df['gdp_zscore'])>3].hist(column = ['gdp'])\n\n\ndf_no_gdp_outliers=df[abs(df['gdp_zscore'])<3]\n\n\ndf_no_gdp_outliers\n\n\ndf_no_gdp_outliers.hist(column=['gdp'], bins=60)\n\n\ncounts_fiscal=df.groupby('region').count()\ncounts_fiscal\n\n\ncounts_fiscal=df.groupby('province').count()\ncounts_fiscal\n\n\n#df_no_gdp_outliers.pivot_table(index='grouping column 1', columns='grouping column 2', values='aggregating column', aggfunc='sum')\n\n\n#pd.crosstab(df_no_gdp_outliers, 'year')\n\n\ndf_no_gdp_outliers_subset = df_no_gdp_outliers[['region', 'gdp', 'fdi', 'it']]\ndf_no_gdp_outliers_subset\n\n\ndef aggregate_and_ttest(dataset, groupby_feature='province', alpha=.05, test_cells = [0, 1]):\n    #Imports\n    from tqdm import tqdm\n    from scipy.stats import ttest_ind_from_stats\n\n    \n    metrics = ['gdp', 'fdi', 'it']\n    \n    feature_size = 'size'\n    feature_mean = 'mean'\n    feature_std = 'std'    \n\n    for metric in tqdm(metrics):\n        \n        #print(metric)\n        crosstab = dataset.groupby(groupby_feature, as_index=False)[metric].agg(['size', 'mean', 'std'])\n        print(crosstab)\n        \n        treatment = crosstab.index[test_cells[0]]\n        control = crosstab.index[test_cells[1]]\n        \n        counts_control = crosstab.loc[control, feature_size]\n        counts_treatment = crosstab.loc[treatment, feature_size]\n\n        mean_control = crosstab.loc[control, feature_mean]\n        mean_treatment = crosstab.loc[treatment, feature_mean]\n\n        standard_deviation_control = crosstab.loc[control, feature_std]\n        standard_deviation_treatment = crosstab.loc[treatment, feature_std]\n        \n        t_statistic, p_value = ttest_ind_from_stats(mean1=mean_treatment, std1=standard_deviation_treatment, nobs1=counts_treatment,mean2=mean_control,std2=standard_deviation_control,nobs2=counts_control)\n        \n        #fstring to print the p value and t statistic\n        print(f\"The t statistic of the comparison of the treatment test cell of {treatment} compared to the control test cell of {control} for the metric of {metric} is {t_statistic} and the p value is {p_value}.\")\n        \n        #f string to say of the comparison is significant at a given alpha level\n\n        if p_value < alpha: \n            print(f'The comparison between {treatment} and {control} is statistically significant at the threshold of {alpha}') \n        else: \n            print(f'The comparison between {treatment} and {control} is not statistically significant at the threshold of {alpha}')\n\n\naggregate_and_ttest(df_no_gdp_outliers, test_cells = [0,2])\n\n\nEastvNorth=pd.DataFrame()\nEastvNorth= aggregate_and_ttest(df_no_gdp_outliers_subset, test_cells = [0,1])\nEastvNorth\n\n\nimport numpy as np\nimport bootstrapped.bootstrap as bs\nimport bootstrapped.stats_functions as bs_stats\n\n\ntest_1=df_no_gdp_outliers[df_no_gdp_outliers['province']=='Beijing']\ntest=test_1['gdp'].to_numpy()\ntest\n\ncontrol_1=df_no_gdp_outliers[df_no_gdp_outliers['province']=='Shanxi']\ncontrol=control_1['gdp'].to_numpy()\ncontrol\n\n\nbins = np.linspace(0, 40, 20)\n\nplt.hist(control, label='Control')\nplt.hist(test, label='Test', color='orange')\nplt.title('Test/Ctrl Data')\nplt.legend()\n\n\nbs.bootstrap_ab(test, control, stat_func=bs_stats.sum, compare_func=bs_compare.percent_change)\n\n\n# run an a/b test simulation considering the lengths of the series (sum)\n# consider the full 'volume' of values that are passed in\n\nprint(bs_compare.percent_change(test.sum(), control.sum()))\n\nprint(bs.bootstrap_ab(\n    test, \n    control, \n    stat_func=bs_stats.sum,\n    compare_func=bs_compare.percent_change\n))\n\n\n# run an a/b test simulation ignoring the lengths of the series (average)\n# just what is the 'typical' value\n# use percent change to compare test and control\n\nprint(bs_compare.difference(test.mean(), control.mean()))\n\n\nprint(bs.bootstrap_ab(test, control, bs_stats.mean, bs_compare.difference))"
  },
  {
    "objectID": "posts/2020-10-14-dask-xgboost-ht-ohe.html",
    "href": "posts/2020-10-14-dask-xgboost-ht-ohe.html",
    "title": "Using dask_ml.preprocessing and OneHotEncoder for categorical encoding with Dask",
    "section": "",
    "text": "import numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine(\"sqlite:///fiscal.db\")\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\n#engine.execute(\"SELECT * FROM fiscal_data LIMIT 1\").fetchall()\n\n\nsql = \"\"\"\nSELECT year\n, region\n, province\n, gdp\n, fdi\n, it\n, specific\nFROM fiscal_table\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n\n\n\n\nClient\n\n  Scheduler: inproc://192.168.1.71/9390/1\n  Dashboard: http://192.168.1.71:8787/status\n\n\n\nCluster\n\n  Workers: 3\n  Cores: 6\n  Memory: 12.00 GB\n\n\n\n\n\n\n\nclient.restart()\n\n\n\n\n\nClient\n\n  Scheduler: inproc://192.168.1.71/9390/1\n  Dashboard: http://192.168.1.71:8787/status\n\n\n\nCluster\n\n  Workers: 3\n  Cores: 6\n  Memory: 12.00 GB\n\n\n\n\n\n\n\nfrom dask import dataframe as dd \n\n\nddf = dd.from_pandas(df, npartitions=5)\n\n\nprint(ddf)\n\nDask DataFrame Structure:\n                year  region province      gdp    fdi     it specific\nnpartitions=5                                                        \n0              int64  object   object  float64  int64  int64  float64\n72               ...     ...      ...      ...    ...    ...      ...\n...              ...     ...      ...      ...    ...    ...      ...\n288              ...     ...      ...      ...    ...    ...      ...\n359              ...     ...      ...      ...    ...    ...      ...\nDask Name: from_pandas, 5 tasks\n\n\n\nddf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.30\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n  \n\n\n\n\n\nclient.id\n\n'Client-0ac0cc94-0e22-11eb-a4ae-d71460f30774'\n\n\n\n# Selecting Features and Target\n\n\nfeat_list = [\"year\", \"fdi\"]\ncat_feat_list = [\"region\", \"province\"]\ntarget = [\"gdp\"]\n\n\nddf[\"year\"] = ddf[\"year\"].astype(int)\nddf[\"fdi\"] = ddf[\"fdi\"].astype(float)\nddf[\"gdp\"] = ddf[\"gdp\"].astype(float)\nddf[\"it\"] = ddf[\"it\"].astype(float)\n\n\n#OHE\nfrom dask_ml.preprocessing import OneHotEncoder\n\n\nddf = ddf.categorize(cat_feat_list)\n\n\nohe = OneHotEncoder(sparse=False)\n\n\nohe_ddf = ohe.fit_transform(ddf[cat_feat_list])\n\n\nfeat_list = feat_list + ohe_ddf.columns.tolist()\nfeat_list = [f for f in feat_list if f not in cat_feat_list]\n#client.close()\n\n\nddf_processed = (dd.concat([ddf,ohe_ddf], axis=1) [feat_list + target])\n\n\nddf_processed.compute()\n\n\n\n\n\n  \n    \n      \n      year\n      fdi\n      region_East China\n      region_North China\n      region_Southwest China\n      region_Northwest China\n      region_South Central China\n      region_Northeast China\n      province_Anhui\n      province_Beijing\n      ...\n      province_Shandong\n      province_Shanghai\n      province_Shanxi\n      province_Sichuan\n      province_Tianjin\n      province_Tibet\n      province_Xinjiang\n      province_Yunnan\n      province_Zhejiang\n      gdp\n    \n  \n  \n    \n      0\n      1996\n      50661.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2093.30\n    \n    \n      1\n      1997\n      43443.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2347.32\n    \n    \n      2\n      1998\n      27673.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2542.96\n    \n    \n      3\n      1999\n      26131.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2712.34\n    \n    \n      4\n      2000\n      31847.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2902.09\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      498055.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      9705.02\n    \n    \n      356\n      2004\n      668128.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      11648.70\n    \n    \n      357\n      2005\n      772000.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      13417.68\n    \n    \n      358\n      2006\n      888935.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      15718.47\n    \n    \n      359\n      2007\n      1036576.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      18753.73\n    \n  \n\n360 rows  39 columns\n\n\n\n\nclient.restart()\n\n\nclient.close()"
  },
  {
    "objectID": "posts/2021-05-30-classification using health data with pycaret 1.html",
    "href": "posts/2021-05-30-classification using health data with pycaret 1.html",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "",
    "text": "#Fetal cardiotocography example\n#Code from https://github.com/pycaret/pycaret/\n#Dataset link: https://www.kaggle.com/akshat0007/fetalhr"
  },
  {
    "objectID": "posts/2021-05-30-classification using health data with pycaret 1.html#performing-some-basic-preprocessing-techniques",
    "href": "posts/2021-05-30-classification using health data with pycaret 1.html#performing-some-basic-preprocessing-techniques",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "Performing some basic preprocessing techniques",
    "text": "Performing some basic preprocessing techniques\n\n## This will print the number of columns and rows\nprint(df.shape)\n\n(2129, 35)\n\n\n\n## Checking for the null values\ndf.isnull().sum()\n\nLBE         3\nLB          3\nAC          3\nFM          2\nUC          2\nASTV        2\nMSTV        2\nALTV        2\nMLTV        2\nDL          1\nDS          1\nDP          1\nDR          1\nWidth       3\nMin         3\nMax         3\nNmax        3\nNzeros      3\nMode        3\nMean        3\nMedian      3\nVariance    3\nTendency    3\nA           3\nB           3\nC           3\nD           3\nE           3\nAD          3\nDE          3\nLD          3\nFS          3\nSUSP        3\nCLASS       3\nNSP         3\ndtype: int64\n\n\n\n## Dropping the the rows containing null values\ndf=df.dropna()\n\n\ndf.isnull().sum()\n\nLBE         0\nLB          0\nAC          0\nFM          0\nUC          0\nASTV        0\nMSTV        0\nALTV        0\nMLTV        0\nDL          0\nDS          0\nDP          0\nDR          0\nWidth       0\nMin         0\nMax         0\nNmax        0\nNzeros      0\nMode        0\nMean        0\nMedian      0\nVariance    0\nTendency    0\nA           0\nB           0\nC           0\nD           0\nE           0\nAD          0\nDE          0\nLD          0\nFS          0\nSUSP        0\nCLASS       0\nNSP         0\ndtype: int64\n\n\n\n## Checking the data type of the columns\ndf.dtypes\n\nLBE         float64\nLB          float64\nAC          float64\nFM          float64\nUC          float64\nASTV        float64\nMSTV        float64\nALTV        float64\nMLTV        float64\nDL          float64\nDS          float64\nDP          float64\nDR          float64\nWidth       float64\nMin         float64\nMax         float64\nNmax        float64\nNzeros      float64\nMode        float64\nMean        float64\nMedian      float64\nVariance    float64\nTendency    float64\nA           float64\nB           float64\nC           float64\nD           float64\nE           float64\nAD          float64\nDE          float64\nLD          float64\nFS          float64\nSUSP        float64\nCLASS       float64\nNSP         float64\ndtype: object"
  },
  {
    "objectID": "posts/2021-05-30-classification using health data with pycaret 1.html#importing-the-pycaret-library",
    "href": "posts/2021-05-30-classification using health data with pycaret 1.html#importing-the-pycaret-library",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "Importing the pycaret library",
    "text": "Importing the pycaret library\n\n# This command will basically import all the modules from pycaret that are necessary for classification tasks\nfrom pycaret.classification import *\n\n\n# Setting up the classifier\n# Pass the complete dataset as data and the featured to be predicted as target\nclf=setup(data=df,target='NSP')\n\n\n                    Description        Value    \n                \n                        0\n                        session_id\n                        7481\n            \n            \n                        1\n                        Target\n                        NSP\n            \n            \n                        2\n                        Target Type\n                        Multiclass\n            \n            \n                        3\n                        Label Encoded\n                        None\n            \n            \n                        4\n                        Original Data\n                        (2126, 35)\n            \n            \n                        5\n                        Missing Values\n                        False\n            \n            \n                        6\n                        Numeric Features\n                        23\n            \n            \n                        7\n                        Categorical Features\n                        11\n            \n            \n                        8\n                        Ordinal Features\n                        False\n            \n            \n                        9\n                        High Cardinality Features\n                        False\n            \n            \n                        10\n                        High Cardinality Method\n                        None\n            \n            \n                        11\n                        Transformed Train Set\n                        (1488, 33)\n            \n            \n                        12\n                        Transformed Test Set\n                        (638, 33)\n            \n            \n                        13\n                        Shuffle Train-Test\n                        True\n            \n            \n                        14\n                        Stratify Train-Test\n                        False\n            \n            \n                        15\n                        Fold Generator\n                        StratifiedKFold\n            \n            \n                        16\n                        Fold Number\n                        10\n            \n            \n                        17\n                        CPU Jobs\n                        -1\n            \n            \n                        18\n                        Use GPU\n                        False\n            \n            \n                        19\n                        Log Experiment\n                        False\n            \n            \n                        20\n                        Experiment Name\n                        clf-default-name\n            \n            \n                        21\n                        USI\n                        3ae4\n            \n            \n                        22\n                        Imputation Type\n                        simple\n            \n            \n                        23\n                        Iterative Imputation Iteration\n                        None\n            \n            \n                        24\n                        Numeric Imputer\n                        mean\n            \n            \n                        25\n                        Iterative Imputation Numeric Model\n                        None\n            \n            \n                        26\n                        Categorical Imputer\n                        constant\n            \n            \n                        27\n                        Iterative Imputation Categorical Model\n                        None\n            \n            \n                        28\n                        Unknown Categoricals Handling\n                        least_frequent\n            \n            \n                        29\n                        Normalize\n                        False\n            \n            \n                        30\n                        Normalize Method\n                        None\n            \n            \n                        31\n                        Transformation\n                        False\n            \n            \n                        32\n                        Transformation Method\n                        None\n            \n            \n                        33\n                        PCA\n                        False\n            \n            \n                        34\n                        PCA Method\n                        None\n            \n            \n                        35\n                        PCA Components\n                        None\n            \n            \n                        36\n                        Ignore Low Variance\n                        False\n            \n            \n                        37\n                        Combine Rare Levels\n                        False\n            \n            \n                        38\n                        Rare Level Threshold\n                        None\n            \n            \n                        39\n                        Numeric Binning\n                        False\n            \n            \n                        40\n                        Remove Outliers\n                        False\n            \n            \n                        41\n                        Outliers Threshold\n                        None\n            \n            \n                        42\n                        Remove Multicollinearity\n                        False\n            \n            \n                        43\n                        Multicollinearity Threshold\n                        None\n            \n            \n                        44\n                        Clustering\n                        False\n            \n            \n                        45\n                        Clustering Iteration\n                        None\n            \n            \n                        46\n                        Polynomial Features\n                        False\n            \n            \n                        47\n                        Polynomial Degree\n                        None\n            \n            \n                        48\n                        Trignometry Features\n                        False\n            \n            \n                        49\n                        Polynomial Threshold\n                        None\n            \n            \n                        50\n                        Group Features\n                        False\n            \n            \n                        51\n                        Feature Selection\n                        False\n            \n            \n                        52\n                        Feature Selection Method\n                        classic\n            \n            \n                        53\n                        Features Selection Threshold\n                        None\n            \n            \n                        54\n                        Feature Interaction\n                        False\n            \n            \n                        55\n                        Feature Ratio\n                        False\n            \n            \n                        56\n                        Interaction Threshold\n                        None\n            \n            \n                        57\n                        Fix Imbalance\n                        False\n            \n            \n                        58\n                        Fix Imbalance Method\n                        SMOTE\n            \n    \n\n\n\n# This model will be used to compare all the model along with the cross validation\ncompare_models()\n\n\n                    Model        Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC        TT (Sec)    \n                \n                        et\n                        Extra Trees Classifier\n                        0.9906\n                        0.9950\n                        0.9786\n                        0.9908\n                        0.9905\n                        0.9733\n                        0.9737\n                        0.0740\n            \n            \n                        rf\n                        Random Forest Classifier\n                        0.9899\n                        0.9979\n                        0.9770\n                        0.9901\n                        0.9898\n                        0.9714\n                        0.9718\n                        0.0940\n            \n            \n                        lightgbm\n                        Light Gradient Boosting Machine\n                        0.9899\n                        0.9983\n                        0.9783\n                        0.9901\n                        0.9898\n                        0.9714\n                        0.9718\n                        0.0780\n            \n            \n                        gbc\n                        Gradient Boosting Classifier\n                        0.9893\n                        0.9978\n                        0.9767\n                        0.9894\n                        0.9891\n                        0.9695\n                        0.9699\n                        0.2870\n            \n            \n                        dt\n                        Decision Tree Classifier\n                        0.9872\n                        0.9812\n                        0.9758\n                        0.9874\n                        0.9872\n                        0.9641\n                        0.9643\n                        0.0090\n            \n            \n                        ridge\n                        Ridge Classifier\n                        0.9845\n                        0.0000\n                        0.9624\n                        0.9848\n                        0.9842\n                        0.9556\n                        0.9565\n                        0.0070\n            \n            \n                        lda\n                        Linear Discriminant Analysis\n                        0.9845\n                        0.9965\n                        0.9624\n                        0.9848\n                        0.9842\n                        0.9556\n                        0.9565\n                        0.0090\n            \n            \n                        lr\n                        Logistic Regression\n                        0.9839\n                        0.9969\n                        0.9690\n                        0.9841\n                        0.9836\n                        0.9542\n                        0.9548\n                        0.4540\n            \n            \n                        nb\n                        Naive Bayes\n                        0.9664\n                        0.9902\n                        0.9696\n                        0.9702\n                        0.9674\n                        0.9096\n                        0.9115\n                        0.0070\n            \n            \n                        svm\n                        SVM - Linear Kernel\n                        0.9153\n                        0.0000\n                        0.7730\n                        0.9222\n                        0.9085\n                        0.7477\n                        0.7579\n                        0.0140\n            \n            \n                        ada\n                        Ada Boost Classifier\n                        0.9100\n                        0.9843\n                        0.9211\n                        0.9629\n                        0.9106\n                        0.8425\n                        0.8624\n                        0.0460\n            \n            \n                        knn\n                        K Neighbors Classifier\n                        0.9079\n                        0.9264\n                        0.7654\n                        0.9066\n                        0.9028\n                        0.7173\n                        0.7250\n                        0.0150\n            \n            \n                        qda\n                        Quadratic Discriminant Analysis\n                        0.7137\n                        0.8002\n                        0.7495\n                        0.8723\n                        0.7475\n                        0.4461\n                        0.5074\n                        0.0070\n            \n    \n\n\nExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n                     criterion='gini', max_depth=None, max_features='auto',\n                     max_leaf_nodes=None, max_samples=None,\n                     min_impurity_decrease=0.0, min_impurity_split=None,\n                     min_samples_leaf=1, min_samples_split=2,\n                     min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n                     oob_score=False, random_state=7481, verbose=0,\n                     warm_start=False)\n\n\n\nThe AUC score is 0.000 because it is not supported for the muli-classification tasks\n\n\nAlso, from the above it is understood that Extreme Gradient Boosting(popularly known as XGBoost) model really performed well. So, we will proceed with Extreme Gradient Boosting model."
  },
  {
    "objectID": "posts/2021-05-30-classification using health data with pycaret 1.html#creating-the-extreme-gradient-boostingxgboost-model",
    "href": "posts/2021-05-30-classification using health data with pycaret 1.html#creating-the-extreme-gradient-boostingxgboost-model",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "Creating the Extreme Gradient Boosting(XGBoost) model",
    "text": "Creating the Extreme Gradient Boosting(XGBoost) model\n\nxgboost_classifier=create_model('rf')\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9866\n                        0.9992\n                        0.9683\n                        0.9868\n                        0.9863\n                        0.9616\n                        0.9624\n            \n            \n                        1\n                        0.9866\n                        0.9990\n                        0.9813\n                        0.9866\n                        0.9866\n                        0.9625\n                        0.9625\n            \n            \n                        2\n                        0.9933\n                        1.0000\n                        0.9841\n                        0.9933\n                        0.9932\n                        0.9810\n                        0.9812\n            \n            \n                        3\n                        1.0000\n                        1.0000\n                        1.0000\n                        1.0000\n                        1.0000\n                        1.0000\n                        1.0000\n            \n            \n                        4\n                        0.9933\n                        1.0000\n                        0.9833\n                        0.9938\n                        0.9933\n                        0.9813\n                        0.9814\n            \n            \n                        5\n                        0.9866\n                        0.9981\n                        0.9667\n                        0.9868\n                        0.9863\n                        0.9617\n                        0.9625\n            \n            \n                        6\n                        0.9866\n                        0.9861\n                        0.9667\n                        0.9868\n                        0.9863\n                        0.9617\n                        0.9625\n            \n            \n                        7\n                        0.9799\n                        0.9974\n                        0.9389\n                        0.9797\n                        0.9795\n                        0.9424\n                        0.9431\n            \n            \n                        8\n                        0.9932\n                        0.9994\n                        0.9972\n                        0.9936\n                        0.9933\n                        0.9810\n                        0.9812\n            \n            \n                        9\n                        0.9932\n                        0.9993\n                        0.9833\n                        0.9933\n                        0.9932\n                        0.9805\n                        0.9807\n            \n            \n                        Mean\n                        0.9899\n                        0.9979\n                        0.9770\n                        0.9901\n                        0.9898\n                        0.9714\n                        0.9718\n            \n            \n                        SD\n                        0.0054\n                        0.0040\n                        0.0168\n                        0.0054\n                        0.0055\n                        0.0155\n                        0.0152\n            \n    \n\n\n\n## Let's now check the model hyperparameters\nprint(xgboost_classifier)\n\nRandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=100,\n                       n_jobs=-1, oob_score=False, random_state=7481, verbose=0,\n                       warm_start=False)"
  },
  {
    "objectID": "posts/2021-05-30-classification using health data with pycaret 1.html#tuning-the-hyperparametes-for-better-performance",
    "href": "posts/2021-05-30-classification using health data with pycaret 1.html#tuning-the-hyperparametes-for-better-performance",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "Tuning the hyperparametes for better performance",
    "text": "Tuning the hyperparametes for better performance\n\n# Whenenver we compare different models or build a model, the model uses deault\n#hyperparameter values. Hence, we need to tune our model to get better performance\n\ntuned_xgboost_classifier=tune_model(xgboost_classifier)\n\n\n                    Accuracy        AUC        Recall        Prec.        F1        Kappa        MCC    \n                \n                        0\n                        0.9866\n                        0.9990\n                        0.9683\n                        0.9868\n                        0.9863\n                        0.9616\n                        0.9624\n            \n            \n                        1\n                        0.9933\n                        0.9988\n                        0.9972\n                        0.9936\n                        0.9934\n                        0.9815\n                        0.9817\n            \n            \n                        2\n                        0.9933\n                        0.9999\n                        0.9841\n                        0.9933\n                        0.9932\n                        0.9810\n                        0.9812\n            \n            \n                        3\n                        1.0000\n                        1.0000\n                        1.0000\n                        1.0000\n                        1.0000\n                        1.0000\n                        1.0000\n            \n            \n                        4\n                        0.9933\n                        0.9992\n                        0.9833\n                        0.9938\n                        0.9933\n                        0.9813\n                        0.9814\n            \n            \n                        5\n                        0.9866\n                        0.9927\n                        0.9667\n                        0.9868\n                        0.9863\n                        0.9617\n                        0.9625\n            \n            \n                        6\n                        0.9866\n                        0.9897\n                        0.9667\n                        0.9868\n                        0.9863\n                        0.9617\n                        0.9625\n            \n            \n                        7\n                        0.9866\n                        0.9995\n                        0.9556\n                        0.9866\n                        0.9864\n                        0.9621\n                        0.9623\n            \n            \n                        8\n                        0.9932\n                        1.0000\n                        0.9972\n                        0.9936\n                        0.9933\n                        0.9810\n                        0.9812\n            \n            \n                        9\n                        0.9865\n                        0.9997\n                        0.9805\n                        0.9865\n                        0.9865\n                        0.9615\n                        0.9615\n            \n            \n                        Mean\n                        0.9906\n                        0.9979\n                        0.9799\n                        0.9908\n                        0.9905\n                        0.9733\n                        0.9737\n            \n            \n                        SD\n                        0.0045\n                        0.0034\n                        0.0145\n                        0.0045\n                        0.0046\n                        0.0128\n                        0.0126\n            \n    \n\n\n\nWe can clearly conclude that our tuned model has performed better than our original model with default hyperparameters. The mean accuracy increased from 0.9899 to 0.9906\n\n\npycaret library really makes the process of tuning hyperparameters easy\n\n\nWe just need to pass the model in the following command\n\n\ntune_model(model_name)"
  },
  {
    "objectID": "posts/2021-05-30-classification using health data with pycaret 1.html#plotting-classification-plots",
    "href": "posts/2021-05-30-classification using health data with pycaret 1.html#plotting-classification-plots",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "Plotting classification plots",
    "text": "Plotting classification plots"
  },
  {
    "objectID": "posts/2021-05-30-classification using health data with pycaret 1.html#classification-report",
    "href": "posts/2021-05-30-classification using health data with pycaret 1.html#classification-report",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "Classification Report",
    "text": "Classification Report\n\nplot_model(tuned_xgboost_classifier,plot='class_report')"
  },
  {
    "objectID": "posts/2021-05-30-classification using health data with pycaret 1.html#plotting-the-confusion-matrix",
    "href": "posts/2021-05-30-classification using health data with pycaret 1.html#plotting-the-confusion-matrix",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "Plotting the confusion matrix",
    "text": "Plotting the confusion matrix\n\nplot_model(tuned_xgboost_classifier,plot='confusion_matrix')"
  },
  {
    "objectID": "posts/2021-05-30-classification using health data with pycaret 1.html#saving-the-model-for-future-predictions",
    "href": "posts/2021-05-30-classification using health data with pycaret 1.html#saving-the-model-for-future-predictions",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "Saving the model for future predictions",
    "text": "Saving the model for future predictions\n\n## This can be used to save our trained model for future use.\nsave_model(tuned_xgboost_classifier,\"XGBOOST CLASSIFIER\")\n\nTransformation Pipeline and Model Succesfully Saved\n\n\n(Pipeline(memory=None,\n          steps=[('dtypes',\n                  DataTypes_Auto_infer(categorical_features=[],\n                                       display_types=True, features_todrop=[],\n                                       id_columns=[],\n                                       ml_usecase='classification',\n                                       numerical_features=[], target='NSP',\n                                       time_features=[])),\n                 ('imputer',\n                  Simple_Imputer(categorical_strategy='not_available',\n                                 fill_value_categorical=None,\n                                 fill_value_numerical=None,\n                                 numeric_strategy...\n                  RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n                                         class_weight='balanced_subsample',\n                                         criterion='entropy', max_depth=11,\n                                         max_features='sqrt',\n                                         max_leaf_nodes=None, max_samples=None,\n                                         min_impurity_decrease=0.002,\n                                         min_impurity_split=None,\n                                         min_samples_leaf=4, min_samples_split=9,\n                                         min_weight_fraction_leaf=0.0,\n                                         n_estimators=130, n_jobs=-1,\n                                         oob_score=False, random_state=7481,\n                                         verbose=0, warm_start=False)]],\n          verbose=False),\n 'XGBOOST CLASSIFIER.pkl')"
  },
  {
    "objectID": "posts/2021-05-30-classification using health data with pycaret 1.html#loading-the-saved-model",
    "href": "posts/2021-05-30-classification using health data with pycaret 1.html#loading-the-saved-model",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "Loading the saved model",
    "text": "Loading the saved model\n\n## This can be used to load our model. We don't need to train our model again and again.\nsaved_model=load_model('XGBOOST CLASSIFIER')\n\nTransformation Pipeline and Model Successfully Loaded"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html",
    "href": "posts/2021-06-04-imbalanced-data.html",
    "title": "Imbalanced data",
    "section": "",
    "text": "#code adapted from https://github.com/thomasjpfan/ml-workshop-advanced"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#load-mammography-data",
    "href": "posts/2021-06-04-imbalanced-data.html#load-mammography-data",
    "title": "Imbalanced data",
    "section": "Load Mammography Data",
    "text": "Load Mammography Data\n\n# %load solutions/classifier_example.py\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\nrf.score(X_test, y_test)\n\ny_pred = rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7139\n           1       0.17      0.01      0.01       129\n\n    accuracy                           0.98      7268\n   macro avg       0.57      0.50      0.50      7268\nweighted avg       0.97      0.98      0.97      7268\n\n\n\n\nfrom sklearn.datasets import fetch_openml\n\n\nnp.bincount(y)\n\narray([28524,   548])"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#split-data-into-train-test-split",
    "href": "posts/2021-06-04-imbalanced-data.html#split-data-into-train-test-split",
    "title": "Imbalanced data",
    "section": "Split data into train test split",
    "text": "Split data into train test split\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, random_state=0)\n\n\nBase models\n\nLinear model\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression\n\n\nbase_log_reg = LogisticRegression(random_state=42)\ncv_results = cross_validate(base_log_reg,\n                            X_train, y_train, scoring=['roc_auc', 'average_precision'])\n\n\ncv_results\n\n{'fit_time': array([0.18456721, 0.18922424, 0.21849942, 0.16009235, 0.14718604]),\n 'score_time': array([0.02156591, 0.01044083, 0.00906014, 0.00876856, 0.00877857]),\n 'test_roc_auc': array([0.85926447, 0.82782335, 0.83645313, 0.82829213, 0.82486117]),\n 'test_average_precision': array([0.08657556, 0.08525184, 0.10518971, 0.0796069 , 0.08259916])}\n\n\n\nlog_reg_base_auc = cv_results['test_roc_auc'].mean()\nlog_reg_base_auc\n\n0.8353388498771366\n\n\n\nlog_reg_base_ap = cv_results['test_average_precision'].mean()\nlog_reg_base_ap\n\n0.08784463474925555\n\n\n\ndef compute_metrics(estimator):\n    cv_results = cross_validate(estimator,\n                                X_train, y_train, scoring=['roc_auc', 'average_precision'])\n    return {\n        \"auc\": cv_results[\"test_roc_auc\"].mean(),\n        \"average_precision\": cv_results[\"test_average_precision\"].mean(),\n    }\n\n\nbase_log_reg_metrics = compute_metrics(base_log_reg)\nbase_log_reg_metrics\n\n{'auc': 0.8353388498771366, 'average_precision': 0.08784463474925555}\n\n\n\n\nRandom Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nbase_rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n\n\nbase_rf_metrics = compute_metrics(base_rf)\nbase_rf_metrics\n\n{'auc': 0.7132260944197908, 'average_precision': 0.04824294956894796}\n\n\n\n\n\nImbalance-learn sampler\n\nUnder sampler\n\nnp.bincount(y_train)\n\narray([21393,   411])\n\n\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\n\nunder_sampler = RandomUnderSampler(random_state=42)\n\n\nX_train_subsample, y_train_subsample = under_sampler.fit_resample(X_train, y_train)\n\n\nX_train.shape\n\n(21804, 5)\n\n\n\nX_train_subsample.shape\n\n(822, 5)\n\n\n\nnp.bincount(y_train_subsample)\n\narray([411, 411])\n\n\n\n\nOversampling\n\nfrom imblearn.over_sampling import RandomOverSampler\n\n\nover_sampler = RandomOverSampler(random_state=42)\n\n\nX_train_subsample, y_train_subsample = over_sampler.fit_resample(X_train, y_train)\n\n\nX_train_subsample.shape\n\n(42786, 5)\n\n\n\nnp.bincount(y_train_subsample)\n\narray([21393, 21393])"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#pipelines-with-imblean",
    "href": "posts/2021-06-04-imbalanced-data.html#pipelines-with-imblean",
    "title": "Imbalanced data",
    "section": "Pipelines with imblean",
    "text": "Pipelines with imblean\n\nLinear model with under sampling\n\nfrom imblearn.pipeline import make_pipeline as make_imb_pipeline\n\n\nunder_log_reg = make_imb_pipeline(\n    RandomUnderSampler(random_state=42), LogisticRegression(random_state=42))\n\n\nbase_log_reg_metrics\n\n{'auc': 0.8353388498771366, 'average_precision': 0.08784463474925555}\n\n\n\ncompute_metrics(under_log_reg)\n\n{'auc': 0.8347615373366913, 'average_precision': 0.08779628352835236}\n\n\n\n\nRandom Forest with under sampling\n\nunder_rf = make_imb_pipeline(\n    RandomUnderSampler(random_state=42), RandomForestClassifier(random_state=42))\n\n\nbase_rf_metrics\n\n{'auc': 0.7132260944197908, 'average_precision': 0.04824294956894796}\n\n\n\ncompute_metrics(under_rf)\n\n{'auc': 0.7930356061088476, 'average_precision': 0.0634524323793531}\n\n\n\n\nLinear model with over sampling\n\nover_log_reg = make_imb_pipeline(\n    RandomOverSampler(), LogisticRegression(random_state=42))\n\n\nbase_log_reg_metrics\n\n{'auc': 0.8353388498771366, 'average_precision': 0.08784463474925555}\n\n\n\ncompute_metrics(over_log_reg)\n\n{'auc': 0.835413689744715, 'average_precision': 0.08676060348173371}"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#exercise-1",
    "href": "posts/2021-06-04-imbalanced-data.html#exercise-1",
    "title": "Imbalanced data",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nUse make_imb_pipeline with RandomOverSampler to create a pipline with random forset called over_rf.\nCompute our metrics using compute_metrics.\n\n\n# %load solutions/02-ex01-solutions.py\nover_rf = make_imb_pipeline(\n    RandomOverSampler(),\n    RandomForestClassifier(random_state=42, n_jobs=-1)\n)\n\nbase_rf_metrics\n\ncompute_metrics(over_rf)\n\n{'auc': 0.713025487676646, 'average_precision': 0.04324731383558929}"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#plotting-curves-for-logistic-regression",
    "href": "posts/2021-06-04-imbalanced-data.html#plotting-curves-for-logistic-regression",
    "title": "Imbalanced data",
    "section": "Plotting curves for logistic regression",
    "text": "Plotting curves for logistic regression\n\nbase_log_reg.fit(X_train, y_train)\nunder_log_reg.fit(X_train, y_train)\nover_log_reg.fit(X_train, y_train);\n\n\nbase_log_reg.score(X_test, y_test)\n\n0.9811502476609797\n\n\n\nPlotting\n\nfrom sklearn.metrics import plot_precision_recall_curve\nfrom sklearn.metrics import plot_roc_curve\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_log_reg, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(under_log_reg, X_test, y_test, ax=ax1, name=\"undersampling\")\nplot_roc_curve(over_log_reg, X_test, y_test, ax=ax1, name=\"oversampling\")\n\nplot_precision_recall_curve(base_log_reg, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(under_log_reg, X_test, y_test, ax=ax2, name=\"undersampling\")\nplot_precision_recall_curve(over_log_reg, X_test, y_test, ax=ax2, name=\"oversampling\");\n\n\n\n\n\n# %load solutions/02-ex02-solutions.py\nbase_rf.fit(X_train, y_train)\nunder_rf.fit(X_train, y_train)\nover_rf.fit(X_train, y_train);\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_rf, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(under_rf, X_test, y_test, ax=ax1, name=\"undersampling\")\nplot_roc_curve(over_rf, X_test, y_test, ax=ax1, name=\"oversampling\")\n\nplot_precision_recall_curve(base_rf, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(under_rf, X_test, y_test, ax=ax2, name=\"undersampling\")\nplot_precision_recall_curve(over_rf, X_test, y_test, ax=ax2, name=\"oversampling\");"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#class-weights",
    "href": "posts/2021-06-04-imbalanced-data.html#class-weights",
    "title": "Imbalanced data",
    "section": "Class-Weights",
    "text": "Class-Weights\n\nLinear model with class weights\n\nclass_weight_log_reg = LogisticRegression(class_weight='balanced', random_state=42)\nclass_weight_log_reg.fit(X_train, y_train)\n\nLogisticRegressionLogisticRegression(class_weight='balanced', random_state=42)\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_log_reg, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(class_weight_log_reg, X_test, y_test, ax=ax1, name=\"class-weighted\")\n\nplot_precision_recall_curve(base_log_reg, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(class_weight_log_reg, X_test, y_test, ax=ax2, name=\"class-weighted\")\n\n<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f4ef4551550>\n\n\n\n\n\n\n\nRandom forest with class weights\n\nclass_weight_rf = RandomForestClassifier(class_weight='balanced', random_state=42)\nclass_weight_rf.fit(X_train, y_train)\n\nRandomForestClassifierRandomForestClassifier(class_weight='balanced', random_state=42)\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_rf, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(class_weight_rf, X_test, y_test, ax=ax1, name=\"class-weighted\")\n\nplot_precision_recall_curve(base_rf, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(class_weight_rf, X_test, y_test, ax=ax2, name=\"class-weighted\")\n\n<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f4ef439b910>"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#ensemble-resampling",
    "href": "posts/2021-06-04-imbalanced-data.html#ensemble-resampling",
    "title": "Imbalanced data",
    "section": "Ensemble Resampling",
    "text": "Ensemble Resampling\n\nfrom imblearn.ensemble import BalancedRandomForestClassifier\n\n\nbalanced_rf = BalancedRandomForestClassifier(random_state=0)\nbalanced_rf.fit(X_train, y_train)\n\nBalancedRandomForestClassifierBalancedRandomForestClassifier(random_state=0)\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_rf, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(under_rf, X_test, y_test, ax=ax1, name=\"undersampling\")\nplot_roc_curve(over_rf, X_test, y_test, ax=ax1, name=\"oversampling\")\nplot_roc_curve(balanced_rf, X_test, y_test, ax=ax1, name=\"balanced bagging\")\n\nplot_precision_recall_curve(base_rf, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(under_rf, X_test, y_test, ax=ax2, name=\"undersampling\")\nplot_precision_recall_curve(over_rf, X_test, y_test, ax=ax2, name=\"oversampling\");\nplot_precision_recall_curve(balanced_rf, X_test, y_test, ax=ax2, name=\"balanced bagging\")\n\n<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f4ef4021fa0>"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#smote",
    "href": "posts/2021-06-04-imbalanced-data.html#smote",
    "title": "Imbalanced data",
    "section": "SMOTE",
    "text": "SMOTE\n\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\nX_train_smote.shape\n\n(42786, 5)\n\n\n\nnp.bincount(y_train_smote)\n\narray([21393, 21393])\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 8))\nsorting = np.argsort(y_train)\n\naxes[0].set_title(\"Original\")\naxes[0].scatter(X_train.iloc[sorting, 3], X_train.iloc[sorting, 4], c=plt.cm.tab10(y_train.iloc[sorting]), alpha=.3, s=2)\n\naxes[1].set_title(\"SMOTE\")\naxes[1].scatter(X_train_smote.iloc[:, 3], X_train_smote.iloc[:, 4], c=plt.cm.tab10(y_train_smote), alpha=.1, s=2)\n\n<matplotlib.collections.PathCollection at 0x7f4ec7f8efa0>\n\n\n\n\n\n\ncv_results\n\n{'fit_time': array([0.18456721, 0.18922424, 0.21849942, 0.16009235, 0.14718604]),\n 'score_time': array([0.02156591, 0.01044083, 0.00906014, 0.00876856, 0.00877857]),\n 'test_roc_auc': array([0.85926447, 0.82782335, 0.83645313, 0.82829213, 0.82486117]),\n 'test_average_precision': array([0.08657556, 0.08525184, 0.10518971, 0.0796069 , 0.08259916])}\n\n\n\nsmote_log_reg = make_imb_pipeline(\n    SMOTE(random_state=42), LogisticRegression(random_state=42))\ncompute_metrics(smote_log_reg)\n\n{'auc': 0.7872750489175167, 'average_precision': 0.06044681160292857}\n\n\n\nbase_rf_metrics\n\n{'auc': 0.7132260944197908, 'average_precision': 0.04824294956894796}\n\n\n\nsmote_rf = make_imb_pipeline(SMOTE(random_state=42), RandomForestClassifier(random_state=42, n_jobs=-1))\ncompute_metrics(smote_rf)\n\n{'auc': 0.7160674179833459, 'average_precision': 0.03946048645139655}"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#plotting-all-the-version-of-random-forest",
    "href": "posts/2021-06-04-imbalanced-data.html#plotting-all-the-version-of-random-forest",
    "title": "Imbalanced data",
    "section": "Plotting all the version of random forest",
    "text": "Plotting all the version of random forest\n\nsmote_rf.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('smote', SMOTE(random_state=42)),\n                ('randomforestclassifier',\n                 RandomForestClassifier(n_jobs=-1, random_state=42))])SMOTESMOTE(random_state=42)RandomForestClassifierRandomForestClassifier(n_jobs=-1, random_state=42)\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_rf, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(under_rf, X_test, y_test, ax=ax1, name=\"undersampling\")\nplot_roc_curve(over_rf, X_test, y_test, ax=ax1, name=\"oversampling\")\nplot_roc_curve(balanced_rf, X_test, y_test, ax=ax1, name=\"balanced bagging\")\nplot_roc_curve(smote_rf, X_test, y_test, ax=ax1, name=\"smote\")\n\nplot_precision_recall_curve(base_rf, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(under_rf, X_test, y_test, ax=ax2, name=\"undersampling\")\nplot_precision_recall_curve(over_rf, X_test, y_test, ax=ax2, name=\"oversampling\");\nplot_precision_recall_curve(balanced_rf, X_test, y_test, ax=ax2, name=\"balanced bagging\")\nplot_precision_recall_curve(smote_rf, X_test, y_test, ax=ax2, name=\"smote\")\n\n<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f4ef4280fa0>\n\n\n\n\n\n\n# %load solutions/02-ex03-solutions.py\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nbase_hist = HistGradientBoostingClassifier(random_state=42)\nbase_hist.fit(X_train, y_train)\n\nsmote_hist = make_imb_pipeline(\n    SMOTE(), HistGradientBoostingClassifier(random_state=42))\nsmote_hist.fit(X_train, y_train)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_hist, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(smote_hist, X_test, y_test, ax=ax1, name=\"smote\")\n\nplot_precision_recall_curve(base_hist, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(smote_hist, X_test, y_test, ax=ax2, name=\"smote\")\n\n<sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f4ef4577e50>"
  },
  {
    "objectID": "posts/2021-05-15-bootstrapping.html",
    "href": "posts/2021-05-15-bootstrapping.html",
    "title": "Bootstrapping in Python",
    "section": "",
    "text": "import numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\nimport seaborn as sns\n\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.stats.power import NormalIndPower, TTestIndPower\nfrom scipy.stats import ttest_ind_from_stats\nimport numpy as np\nimport scipy"
  },
  {
    "objectID": "posts/2021-05-15-bootstrapping.html#distributions-of-dependant-variables",
    "href": "posts/2021-05-15-bootstrapping.html#distributions-of-dependant-variables",
    "title": "Bootstrapping in Python",
    "section": "Distributions of Dependant Variables",
    "text": "Distributions of Dependant Variables\n\nRight skew\n\ndf.hist(column=['fdi'], bins=60)\n\narray([[<AxesSubplot:title={'center':'fdi'}>]], dtype=object)\n\n\n\n\n\n\nsns.histplot(df['fdi'])\n\n<AxesSubplot:xlabel='fdi', ylabel='Count'>\n\n\n\n\n\n\nsns.displot(df['gdp'])\n\n<seaborn.axisgrid.FacetGrid at 0x7faf8b829160>\n\n\n\n\n\n\nsns.displot(df['fdi'])\n\n<seaborn.axisgrid.FacetGrid at 0x7fafbfac57f0>\n\n\n\n\n\n\nsns.displot(df['it'])\n\n<seaborn.axisgrid.FacetGrid at 0x7faf8b3b7220>\n\n\n\n\n\n\nsns.displot(df['specific'].dropna())\n\n<seaborn.axisgrid.FacetGrid at 0x7faf8bbe62e0>\n\n\n\n\n\n\ndf.hist(column=['fdi'], bins=60)\n\narray([[<AxesSubplot:title={'center':'fdi'}>]], dtype=object)"
  },
  {
    "objectID": "posts/2021-05-15-bootstrapping.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "href": "posts/2021-05-15-bootstrapping.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "title": "Bootstrapping in Python",
    "section": "Removal of GDP value outliers more than 3 standard deviations away from the mean",
    "text": "Removal of GDP value outliers more than 3 standard deviations away from the mean"
  },
  {
    "objectID": "posts/2021-05-15-bootstrapping.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "href": "posts/2021-05-15-bootstrapping.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "title": "Bootstrapping in Python",
    "section": "outlier removal of rows with GDP values that are > 3 standard deviations away form the mean",
    "text": "outlier removal of rows with GDP values that are > 3 standard deviations away form the mean\n\nimport scipy.stats as stats\n\n\ndf['gdp_zscore'] = stats.zscore(df['gdp'])"
  },
  {
    "objectID": "posts/2021-05-15-bootstrapping.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "href": "posts/2021-05-15-bootstrapping.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "title": "Bootstrapping in Python",
    "section": "these are the observations more then > 3 SDs away from the mean of gdp that will be dropped",
    "text": "these are the observations more then > 3 SDs away from the mean of gdp that will be dropped\n\ndf[abs(df['gdp_zscore'])>3].hist(column = ['gdp'])\n\narray([[<AxesSubplot:title={'center':'gdp'}>]], dtype=object)\n\n\n\n\n\n\ndf_no_gdp_outliers=df[abs(df['gdp_zscore'])<3]\n\n\ndf_no_gdp_outliers\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n      gdp_zscore\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.30\n      50661\n      631930\n      147002.0\n      -0.521466\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n      -0.464746\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n      -0.421061\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n      -0.383239\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n      -0.340870\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      354\n      2002\n      East China\n      Zhejiang\n      8003.67\n      307610\n      1962633\n      365437.0\n      0.798274\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n      1.178172\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.70\n      668128\n      3162299\n      656175.0\n      1.612181\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n      2.007180\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n      2.520929\n    \n  \n\n350 rows  8 columns\n\n\n\n\ndf_no_gdp_outliers.hist(column=['gdp'], bins=60)\n\narray([[<AxesSubplot:title={'center':'gdp'}>]], dtype=object)\n\n\n\n\n\n\ncounts_fiscal=df.groupby('region').count()\ncounts_fiscal\n\n\n\n\n\n  \n    \n      \n      year\n      province\n      gdp\n      fdi\n      it\n      specific\n      gdp_zscore\n    \n    \n      region\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      East China\n      84\n      84\n      84\n      84\n      84\n      84\n      84\n    \n    \n      North China\n      48\n      48\n      48\n      48\n      48\n      47\n      48\n    \n    \n      Northeast China\n      36\n      36\n      36\n      36\n      36\n      36\n      36\n    \n    \n      Northwest China\n      60\n      60\n      60\n      60\n      60\n      60\n      60\n    \n    \n      South Central China\n      72\n      72\n      72\n      72\n      72\n      72\n      72\n    \n    \n      Southwest China\n      60\n      60\n      60\n      60\n      60\n      57\n      60\n    \n  \n\n\n\n\n\ncounts_fiscal=df.groupby('province').count()\ncounts_fiscal\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      gdp\n      fdi\n      it\n      specific\n      gdp_zscore\n    \n    \n      province\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Anhui\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Beijing\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Chongqing\n      12\n      12\n      12\n      12\n      12\n      9\n      12\n    \n    \n      Fujian\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Gansu\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Guangdong\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Guangxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Guizhou\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hainan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hebei\n      12\n      12\n      12\n      12\n      12\n      11\n      12\n    \n    \n      Heilongjiang\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Henan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hubei\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hunan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Jiangsu\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Jiangxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Jilin\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Liaoning\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Ningxia\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Qinghai\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shaanxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shandong\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shanghai\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shanxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Sichuan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Tianjin\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Tibet\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Xinjiang\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Yunnan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Zhejiang\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n  \n\n\n\n\n\n#df_no_gdp_outliers.pivot_table(index='grouping column 1', columns='grouping column 2', values='aggregating column', aggfunc='sum')\n\n\n#pd.crosstab(df_no_gdp_outliers, 'year')\n\n\ndf_no_gdp_outliers_subset = df_no_gdp_outliers[['region', 'gdp', 'fdi', 'it']]\ndf_no_gdp_outliers_subset\n\n\n\n\n\n  \n    \n      \n      region\n      gdp\n      fdi\n      it\n    \n  \n  \n    \n      0\n      East China\n      2093.30\n      50661\n      631930\n    \n    \n      1\n      East China\n      2347.32\n      43443\n      657860\n    \n    \n      2\n      East China\n      2542.96\n      27673\n      889463\n    \n    \n      3\n      East China\n      2712.34\n      26131\n      1227364\n    \n    \n      4\n      East China\n      2902.09\n      31847\n      1499110\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      354\n      East China\n      8003.67\n      307610\n      1962633\n    \n    \n      355\n      East China\n      9705.02\n      498055\n      2261631\n    \n    \n      356\n      East China\n      11648.70\n      668128\n      3162299\n    \n    \n      357\n      East China\n      13417.68\n      772000\n      2370200\n    \n    \n      358\n      East China\n      15718.47\n      888935\n      2553268\n    \n  \n\n350 rows  4 columns\n\n\n\n\ndef aggregate_and_ttest(dataset, groupby_feature='province', alpha=.05, test_cells = [0, 1]):\n    #Imports\n    from tqdm import tqdm\n    from scipy.stats import ttest_ind_from_stats\n\n    \n    metrics = ['gdp', 'fdi', 'it']\n    \n    feature_size = 'size'\n    feature_mean = 'mean'\n    feature_std = 'std'    \n\n    for metric in tqdm(metrics):\n        \n        #print(metric)\n        crosstab = dataset.groupby(groupby_feature, as_index=False)[metric].agg(['size', 'mean', 'std'])\n        print(crosstab)\n        \n        treatment = crosstab.index[test_cells[0]]\n        control = crosstab.index[test_cells[1]]\n        \n        counts_control = crosstab.loc[control, feature_size]\n        counts_treatment = crosstab.loc[treatment, feature_size]\n\n        mean_control = crosstab.loc[control, feature_mean]\n        mean_treatment = crosstab.loc[treatment, feature_mean]\n\n        standard_deviation_control = crosstab.loc[control, feature_std]\n        standard_deviation_treatment = crosstab.loc[treatment, feature_std]\n        \n        t_statistic, p_value = ttest_ind_from_stats(mean1=mean_treatment, std1=standard_deviation_treatment, nobs1=counts_treatment,mean2=mean_control,std2=standard_deviation_control,nobs2=counts_control)\n        \n        #fstring to print the p value and t statistic\n        print(f\"The t statistic of the comparison of the treatment test cell of {treatment} compared to the control test cell of {control} for the metric of {metric} is {t_statistic} and the p value is {p_value}.\")\n        \n        #f string to say of the comparison is significant at a given alpha level\n\n        if p_value < alpha: \n            print(f'The comparison between {treatment} and {control} is statistically significant at the threshold of {alpha}') \n        else: \n            print(f'The comparison between {treatment} and {control} is not statistically significant at the threshold of {alpha}')\n\n\naggregate_and_ttest(df_no_gdp_outliers, test_cells = [0,2])\n\n100%|| 3/3 [00:00<00:00, 73.44it/s]\n\n\n              size          mean          std\nprovince                                     \nAnhui           12   3905.870000  1657.186350\nBeijing         12   4673.453333  2585.218431\nChongqing       12   2477.712500  1073.374101\nFujian          12   4864.023333  2065.665290\nGansu           12   1397.832500   628.751284\nGuangdong        8  10564.827500  3076.928885\nGuangxi         12   2924.104167  1316.680079\nGuizhou         12   1422.010833   679.163186\nHainan          12    686.714167   277.167010\nHebei           12   6936.825000  3266.776349\nHeilongjiang    12   4041.241667  1531.676708\nHenan           12   7208.966667  3669.236184\nHubei           12   4772.503333  2121.833184\nHunan           12   4765.891667  2159.588877\nJiangsu         10   8880.142000  3069.858941\nJiangxi         12   2460.782500  1125.673920\nJilin           12   2274.854167   975.812431\nLiaoning        12   5231.135000  1988.700441\nNingxia         12    432.268333   224.934621\nQinghai         12    383.099167   194.618478\nShaanxi         12   2658.034167  1461.540671\nShandong         9   9093.784444  2952.172758\nShanghai        12   6432.454167  3049.477185\nShanxi          12   2817.210833  1531.856025\nSichuan         12   5377.790000  2412.985532\nTianjin         12   2528.665000  1367.201360\nTibet           12    170.426667    88.715089\nXinjiang        12   1828.896667   848.752092\nYunnan          12   2604.054167  1016.828525\nZhejiang        11   8264.008182  3870.124534\nThe t statistic of the comparison of the treatment test cell of Anhui compared to the control test cell of Chongqing for the metric of gdp is 2.505668475205307 and the p value is 0.020116468101911197.\nThe comparison between Anhui and Chongqing is statistically significant at the threshold of 0.05\n              size          mean            std\nprovince                                       \nAnhui           12  7.095308e+04   78371.990245\nBeijing         12  2.573693e+05  121078.451044\nChongqing       12  4.112783e+04   25850.251481\nFujian          12  3.744664e+05   65608.304198\nGansu           12  5.295500e+03    2941.514777\nGuangdong        8  1.117272e+06  137741.790514\nGuangxi         12  5.514783e+04   19725.422944\nGuizhou         12  5.812333e+03    3337.775071\nHainan          12  6.436600e+04   19972.968837\nHebei           12  1.322308e+05   56541.699667\nHeilongjiang    12  8.271933e+04   62818.132171\nHenan           12  9.442600e+04   78299.340203\nHubei           12  1.497132e+05   70692.266346\nHunan           12  1.321102e+05   86224.990870\nJiangsu         10  7.422869e+05  257982.284030\nJiangxi         12  1.037352e+05   94052.957802\nJilin           12  4.122658e+04   16166.473875\nLiaoning        12  2.859253e+05  152318.549543\nNingxia         12  3.950417e+03    3662.459319\nQinghai         12  1.098408e+04   12241.262884\nShaanxi         12  5.089258e+04   29331.097377\nShandong         9  3.943093e+05  219313.559118\nShanghai        12  5.082483e+05  166880.730080\nShanxi          12  3.862883e+04   32974.368539\nSichuan         12  6.219717e+04   39329.514938\nTianjin         12  2.501733e+05  119418.314501\nTibet           12  8.397500e+02     922.467750\nXinjiang        12  4.433083e+03    3630.847471\nYunnan          12  1.704833e+04    9213.888976\nZhejiang        11  3.704169e+05  286211.503281\nThe t statistic of the comparison of the treatment test cell of Anhui compared to the control test cell of Chongqing for the metric of fdi is 1.251953695962862 and the p value is 0.2237342006262051.\nThe comparison between Anhui and Chongqing is not statistically significant at the threshold of 0.05\n              size          mean           std\nprovince                                      \nAnhui           12  2.649674e+06  1.966030e+06\nBeijing         12  1.175965e+06  4.944598e+05\nChongqing       12  1.636146e+06  1.155977e+06\nFujian          12  1.274117e+06  6.641800e+05\nGansu           12  2.045347e+06  1.432134e+06\nGuangdong        8  2.269997e+06  9.845008e+05\nGuangxi         12  2.326539e+06  1.691126e+06\nGuizhou         12  2.132636e+06  1.553924e+06\nHainan          12  5.404872e+05  4.064191e+05\nHebei           12  2.944163e+06  2.160958e+06\nHeilongjiang    12  3.230451e+06  2.227509e+06\nHenan           12  3.671971e+06  2.987163e+06\nHubei           12  2.904660e+06  2.189358e+06\nHunan           12  3.215128e+06  2.351869e+06\nJiangsu         10  1.563339e+06  8.478538e+05\nJiangxi         12  1.760613e+06  1.208039e+06\nJilin           12  2.136635e+06  1.375026e+06\nLiaoning        12  2.628358e+06  1.563158e+06\nNingxia         12  7.872062e+05  5.531448e+05\nQinghai         12  9.600921e+05  6.721767e+05\nShaanxi         12  2.474031e+06  1.786697e+06\nShandong         9  1.965966e+06  8.946962e+05\nShanghai        12  1.569143e+06  6.996706e+05\nShanxi          12  1.983718e+06  1.491559e+06\nSichuan         12  4.016480e+06  2.923696e+06\nTianjin         12  8.310284e+05  4.641450e+05\nTibet           12  1.174176e+06  7.959846e+05\nXinjiang        12  2.251012e+06  1.612187e+06\nYunnan          12  3.165419e+06  1.652014e+06\nZhejiang        11  1.648032e+06  8.385722e+05\nThe t statistic of the comparison of the treatment test cell of Anhui compared to the control test cell of Chongqing for the metric of it is 1.5394289719091268 and the p value is 0.13796027793319976.\nThe comparison between Anhui and Chongqing is not statistically significant at the threshold of 0.05\n\n\n\n\n\n\nEastvNorth=pd.DataFrame()\nEastvNorth= aggregate_and_ttest(df_no_gdp_outliers_subset, test_cells = [0,1])\nEastvNorth\n\n100%|| 3/3 [00:00<00:00, 138.11it/s]\n\n\n                     size         mean          std\nregion                                             \nEast China             78  6070.604231  3500.372702\nNorth China            48  4239.038542  2866.705149\nNortheast China        36  3849.076944  1948.531835\nNorthwest China        60  1340.026167  1174.399739\nSouth Central China    68  4835.540882  3697.129915\nSouthwest China        60  2410.398833  2144.589994\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 3.0488753833171947 and the p value is 0.0028085413359212334.\nThe comparison between East China and North China is statistically significant at the threshold of 0.05\n                     size           mean            std\nregion                                                 \nEast China             78  355577.897436  275635.866746\nNorth China            48  169600.583333  127011.475909\nNortheast China        36  136623.750000  142734.495232\nNorthwest China        60   15111.133333   22954.193559\nSouth Central China    68  218931.426471  339981.399823\nSouthwest China        60   25405.083333   31171.373876\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 4.391461461316698 and the p value is 2.3859390186769955e-05.\nThe comparison between East China and North China is statistically significant at the threshold of 0.05\n                     size          mean           std\nregion                                               \nEast China             78  1.775615e+06  1.153030e+06\nNorth China            48  1.733719e+06  1.548794e+06\nNortheast China        36  2.665148e+06  1.768442e+06\nNorthwest China        60  1.703538e+06  1.446408e+06\nSouth Central China    68  2.500962e+06  2.196436e+06\nSouthwest China        60  2.424971e+06  2.002198e+06\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 0.17339716493934587 and the p value is 0.862621991978372.\nThe comparison between East China and North China is not statistically significant at the threshold of 0.05\n\n\n\n\n\n\nimport numpy as np\nimport bootstrapped.bootstrap as bs\nimport bootstrapped.stats_functions as bs_stats\n\n\ntest_1=df_no_gdp_outliers[df_no_gdp_outliers['province']=='Beijing']\ntest=test_1['gdp'].to_numpy()\ntest\n\ncontrol_1=df_no_gdp_outliers[df_no_gdp_outliers['province']=='Shanxi']\ncontrol=control_1['gdp'].to_numpy()\ncontrol\n\narray([1292.11, 1476.  , 1611.08, 1667.1 , 1845.72, 2029.53, 2324.8 ,\n       2855.23, 3571.37, 4230.53, 4878.61, 6024.45])\n\n\n\nbins = np.linspace(0, 40, 20)\n\nplt.hist(control, label='Control')\nplt.hist(test, label='Test', color='orange')\nplt.title('Test/Ctrl Data')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7faf8596a190>\n\n\n\n\n\n\nbs.bootstrap_ab(test, control, stat_func=bs_stats.sum, compare_func=bs_compare.percent_change)\n\n65.88937107712621    (-19.58634300490877, 124.01876332252021)\n\n\n\n# run an a/b test simulation considering the lengths of the series (sum)\n# consider the full 'volume' of values that are passed in\n\nprint(bs_compare.percent_change(test.sum(), control.sum()))\n\nprint(bs.bootstrap_ab(\n    test, \n    control, \n    stat_func=bs_stats.sum,\n    compare_func=bs_compare.percent_change\n))\n\n65.88937107712621\n65.88937107712621    (-20.064956167313596, 124.54556877143521)\n\n\n\n# run an a/b test simulation ignoring the lengths of the series (average)\n# just what is the 'typical' value\n# use percent change to compare test and control\n\nprint(bs_compare.difference(test.mean(), control.mean()))\n\n1856.2424999999998\n\n\n\nprint(bs.bootstrap_ab(test, control, bs_stats.mean, bs_compare.difference))\n\n1856.2424999999998    (218.25606250000146, 3411.9760624999994)"
  },
  {
    "objectID": "posts/2020-10-13-dask-xgboost-fiscal-data.html",
    "href": "posts/2020-10-13-dask-xgboost-fiscal-data.html",
    "title": "Moving Dask XGboost with Fiscal Data, saving and loading Dask XGboost models",
    "section": "",
    "text": "import numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///fiscal.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nengine.execute(\"SELECT * FROM fiscal_table LIMIT 10\").fetchall()\n\n[(1996, 'East China', 'Anhui', 631930, 147002, 2093.3, 50661),\n (1997, 'East China', 'Anhui', 657860, 151981, 2347.32, 43443),\n (1998, 'East China', 'Anhui', 889463, 174930, 2542.96, 27673),\n (1999, 'East China', 'Anhui', 1227364, 285324, 2712.34, 26131),\n (2000, 'East China', 'Anhui', 1499110, 195580, 2902.09, 31847),\n (2001, 'East China', 'Anhui', 2165189, 250898, 3246.71, 33672),\n (2002, 'East China', 'Anhui', 2404936, 434149, 3519.72, 38375),\n (2003, 'East China', 'Anhui', 2815820, 619201, 3923.11, 36720),\n (2004, 'East China', 'Anhui', 3422176, 898441, 4759.3, 54669),\n (2005, 'East China', 'Anhui', 3874846, 898441, 5350.17, 69000)]\n\n\n\nsql = \"\"\"\nSELECT year\n, region\n, province\n, gdp\n, fdi\n, it\n, specific\nFROM fiscal_table\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.30\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.70\n      668128\n      3162299\n      656175.0\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      18753.73\n      1036576\n      2939778\n      844647.0\n    \n  \n\n360 rows  7 columns\n\n\n\n\ndf.columns\n\nIndex(['year', 'region', 'province', 'gdp', 'fdi', 'it', 'specific'], dtype='object')\n\n\n\ndf.gdp.hist()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f2677958128>\n\n\n\n\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n\n\n\n\nClient\n\n  Scheduler: inproc://192.168.1.71/13442/1\n  Dashboard: http://192.168.1.71:8787/status\n\n\n\nCluster\n\n  Workers: 3\n  Cores: 6\n  Memory: 12.00 GB\n\n\n\n\n\n\n\nclient.restart()\n\n\n\n\n\nClient\n\n  Scheduler: inproc://192.168.1.71/13442/1\n  Dashboard: http://192.168.1.71:8787/status\n\n\n\nCluster\n\n  Workers: 3\n  Cores: 6\n  Memory: 12.00 GB\n\n\n\n\n\n\n\nfrom dask import dataframe as dd \n\n\nddf = dd.from_pandas(df, npartitions=5)\n\n\nprint(ddf)\n\nDask DataFrame Structure:\n                year  region province      gdp    fdi     it specific\nnpartitions=5                                                        \n0              int64  object   object  float64  int64  int64  float64\n72               ...     ...      ...      ...    ...    ...      ...\n...              ...     ...      ...      ...    ...    ...      ...\n288              ...     ...      ...      ...    ...    ...      ...\n359              ...     ...      ...      ...    ...    ...      ...\nDask Name: from_pandas, 5 tasks\n\n\n\nddf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.30\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n  \n\n\n\n\n\nclient.id\n\n'Client-e79fe0fe-0d59-11eb-b482-f9dc9eaa58ee'\n\n\n\nfeat_list = [\"year\", \"fdi\"]\ncat_feat_list = [\"region\", \"province\"]\ntarget = [\"gdp\"]\n\n\nddf[\"year\"] = ddf[\"year\"].astype(int)\nddf[\"fdi\"] = ddf[\"fdi\"].astype(float)\nddf[\"gdp\"] = ddf[\"gdp\"].astype(float)\nddf[\"it\"] = ddf[\"it\"].astype(float)\n# ddf[\"province\"] = ddf[\"province\"].astype(float)\n# ddf[\"region\"] = ddf[\"region\"].astype(float)\n\n\nx=ddf[feat_list].persist()\ny=ddf[target].persist()\n\n\nx\n\n\nDask DataFrame Structure:\n\n\n\n  \n    \n      \n      year\n      fdi\n    \n    \n      npartitions=5\n      \n      \n    \n  \n  \n    \n      0\n      int64\n      float64\n    \n    \n      72\n      ...\n      ...\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      288\n      ...\n      ...\n    \n    \n      359\n      ...\n      ...\n    \n  \n\n\nDask Name: getitem, 5 tasks\n\n\n\ny.compute()\n\n\n\n\n\n  \n    \n      \n      gdp\n    \n  \n  \n    \n      0\n      2093.30\n    \n    \n      1\n      2347.32\n    \n    \n      2\n      2542.96\n    \n    \n      3\n      2712.34\n    \n    \n      4\n      2902.09\n    \n    \n      ...\n      ...\n    \n    \n      355\n      9705.02\n    \n    \n      356\n      11648.70\n    \n    \n      357\n      13417.68\n    \n    \n      358\n      15718.47\n    \n    \n      359\n      18753.73\n    \n  \n\n360 rows  1 columns\n\n\n\n\nprint(x.shape,y.shape)\n\n(Delayed('int-97d0cf00-db85-425b-a0d2-08297142db86'), 2) (Delayed('int-01cdae78-a995-48c1-9b93-277a008ad57a'), 1)\n\n\n\nx.count().compute()\n\nyear    360\nfdi     360\ndtype: int64\n\n\n\nfrom dask_ml.xgboost import XGBRegressor\n\n\nXGBR = XGBRegressor()\n\n\n%%time\nXGBR_model = XGBR.fit(x,y)\n\nCPU times: user 54.2 s, sys: 1.02 s, total: 55.2 s\nWall time: 18.4 s\n\n\n\nXGBR_model\n\nXGBRegressor()\n\n\n\nXGBR_model.save_model('fiscal_model')\n\n\nXGBR_model.load_model('fiscal_model')\n\n[08:43:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror."
  },
  {
    "objectID": "posts/2020-09-03-nlp_seaborn_heatmaps.html",
    "href": "posts/2020-09-03-nlp_seaborn_heatmaps.html",
    "title": "NLP Heatmaps with Seaborn",
    "section": "",
    "text": "df=pd.read_csv('../processed_data/nf_complete.csv')\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 126 entries, 0 to 125\nData columns (total 23 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   Unnamed: 0             126 non-null    int64 \n 1   year                   126 non-null    int64 \n 2   title                  126 non-null    object\n 3   abstract               126 non-null    object\n 4   theme                  126 non-null    object\n 5   China                  126 non-null    int64 \n 6   Russia                 126 non-null    int64 \n 7   War                    126 non-null    int64 \n 8   President              126 non-null    int64 \n 9   US                     126 non-null    int64 \n 10  Vietnam                126 non-null    int64 \n 11  Cold War               126 non-null    int64 \n 12  World War              126 non-null    int64 \n 13  Vietnam War            126 non-null    int64 \n 14  Korean War             126 non-null    int64 \n 15  Survey                 126 non-null    int64 \n 16  Case Study             126 non-null    int64 \n 17  Trade                  126 non-null    int64 \n 18  Humanitarian           126 non-null    int64 \n 19  fixed_effects          126 non-null    int64 \n 20  instrumental_variable  126 non-null    int64 \n 21  regression             126 non-null    int64 \n 22  experimental           126 non-null    int64 \ndtypes: int64(20), object(3)\nmemory usage: 22.8+ KB\n\n\n\nimport plotly_express as ple\nple.histogram(df.sort_values('year').groupby(['year','theme'])['Cold War'].sum().reset_index(), x=\"year\", y=\"Cold War\", histfunc=\"sum\", color=\"theme\")\n\n\n\n        \n        \n            \n            \n        \n\n\n\n# ple.lidifferences(dfm_regional.sort_values('year').groupby(['year','theme'])['Cold War'].sum().reset_index(),\n#          x='year',\n#          y='Cold War',\n#          line_group='theme',\n#          color='theme'\n#         )\n\n\n# Create the crosstab DataFrame\npd_crosstab = pd.crosstab(df[\"theme\"], df[\"year\"])\n\n# Plot a heatmap of the table with no color bar and using the BuGn palette\nsns.heatmap(pd_crosstab, cbar=False, cmap=\"GnBu\", linewidths=0.3)\n\n# Rotate tick marks for visibility\nplt.yticks(rotation=0)\nplt.xticks(rotation=90)\n\nplt.tight_layout()\n#plt.savefig('./img/theme_heat_1.png', bbox_inches='tight', dpi=500)\n\n\n#Show the plot\nplt.show()\nplt.clf()\n\n\n\n\n<Figure size 720x648 with 0 Axes>\n\n\n\nsns.clustermap(pd_crosstab, cmap='Greens', robust=True)\n\n# plot using a color palette\n#sns.heatmap(df, cmap=\"YlGnBu\")\n#sns.heatmap(df, cmap=\"Blues\")\n#sns.heatmap(df, cmap=\"BuPu\")\n#sns.heatmap(df, cmap=\"Greens\")\n\n<seaborn.matrix.ClusterGrid at 0x7f978c07a470>\n\n\n\n\n\n\n# Import seaborn library\nimport seaborn as sns\n\n# Get correlation matrix of the meat DataFrame\ncorr_meat = df.corr(method='pearson')\n\n# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\nfig = sns.clustermap(pd_crosstab,\n                     row_cluster=True,\n                     col_cluster=True,\n                     figsize=(10, 10))\n\nplt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\nplt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\nplt.show()\n\n\n\n\n\ndata_normalized = pd_crosstab\n\n# Standardize the mean and variance within a stat, so different stats can be comparable\n# (This is the same as changing all the columns to Z-scores)\ndata_normalized = (data_normalized - data_normalized.mean())/data_normalized.var()\n\n# Normalize these values to range from -1 to 1\ndata_normalized = (data_normalized)/(data_normalized.max() - data_normalized.min())\n\ndata_normalized = data_normalized.T\n\n# Can use a semicolon after the command to suppress output of the row_dendrogram and col_dendrogram.\nsns.clustermap(data_normalized, cmap='Blues');\n\n\n\n\n\ndata_normalized = pd_crosstab\n\n# Standardize the mean and variance within a stat, so different stats can be comparable\n# (This is the same as changing all the columns to Z-scores)\ndata_normalized = (data_normalized - data_normalized.mean())/data_normalized.var()\n\n# Normalize these values to range from -1 to 1\ndata_normalized = (data_normalized)/(data_normalized.max() - data_normalized.min())\n\n#data_normalized = data_normalized.T\n\n# Can use a semicolon after the command to suppress output of the row_dendrogram and col_dendrogram.\nsns.clustermap(data_normalized, cmap='BuPu');\n\n\n\n\n\nimport matplotlib.pyplot as plt\nsns.clustermap(data_normalized);\nfig = plt.gcf()\nfig.savefig('clusteredheatmap_bbox_tight.png', bbox_inches='tight')\n\n\n\n\n\ntidy_df = pd.melt(df.reset_index(), id_vars='index')\ndf.T.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      116\n      117\n      118\n      119\n      120\n      121\n      122\n      123\n      124\n      125\n    \n  \n  \n    \n      Unnamed: 0\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      14\n      13\n      ...\n      128\n      130\n      123\n      125\n      131\n      132\n      133\n      134\n      135\n      136\n    \n    \n      year\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2001\n      2001\n      ...\n      2017\n      2017\n      2017\n      2017\n      2018\n      2018\n      2018\n      2018\n      2018\n      2018\n    \n    \n      title\n      \"Institutions at the Domestic/International Ne...\n      Born to Lose and Doomed to Survive: State Deat...\n      The significance of allegiance in internatio...\n      The significance of allegiance in internatio...\n      Truth-Telling and Mythmaking in Post-Soviet Ru...\n      Building a Cape Fear Metropolis: Fort Bragg, F...\n      The Glories and the Sadness: Shaping the natio...\n      What leads longstanding adversaries to engage ...\n      A School for the Nation: Military  Institution...\n      The 'American Century' Army:  The Origins of t...\n      ...\n      Fully Committed? Religiously Committed State P...\n      Straddling the Threshold of Two Worlds: Soldie...\n      U.S. Armys Investigation and Adjudication of ...\n      Grand Strategic Crucibles: The Lasting Effects...\n      Trust in International Politics: The Role of L...\n      Planning for the Short Haul: Trade Among Belli...\n      Clinging to the Anti-Imperial Mantle: The Repu...\n      The New Navy's Pacific Wars:  Peripheral Confl...\n      Stop or I'll Shoot, Comply and I Won't: The Di...\n      Unexpected Humanitarians: Albania, the U.S. Mi...\n    \n    \n      abstract\n      Civil-military relations are frequently studie...\n      Under what conditions do states die, or exit t...\n      My dissertation employs original and secondary...\n      \\nThis study revises prevailing interpretation...\n      Can distorted and pernicious ideas r histo...\n      My dissertation examines the cultural and econ...\n      In my dissertation I compare the ways in whic...\n      This dissertation develops a socio-psychoanal...\n      Beginning in Europe in the latter half of the ...\n      This dissertation covers the period 1949-1959 ...\n      ...\n      This dissertation argues that the higher the l...\n      This dissertation explores how American soldie...\n      This dissertation examines the U.S. Armys res...\n      When and how do military interventions shape g...\n      In my dissertation, I focus on how leader rela...\n      In times of war, why do belligerents continue ...\n      My dissertation project, Clinging to the Anti-...\n      Using a transnational methodology and sources ...\n      There is a dilemma at the heart of coercion. S...\n      Using archives and oral history, this disserta...\n    \n    \n      theme\n      IR scholarship\n      IR scholarship\n      IR scholarship\n      Conflit Between States\n      Conflict Between States\n      Domestic Military History\n      Culture\n      Culture / Peace Process\n      Military History\n      Military History\n      ...\n      IR Scholarship\n      Military History\n      Military History\n      IR Scholarship\n      Nuclear Weapons\n      Conflict between states\n      Cold War\n      Military History\n      IR Scholarship\n      Military History\n    \n  \n\n5 rows  126 columns"
  },
  {
    "objectID": "posts/2020-12-31-TextSummarizer-Dickens.html",
    "href": "posts/2020-12-31-TextSummarizer-Dickens.html",
    "title": "didactic",
    "section": "",
    "text": "# Natural Language Tool Kit (NLTK)\nimport nltk\nnltk.download('stopwords')\n\n# Regular Expression for text preprocessing\nimport re\n\n# Heap (priority) queue algorithm to get the top sentences\nimport heapq\n\n# NumPy for numerical computing\nimport numpy as np\n\n# pandas for creating DataFrames\nimport pandas as pd\n\n# matplotlib for plot\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\n\n\n\n\n\nimport requests\nimport re\n\nr = requests.get(\"https://www.gutenberg.org/files/98/98-0.txt\")\nraw_text = r.text\n\n\nprint(raw_text[0:1000])\n\nThe Project Gutenberg EBook of A Tale of Two Cities, by Charles Dickens\n\nThis eBook is for the use of anyone anywhere in the United States and most\nother parts of the world at no cost and with almost no restrictions\nwhatsoever.  You may copy it, give it away or re-use it under the terms of\nthe Project Gutenberg License included with this eBook or online at\nwww.gutenberg.org.  If you are not located in the United States, you'll have\nto check the laws of the country where you are located before using this ebook.\n\nTitle: A Tale of Two Cities\n       A Story of the French Revolution\n       \nAuthor: Charles Dickens\n\nRelease Date: January, 1994 [EBook #98]\n[Most recently updated: December 20, 2020]\n\nLanguage: English\n\nCharacter set encoding: UTF-8\n\n*** START OF THIS PROJECT GUTENBERG EBOOK A TALE OF TWO CITIES ***\n\n\n\n\nProduced by Judith Boss, and David Widger\n\n\n\n\nA TALE OF TWO CITIES\n\nA STORY OF THE FRENCH REVOLUTION\n\nBy Charles Dickens\n\n\nCONTENTS\n\n\n\n\n\n# # load text file\n# with open('https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/NotesfromtheUnderground_Dostoevsky.txt', 'r') as f:\n#     file_data = f.read()\n\n\n# text_file = open(\"https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/NotesfromtheUnderground_Dostoevsky.txt\", \"r\")\n# lines = raw_text.readlines()\n\n\n# lines = raw_text.readlines()\n\n\n# text_file.close()\n\n\n# df = pd.read_txt('https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/NotesfromtheUnderground_Dostoevsky.txt')\n\n\n# # view text data\n# print(lines)\n\n\n\n\n\ntext = raw_text\ntext = re.sub(r'\\[[0-9]*\\]',' ',text) # replace reference number i.e. [1], [10], [20] with empty space, if any..\ntext = re.sub(r'\\s+',' ',text) # replace one or more spaces with single space\n#print(text)\n\nNext, we form a clean text with lower case (without special characters, digits and extra spaces) and split it into individual word, for word score computation and formation of the word histogram.\nThe reason to form a clean text is so that the algorithm wont treat, i.e.understanding and understanding, as two different words.\n\n# generate clean text\nclean_text = text.lower() # convert all uppercase characters into lowercase characters\nclean_text = re.sub(r'\\W',' ',clean_text) # replace character other than [a-zA-Z0-9] with empty space\nclean_text = re.sub(r'\\d',' ',clean_text) # replace digit with empty space\nclean_text = re.sub(r'\\s+',' ',clean_text) # replace one or more spaces with a single space\n\n#print(clean_text)\n\n\n\n\nWe split (tokenize) the text into sentences using NLTK sent_tokenize() method. We will evaluate the importance of each of sentences, then decide if we should each include in our summary.\n\n# split (tokenize) the sentences\nsentences = nltk.sent_tokenize(text)\n#print(sentences)\n\n\nimport nltk\nnltk.download('punkt')\n\nfrom nltk import word_tokenize,sent_tokenize\n\n[nltk_data] Downloading package punkt to /home/david/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n\n\n\n\n\nStop words are English words which do not add much meaning to a sentence. They can be safely ignored without sacrificing the meaning of the sentence. We already downloaded a file with English stop words in the first section of the notebook.\nHere, we will get the list of stop words and store them in stop_word variable.\n\n# get stop words list\nstop_words = nltk.corpus.stopwords.words('english')\nprint(stop_words)\n\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n\n\n\n\n\n\n# create an empty dictionary to house the word count\nword_count = {}\n\n# loop through tokenized words, remove stop words and save word count to dictionary\nfor word in nltk.word_tokenize(clean_text):\n    # remove stop words\n    if word not in stop_words:\n        # save word count to dictionary\n        if word not in word_count.keys():\n            word_count[word] = 1\n        else:\n            word_count[word] += 1\n\nLets plot the word histogram and see the results.\n\n# plt.figure(figsize=(16,10))\n# plt.xticks(rotation = 90)\n# plt.bar(word_count.keys(), word_count.values())\n# plt.show()\n\n\ndef plot_top_words(word_count_dict, show_top_n=20):\n    word_count_table = pd.DataFrame.from_dict(word_count_dict, orient = 'index').rename(columns={0: 'score'})\n    word_count_table.sort_values(by='score').tail(show_top_n).plot(kind='barh', figsize=(10,10))\n    plt.show()\n\n\nplot_top_words(word_count, 20)\n\n\n\n\n\nsentence_score = {}\n\n# loop through tokenized sentence, only take sentences that have less than 30 words, then add word score to form sentence score\nfor sentence in sentences:\n    # check if word in sentence is in word_count dictionary\n    for word in nltk.word_tokenize(sentence.lower()):\n        if word in word_count.keys():\n            # only take sentence that has less than 30 words\n            if len(sentence.split(' ')) < 30:\n                # add word score to sentence score\n                if sentence not in sentence_score.keys():\n                    sentence_score[sentence] = word_count[word]\n                else:\n                    sentence_score[sentence] += word_count[word]\n\n\ndf_sentence_score = pd.DataFrame.from_dict(sentence_score, orient = 'index').rename(columns={0: 'score'})\ndf_sentence_score.sort_values(by='score', ascending = False)\n\n\n\n\n\n  \n    \n      \n      score\n    \n  \n  \n    \n      I know this is a confidence, she modestly said, after a little hesitation, and in earnest tears, I know you would say this to no one else.\n      2445\n    \n    \n      Thus engaged, with her right elbow supported by her left hand, Madame Defarge said nothing when her lord came in, but coughed just one grain of cough.\n      2373\n    \n    \n      Vengeance and retribution require a long time; it is the rule. It does not take a long time to strike a man with Lightning, said Defarge.\n      2312\n    \n    \n      We have borne this a long time, said Madame Defarge, turning her eyes again upon Lucie.\n      2208\n    \n    \n      You had better, Lucie, said Mr. Lorry, doing all he could to propitiate, by tone and manner, have the dear child here, and our good Pross.\n      2197\n    \n    \n      ...\n      ...\n    \n    \n      What are you hooroaring at?\n      1\n    \n    \n      Him and his hooroars!\n      1\n    \n    \n      Come on at a footpace!\n      1\n    \n    \n      But he was not persuaded.\n      1\n    \n    \n      Dont be ungrateful.\n      1\n    \n  \n\n3832 rows  1 columns\n\n\n\n\nbest_sentences = heapq.nlargest(3, sentence_score, key=sentence_score.get)\n\n\nprint('SUMMARY')\nprint('------------------------')\n\nfor sentence in sentences:\n    if sentence in best_sentences:\n        print (sentence)\n\nSUMMARY\n------------------------\nThus engaged, with her right elbow supported by her left hand, Madame Defarge said nothing when her lord came in, but coughed just one grain of cough.\nI know this is a confidence, she modestly said, after a little hesitation, and in earnest tears, I know you would say this to no one else.\nVengeance and retribution require a long time; it is the rule. It does not take a long time to strike a man with Lightning, said Defarge."
  },
  {
    "objectID": "posts/2020-12-23_Healthcare_Modeling_app-Copy1.html",
    "href": "posts/2020-12-23_Healthcare_Modeling_app-Copy1.html",
    "title": "didactic",
    "section": "",
    "text": "Random Forest Classifier\n\n\nFeature and Target Selection\n\n# Select feature and target variables:\nX = df.drop(['stroke'], axis=1)\ny = df[['stroke']]\n\n\n#One-hot encode the data using pandas get_dummies\nX = pd.get_dummies(X)\n\n\n#rus = RandomUnderSampler(random_state=0, replacement=True)\n#X_resampled, y_resampled = rus.fit_resample(X, y)\n#print(np.vstack(np.unique([tuple(row) for row in X_resampled], axis=0)).shape)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\ncolumns = X_train.columns\n\nsm = SMOTE(random_state=1)\nX_train_SMOTE, y_train_SMOTE = sm.fit_sample(X_train, y_train)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\n\nmodel = RandomForestClassifier(n_estimators=100, bootstrap=True,\n                               max_features='sqrt', n_jobs=3, verbose=1, class_weight=\"balanced\")\n\nmodel.fit(X_train_SMOTE, y_train_SMOTE)\n\ny_pred = model.predict(X_test)\n\nDataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  model.fit(X_train_SMOTE, y_train_SMOTE)\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.9s\n[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    2.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.1s finished\n\n\n\nfrom sklearn.metrics import roc_auc_score\n\n# Calculate roc auc\nroc_value = roc_auc_score(y_test, y_pred)\nroc_value\n\n0.5381645695594856\n\n\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\n\nAccuracy: 0.9531490015360983\nPrecision: 0.05491990846681922\nRecall: 0.1085972850678733\n\n\n\ny_pred_proba = model.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()\n\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.2s finished\n\n\n\n\n\n\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\nfrom inspect import signature\n\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\n\nplt.plot(precision,recall)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n\nText(0, 0.5, 'Precision')\n\n\n\n\n\n\n# Import numpy and matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Construct the histogram with a flattened 3d array and a range of bins\nplt.hist(y_pred_proba.ravel())\n\n# Add a title to the plot\nplt.title('Predicted Probability of Stroke')\n\n# Show the plot\nplt.show()\n\n\n\n\n\nlen(y_pred_proba)\n\n13020\n\n\n\ny_pred\n\narray([1, 0, 0, ..., 0, 0, 1])\n\n\n# Get feature importances for interpretability\n\n# Get numerical feature importances\nimportances = list(model.feature_importances_)\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(X, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the features and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n\nVariable: age                  Importance: 0.44\nVariable: avg_glucose_level    Importance: 0.19\nVariable: bmi                  Importance: 0.15\nVariable: work_type            Importance: 0.06\nVariable: smoking_status       Importance: 0.05\nVariable: gender               Importance: 0.03\nVariable: Residence_type       Importance: 0.03\nVariable: hypertension         Importance: 0.02\nVariable: ever_married         Importance: 0.02\nVariable: heart_disease        Importance: 0.01\n\n\n\nplt.figure(1)\nplt.title('Feature Importance')\nx_values = list(range(len(importances)))\nplt.barh(x_values,  importances, align='center')\nplt.yticks(x_values, X)\nplt.xlabel('Relative Importance')\nplt.tight_layout() \n\n\n\n\n\nimport pandas as pd\nfeature_importances = pd.DataFrame(model.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance', ascending=False)\n\n\nimportances\n\n[0.030891482100094805,\n 0.4448331026265109,\n 0.017695413344245573,\n 0.009520184938617332,\n 0.01937533663501595,\n 0.06499114062861666,\n 0.026702090192497516,\n 0.18974364950033454,\n 0.14563102038830425,\n 0.050616579645762515]\n\n\n\n\nConfusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\ncnf_matrix\n\narray([[12386,   413],\n       [  197,    24]])\n\n\n\nsns.set(font_scale=5.0)\nconf_mat = confusion_matrix(y_test, y_pred)\ncm_normalized = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\nfig, ax = plt.subplots(figsize=(30,30), dpi = 100)\nsns.heatmap(cm_normalized, annot=True, cmap=\"Blues\")\nsns.set(font_scale=1)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\n#fig.savefig('cm_augmented.png', dpi=fig.dpi, transparent=True)\nplt.show()\n\n\n\n\n\ncm_normalized\n\narray([[0.96773185, 0.03226815],\n       [0.89140271, 0.10859729]])\n\n\n\nfig, ax = plt.subplots()\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"Blues\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.5)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.tick_params(axis='both', which='major', labelsize=10, labelbottom = False, bottom=False, top = True, labeltop=True)\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\n\nmodel = RandomForestClassifier(n_estimators=100, bootstrap=True,\n                               max_features='sqrt', n_jobs=3, verbose=1, class_weight=\"balanced\")\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nDataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  model.fit(X_train, y_train)\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.6s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.1s finished\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\n\nmodel = RandomForestClassifier(n_estimators=100, bootstrap=True,\n                               max_features='sqrt', n_jobs=3, verbose=1, class_weight=\"balanced\")\n\nmodel.fit(X_train_SMOTE, y_train_SMOTE)\n\ny_pred = model.predict(X_test)\n\nDataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  model.fit(X_train_SMOTE, y_train_SMOTE)\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    1.9s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.1s finished\n\n\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\n\nAccuracy: 0.9541474654377881\nPrecision: 0.05660377358490566\nRecall: 0.1085972850678733\n\n\n\ny_pred = model.predict_proba(X_test)[:,1]\ntrain_proba = pd.DataFrame({'predicted_probability': y_pred})\ntrain_proba.info()\n\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 13020 entries, 0 to 13019\nData columns (total 1 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   predicted_probability  13020 non-null  float64\ndtypes: float64(1)\nmemory usage: 101.8 KB\n\n\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.2s finished\n\n\n\n##check whether y_train indexes are the same as X_train indexes \nsame_index = y_test.index == X_test.index\nsame_index.all()\n\nTrue\n\n\n\n## get them into the same pandas frame\ntable = pd.concat([y_test.reset_index(drop=True), train_proba.reset_index(drop=True)], axis=1)\ntable\n\n\n\n\n\n  \n    \n      \n      stroke\n      predicted_probability\n    \n  \n  \n    \n      0\n      0\n      0.63\n    \n    \n      1\n      0\n      0.25\n    \n    \n      2\n      0\n      0.00\n    \n    \n      3\n      0\n      0.00\n    \n    \n      4\n      0\n      0.00\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      13015\n      0\n      0.00\n    \n    \n      13016\n      0\n      0.00\n    \n    \n      13017\n      0\n      0.00\n    \n    \n      13018\n      0\n      0.00\n    \n    \n      13019\n      1\n      0.70\n    \n  \n\n13020 rows  2 columns\n\n\n\n\ntable.stroke.value_counts()\n\n0    12799\n1      221\nName: stroke, dtype: int64\n\n\n\ntable.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 13020 entries, 0 to 13019\nData columns (total 2 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   stroke                 13020 non-null  int64  \n 1   predicted_probability  13020 non-null  float64\ndtypes: float64(1), int64(1)\nmemory usage: 203.6 KB\n\n\n\ntable.to_csv('../processed_csvs/healthcare_table.csv')\n\n\n\nCross-Validation Precision\n\nfrom sklearn.ensemble import RandomForestClassifier\nforest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n\n#cross validation predictions for test set\ny_test_pred = cross_val_predict(forest_clf, X_test, y_test, cv=5)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_test_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_test_pred))\n\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n\n\nAccuracy: 0.9827188940092166\nPrecision: 0.0\nRecall: 0.0\n\n\n\n#cross validation predictions for full dataset\ny_pred = cross_val_predict(forest_clf, X, y, cv=5)\n\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n\n\n\nprint(\"Accuracy:\",metrics.accuracy_score(y, y_pred))\nprint(\"Precision:\",metrics.precision_score(y, y_pred))\nprint(\"Recall:\",metrics.recall_score(y, y_pred))\n\nAccuracy: 0.9817741935483871\nPrecision: 0.0\nRecall: 0.0\n\n\n\ntest_proba = pd.DataFrame({'predicted_probability': y_pred})\ntest_proba.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 43400 entries, 0 to 43399\nData columns (total 1 columns):\n #   Column                 Non-Null Count  Dtype\n---  ------                 --------------  -----\n 0   predicted_probability  43400 non-null  int64\ndtypes: int64(1)\nmemory usage: 339.2 KB\n\n\n\n##check whether y_test indexes are the same as X_test indexes \nsame_index = y.index == X.index\nsame_index.all()\n\nTrue\n\n\n\n## get them into the same pandas frame\ntable = pd.concat([y.reset_index(drop=True), test_proba.reset_index(drop=True)], axis=1)\ntable\n\n\n\n\n\n  \n    \n      \n      stroke\n      predicted_probability\n    \n  \n  \n    \n      0\n      0\n      0\n    \n    \n      1\n      0\n      0\n    \n    \n      2\n      0\n      0\n    \n    \n      3\n      0\n      0\n    \n    \n      4\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      43395\n      0\n      0\n    \n    \n      43396\n      0\n      0\n    \n    \n      43397\n      0\n      0\n    \n    \n      43398\n      0\n      0\n    \n    \n      43399\n      0\n      0\n    \n  \n\n43400 rows  2 columns\n\n\n\n\ntable.stroke.value_counts()\n\n0    42617\n1      783\nName: stroke, dtype: int64\n\n\n\ntable.to_csv('../processed_csvs/final_model_table.csv')\n\n\n\n5-Fold Cross Validation\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\n\n\nfrom sklearn.model_selection import cross_val_score\nmodels = [\n    LogisticRegression(solver=\"liblinear\", random_state=42),\n    RandomForestClassifier(n_estimators=10, random_state=42),\n    KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2),\n    GaussianNB(),\n]\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\nentries = []\nfor model in models:\n  model_name = model.__class__.__name__\n  accuracies = cross_val_score(model, X, y, scoring='precision', cv=CV)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))\ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'precision'])\n\nsns.boxplot(x='model_name', y='precision', data=cv_df)\nsns.stripplot(x='model_name', y='precision', data=cv_df, \n              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)"
  },
  {
    "objectID": "posts/2021-01-01-SNP-trend-minus-cycle.html",
    "href": "posts/2021-01-01-SNP-trend-minus-cycle.html",
    "title": "didactic",
    "section": "",
    "text": "This post includes code and notes adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport pandas_datareader\nimport datetime\nimport pandas_datareader.data as web\n\n\nimport statsmodels.api as sm\nimport quandl\n\n\nstart = datetime.datetime(2019, 1, 1)\nend = pd.to_datetime('today')\n\n\nSP500 = quandl.get(\"MULTPL/SP500_REAL_PRICE_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2019-01-01\n      2607.39\n    \n    \n      2019-02-01\n      2754.86\n    \n    \n      2019-03-01\n      2803.98\n    \n    \n      2019-04-01\n      2903.80\n    \n    \n      2019-05-01\n      2854.71\n    \n    \n      2019-05-31\n      2752.08\n    \n    \n      2019-06-01\n      2890.17\n    \n    \n      2019-07-01\n      2996.11\n    \n    \n      2019-08-01\n      2897.50\n    \n    \n      2019-09-01\n      2982.16\n    \n    \n      2019-10-01\n      2977.68\n    \n    \n      2019-11-01\n      3104.90\n    \n    \n      2019-12-01\n      3176.75\n    \n    \n      2019-12-31\n      3230.58\n    \n    \n      2020-01-01\n      3278.20\n    \n    \n      2020-01-31\n      3225.04\n    \n    \n      2020-02-01\n      3277.31\n    \n    \n      2020-02-28\n      2954.81\n    \n    \n      2020-03-01\n      2652.39\n    \n    \n      2020-03-31\n      2584.59\n    \n    \n      2020-04-01\n      2761.98\n    \n    \n      2020-04-30\n      2912.43\n    \n    \n      2020-05-01\n      2919.61\n    \n    \n      2020-06-01\n      3104.66\n    \n    \n      2020-06-30\n      3100.29\n    \n    \n      2020-07-01\n      3207.62\n    \n    \n      2020-07-31\n      3271.12\n    \n    \n      2020-08-01\n      3391.71\n    \n    \n      2020-08-31\n      3500.31\n    \n    \n      2020-09-01\n      3365.52\n    \n    \n      2020-09-30\n      3363.00\n    \n    \n      2020-10-01\n      3418.70\n    \n    \n      2020-11-01\n      3429.33\n    \n    \n      2020-11-30\n      3621.63\n    \n    \n      2020-12-01\n      3662.45\n    \n  \n\n\n\n\n\nSP500.plot()\n\n<AxesSubplot:xlabel='Date'>\n\n\n\n\n\n\nSP500.index\n\nDatetimeIndex(['2019-01-01', '2019-02-01', '2019-03-01', '2019-04-01',\n               '2019-05-01', '2019-05-31', '2019-06-01', '2019-07-01',\n               '2019-08-01', '2019-09-01', '2019-10-01', '2019-11-01',\n               '2019-12-01', '2019-12-31', '2020-01-01', '2020-01-31',\n               '2020-02-01', '2020-02-28', '2020-03-01', '2020-03-31',\n               '2020-04-01', '2020-04-30', '2020-05-01', '2020-06-01',\n               '2020-06-30', '2020-07-01', '2020-07-31', '2020-08-01',\n               '2020-08-31', '2020-09-01', '2020-09-30', '2020-10-01',\n               '2020-11-01', '2020-11-30', '2020-12-01'],\n              dtype='datetime64[ns]', name='Date', freq=None)\n\n\n\nSP500.head()\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2019-01-01\n      2607.39\n    \n    \n      2019-02-01\n      2754.86\n    \n    \n      2019-03-01\n      2803.98\n    \n    \n      2019-04-01\n      2903.80\n    \n    \n      2019-05-01\n      2854.71\n    \n  \n\n\n\n\n\nSP500['Value'].plot()\nplt.ylabel(\"SP500 Value\")\n\nText(0, 0.5, 'SP500 Value')\n\n\n\n\n\n\n\n\n# Tuple unpacking\nSP500_cycle, SP500_trend = sm.tsa.filters.hpfilter(SP500.Value)\n\n\nSP500_cycle\n\nDate\n2019-01-01   -159.626002\n2019-02-01    -32.234117\n2019-03-01     -3.092466\n2019-04-01     76.968864\n2019-05-01      8.461718\n2019-05-31   -113.170164\n2019-06-01      6.401669\n2019-07-01     94.446400\n2019-08-01    -21.300789\n2019-09-01     47.056253\n2019-10-01     27.196993\n2019-11-01    140.021486\n2019-12-01    198.502790\n2019-12-31    239.946447\n2020-01-01    275.993938\n2020-01-31    211.756776\n2020-02-01    252.953978\n2020-02-28    -81.237788\n2020-03-01   -396.749949\n2020-03-31   -479.773159\n2020-04-01   -320.220102\n2020-04-30   -190.403605\n2020-05-01   -206.636358\n2020-06-01    -47.642048\n2020-06-30    -80.445214\n2020-07-01     -3.630618\n2020-07-31     27.617253\n2020-08-01    114.566186\n2020-08-31    188.466703\n2020-09-01     18.177725\n2020-09-30    -20.499622\n2020-10-01     -1.485569\n2020-11-01    -27.927537\n2020-11-30    127.057981\n2020-12-01    130.481948\nName: Value_cycle, dtype: float64\n\n\n\ntype(SP500_cycle)\n\npandas.core.series.Series\n\n\n\nSP500[\"trend\"] = SP500_trend\n\n\nSP500[['trend','Value']].plot()\n\n<AxesSubplot:xlabel='Date'>\n\n\n\n\n\n\nSP500[['trend','Value']][\"2000-03-31\":].plot(figsize=(12,8))\n\n<AxesSubplot:xlabel='Date'>"
  },
  {
    "objectID": "posts/2020-12-23_Healthcare_Modeling_app.html",
    "href": "posts/2020-12-23_Healthcare_Modeling_app.html",
    "title": "didactic",
    "section": "",
    "text": "This notebook uses SMOTE and cross-validation.\n\n\nimport sys\nimport os\n\nfrom scipy import stats\nfrom datetime import datetime, date\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nimport xgboost as xgb\n\n\n%matplotlib inline\nplt.style.use(\"fivethirtyeight\")\nsns.set_context(\"notebook\")\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport pickle\nfrom sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict,cross_validate\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix"
  },
  {
    "objectID": "posts/2020-12-23_Healthcare_Modeling_app.html#data-prep",
    "href": "posts/2020-12-23_Healthcare_Modeling_app.html#data-prep",
    "title": "didactic",
    "section": "Data Prep",
    "text": "Data Prep\n\ndf = df.drop(columns = ['id'])\n\n\n# Label Encoding\nfor f in df.columns:\n    if df[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df[f].values))\n        df[f] = lbl.transform(list(df[f].values))\n\n\npct_list = []\nfor col in df.columns:\n    pct_missing = np.mean(df[col].isnull())\n    if round(pct_missing*100) >0:\n        pct_list.append([col, round(pct_missing*100)])\n    print('{} - {}%'.format(col, round(pct_missing*100)))\n\ngender - 0%\nage - 0%\nhypertension - 0%\nheart_disease - 0%\never_married - 0%\nwork_type - 0%\nResidence_type - 0%\navg_glucose_level - 0%\nbmi - 3%\nsmoking_status - 0%\nstroke - 0%\n\n\n\ndf = df.fillna(df.mean())\n\n\ndf=df.dropna()\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 43400 entries, 0 to 43399\nData columns (total 11 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   gender             43400 non-null  int64  \n 1   age                43400 non-null  float64\n 2   hypertension       43400 non-null  int64  \n 3   heart_disease      43400 non-null  int64  \n 4   ever_married       43400 non-null  int64  \n 5   work_type          43400 non-null  int64  \n 6   Residence_type     43400 non-null  int64  \n 7   avg_glucose_level  43400 non-null  float64\n 8   bmi                43400 non-null  float64\n 9   smoking_status     43400 non-null  int64  \n 10  stroke             43400 non-null  int64  \ndtypes: float64(3), int64(8)\nmemory usage: 4.0 MB\n\n\n\nFeatures = ['age','heart_disease','ever_married']\nx = df[Features]\ny = df[\"stroke\"]\n\n\n# Train Test split\nX_train, X_test,y_train,y_test = train_test_split(x,y, test_size=0.2, random_state=2)\n\n\n#### Data Preprocessing\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_validate\nimport xgboost as xgb\n\n\n\nclf = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=9,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    missing=-999,\n    random_state=2021,\n    tree_method='auto'\n#    tree_method='hist'\n#    tree_method='gpu_hist'\n)\n\n\nkfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n\n\nparam_grid = { \n    'colsample_bytree':[.75,1],\n    'learning_rate':[0.01,0.05,0.1,0.3,0.5],\n    'max_depth':[1,2,3,5],\n    'subsample':[.75,1],\n    'n_estimators': list(range(50, 400, 50))\n}\n\n\ngrid_search = GridSearchCV(estimator=clf, scoring='roc_auc', param_grid=param_grid, n_jobs=-1, cv=kfold)\n\n\n%%time\ngrid_result = grid_search.fit(X_train, y_train)\n\n/home/david/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n\n\n[12:38:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nCPU times: user 9.02 s, sys: 860 ms, total: 9.88 s\nWall time: 12min 43s\n\n\n\nprint(f'Best: {grid_result.best_score_} using {grid_result.best_params_}','\\n')\n\nBest: 0.8410524780191915 using {'colsample_bytree': 0.75, 'learning_rate': 0.05, 'max_depth': 1, 'n_estimators': 200, 'subsample': 0.75} \n\n\n\n\n#Set our final hyperparameters to the tuned values\nxgbcl = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,\n         gamma=0.0, max_delta_step=0.0, min_child_weight=1.0,\n         missing=None, n_jobs=-1, objective='binary:logistic', random_state=42, reg_alpha=0.0,\n         reg_lambda=1.0, scale_pos_weight=1.0, tree_method='auto',\n         colsample_bytree = grid_result.best_params_['colsample_bytree'], \n         learning_rate = grid_result.best_params_['learning_rate'], \n         max_depth = grid_result.best_params_['max_depth'], \n         subsample = grid_result.best_params_['subsample'], \n         n_estimators = grid_result.best_params_['n_estimators'])\n\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n#refit the model on k-folds to get stable avg error metrics\nscores = cross_validate(estimator=xgbcl, X=X_train, y=y_train, cv=kfold, n_jobs=-1, \n                        scoring=['accuracy', 'roc_auc', 'precision', 'recall', 'f1'])\n\nprint('Training 5-fold Cross Validation Results:\\n')\nprint('AUC: ', scores['test_roc_auc'].mean())\nprint('Accuracy: ', scores['test_accuracy'].mean())\nprint('Precision: ', scores['test_precision'].mean())\nprint('Recall: ', scores['test_recall'].mean())\nprint('F1: ', scores['test_f1'].mean(), '\\n')\n\nTraining 5-fold Cross Validation Results:\n\nAUC:  0.8429184127269972\nAccuracy:  0.9820852534562212\nPrecision:  0.0\nRecall:  0.0\nF1:  0.0 \n\n\n\n\nimport sklearn.metrics as metrics\n\n\n#Fit the final model\nxgbcl.fit(X_train, y_train)\n\n#Generate predictions against our training and test data\npred_train = xgbcl.predict(X_train)\nproba_train = xgbcl.predict_proba(X_train)\npred_test = xgbcl.predict(X_test)\nproba_test = xgbcl.predict_proba(X_test)\n\n# Print model report\nprint(\"Classification report (Test): \\n\")\nprint(metrics.classification_report(y_test, pred_test))\nprint(\"Confusion matrix (Test): \\n\")\nprint(metrics.confusion_matrix(y_test, pred_test)/len(y_test))\n\nprint ('\\nTrain Accuracy:', metrics.accuracy_score(y_train, pred_train))\nprint ('Test Accuracy:', metrics.accuracy_score(y_test, pred_test))\n\nprint ('\\nTrain AUC:', metrics.roc_auc_score(y_train, proba_train[:,1]))\nprint ('Test AUC:', metrics.roc_auc_score(y_test, proba_test[:,1]))\n\n# calculate the fpr and tpr for all thresholds of the classification\ntrain_fpr, train_tpr, train_threshold = metrics.roc_curve(y_train, proba_train[:,1])\ntest_fpr, test_tpr, test_threshold = metrics.roc_curve(y_test, proba_test[:,1])\n\ntrain_roc_auc = metrics.auc(train_fpr, train_tpr)\ntest_roc_auc = metrics.auc(test_fpr, test_tpr)\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=[7,5])\nplt.title('Receiver Operating Characteristic')\nplt.plot(train_fpr, train_tpr, 'b', label = 'Train AUC = %0.2f' % train_roc_auc)\nplt.plot(test_fpr, test_tpr, 'g', label = 'Test AUC = %0.2f' % test_roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n# plot feature importance\nxgb.plot_importance(xgbcl, importance_type='gain');\n\n[12:38:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n\n\n/home/david/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n\n\nClassification report (Test): \n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      8519\n           1       0.00      0.00      0.00       161\n\n    accuracy                           0.98      8680\n   macro avg       0.49      0.50      0.50      8680\nweighted avg       0.96      0.98      0.97      8680\n\nConfusion matrix (Test): \n\n[[0.98145161 0.        ]\n [0.01854839 0.        ]]\n\nTrain Accuracy: 0.9820852534562212\nTest Accuracy: 0.9814516129032258\n\nTrain AUC: 0.8465318377764562\nTest AUC: 0.8563193417126058\n\n\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\n\n\n\n\n\npickle.dump(xgbcl, open('stroke_xgboost_model.pkl', 'wb'))\npickle.dump(scaler, open('scaler.pkl', 'wb'))\n\n\nmodel = pickle.load(open('stroke_xgboost_model.pkl', 'rb'))\nprint(model)\n\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,\n              colsample_bynode=1, colsample_bytree=0.75, gamma=0.0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.05, max_delta_step=0.0, max_depth=1,\n              min_child_weight=1.0, missing=None, monotone_constraints='()',\n              n_estimators=200, n_jobs=-1, num_parallel_tree=1, random_state=42,\n              reg_alpha=0.0, reg_lambda=1.0, scale_pos_weight=1.0,\n              subsample=0.75, tree_method='auto', validate_parameters=1,\n              verbosity=None)"
  },
  {
    "objectID": "posts/2020-12-30-TextSummarizer-tieng-viet.html",
    "href": "posts/2020-12-30-TextSummarizer-tieng-viet.html",
    "title": "didactic",
    "section": "",
    "text": "# Natural Language Tool Kit (NLTK)\nimport nltk\nnltk.download('stopwords')\n\n# Regular Expression for text preprocessing\nimport re\n\n# Heap (priority) queue algorithm to get the top sentences\nimport heapq\n\n# NumPy for numerical computing\nimport numpy as np\n\n# pandas for creating DataFrames\nimport pandas as pd\n\n# matplotlib for plot\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\n\n\n\n\n# load text file\nwith open('viet_article1.txt', 'r') as f:\n    file_data = f.read()\n\n\n# view text data\nprint(file_data)\n\nThm 9 ngi nhp cnh nhim nCoV\n\nNgy 1/1, B Y t ghi nhn ca dng tnh nCoV, u l ngi nhp cnh c cch ly ngay ti Khnh Ha, Bnh Dng v Long An. Tng ca nhim ln 1.474.\n\n\"Bnh nhn 1466\", nam, 35 tui,  TP Hng Yn, tnh Hng Yn. \"Bnh nhn 1467\" nam, 38, tui,  huyn Thanh H, tnh Hi Dng. \"Bnh nhn 1468\", nam, 51 tui,  huyn Thanh Chng, tnh Ngh An. Ba ngi ny t Nga nhp cnh sn bay Cam Ranh trn chuyn bay VN5062 ngy 25/12, cch ly ti tnh Khnh Ha. Kt qu xt nghim ln mt ngy 25/12 m tnh; ly mu ln hai ngy 30/12 kt qu xt nghim ti Trung tm Kim sot Bnh tt tnh Khnh Ha dng tnh vi nCoV, cc bnh nhn iu tr ti Bnh vin Bnh nhit i tnh Khnh Ha.\n\nTrc  trn chuyn bay ny  ghi nhn 9 trng hp dng tnh vi nCoV.\n\n\"Bnh nhn 1469\", nam, 32 tui,  qun Hi Chu, TP  Nng. \"Bnh nhn 1470\", n, 40 tui,  huyn Ba V, v \"bnh nhn 1471\", nam, 37 tui,  huyn Chng M, TP H Ni. \"Bnh nhn 1472\", nam, 29 tui,  huyn Kim Thnh, v \"bnh nhn 1473\", nam, 37 tui,  huyn Thanh H, tnh Hi Dng. H t c nhp cnh sn bay Tn Sn Nht trn chuyn bay VN30 ngy 30/12 c cch ly ti tnh Bnh Dng. Kt qu xt nghim ngy 31/12 ti Vin Pasteur TP HCM dng tnh vi nCoV, cc bnh nhn iu tr ti Bnh vin a khoa tnh Bnh Dng.\n\n\"Bnh nhn 1474\", nam, 50 tui,  Qun 7, TP HCM. ng t Canada qu cnh sn bay Incheon (Hn Quc), sau  nhp cnh sn bay Tn Sn Nht trn chuyn bay VN405 ngy 29/12, c cch ly ti tnh Long An. Ly mu ngy 30/12, kt qu xt nghim ti Trung tm Kim sot bnh tt tnh Long An v Vin Pasteur TP HCM u dng tnh vi nCoV, bnh nhn iu tr ti Bnh vin a khoa tnh Long An.\n\nNh vy, tng ca nhim ln 1.474, tng s khi 1.325. S ngi t vong do Covid-19 l 35, bn ngi t vong sau ba n bn ln xt nghim m tnh. Cc bnh nhn cn li a s sc khe n nh, trong  7 ngi xt nghim m tnh nCoV ln mt, 11 ngi m tnh ln hai v 10 ngi m tnh ln ba.\n\nTng s ngi tip xc gn v nhp cnh t vng dch ang c cch ly hn 17.000. Trong , cch ly tp trung ti bnh vin 150; cch ly tp trung hn 15.000, cn li  nh hoc ni lu tr.\n\nVit Nam  30 ngy khng ghi nhn ca nhim mi trong cng ng. Song, nguy c dch xm nhp lun thng trc, c bit khi tng s lng chuyn bay a cng dn Vit Nam, chuyn gia v nc. Nh chc trch lun cnh bo nguy c dch xm nhp t ngi nhp cnh tri php.\n\nB Y t khuyn co ngi dn tip tc thc hin tt \"Thng ip 5K\", nht l eo khu trang v ra tay bng x phng, dung dch st khun.\n\nTh gii ghi nhn hn 1,7 triu ngi cht v nCoV trong hn 83 triu ngi nhim. M l quc gia chu nh hng nghim trng nht ca dch, tip theo l n  v Brazil.\n\n\n\n\ntext = file_data\ntext = re.sub(r'\\[[0-9]*\\]',' ',text) # replace reference number i.e. [1], [10], [20] with empty space, if any..\ntext = re.sub(r'\\s+',' ',text) # replace one or more spaces with single space\nprint(text)\n\nThm 9 ngi nhp cnh nhim nCoV Ngy 1/1, B Y t ghi nhn ca dng tnh nCoV, u l ngi nhp cnh c cch ly ngay ti Khnh Ha, Bnh Dng v Long An. Tng ca nhim ln 1.474. \"Bnh nhn 1466\", nam, 35 tui,  TP Hng Yn, tnh Hng Yn. \"Bnh nhn 1467\" nam, 38, tui,  huyn Thanh H, tnh Hi Dng. \"Bnh nhn 1468\", nam, 51 tui,  huyn Thanh Chng, tnh Ngh An. Ba ngi ny t Nga nhp cnh sn bay Cam Ranh trn chuyn bay VN5062 ngy 25/12, cch ly ti tnh Khnh Ha. Kt qu xt nghim ln mt ngy 25/12 m tnh; ly mu ln hai ngy 30/12 kt qu xt nghim ti Trung tm Kim sot Bnh tt tnh Khnh Ha dng tnh vi nCoV, cc bnh nhn iu tr ti Bnh vin Bnh nhit i tnh Khnh Ha. Trc  trn chuyn bay ny  ghi nhn 9 trng hp dng tnh vi nCoV. \"Bnh nhn 1469\", nam, 32 tui,  qun Hi Chu, TP  Nng. \"Bnh nhn 1470\", n, 40 tui,  huyn Ba V, v \"bnh nhn 1471\", nam, 37 tui,  huyn Chng M, TP H Ni. \"Bnh nhn 1472\", nam, 29 tui,  huyn Kim Thnh, v \"bnh nhn 1473\", nam, 37 tui,  huyn Thanh H, tnh Hi Dng. H t c nhp cnh sn bay Tn Sn Nht trn chuyn bay VN30 ngy 30/12 c cch ly ti tnh Bnh Dng. Kt qu xt nghim ngy 31/12 ti Vin Pasteur TP HCM dng tnh vi nCoV, cc bnh nhn iu tr ti Bnh vin a khoa tnh Bnh Dng. \"Bnh nhn 1474\", nam, 50 tui,  Qun 7, TP HCM. ng t Canada qu cnh sn bay Incheon (Hn Quc), sau  nhp cnh sn bay Tn Sn Nht trn chuyn bay VN405 ngy 29/12, c cch ly ti tnh Long An. Ly mu ngy 30/12, kt qu xt nghim ti Trung tm Kim sot bnh tt tnh Long An v Vin Pasteur TP HCM u dng tnh vi nCoV, bnh nhn iu tr ti Bnh vin a khoa tnh Long An. Nh vy, tng ca nhim ln 1.474, tng s khi 1.325. S ngi t vong do Covid-19 l 35, bn ngi t vong sau ba n bn ln xt nghim m tnh. Cc bnh nhn cn li a s sc khe n nh, trong  7 ngi xt nghim m tnh nCoV ln mt, 11 ngi m tnh ln hai v 10 ngi m tnh ln ba. Tng s ngi tip xc gn v nhp cnh t vng dch ang c cch ly hn 17.000. Trong , cch ly tp trung ti bnh vin 150; cch ly tp trung hn 15.000, cn li  nh hoc ni lu tr. Vit Nam  30 ngy khng ghi nhn ca nhim mi trong cng ng. Song, nguy c dch xm nhp lun thng trc, c bit khi tng s lng chuyn bay a cng dn Vit Nam, chuyn gia v nc. Nh chc trch lun cnh bo nguy c dch xm nhp t ngi nhp cnh tri php. B Y t khuyn co ngi dn tip tc thc hin tt \"Thng ip 5K\", nht l eo khu trang v ra tay bng x phng, dung dch st khun. Th gii ghi nhn hn 1,7 triu ngi cht v nCoV trong hn 83 triu ngi nhim. M l quc gia chu nh hng nghim trng nht ca dch, tip theo l n  v Brazil. \n\n\n\n# generate clean text\nclean_text = text.lower() # convert all uppercase characters into lowercase characters\n\n# replace characters other than [a-zA-Z0-9], digits & one or more spaces with single space\nregex_patterns = [r'\\W',r'\\d',r'\\s+']\nfor regex in regex_patterns:\n    clean_text = re.sub(regex,' ',clean_text)\n\nprint(clean_text)\n\nthm ngi nhp cnh nhim ncov ngy b y t ghi nhn ca dng tnh ncov u l ngi nhp cnh c cch ly ngay ti khnh ha bnh dng v long an tng ca nhim ln bnh nhn nam tui  tp hng yn tnh hng yn bnh nhn nam tui  huyn thanh h tnh hi dng bnh nhn nam tui  huyn thanh chng tnh ngh an ba ngi ny t nga nhp cnh sn bay cam ranh trn chuyn bay vn ngy cch ly ti tnh khnh ha kt qu xt nghim ln mt ngy m tnh ly mu ln hai ngy kt qu xt nghim ti trung tm kim sot bnh tt tnh khnh ha dng tnh vi ncov cc bnh nhn iu tr ti bnh vin bnh nhit i tnh khnh ha trc  trn chuyn bay ny  ghi nhn trng hp dng tnh vi ncov bnh nhn nam tui  qun hi chu tp  nng bnh nhn n tui  huyn ba v v bnh nhn nam tui  huyn chng m tp h ni bnh nhn nam tui  huyn kim thnh v bnh nhn nam tui  huyn thanh h tnh hi dng h t c nhp cnh sn bay tn sn nht trn chuyn bay vn ngy c cch ly ti tnh bnh dng kt qu xt nghim ngy ti vin pasteur tp hcm dng tnh vi ncov cc bnh nhn iu tr ti bnh vin a khoa tnh bnh dng bnh nhn nam tui  qun tp hcm ng t canada qu cnh sn bay incheon hn quc sau  nhp cnh sn bay tn sn nht trn chuyn bay vn ngy c cch ly ti tnh long an ly mu ngy kt qu xt nghim ti trung tm kim sot bnh tt tnh long an v vin pasteur tp hcm u dng tnh vi ncov bnh nhn iu tr ti bnh vin a khoa tnh long an nh vy tng ca nhim ln tng s khi s ngi t vong do covid l bn ngi t vong sau ba n bn ln xt nghim m tnh cc bnh nhn cn li a s sc khe n nh trong  ngi xt nghim m tnh ncov ln mt ngi m tnh ln hai v ngi m tnh ln ba tng s ngi tip xc gn v nhp cnh t vng dch ang c cch ly hn trong  cch ly tp trung ti bnh vin cch ly tp trung hn cn li  nh hoc ni lu tr vit nam  ngy khng ghi nhn ca nhim mi trong cng ng song nguy c dch xm nhp lun thng trc c bit khi tng s lng chuyn bay a cng dn vit nam chuyn gia v nc nh chc trch lun cnh bo nguy c dch xm nhp t ngi nhp cnh tri php b y t khuyn co ngi dn tip tc thc hin tt thng ip k nht l eo khu trang v ra tay bng x phng dung dch st khun th gii ghi nhn hn triu ngi cht v ncov trong hn triu ngi nhim m l quc gia chu nh hng nghim trng nht ca dch tip theo l n  v brazil \n\n\n\n# split (tokenize) the sentences\nsentences = nltk.sent_tokenize(text)\nprint(sentences)\n\n['Thm 9 ngi nhp cnh nhim nCoV Ngy 1/1, B Y t ghi nhn ca dng tnh nCoV, u l ngi nhp cnh c cch ly ngay ti Khnh Ha, Bnh Dng v Long An.', 'Tng ca nhim ln 1.474.', '\"Bnh nhn 1466\", nam, 35 tui,  TP Hng Yn, tnh Hng Yn.', '\"Bnh nhn 1467\" nam, 38, tui,  huyn Thanh H, tnh Hi Dng.', '\"Bnh nhn 1468\", nam, 51 tui,  huyn Thanh Chng, tnh Ngh An.', 'Ba ngi ny t Nga nhp cnh sn bay Cam Ranh trn chuyn bay VN5062 ngy 25/12, cch ly ti tnh Khnh Ha.', 'Kt qu xt nghim ln mt ngy 25/12 m tnh; ly mu ln hai ngy 30/12 kt qu xt nghim ti Trung tm Kim sot Bnh tt tnh Khnh Ha dng tnh vi nCoV, cc bnh nhn iu tr ti Bnh vin Bnh nhit i tnh Khnh Ha.', 'Trc  trn chuyn bay ny  ghi nhn 9 trng hp dng tnh vi nCoV.', '\"Bnh nhn 1469\", nam, 32 tui,  qun Hi Chu, TP  Nng.', '\"Bnh nhn 1470\", n, 40 tui,  huyn Ba V, v \"bnh nhn 1471\", nam, 37 tui,  huyn Chng M, TP H Ni.', '\"Bnh nhn 1472\", nam, 29 tui,  huyn Kim Thnh, v \"bnh nhn 1473\", nam, 37 tui,  huyn Thanh H, tnh Hi Dng.', 'H t c nhp cnh sn bay Tn Sn Nht trn chuyn bay VN30 ngy 30/12 c cch ly ti tnh Bnh Dng.', 'Kt qu xt nghim ngy 31/12 ti Vin Pasteur TP HCM dng tnh vi nCoV, cc bnh nhn iu tr ti Bnh vin a khoa tnh Bnh Dng.', '\"Bnh nhn 1474\", nam, 50 tui,  Qun 7, TP HCM.', 'ng t Canada qu cnh sn bay Incheon (Hn Quc), sau  nhp cnh sn bay Tn Sn Nht trn chuyn bay VN405 ngy 29/12, c cch ly ti tnh Long An.', 'Ly mu ngy 30/12, kt qu xt nghim ti Trung tm Kim sot bnh tt tnh Long An v Vin Pasteur TP HCM u dng tnh vi nCoV, bnh nhn iu tr ti Bnh vin a khoa tnh Long An.', 'Nh vy, tng ca nhim ln 1.474, tng s khi 1.325.', 'S ngi t vong do Covid-19 l 35, bn ngi t vong sau ba n bn ln xt nghim m tnh.', 'Cc bnh nhn cn li a s sc khe n nh, trong  7 ngi xt nghim m tnh nCoV ln mt, 11 ngi m tnh ln hai v 10 ngi m tnh ln ba.', 'Tng s ngi tip xc gn v nhp cnh t vng dch ang c cch ly hn 17.000.', 'Trong , cch ly tp trung ti bnh vin 150; cch ly tp trung hn 15.000, cn li  nh hoc ni lu tr.', 'Vit Nam  30 ngy khng ghi nhn ca nhim mi trong cng ng.', 'Song, nguy c dch xm nhp lun thng trc, c bit khi tng s lng chuyn bay a cng dn Vit Nam, chuyn gia v nc.', 'Nh chc trch lun cnh bo nguy c dch xm nhp t ngi nhp cnh tri php.', 'B Y t khuyn co ngi dn tip tc thc hin tt \"Thng ip 5K\", nht l eo khu trang v ra tay bng x phng, dung dch st khun.', 'Th gii ghi nhn hn 1,7 triu ngi cht v nCoV trong hn 83 triu ngi nhim.', 'M l quc gia chu nh hng nghim trng nht ca dch, tip theo l n  v Brazil.']\n\n\n\n# # get stop words list\n# stop_words = nltk.corpus.stopwords.words('Vietnamese')\n# print(stop_words)\n\n\n# create an empty dictionary to house the word count\nword_count = {}\n\n# loop through tokenized words, remove stop words and save word count to dictionary\nfor word in nltk.word_tokenize(clean_text):\n        # save word count to dictionary\n        if word not in word_count.keys():\n            word_count[word] = 1\n        else:\n            word_count[word] += 1\n\n\nplt.figure(figsize=(16,10))\nplt.xticks(rotation = 90)\nplt.bar(word_count.keys(), word_count.values())\nplt.show()\n\n\n\n\n\n# helper function for plotting the top words.\ndef plot_top_words(word_count_dict, show_top_n=20):\n    word_count_table = pd.DataFrame.from_dict(word_count_dict, orient = 'index').rename(columns={0: 'score'})\n    word_count_table.sort_values(by='score').tail(show_top_n).plot(kind='barh', figsize=(10,10))\n    plt.show()\n\n\nplot_top_words(word_count, 20)\n\n\n\n\n\n# create empty dictionary to house sentence score    \nsentence_score = {}\n\n# loop through tokenized sentence, only take sentences that have less than 30 words, then add word score to form sentence score\nfor sentence in sentences:\n    # check if word in sentence is in word_count dictionary\n    for word in nltk.word_tokenize(sentence.lower()):\n        if word in word_count.keys():\n            # only take sentence that has less than 30 words\n            if len(sentence.split(' ')) < 30:\n                # add word score to sentence score\n                if sentence not in sentence_score.keys():\n                    sentence_score[sentence] = word_count[word]\n                else:\n                    sentence_score[sentence] += word_count[word]\n\n\ndf_sentence_score = pd.DataFrame.from_dict(sentence_score, orient = 'index').rename(columns={0: 'score'})\ndf_sentence_score.sort_values(by='score', ascending = False)\n\n\n\n\n\n  \n    \n      \n      score\n    \n  \n  \n    \n      Kt qu xt nghim ngy 31/12 ti Vin Pasteur TP HCM dng tnh vi nCoV, cc bnh nhn iu tr ti Bnh vin a khoa tnh Bnh Dng.\n      198\n    \n    \n      \"Bnh nhn 1472\", nam, 29 tui,  huyn Kim Thnh, v \"bnh nhn 1473\", nam, 37 tui,  huyn Thanh H, tnh Hi Dng.\n      177\n    \n    \n      \"Bnh nhn 1470\", n, 40 tui,  huyn Ba V, v \"bnh nhn 1471\", nam, 37 tui,  huyn Chng M, TP H Ni.\n      155\n    \n    \n      Ba ngi ny t Nga nhp cnh sn bay Cam Ranh trn chuyn bay VN5062 ngy 25/12, cch ly ti tnh Khnh Ha.\n      130\n    \n    \n      H t c nhp cnh sn bay Tn Sn Nht trn chuyn bay VN30 ngy 30/12 c cch ly ti tnh Bnh Dng.\n      127\n    \n    \n      Trong , cch ly tp trung ti bnh vin 150; cch ly tp trung hn 15.000, cn li  nh hoc ni lu tr.\n      109\n    \n    \n      \"Bnh nhn 1467\" nam, 38, tui,  huyn Thanh H, tnh Hi Dng.\n      99\n    \n    \n      \"Bnh nhn 1468\", nam, 51 tui,  huyn Thanh Chng, tnh Ngh An.\n      91\n    \n    \n      S ngi t vong do Covid-19 l 35, bn ngi t vong sau ba n bn ln xt nghim m tnh.\n      89\n    \n    \n      \"Bnh nhn 1466\", nam, 35 tui,  TP Hng Yn, tnh Hng Yn.\n      88\n    \n    \n      Tng s ngi tip xc gn v nhp cnh t vng dch ang c cch ly hn 17.000.\n      87\n    \n    \n      \"Bnh nhn 1469\", nam, 32 tui,  qun Hi Chu, TP  Nng.\n      76\n    \n    \n      Nh chc trch lun cnh bo nguy c dch xm nhp t ngi nhp cnh tri php.\n      74\n    \n    \n      \"Bnh nhn 1474\", nam, 50 tui,  Qun 7, TP HCM.\n      73\n    \n    \n      Song, nguy c dch xm nhp lun thng trc, c bit khi tng s lng chuyn bay a cng dn Vit Nam, chuyn gia v nc.\n      70\n    \n    \n      Trc  trn chuyn bay ny  ghi nhn 9 trng hp dng tnh vi nCoV.\n      69\n    \n    \n      Th gii ghi nhn hn 1,7 triu ngi cht v nCoV trong hn 83 triu ngi nhim.\n      68\n    \n    \n      Vit Nam  30 ngy khng ghi nhn ca nhim mi trong cng ng.\n      48\n    \n    \n      M l quc gia chu nh hng nghim trng nht ca dch, tip theo l n  v Brazil.\n      46\n    \n    \n      Nh vy, tng ca nhim ln 1.474, tng s khi 1.325.\n      27\n    \n    \n      Tng ca nhim ln 1.474.\n      15\n    \n  \n\n\n\n\n\n# get the best 3 sentences for summary             \nbest_sentences = heapq.nlargest(3, sentence_score, key=sentence_score.get)\n\n\nprint('SUMMARY')\nprint('------------------------')\n\n# display top sentences based on their sentence sequence in the original text\nfor sentence in sentences:\n    if sentence in best_sentences:\n        print (sentence)\n\nSUMMARY\n------------------------\n\"Bnh nhn 1470\", n, 40 tui,  huyn Ba V, v \"bnh nhn 1471\", nam, 37 tui,  huyn Chng M, TP H Ni.\n\"Bnh nhn 1472\", nam, 29 tui,  huyn Kim Thnh, v \"bnh nhn 1473\", nam, 37 tui,  huyn Thanh H, tnh Hi Dng.\nKt qu xt nghim ngy 31/12 ti Vin Pasteur TP HCM dng tnh vi nCoV, cc bnh nhn iu tr ti Bnh vin a khoa tnh Bnh Dng.\n\n\n\nimport wordcloud\n\n# Wordcloud of training set\ncloud = np.array(file_data).flatten()\nplt.figure(figsize=(20,10))\nword_cloud = wordcloud.WordCloud(\n    max_words=200,background_color =\"black\",\n    width=2000,height=1000,mode=\"RGB\"\n).generate(str(cloud))\nplt.axis(\"off\")\nplt.imshow(word_cloud)\n\n<matplotlib.image.AxesImage at 0x7f3cc9e372b0>\n\n\n\n\n\n\nimport wordcloud\n\n# Wordcloud of training set\ncloud = np.array(best_sentences).flatten()\nplt.figure(figsize=(20,10))\nword_cloud = wordcloud.WordCloud(\n    max_words=200,background_color =\"black\",\n    width=2000,height=1000,mode=\"RGB\"\n).generate(str(cloud))\nplt.axis(\"off\")\nplt.imshow(word_cloud)\n\n<matplotlib.image.AxesImage at 0x7f3d09c0b190>"
  },
  {
    "objectID": "posts/2020-12-12_Healthcare_Modeling-Copy1.html",
    "href": "posts/2020-12-12_Healthcare_Modeling-Copy1.html",
    "title": "didactic",
    "section": "",
    "text": "This notebook uses SMOTE and cross-validation.\n\n\nimport sys\nimport os\n\nfrom scipy import stats\nfrom datetime import datetime, date\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nimport xgboost as xgb\n\n\n%matplotlib inline\nplt.style.use(\"fivethirtyeight\")\nsns.set_context(\"notebook\")\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing"
  },
  {
    "objectID": "posts/2020-12-12_Healthcare_Modeling-Copy1.html#data-prep",
    "href": "posts/2020-12-12_Healthcare_Modeling-Copy1.html#data-prep",
    "title": "didactic",
    "section": "Data Prep",
    "text": "Data Prep\n\ndf = df.drop(columns = ['id'])\n\n\n# Label Encoding\nfor f in df.columns:\n    if df[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df[f].values))\n        df[f] = lbl.transform(list(df[f].values))\n\n\npct_list = []\nfor col in df.columns:\n    pct_missing = np.mean(df[col].isnull())\n    if round(pct_missing*100) >0:\n        pct_list.append([col, round(pct_missing*100)])\n    print('{} - {}%'.format(col, round(pct_missing*100)))\n\ngender - 0%\nage - 0%\nhypertension - 0%\nheart_disease - 0%\never_married - 0%\nwork_type - 0%\nResidence_type - 0%\navg_glucose_level - 0%\nbmi - 3%\nsmoking_status - 0%\nstroke - 0%\n\n\n\ndf = df.fillna(df.mean())\n\n\ndf=df.dropna()\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 43400 entries, 0 to 43399\nData columns (total 11 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   gender             43400 non-null  int64  \n 1   age                43400 non-null  float64\n 2   hypertension       43400 non-null  int64  \n 3   heart_disease      43400 non-null  int64  \n 4   ever_married       43400 non-null  int64  \n 5   work_type          43400 non-null  int64  \n 6   Residence_type     43400 non-null  int64  \n 7   avg_glucose_level  43400 non-null  float64\n 8   bmi                43400 non-null  float64\n 9   smoking_status     43400 non-null  int64  \n 10  stroke             43400 non-null  int64  \ndtypes: float64(3), int64(8)\nmemory usage: 4.0 MB"
  },
  {
    "objectID": "posts/2020-12-10_EDA_Health_Data.html",
    "href": "posts/2020-12-10_EDA_Health_Data.html",
    "title": "didactic",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n  \n    \n      \n      id\n      gender\n      age\n      hypertension\n      heart_disease\n      ever_married\n      work_type\n      Residence_type\n      avg_glucose_level\n      bmi\n      smoking_status\n      stroke\n    \n  \n  \n    \n      0\n      30669\n      Male\n      3.0\n      0\n      0\n      No\n      children\n      Rural\n      95.12\n      18.0\n      NaN\n      0\n    \n    \n      1\n      30468\n      Male\n      58.0\n      1\n      0\n      Yes\n      Private\n      Urban\n      87.96\n      39.2\n      never smoked\n      0\n    \n    \n      2\n      16523\n      Female\n      8.0\n      0\n      0\n      No\n      Private\n      Urban\n      110.89\n      17.6\n      NaN\n      0\n    \n    \n      3\n      56543\n      Female\n      70.0\n      0\n      0\n      Yes\n      Private\n      Rural\n      69.04\n      35.9\n      formerly smoked\n      0\n    \n    \n      4\n      46136\n      Male\n      14.0\n      0\n      0\n      No\n      Never_worked\n      Rural\n      161.28\n      19.1\n      NaN\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      43395\n      56196\n      Female\n      10.0\n      0\n      0\n      No\n      children\n      Urban\n      58.64\n      20.4\n      never smoked\n      0\n    \n    \n      43396\n      5450\n      Female\n      56.0\n      0\n      0\n      Yes\n      Govt_job\n      Urban\n      213.61\n      55.4\n      formerly smoked\n      0\n    \n    \n      43397\n      28375\n      Female\n      82.0\n      1\n      0\n      Yes\n      Private\n      Urban\n      91.94\n      28.9\n      formerly smoked\n      0\n    \n    \n      43398\n      27973\n      Male\n      40.0\n      0\n      0\n      Yes\n      Private\n      Urban\n      99.16\n      33.2\n      never smoked\n      0\n    \n    \n      43399\n      36271\n      Female\n      82.0\n      0\n      0\n      Yes\n      Private\n      Urban\n      79.48\n      20.6\n      never smoked\n      0\n    \n  \n\n43400 rows  12 columns\n\n\n\n\n#df = df.drop(columns = ['id'])\n\n\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\n\nprofile = ProfileReport(df, title='Pandas Profiling Report')\n\n\nprofile.to_widgets()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      id\n      gender\n      age\n      hypertension\n      heart_disease\n      ever_married\n      work_type\n      Residence_type\n      avg_glucose_level\n      bmi\n      smoking_status\n      stroke\n    \n  \n  \n    \n      0\n      30669\n      Male\n      3.0\n      0\n      0\n      No\n      children\n      Rural\n      95.12\n      18.0\n      NaN\n      0\n    \n    \n      1\n      30468\n      Male\n      58.0\n      1\n      0\n      Yes\n      Private\n      Urban\n      87.96\n      39.2\n      never smoked\n      0\n    \n    \n      2\n      16523\n      Female\n      8.0\n      0\n      0\n      No\n      Private\n      Urban\n      110.89\n      17.6\n      NaN\n      0\n    \n    \n      3\n      56543\n      Female\n      70.0\n      0\n      0\n      Yes\n      Private\n      Rural\n      69.04\n      35.9\n      formerly smoked\n      0\n    \n    \n      4\n      46136\n      Male\n      14.0\n      0\n      0\n      No\n      Never_worked\n      Rural\n      161.28\n      19.1\n      NaN\n      0\n    \n    \n      5\n      32257\n      Female\n      47.0\n      0\n      0\n      Yes\n      Private\n      Urban\n      210.95\n      50.1\n      NaN\n      0\n    \n    \n      6\n      52800\n      Female\n      52.0\n      0\n      0\n      Yes\n      Private\n      Urban\n      77.59\n      17.7\n      formerly smoked\n      0\n    \n    \n      7\n      41413\n      Female\n      75.0\n      0\n      1\n      Yes\n      Self-employed\n      Rural\n      243.53\n      27.0\n      never smoked\n      0\n    \n    \n      8\n      15266\n      Female\n      32.0\n      0\n      0\n      Yes\n      Private\n      Rural\n      77.67\n      32.3\n      smokes\n      0\n    \n    \n      9\n      28674\n      Female\n      74.0\n      1\n      0\n      Yes\n      Self-employed\n      Urban\n      205.84\n      54.6\n      never smoked\n      0\n    \n  \n\n\n\n\n\n\n\n\n  \n    \n      \n      id\n      gender\n      age\n      hypertension\n      heart_disease\n      ever_married\n      work_type\n      Residence_type\n      avg_glucose_level\n      bmi\n      smoking_status\n      stroke\n    \n  \n  \n    \n      43390\n      10096\n      Female\n      69.0\n      0\n      0\n      Yes\n      Self-employed\n      Urban\n      229.85\n      31.2\n      never smoked\n      0\n    \n    \n      43391\n      30077\n      Male\n      6.0\n      0\n      0\n      No\n      children\n      Urban\n      77.48\n      19.1\n      NaN\n      0\n    \n    \n      43392\n      45266\n      Female\n      18.0\n      0\n      0\n      No\n      Private\n      Urban\n      131.96\n      22.8\n      NaN\n      0\n    \n    \n      43393\n      69344\n      Male\n      39.0\n      0\n      0\n      Yes\n      Private\n      Rural\n      132.22\n      31.6\n      never smoked\n      0\n    \n    \n      43394\n      52380\n      Male\n      47.0\n      0\n      0\n      No\n      Govt_job\n      Urban\n      68.52\n      25.2\n      formerly smoked\n      0\n    \n    \n      43395\n      56196\n      Female\n      10.0\n      0\n      0\n      No\n      children\n      Urban\n      58.64\n      20.4\n      never smoked\n      0\n    \n    \n      43396\n      5450\n      Female\n      56.0\n      0\n      0\n      Yes\n      Govt_job\n      Urban\n      213.61\n      55.4\n      formerly smoked\n      0\n    \n    \n      43397\n      28375\n      Female\n      82.0\n      1\n      0\n      Yes\n      Private\n      Urban\n      91.94\n      28.9\n      formerly smoked\n      0\n    \n    \n      43398\n      27973\n      Male\n      40.0\n      0\n      0\n      Yes\n      Private\n      Urban\n      99.16\n      33.2\n      never smoked\n      0\n    \n    \n      43399\n      36271\n      Female\n      82.0\n      0\n      0\n      Yes\n      Private\n      Urban\n      79.48\n      20.6\n      never smoked\n      0\n    \n  \n\n\n\n\n\n\n\n\nprofile.to_notebook_iframe()\n\n\n\n\n\n\n\n\n\n\n\n\nprofile.to_file(output_file=\"pandas_profiling.html\")\n\n\n\n\n\n\n\n\n\n\npct_list = []\nfor col in df.columns:\n    pct_missing = np.mean(df[col].isnull())\n    if round(pct_missing*100) >0:\n        pct_list.append([col, round(pct_missing*100)])\n    print('{} - {}%'.format(col, round(pct_missing*100)))\n\nid - 0%\ngender - 0%\nage - 0%\nhypertension - 0%\nheart_disease - 0%\never_married - 0%\nwork_type - 0%\nResidence_type - 0%\navg_glucose_level - 0%\nbmi - 3%\nsmoking_status - 31%\nstroke - 0%\n\n\n\ncols = df.columns \ncolours = ['darkblue', 'red'] \nsns.heatmap(df[cols].isnull(), cmap=sns.color_palette(colours))\n\n<AxesSubplot:>\n\n\n\n\n\n\ndf.groupby([\"age\", 'heart_disease'])['stroke'].agg(['sum']).round(0)\n\n\n\n\n\n  \n    \n      \n      \n      sum\n    \n    \n      age\n      heart_disease\n      \n    \n  \n  \n    \n      0.08\n      0\n      0\n    \n    \n      0.16\n      0\n      0\n    \n    \n      0.24\n      0\n      0\n    \n    \n      0.32\n      0\n      0\n    \n    \n      0.40\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      80.00\n      1\n      13\n    \n    \n      81.00\n      0\n      32\n    \n    \n      1\n      11\n    \n    \n      82.00\n      0\n      24\n    \n    \n      1\n      12\n    \n  \n\n163 rows  1 columns\n\n\n\n\ndf.dtypes\n\nid                     int64\ngender                object\nage                  float64\nhypertension           int64\nheart_disease          int64\never_married          object\nwork_type             object\nResidence_type        object\navg_glucose_level    float64\nbmi                  float64\nsmoking_status        object\nstroke                 int64\ndtype: object\n\n\n\n# Discretize with respective equal-width bin\ndf['age_binned'] = pd.cut(df['age'], np.arange(0, 91, 5))\ndf['avg_glucose_level_binned'] = pd.cut(df['avg_glucose_level'], np.arange(0, 301, 10))\ndf['bmi_binned'] = pd.cut(df['bmi'], np.arange(0, 101, 5))\n\n\nimport seaborn as sns\n\n\n# Create the correlation heatmap\nheatmap = sns.heatmap(df[['age', 'avg_glucose_level', 'bmi']].corr(), vmin=-1, vmax=1, annot=True)\n# Create the title\nheatmap.set_title('Correlation Heatmap');\n\n\n\n\n\ndef get_stacked_bar_chart(column):\n    # Get the count of records by column and stroke    \n    df_pct = df.groupby([column, 'stroke'])['age'].count()\n    # Create proper DataFrame's format\n    df_pct = df_pct.unstack()    \n    return df_pct.plot.bar(stacked=True, figsize=(6,6), width=1);\n\n\ndef get_100_percent_stacked_bar_chart(column, width = 0.5):\n    # Get the count of records by column and stroke\n    df_breakdown = df.groupby([column, 'stroke'])['age'].count()\n    # Get the count of records by gender\n    df_total = df.groupby([column])['age'].count()\n    # Get the percentage for 100% stacked bar chart\n    df_pct = df_breakdown / df_total * 100\n    # Create proper DataFrame's format\n    df_pct = df_pct.unstack()\n    return df_pct.plot.bar(stacked=True, figsize=(6,6), width=width);\n\n\n# Age related to risk\nget_stacked_bar_chart('age_binned')\n\n<AxesSubplot:xlabel='age_binned'>\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('age_binned', width = 0.9)\n\n<AxesSubplot:xlabel='age_binned'>\n\n\n\n\n\n\nget_stacked_bar_chart('bmi_binned')\nget_100_percent_stacked_bar_chart('bmi_binned', width = 0.9)\n\n<AxesSubplot:xlabel='bmi_binned'>\n\n\n\n\n\n\n\n\n\nget_stacked_bar_chart('avg_glucose_level_binned')\nget_100_percent_stacked_bar_chart('avg_glucose_level_binned', width = 0.9)\n\n<AxesSubplot:xlabel='avg_glucose_level_binned'>\n\n\n\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('hypertension')\nget_100_percent_stacked_bar_chart('heart_disease')\n\n<AxesSubplot:xlabel='heart_disease'>\n\n\n\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('gender')\nget_100_percent_stacked_bar_chart('Residence_type')\n\n<AxesSubplot:xlabel='Residence_type'>\n\n\n\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('work_type')\ndf.groupby(['work_type'])[['age']].agg(['count', 'mean'])\n\n\n\n\n\n  \n    \n      \n      age\n    \n    \n      \n      count\n      mean\n    \n    \n      work_type\n      \n      \n    \n  \n  \n    \n      Govt_job\n      5440\n      49.097610\n    \n    \n      Never_worked\n      177\n      17.757062\n    \n    \n      Private\n      24834\n      45.015060\n    \n    \n      Self-employed\n      6793\n      59.307817\n    \n    \n      children\n      6156\n      6.699253\n    \n  \n\n\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('ever_married')\ndf.groupby(['ever_married'])[['age']].agg(['count', 'mean'])\n\n\n\n\n\n  \n    \n      \n      age\n    \n    \n      \n      count\n      mean\n    \n    \n      ever_married\n      \n      \n    \n  \n  \n    \n      No\n      15462\n      21.238236\n    \n    \n      Yes\n      27938\n      53.828871\n    \n  \n\n\n\n\n\n\n\n\ng = sns.catplot(x=\"Residence_type\", hue=\"smoking_status\", col=\"work_type\",\n                data=df, kind=\"count\",\n                height=4, aspect=.7)\n\n\n\n\n\nimport missingno\nmissingno.matrix(df, figsize = (30,5))\n\n<AxesSubplot:>\n\n\n\n\n\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(1,4,figsize=(25,7))\n\nfig.suptitle(\"Countplot for the dataset\", fontsize=35)\n\nsns.countplot(x=\"gender\", data=df,ax=ax1)\nsns.countplot(x=\"stroke\", data=df,ax=ax2)\nsns.countplot(x=\"ever_married\", data=df,ax=ax3)\nsns.countplot(x=\"hypertension\", data=df,ax=ax4)\n\n<AxesSubplot:xlabel='hypertension', ylabel='count'>\n\n\n\n\n\n\nsns.displot(x=\"age\", data=df, kind=\"kde\", hue=\"gender\", col=\"smoking_status\", row=\"Residence_type\")\n\n/home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n  warnings.warn(msg, UserWarning)\n/home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n  warnings.warn(msg, UserWarning)\n/home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n  warnings.warn(msg, UserWarning)\n\n\n<seaborn.axisgrid.FacetGrid at 0x7fb1c3d08f70>\n\n\n\n\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,7))\nfig.suptitle(\"Boxplot for Dataset\", fontsize=35)\n\nsns.boxplot(x=\"stroke\", y=\"avg_glucose_level\", data=df,ax=ax1)\nsns.boxplot(x=\"stroke\", y=\"bmi\", data=df,ax=ax2)\nsns.boxplot(x=\"stroke\", y=\"age\", data=df,ax=ax3)\n\n<AxesSubplot:xlabel='stroke', ylabel='age'>\n\n\n\n\n\n\n# Compute a correlation matrix and convert to long-form\ncorr_mat = df.corr(\"kendall\").stack().reset_index(name=\"correlation\")\n\n# Draw each cell as a scatter point with varying size and color\ng = sns.relplot(\n    data=corr_mat,\n    x=\"level_0\", y=\"level_1\", hue=\"correlation\", size=\"correlation\",\n    palette=\"vlag\", hue_norm=(-1, 1), edgecolor=\".7\",\n    height=5, sizes=(50, 250), size_norm=(-.2, .8),\n)\n\n# Tweak the figure to finalize\ng.set(xlabel=\"\", ylabel=\"\", aspect=\"equal\")\ng.despine(left=True, bottom=True)\ng.ax.margins(0.25)\nfor label in g.ax.get_xticklabels():\n    label.set_rotation(90)\nfor artist in g.legend.legendHandles:\n    artist.set_edgecolor(\".1\")\n\n\n\n\n\nstrokes_temp_df=df\nstrokes_temp_df[['stroke','hypertension']] = df[['stroke','hypertension']].astype('int')\ncorr = strokes_temp_df.corr()\ncorr.style.background_gradient()\ncorr.style.background_gradient().set_precision(2)\n\n\n                    id        age        hypertension        heart_disease        avg_glucose_level        bmi        stroke    \n                \n                        id\n                        1.00\n                        0.01\n                        0.01\n                        0.01\n                        0.02\n                        0.02\n                        0.00\n            \n            \n                        age\n                        0.01\n                        1.00\n                        0.27\n                        0.25\n                        0.24\n                        0.36\n                        0.16\n            \n            \n                        hypertension\n                        0.01\n                        0.27\n                        1.00\n                        0.12\n                        0.16\n                        0.16\n                        0.08\n            \n            \n                        heart_disease\n                        0.01\n                        0.25\n                        0.12\n                        1.00\n                        0.15\n                        0.06\n                        0.11\n            \n            \n                        avg_glucose_level\n                        0.02\n                        0.24\n                        0.16\n                        0.15\n                        1.00\n                        0.19\n                        0.08\n            \n            \n                        bmi\n                        0.02\n                        0.36\n                        0.16\n                        0.06\n                        0.19\n                        1.00\n                        0.02\n            \n            \n                        stroke\n                        0.00\n                        0.16\n                        0.08\n                        0.11\n                        0.08\n                        0.02\n                        1.00"
  },
  {
    "objectID": "posts/2020-12-09-Health-Data-EDA.html",
    "href": "posts/2020-12-09-Health-Data-EDA.html",
    "title": "didactic",
    "section": "",
    "text": "Credit: code from https://towardsdatascience.com/step-by-step-exploratory-data-analysis-on-stroke-dataset-840aefea8739 and https://www.kaggle.com/lirilkumaramal/heart-stroke\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n  \n    \n      \n      id\n      gender\n      age\n      hypertension\n      heart_disease\n      ever_married\n      work_type\n      Residence_type\n      avg_glucose_level\n      bmi\n      smoking_status\n      stroke\n    \n  \n  \n    \n      0\n      30669\n      Male\n      3.0\n      0\n      0\n      No\n      children\n      Rural\n      95.12\n      18.0\n      NaN\n      0\n    \n    \n      1\n      30468\n      Male\n      58.0\n      1\n      0\n      Yes\n      Private\n      Urban\n      87.96\n      39.2\n      never smoked\n      0\n    \n    \n      2\n      16523\n      Female\n      8.0\n      0\n      0\n      No\n      Private\n      Urban\n      110.89\n      17.6\n      NaN\n      0\n    \n    \n      3\n      56543\n      Female\n      70.0\n      0\n      0\n      Yes\n      Private\n      Rural\n      69.04\n      35.9\n      formerly smoked\n      0\n    \n    \n      4\n      46136\n      Male\n      14.0\n      0\n      0\n      No\n      Never_worked\n      Rural\n      161.28\n      19.1\n      NaN\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      43395\n      56196\n      Female\n      10.0\n      0\n      0\n      No\n      children\n      Urban\n      58.64\n      20.4\n      never smoked\n      0\n    \n    \n      43396\n      5450\n      Female\n      56.0\n      0\n      0\n      Yes\n      Govt_job\n      Urban\n      213.61\n      55.4\n      formerly smoked\n      0\n    \n    \n      43397\n      28375\n      Female\n      82.0\n      1\n      0\n      Yes\n      Private\n      Urban\n      91.94\n      28.9\n      formerly smoked\n      0\n    \n    \n      43398\n      27973\n      Male\n      40.0\n      0\n      0\n      Yes\n      Private\n      Urban\n      99.16\n      33.2\n      never smoked\n      0\n    \n    \n      43399\n      36271\n      Female\n      82.0\n      0\n      0\n      Yes\n      Private\n      Urban\n      79.48\n      20.6\n      never smoked\n      0\n    \n  \n\n43400 rows  12 columns\n\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 43400 entries, 0 to 43399\nData columns (total 12 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   id                 43400 non-null  int64  \n 1   gender             43400 non-null  object \n 2   age                43400 non-null  float64\n 3   hypertension       43400 non-null  int64  \n 4   heart_disease      43400 non-null  int64  \n 5   ever_married       43400 non-null  object \n 6   work_type          43400 non-null  object \n 7   Residence_type     43400 non-null  object \n 8   avg_glucose_level  43400 non-null  float64\n 9   bmi                41938 non-null  float64\n 10  smoking_status     30108 non-null  object \n 11  stroke             43400 non-null  int64  \ndtypes: float64(3), int64(4), object(5)\nmemory usage: 4.0+ MB\n\n\n\ndf['stroke'].value_counts()\n\n0    42617\n1      783\nName: stroke, dtype: int64\n\n\n\n# labeled target is unbalanced\n\n\n# Drop the id column\ndf.drop(columns=['id'], inplace=True)\n\n\n# Showing records where patient suffered from stroke but had missing value in bmi attribute.\ndf[df['bmi'].isna() & df['stroke'] == 1]\n\n\n\n\n\n  \n    \n      \n      gender\n      age\n      hypertension\n      heart_disease\n      ever_married\n      work_type\n      Residence_type\n      avg_glucose_level\n      bmi\n      smoking_status\n      stroke\n    \n  \n  \n    \n      81\n      Female\n      61.0\n      0\n      0\n      Yes\n      Self-employed\n      Rural\n      202.21\n      NaN\n      never smoked\n      1\n    \n    \n      407\n      Female\n      59.0\n      0\n      0\n      Yes\n      Private\n      Rural\n      76.15\n      NaN\n      NaN\n      1\n    \n    \n      747\n      Male\n      78.0\n      0\n      1\n      Yes\n      Private\n      Urban\n      219.84\n      NaN\n      NaN\n      1\n    \n    \n      1139\n      Male\n      57.0\n      0\n      1\n      No\n      Govt_job\n      Urban\n      217.08\n      NaN\n      NaN\n      1\n    \n    \n      1613\n      Male\n      58.0\n      0\n      0\n      Yes\n      Private\n      Rural\n      189.84\n      NaN\n      NaN\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      42530\n      Male\n      66.0\n      0\n      0\n      Yes\n      Self-employed\n      Urban\n      182.89\n      NaN\n      never smoked\n      1\n    \n    \n      42839\n      Female\n      67.0\n      1\n      0\n      Yes\n      Govt_job\n      Urban\n      234.43\n      NaN\n      never smoked\n      1\n    \n    \n      43007\n      Female\n      69.0\n      0\n      1\n      Yes\n      Self-employed\n      Rural\n      89.19\n      NaN\n      smokes\n      1\n    \n    \n      43100\n      Male\n      67.0\n      0\n      0\n      Yes\n      Self-employed\n      Urban\n      136.79\n      NaN\n      smokes\n      1\n    \n    \n      43339\n      Female\n      76.0\n      0\n      0\n      No\n      Private\n      Rural\n      100.55\n      NaN\n      never smoked\n      1\n    \n  \n\n140 rows  11 columns\n\n\n\n\n# Replace the missing values with mean of bmi attribute\ndf['bmi'].fillna(np.round(df['bmi'].mean(), 1), inplace = True)\n\n\n# Create a new category named 'not known'\ndf['smoking_status'].fillna('not known', inplace=True)\nprint(df['smoking_status'].value_counts())\n\nnever smoked       16053\nnot known          13292\nformerly smoked     7493\nsmokes              6562\nName: smoking_status, dtype: int64\n\n\n\n# Discretize with respective equal-width bin\ndf['age_binned'] = pd.cut(df['age'], np.arange(0, 91, 5))\ndf['avg_glucose_level_binned'] = pd.cut(df['avg_glucose_level'], np.arange(0, 301, 10))\ndf['bmi_binned'] = pd.cut(df['bmi'], np.arange(0, 101, 5))\n\nSettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['age_binned'] = pd.cut(df['age'], np.arange(0, 91, 5))\n<ipython-input-23-b4c4cb89b1bd>:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['avg_glucose_level_binned'] = pd.cut(df['avg_glucose_level'], np.arange(0, 301, 10))\n<ipython-input-23-b4c4cb89b1bd>:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['bmi_binned'] = pd.cut(df['bmi'], np.arange(0, 101, 5))\n\n\n\n# Create the correlation heatmap\nheatmap = sns.heatmap(df[['age_norm', 'avg_glucose_level_norm', 'bmi_norm']].corr(), vmin=-1, vmax=1, annot=True)\n# Create the title\nheatmap.set_title('Correlation Heatmap');\n\n\n\n\n\ndef get_stacked_bar_chart(column):\n    # Get the count of records by column and stroke    \n    df_pct = df.groupby([column, 'stroke'])['age'].count()\n    # Create proper DataFrame's format\n    df_pct = df_pct.unstack()    \n    return df_pct.plot.bar(stacked=True, figsize=(6,6), width=1);\n\n\ndef get_100_percent_stacked_bar_chart(column, width = 0.5):\n    # Get the count of records by column and stroke\n    df_breakdown = df.groupby([column, 'stroke'])['age'].count()\n    # Get the count of records by gender\n    df_total = df.groupby([column])['age'].count()\n    # Get the percentage for 100% stacked bar chart\n    df_pct = df_breakdown / df_total * 100\n    # Create proper DataFrame's format\n    df_pct = df_pct.unstack()\n    return df_pct.plot.bar(stacked=True, figsize=(6,6), width=width);\n\n\n# Age related to risk\nget_stacked_bar_chart('age_binned')\n\n<AxesSubplot:xlabel='age_binned'>\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('age_binned', width = 0.9)\n\n<AxesSubplot:xlabel='age_binned'>\n\n\n\n\n\n\nget_stacked_bar_chart('bmi_binned')\nget_100_percent_stacked_bar_chart('bmi_binned', width = 0.9)\n\n<AxesSubplot:xlabel='bmi_binned'>\n\n\n\n\n\n\n\n\n\nget_stacked_bar_chart('avg_glucose_level_binned')\nget_100_percent_stacked_bar_chart('avg_glucose_level_binned', width = 0.9)\n\n<AxesSubplot:xlabel='avg_glucose_level_binned'>\n\n\n\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('hypertension')\nget_100_percent_stacked_bar_chart('heart_disease')\n\n<AxesSubplot:xlabel='heart_disease'>\n\n\n\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('gender')\nget_100_percent_stacked_bar_chart('Residence_type')\n\n<AxesSubplot:xlabel='Residence_type'>\n\n\n\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('work_type')\ndf.groupby(['work_type'])[['age']].agg(['count', 'mean'])\n\n\n\n\n\n  \n    \n      \n      age\n    \n    \n      \n      count\n      mean\n    \n    \n      work_type\n      \n      \n    \n  \n  \n    \n      Govt_job\n      5438\n      49.098750\n    \n    \n      Never_worked\n      177\n      17.757062\n    \n    \n      Private\n      24827\n      45.016837\n    \n    \n      Self-employed\n      6793\n      59.307817\n    \n    \n      children\n      6154\n      6.698018\n    \n  \n\n\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('ever_married')\ndf.groupby(['ever_married'])[['age']].agg(['count', 'mean'])\n\n\n\n\n\n  \n    \n      \n      age\n    \n    \n      \n      count\n      mean\n    \n    \n      ever_married\n      \n      \n    \n  \n  \n    \n      No\n      15456\n      21.237487\n    \n    \n      Yes\n      27933\n      53.829735\n    \n  \n\n\n\n\n\n\n\n\ng = sns.catplot(x=\"Residence_type\", hue=\"smoking_status\", col=\"work_type\",\n                data=df, kind=\"count\",\n                height=4, aspect=.7)\n\n\n\n\n\nmissingno.matrix(df, figsize = (30,5))\n\nNameError: name 'missingno' is not defined\n\n\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(1,4,figsize=(25,7))\n\nfig.suptitle(\"Countplot for the dataset\", fontsize=35)\n\nsns.countplot(x=\"gender\", data=df,ax=ax1)\nsns.countplot(x=\"stroke\", data=df,ax=ax2)\nsns.countplot(x=\"ever_married\", data=df,ax=ax3)\nsns.countplot(x=\"hypertension\", data=df,ax=ax4)\n\n<AxesSubplot:xlabel='hypertension', ylabel='count'>\n\n\n\n\n\n\nsns.displot(x=\"age\", data=df, kind=\"kde\", hue=\"gender\", col=\"smoking_status\", row=\"Residence_type\")\n\n<seaborn.axisgrid.FacetGrid at 0x7fe05aa233d0>\n\n\n\n\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,7))\nfig.suptitle(\"Boxplot for Dataset\", fontsize=35)\n\nsns.boxplot(x=\"stroke\", y=\"avg_glucose_level\", data=df,ax=ax1)\nsns.boxplot(x=\"stroke\", y=\"bmi\", data=df,ax=ax2)\nsns.boxplot(x=\"stroke\", y=\"age\", data=df,ax=ax3)\n\n<AxesSubplot:xlabel='stroke', ylabel='age'>\n\n\n\n\n\n\n# Compute a correlation matrix and convert to long-form\ncorr_mat = df.corr(\"kendall\").stack().reset_index(name=\"correlation\")\n\n# Draw each cell as a scatter point with varying size and color\ng = sns.relplot(\n    data=corr_mat,\n    x=\"level_0\", y=\"level_1\", hue=\"correlation\", size=\"correlation\",\n    palette=\"vlag\", hue_norm=(-1, 1), edgecolor=\".7\",\n    height=5, sizes=(50, 250), size_norm=(-.2, .8),\n)\n\n# Tweak the figure to finalize\ng.set(xlabel=\"\", ylabel=\"\", aspect=\"equal\")\ng.despine(left=True, bottom=True)\ng.ax.margins(0.25)\nfor label in g.ax.get_xticklabels():\n    label.set_rotation(90)\nfor artist in g.legend.legendHandles:\n    artist.set_edgecolor(\".1\")\n\n\n\n\n\nstrokes_temp_df=df\nstrokes_temp_df[['stroke','hypertension']] = df[['stroke','hypertension']].astype('int')\ncorr = strokes_temp_df.corr()\ncorr.style.background_gradient()\ncorr.style.background_gradient().set_precision(2)\n\n\n                    age        hypertension        heart_disease        avg_glucose_level        bmi        stroke        age_norm        avg_glucose_level_norm        bmi_norm    \n                \n                        age\n                        1.00\n                        0.27\n                        0.25\n                        0.24\n                        0.35\n                        0.16\n                        1.00\n                        0.24\n                        0.35\n            \n            \n                        hypertension\n                        0.27\n                        1.00\n                        0.12\n                        0.16\n                        0.15\n                        0.08\n                        0.27\n                        0.16\n                        0.15\n            \n            \n                        heart_disease\n                        0.25\n                        0.12\n                        1.00\n                        0.15\n                        0.05\n                        0.11\n                        0.25\n                        0.15\n                        0.05\n            \n            \n                        avg_glucose_level\n                        0.24\n                        0.16\n                        0.15\n                        1.00\n                        0.18\n                        0.08\n                        0.24\n                        1.00\n                        0.18\n            \n            \n                        bmi\n                        0.35\n                        0.15\n                        0.05\n                        0.18\n                        1.00\n                        0.02\n                        0.35\n                        0.18\n                        1.00\n            \n            \n                        stroke\n                        0.16\n                        0.08\n                        0.11\n                        0.08\n                        0.02\n                        1.00\n                        0.16\n                        0.08\n                        0.02\n            \n            \n                        age_norm\n                        1.00\n                        0.27\n                        0.25\n                        0.24\n                        0.35\n                        0.16\n                        1.00\n                        0.24\n                        0.35\n            \n            \n                        avg_glucose_level_norm\n                        0.24\n                        0.16\n                        0.15\n                        1.00\n                        0.18\n                        0.08\n                        0.24\n                        1.00\n                        0.18\n            \n            \n                        bmi_norm\n                        0.35\n                        0.15\n                        0.05\n                        0.18\n                        1.00\n                        0.02\n                        0.35\n                        0.18\n                        1.00"
  },
  {
    "objectID": "posts/2020-12-11_Xgboost_Health_data.html",
    "href": "posts/2020-12-11_Xgboost_Health_data.html",
    "title": "didactic",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sqlalchemy as db\nimport sqlite3\nimport pandas as pd\nimport numpy as np\n\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n  \n    \n      \n      id\n      gender\n      age\n      hypertension\n      heart_disease\n      ever_married\n      work_type\n      Residence_type\n      avg_glucose_level\n      bmi\n      smoking_status\n      stroke\n    \n  \n  \n    \n      0\n      30669\n      Male\n      3.0\n      0\n      0\n      No\n      children\n      Rural\n      95.12\n      18.0\n      NaN\n      0\n    \n    \n      1\n      30468\n      Male\n      58.0\n      1\n      0\n      Yes\n      Private\n      Urban\n      87.96\n      39.2\n      never smoked\n      0\n    \n    \n      2\n      16523\n      Female\n      8.0\n      0\n      0\n      No\n      Private\n      Urban\n      110.89\n      17.6\n      NaN\n      0\n    \n    \n      3\n      56543\n      Female\n      70.0\n      0\n      0\n      Yes\n      Private\n      Rural\n      69.04\n      35.9\n      formerly smoked\n      0\n    \n    \n      4\n      46136\n      Male\n      14.0\n      0\n      0\n      No\n      Never_worked\n      Rural\n      161.28\n      19.1\n      NaN\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      43395\n      56196\n      Female\n      10.0\n      0\n      0\n      No\n      children\n      Urban\n      58.64\n      20.4\n      never smoked\n      0\n    \n    \n      43396\n      5450\n      Female\n      56.0\n      0\n      0\n      Yes\n      Govt_job\n      Urban\n      213.61\n      55.4\n      formerly smoked\n      0\n    \n    \n      43397\n      28375\n      Female\n      82.0\n      1\n      0\n      Yes\n      Private\n      Urban\n      91.94\n      28.9\n      formerly smoked\n      0\n    \n    \n      43398\n      27973\n      Male\n      40.0\n      0\n      0\n      Yes\n      Private\n      Urban\n      99.16\n      33.2\n      never smoked\n      0\n    \n    \n      43399\n      36271\n      Female\n      82.0\n      0\n      0\n      Yes\n      Private\n      Urban\n      79.48\n      20.6\n      never smoked\n      0\n    \n  \n\n43400 rows  12 columns\n\n\n\n\ndf = df.drop(columns = ['id'])\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\n\ntarget = df.stroke.values\nfeatures = df.drop(columns =[\"stroke\"])\n\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=0)\n\n\ndf.dtypes\n\ngender                object\nage                  float64\nhypertension           int64\nheart_disease          int64\never_married          object\nwork_type             object\nResidence_type        object\navg_glucose_level    float64\nbmi                  float64\nsmoking_status        object\nstroke                 int64\ndtype: object\n\n\n\n# Label Encoding\nfor f in df.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))   \n\n\nfor f in df.columns:\n    print(f)\n\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n# Label Encoding\nfor f in df.columns:\n    if df[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df[f].values))\n        df[f] = lbl.transform(list(df[f].values))\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      gender\n      age\n      hypertension\n      heart_disease\n      ever_married\n      work_type\n      Residence_type\n      avg_glucose_level\n      bmi\n      smoking_status\n      stroke\n    \n  \n  \n    \n      0\n      1\n      3.0\n      0\n      0\n      0\n      4\n      0\n      95.12\n      18.0\n      1\n      0\n    \n    \n      1\n      1\n      58.0\n      1\n      0\n      1\n      2\n      1\n      87.96\n      39.2\n      2\n      0\n    \n    \n      2\n      0\n      8.0\n      0\n      0\n      0\n      2\n      1\n      110.89\n      17.6\n      1\n      0\n    \n    \n      3\n      0\n      70.0\n      0\n      0\n      1\n      2\n      0\n      69.04\n      35.9\n      0\n      0\n    \n    \n      4\n      1\n      14.0\n      0\n      0\n      0\n      1\n      0\n      161.28\n      19.1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      43395\n      0\n      10.0\n      0\n      0\n      0\n      4\n      1\n      58.64\n      20.4\n      2\n      0\n    \n    \n      43396\n      0\n      56.0\n      0\n      0\n      1\n      0\n      1\n      213.61\n      55.4\n      0\n      0\n    \n    \n      43397\n      0\n      82.0\n      1\n      0\n      1\n      2\n      1\n      91.94\n      28.9\n      0\n      0\n    \n    \n      43398\n      1\n      40.0\n      0\n      0\n      1\n      2\n      1\n      99.16\n      33.2\n      2\n      0\n    \n    \n      43399\n      0\n      82.0\n      0\n      0\n      1\n      2\n      1\n      79.48\n      20.6\n      2\n      0\n    \n  \n\n43400 rows  11 columns\n\n\n\n\n# Label Encoding\nfor f in X_train.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))   \n\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_validate\n\n\nimport xgboost as xgb\n\n\nclf = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=9,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    missing=-999,\n    random_state=2021,\n    tree_method='auto'\n#    tree_method='hist'\n#    tree_method='gpu_hist'\n)\n\n\n## K Fold Cross Validation (5 Folds)\n\n\nkfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n\n\n# Define Parameter grid for hyperparameter search process\n\n\nparam_grid = { \n    'colsample_bytree':[.75,1],\n    'learning_rate':[0.01,0.05,0.1,0.3,0.5],\n    'max_depth':[1,2,3,5],\n    'subsample':[.75,1],\n    'n_estimators': list(range(50, 400, 50))\n}\n\n\n## Run the GridSearch,optimizing on ROC\n\n\ngrid_search = GridSearchCV(estimator=clf, scoring='roc_auc', param_grid=param_grid, n_jobs=-1, cv=kfold)\n\n\n%%time\ngrid_result = grid_search.fit(X_train, y_train)\n\n/home/david/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n\n\n[09:05:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nCPU times: user 27 s, sys: 996 ms, total: 28 s\nWall time: 20min 15s\n\n\n\nprint(f'Best: {grid_result.best_score_} using {grid_result.best_params_}','\\n')\n\nBest: 0.8617766137590029 using {'colsample_bytree': 0.75, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 300, 'subsample': 0.75} \n\n\n\n\n#Set our final hyperparameters to the tuned values\nxgbcl = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,\n         gamma=0.0, max_delta_step=0.0, min_child_weight=1.0,\n         missing=None, n_jobs=-1, objective='binary:logistic', random_state=42, reg_alpha=0.0,\n         reg_lambda=1.0, scale_pos_weight=1.0, tree_method='auto',\n         colsample_bytree = grid_result.best_params_['colsample_bytree'], \n         learning_rate = grid_result.best_params_['learning_rate'], \n         max_depth = grid_result.best_params_['max_depth'], \n         subsample = grid_result.best_params_['subsample'], \n         n_estimators = grid_result.best_params_['n_estimators'])\n\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n#refit the model on k-folds to get stable avg error metrics\nscores = cross_validate(estimator=xgbcl, X=X_train, y=y_train, cv=kfold, n_jobs=-1, \n                        scoring=['accuracy', 'roc_auc', 'precision', 'recall', 'f1'])\n\nprint('Training 5-fold Cross Validation Results:\\n')\nprint('AUC: ', scores['test_roc_auc'].mean())\nprint('Accuracy: ', scores['test_accuracy'].mean())\nprint('Precision: ', scores['test_precision'].mean())\nprint('Recall: ', scores['test_recall'].mean())\nprint('F1: ', scores['test_f1'].mean(), '\\n')\n\nTraining 5-fold Cross Validation Results:\n\nAUC:  0.8632093427558646\nAccuracy:  0.981278801843318\nPrecision:  0.0\nRecall:  0.0\nF1:  0.0 \n\n\n\n\nimport sklearn.metrics as metrics\n\n\n#Fit the final model\nxgbcl.fit(X_train, y_train)\n\n#Generate predictions against our training and test data\npred_train = xgbcl.predict(X_train)\nproba_train = xgbcl.predict_proba(X_train)\npred_test = xgbcl.predict(X_test)\nproba_test = xgbcl.predict_proba(X_test)\n\n# Print model report\nprint(\"Classification report (Test): \\n\")\nprint(metrics.classification_report(y_test, pred_test))\nprint(\"Confusion matrix (Test): \\n\")\nprint(metrics.confusion_matrix(y_test, pred_test)/len(y_test))\n\nprint ('\\nTrain Accuracy:', metrics.accuracy_score(y_train, pred_train))\nprint ('Test Accuracy:', metrics.accuracy_score(y_test, pred_test))\n\nprint ('\\nTrain AUC:', metrics.roc_auc_score(y_train, proba_train[:,1]))\nprint ('Test AUC:', metrics.roc_auc_score(y_test, proba_test[:,1]))\n\n# calculate the fpr and tpr for all thresholds of the classification\ntrain_fpr, train_tpr, train_threshold = metrics.roc_curve(y_train, proba_train[:,1])\ntest_fpr, test_tpr, test_threshold = metrics.roc_curve(y_test, proba_test[:,1])\n\ntrain_roc_auc = metrics.auc(train_fpr, train_tpr)\ntest_roc_auc = metrics.auc(test_fpr, test_tpr)\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=[7,5])\nplt.title('Receiver Operating Characteristic')\nplt.plot(train_fpr, train_tpr, 'b', label = 'Train AUC = %0.2f' % train_roc_auc)\nplt.plot(test_fpr, test_tpr, 'g', label = 'Test AUC = %0.2f' % test_roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n# plot feature importance\nxgb.plot_importance(xgbcl, importance_type='gain');\n\n[20:30:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nClassification report (Test): \n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      8547\n           1       0.00      0.00      0.00       133\n\n    accuracy                           0.98      8680\n   macro avg       0.49      0.50      0.50      8680\nweighted avg       0.97      0.98      0.98      8680\n\nConfusion matrix (Test): \n\n[[0.98467742 0.        ]\n [0.01532258 0.        ]]\n\nTrain Accuracy: 0.981278801843318\nTest Accuracy: 0.9846774193548387\n\nTrain AUC: 0.8907315481700571\nTest AUC: 0.8661175578468812\n\n\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\n\n\n\n\n\n#take a random row of data\nX_rand = features.sample(1, random_state = 5)\ndisplay(df.iloc[X_rand.index])\n\n\n\n\n\n  \n    \n      \n      gender\n      age\n      hypertension\n      heart_disease\n      ever_married\n      work_type\n      Residence_type\n      avg_glucose_level\n      bmi\n      smoking_status\n      stroke\n    \n  \n  \n    \n      33658\n      0\n      60.0\n      0\n      0\n      1\n      2\n      0\n      108.13\n      28.6\n      2\n      0\n    \n  \n\n\n\n\n\nX = features\n\n\n#Set our final hyperparameters to the tuned values\nxgbcl = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,\n         gamma=0.0, max_delta_step=0.0, min_child_weight=1.0,\n         missing=None, n_jobs=-1, objective='binary:logistic', random_state=42, reg_alpha=0.0,\n         reg_lambda=1.0, scale_pos_weight=1.0, tree_method='auto',\n         colsample_bytree = grid_result.best_params_['colsample_bytree'], \n         learning_rate = grid_result.best_params_['learning_rate'], \n         max_depth = grid_result.best_params_['max_depth'], \n         subsample = grid_result.best_params_['subsample'], \n         n_estimators = grid_result.best_params_['n_estimators'])\n\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n#refit the model on k-folds to get stable avg error metrics\nscores = cross_validate(estimator=xgbcl, X=X_train, y=y_train, cv=kfold, n_jobs=-1, \n                        scoring=['accuracy', 'roc_auc', 'precision', 'recall', 'f1'])\n\nprint('Training 5-fold Cross Validation Results:\\n')\nprint('AUC: ', scores['test_roc_auc'].mean())\nprint('Accuracy: ', scores['test_accuracy'].mean())\nprint('Precision: ', scores['test_precision'].mean())\nprint('Recall: ', scores['test_recall'].mean())\nprint('F1: ', scores['test_f1'].mean(), '\\n')\n\nNameError: name 'grid_result' is not defined\n\n\n\n#Generate predictions against our training and test data\npred_train = clf.predict(X_train)\nproba_train = clf.predict_proba(X_train)\npred_test = clf.predict(X_test)\nproba_test = clf.predict_proba(X_test)\n\n\nimport sklearn.metrics as metrics\n# Print model report\nprint(\"Classification report (Test): \\n\")\nprint(metrics.classification_report(y_test, pred_test))\nprint(\"Confusion matrix (Test): \\n\")\nprint(metrics.confusion_matrix(y_test, pred_test)/len(y_test))\n\nprint ('\\nTrain Accuracy:', metrics.accuracy_score(y_train, pred_train))\nprint ('Test Accuracy:', metrics.accuracy_score(y_test, pred_test))\n\nprint ('\\nTrain AUC:', metrics.roc_auc_score(y_train, proba_train[:,1]))\nprint ('Test AUC:', metrics.roc_auc_score(y_test, proba_test[:,1]))\n\nClassification report (Test): \n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      8547\n           1       0.20      0.02      0.03       133\n\n    accuracy                           0.98      8680\n   macro avg       0.59      0.51      0.51      8680\nweighted avg       0.97      0.98      0.98      8680\n\nConfusion matrix (Test): \n\n[[9.83755760e-01 9.21658986e-04]\n [1.50921659e-02 2.30414747e-04]]\n\nTrain Accuracy: 0.9959389400921659\nTest Accuracy: 0.9839861751152074\n\nTrain AUC: 0.9999601273396401\nTest AUC: 0.835284948066903\n\n\n\n# calculate the fpr and tpr for all thresholds of the classification\ntrain_fpr, train_tpr, train_threshold = metrics.roc_curve(y_train, proba_train[:,1])\ntest_fpr, test_tpr, test_threshold = metrics.roc_curve(y_test, proba_test[:,1])\n\ntrain_roc_auc = metrics.auc(train_fpr, train_tpr)\ntest_roc_auc = metrics.auc(test_fpr, test_tpr)\n\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=[7,5])\nplt.title('Receiver Operating Characteristic')\nplt.plot(train_fpr, train_tpr, 'b', label = 'Train AUC = %0.2f' % train_roc_auc)\nplt.plot(test_fpr, test_tpr, 'g', label = 'Test AUC = %0.2f' % test_roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n# plot feature importance\nxgb.plot_importance(clf, importance_type='gain');\n\n\n\n\n\n\n\n\n#take a random row of data\nX_rand = features.sample(1, random_state = 5)\ndisplay(df.iloc[X_rand.index])\n\n\n\n\n\n  \n    \n      \n      gender\n      age\n      hypertension\n      heart_disease\n      ever_married\n      work_type\n      Residence_type\n      avg_glucose_level\n      bmi\n      smoking_status\n      stroke\n    \n  \n  \n    \n      33658\n      Female\n      60.0\n      0\n      0\n      Yes\n      Private\n      Rural\n      108.13\n      28.6\n      never smoked\n      0\n    \n  \n\n\n\n\n\nimport shap\n\n\n## kernel shap sends data as numpy array which has no column names, so we fix it\ndef xgb_predict(data_asarray):\n    data_asframe =  pd.DataFrame(data_asarray, columns=feature_names)\n    return estimator.predict(data_asframe)\n\n\n#### Kernel SHAP\nX_summary = shap.kmeans(X_train, 10)\n\n\n#shap_kernel_explainer = shap.KernelExplainer(xgbcl, X_summary)\n\n\ndef xgb_predict(df):\n    data_asframe =  pd.DataFrame(df, columns=feature_names)\n    return estimator.predict(data_asframe)\n\n\n#### Tree SHAP\nshap_tree_explainer = shap.TreeExplainer(xgbcl, feature_perturbation = \"interventional\")\n\n\nshap.initjs()\n## shapely values with kernel SHAP\nshap.force_plot(shap_tree_explainer.expected_value, shap_values_single, X_test.iloc[[5]])\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\nshap.initjs()\n## shapely values with Tree SHAP\nshap_values_single = shap_tree_explainer.shap_values(X_test.iloc[[10]])\nshap.force_plot(shap_tree_explainer.expected_value, shap_values_single, X_test.iloc[[5]])\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\nshap_values = shap_tree_explainer.shap_values(X_train)\nshap.summary_plot(shap_values, features)\n\nAssertionError: Feature and SHAP matrices must have the same number of rows!\n\n\n\n\n\n\n# #Display all features and SHAP values\n# display(pd.DataFrame(data=shap_values, columns=X_train.columns, index=[126]).transpose().sort_values(by=126, ascending=True))\n\n\nX_train.columns\n\nIndex(['gender', 'age', 'hypertension', 'heart_disease', 'ever_married',\n       'work_type', 'Residence_type', 'avg_glucose_level', 'bmi',\n       'smoking_status'],\n      dtype='object')\n\n\n\nshap.dependence_plot('age', shap_values, X_train, interaction_index='bmi')\nshap.dependence_plot('bmi', shap_values, X_train) #when we don't specify an interaction_index, the strongest one is automatically chosen for us\nshap.dependence_plot('heart_disease', shap_values, X_train, interaction_index='age')"
  },
  {
    "objectID": "posts/2020-11-20-Dask-Diab.html",
    "href": "posts/2020-11-20-Dask-Diab.html",
    "title": "didactic",
    "section": "",
    "text": "categories: [Big Data]\n\n\n\n\nimport dask\nfrom dask.distributed import Client, progress\n\n\nfrom dask.distributed import Client\n\nclient = Client(n_workers=4)\n\nclient\n\n\n\n\n\n\n\n  Scheduler: tcp://127.0.0.1:41309\n  Dashboard: http://127.0.0.1:8787/status\n\n\n\n\n\n  Workers: 4\n  Cores: 8\n  Memory: 16.57 GB\n\n\n\n\n\n\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/diabetes.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n  \n    \n      \n      Pregnancies\n      Glucose\n      BloodPressure\n      SkinThickness\n      Insulin\n      BMI\n      DiabetesPedigreeFunction\n      Age\n      Outcome\n    \n  \n  \n    \n      0\n      6\n      148\n      72\n      35\n      0\n      33.6\n      0.627\n      50\n      1\n    \n    \n      1\n      1\n      85\n      66\n      29\n      0\n      26.6\n      0.351\n      31\n      0\n    \n    \n      2\n      8\n      183\n      64\n      0\n      0\n      23.3\n      0.672\n      32\n      1\n    \n    \n      3\n      1\n      89\n      66\n      23\n      94\n      28.1\n      0.167\n      21\n      0\n    \n    \n      4\n      0\n      137\n      40\n      35\n      168\n      43.1\n      2.288\n      33\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      763\n      10\n      101\n      76\n      48\n      180\n      32.9\n      0.171\n      63\n      0\n    \n    \n      764\n      2\n      122\n      70\n      27\n      0\n      36.8\n      0.340\n      27\n      0\n    \n    \n      765\n      5\n      121\n      72\n      23\n      112\n      26.2\n      0.245\n      30\n      0\n    \n    \n      766\n      1\n      126\n      60\n      0\n      0\n      30.1\n      0.349\n      47\n      1\n    \n    \n      767\n      1\n      93\n      70\n      31\n      0\n      30.4\n      0.315\n      23\n      0\n    \n  \n\n768 rows  9 columns\n\n\n\n\nfrom dask import dataframe as dd \n\nddf = dd.from_pandas(df, npartitions=5)\n\nddf\n\n\nDask DataFrame Structure:\n\n\n\n  \n    \n      \n      Pregnancies\n      Glucose\n      BloodPressure\n      SkinThickness\n      Insulin\n      BMI\n      DiabetesPedigreeFunction\n      Age\n      Outcome\n    \n    \n      npartitions=5\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      int64\n      int64\n      int64\n      int64\n      int64\n      float64\n      float64\n      int64\n      int64\n    \n    \n      154\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      616\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      767\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n  \n\n\nDask Name: from_pandas, 5 tasks\n\n\n\nimport dask.dataframe as dd\n\n# Subset of the columns to use\ncols = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n        'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n\n\nddf2 = ddf.sample(frac=0.2) # XGBoost requires a bit of RAM, we need a larger cluster\n\n\nddf2\n\n\nDask DataFrame Structure:\n\n\n\n  \n    \n      \n      Pregnancies\n      Glucose\n      BloodPressure\n      SkinThickness\n      Insulin\n      BMI\n      DiabetesPedigreeFunction\n      Age\n      Outcome\n    \n    \n      npartitions=5\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      int64\n      int64\n      int64\n      int64\n      int64\n      float64\n      float64\n      int64\n      int64\n    \n    \n      154\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      616\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      767\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n  \n\n\nDask Name: sample, 10 tasks\n\n\n\ndiab_diag = (ddf.Outcome)  # column of labels\n\n\ndel ddf['Outcome']  # Remove delay information from training dataframe\n\n\nddf, diab_diag = dask.persist(ddf, diab_diag)  # start work in the background\n\n\ndiab_diag.head()\n\n0    1\n1    0\n2    1\n3    0\n4    1\nName: Outcome, dtype: int64\n\n\n\ndf2 = dd.get_dummies(ddf.categorize()).persist()\n\n\nlen(df2.columns)\n\n8\n\n\n\ndata_train, data_test = df2.random_split([0.9, 0.1],\n                                         random_state=1234)\nlabels_train, labels_test = diab_diag.random_split([0.9, 0.1],\n                                                    random_state=1234)\n\n\n%%time\nimport dask_xgboost as dxgb\n\nparams = {'objective': 'binary:logistic', 'nround': 1000,\n          'max_depth': 16, 'eta': 0.01, 'subsample': 0.5,\n          'min_child_weight': 1, 'tree_method': 'hist',\n          'grow_policy': 'lossguide'}\n\nbst = dxgb.train(client, params, data_train, labels_train)\n\nCPU times: user 1.23 s, sys: 607 ms, total: 1.84 s\nWall time: 3.56 s\n\n\n\nbst\n\n<xgboost.core.Booster at 0x7fde80f05f98>\n\n\n\nimport xgboost as xgb\npandas_df = data_test.head()\ndtest = xgb.DMatrix(pandas_df)\n\n\nbst.predict(dtest)\n\narray([0.52612805, 0.51560616, 0.47321838, 0.5084377 , 0.45707062],\n      dtype=float32)\n\n\n\npredictions = dxgb.predict(client, bst, data_test).persist()\n\n\npredictions\n\n\n\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  unknown   unknown \n     Shape  (nan,)   (nan,) \n     Count  5 Tasks  5 Chunks \n     Type  float32  numpy.ndarray \n  \n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\n\nprint(roc_auc_score(labels_test.compute(), predictions.compute()))\n\n0.7775157232704403\n\n\n\nimport matplotlib.pyplot as plt\nfpr, tpr, _ = roc_curve(labels_test.compute(), predictions.compute())\n# Taken from\n# http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\nplt.figure(figsize=(8, 8))\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\nimport dask\nimport xgboost\nimport dask_xgboost\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nax = xgboost.plot_importance(bst, height=0.8, max_num_features=9)\nax.grid(False, axis=\"y\")\nax.set_title('Estimated feature importance')\nplt.show()\n\n\n\n\n\ny_hat = dask_xgboost.predict(client, bst, data_test).persist()\ny_hat\n\n\n\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  unknown   unknown \n     Shape  (nan,)   (nan,) \n     Count  5 Tasks  5 Chunks \n     Type  float32  numpy.ndarray \n  \n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import roc_curve\n\nlabels_test, y_hat = dask.compute(labels_test, y_hat)\nfpr, tpr, _ = roc_curve(labels_test, y_hat)\n\n\nfrom sklearn.metrics import auc\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.plot(fpr, tpr, lw=3,\n        label='ROC Curve (area = {:.2f})'.format(auc(fpr, tpr)))\nax.plot([0, 1], [0, 1], 'k--', lw=2)\nax.set(\n    xlim=(0, 1),\n    ylim=(0, 1),\n    title=\"ROC Curve\",\n    xlabel=\"False Positive Rate\",\n    ylabel=\"True Positive Rate\",\n)\nax.legend();\nplt.show()"
  }
]