[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "didactic",
    "section": "",
    "text": "news\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 13, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 8, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 7, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 5, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 28, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 24, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 3, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 19, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2020\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2020-08-30-nlp with pyspark.html",
    "href": "posts/2020-08-30-nlp with pyspark.html",
    "title": "NLP with Pyspark",
    "section": "",
    "text": "This post includes code from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-08-30-nlp with pyspark.html#using-tokenizer-and-regextokenizer",
    "href": "posts/2020-08-30-nlp with pyspark.html#using-tokenizer-and-regextokenizer",
    "title": "NLP with Pyspark",
    "section": "Using Tokenizer and RegexTokenizer",
    "text": "Using Tokenizer and RegexTokenizer\n\ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n\nregexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n# alternatively, pattern=\"\\\\w+\", gaps(False)\n\ncountTokens = udf(lambda words: len(words), IntegerType())\n\ntokenized = tokenizer.transform(sentenceDataFrame)\ntokenized.select(\"sentence\", \"words\")\\\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n\nregexTokenized = regexTokenizer.transform(sentenceDataFrame)\nregexTokenized.select(\"sentence\", \"words\") \\\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n\n\n+-----------------------------------+------------------------------------------+------+\nsentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\nHi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\nI wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\nLogistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n+-----------------------------------+------------------------------------------+------+\n\n+-----------------------------------+------------------------------------------+------+\nsentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\nHi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\nI wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\nLogistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n+-----------------------------------+------------------------------------------+------+"
  },
  {
    "objectID": "posts/2020-08-30-nlp with pyspark.html#removing-stop-words",
    "href": "posts/2020-08-30-nlp with pyspark.html#removing-stop-words",
    "title": "NLP with Pyspark",
    "section": "Removing Stop Words",
    "text": "Removing Stop Words\n\nfrom pyspark.ml.feature import StopWordsRemover\n\nsentenceData = spark.createDataFrame([\n    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n], [\"id\", \"raw\"])\n\nremover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\nremover.transform(sentenceData).show(truncate=False)\n\n\n+---+----------------------------+--------------------+\nid |raw                         |filtered            |\n+---+----------------------------+--------------------+\n0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n+---+----------------------------+--------------------+"
  },
  {
    "objectID": "posts/2020-08-30-nlp with pyspark.html#n-grams",
    "href": "posts/2020-08-30-nlp with pyspark.html#n-grams",
    "title": "NLP with Pyspark",
    "section": "n-grams",
    "text": "n-grams\n\nfrom pyspark.ml.feature import NGram\n\nwordDataFrame = spark.createDataFrame([\n    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n], [\"id\", \"words\"])\n\nngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n\nngramDataFrame = ngram.transform(wordDataFrame)\nngramDataFrame.select(\"ngrams\").show(truncate=False)\n\n\n+------------------------------------------------------------------+\nngrams                                                            |\n+------------------------------------------------------------------+\n[Hi I, I heard, heard about, about Spark]                         |\n[I wish, wish Java, Java could, could use, use case, case classes]|\n[Logistic regression, regression models, models are, are neat]    |\n+------------------------------------------------------------------+\n\n\n\n\n\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer\n\nsentenceData = spark.createDataFrame([\n    (0.0, \"Hi I heard about Spark\"),\n    (0.0, \"I wish Java could use case classes\"),\n    (1.0, \"Logistic regression models are neat\")\n], [\"label\", \"sentence\"])\n\nsentenceData.show()\n\n\n+-----+--------------------+\nlabel|            sentence|\n+-----+--------------------+\n  0.0|Hi I heard about ...|\n  0.0|I wish Java could...|\n  1.0|Logistic regressi...|\n+-----+--------------------+\n\n\n\n\n\ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\nwordsData = tokenizer.transform(sentenceData)\nwordsData.show()\n\n\n+-----+--------------------+--------------------+\nlabel|            sentence|               words|\n+-----+--------------------+--------------------+\n  0.0|Hi I heard about ...|[hi, i, heard, ab...|\n  0.0|I wish Java could...|[i, wish, java, c...|\n  1.0|Logistic regressi...|[logistic, regres...|\n+-----+--------------------+--------------------+\n\n\n\n\n\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\nfeaturizedData = hashingTF.transform(wordsData)\n# alternatively, CountVectorizer can also be used to get term frequency vectors\n\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData)\nrescaledData = idfModel.transform(featurizedData)\n\nrescaledData.select(\"label\", \"features\").show()\n\n\n+-----+--------------------+\nlabel|            features|\n+-----+--------------------+\n  0.0|(20,[6,8,13,16],[...|\n  0.0|(20,[0,2,7,13,15,...|\n  1.0|(20,[3,4,6,11,19]...|\n+-----+--------------------+"
  },
  {
    "objectID": "posts/2020-08-30-nlp with pyspark.html#countvectorizer",
    "href": "posts/2020-08-30-nlp with pyspark.html#countvectorizer",
    "title": "NLP with Pyspark",
    "section": "CountVectorizer",
    "text": "CountVectorizer\n\nfrom pyspark.ml.feature import CountVectorizer\n\n# Input data: Each row is a bag of words with a ID.\ndf = spark.createDataFrame([\n    (0, \"a b c\".split(\" \")),\n    (1, \"a b b c a\".split(\" \"))\n], [\"id\", \"words\"])\n\n# fit a CountVectorizerModel from the corpus.\ncv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n\nmodel = cv.fit(df)\n\nresult = model.transform(df)\nresult.show(truncate=False)\n\n\n+---+---------------+-------------------------+\nid |words          |features                 |\n+---+---------------+-------------------------+\n0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+---+---------------+-------------------------+\n\n\n\n\n\ndf = spark.read.load(\"/FileStore/tables/SMSSpamCollection\",\n                     format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"false\")\n\n\n\n\n\n\ndf.printSchema()\n\n\nroot\n-- _c0: string (nullable = true)\n-- _c1: string (nullable = true)\n\n\n\n\n\ndata = df.withColumnRenamed('_c0','class').withColumnRenamed('_c1','text')\n\n\n\n\n\n\ndata.printSchema()\n\n\nroot\n-- class: string (nullable = true)\n-- text: string (nullable = true)"
  },
  {
    "objectID": "posts/2020-08-30-nlp with pyspark.html#clean-and-prepare-the-data",
    "href": "posts/2020-08-30-nlp with pyspark.html#clean-and-prepare-the-data",
    "title": "NLP with Pyspark",
    "section": "Clean and Prepare the Data",
    "text": "Clean and Prepare the Data\n\nfrom pyspark.sql.functions import length\n\n\n\n\n\n\ndata = data.withColumn('length',length(data['text']))\n\n\n\n\n\n\ndata.printSchema()\n\n\nroot\n-- class: string (nullable = true)\n-- text: string (nullable = true)\n-- length: integer (nullable = true)\n\n\n\n\n\n# Pretty Clear Difference\ndata.groupby('class').mean().show()\n\n\n+-----+-----------------+\nclass|      avg(length)|\n+-----+-----------------+\n  ham| 71.4545266210897|\n spam|138.6706827309237|\n+-----+-----------------+\n\n\n\n\n\nfrom pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer\n\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\nstopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\ncount_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\nidf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\nham_spam_to_num = StringIndexer(inputCol='class',outputCol='label')\n\n\n\n\n\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.linalg import Vector\n\n\n\n\n\n\nclean_up = VectorAssembler(inputCols=['tf_idf','length'],outputCol='features')\n\n\n\n\n\n\nNaive Bayes\n\nfrom pyspark.ml.classification import NaiveBayes\n\n\n\n\n\n\n# Use defaults\nnb = NaiveBayes()\n\n\n\n\n\n\n### Pipeline\n\n\nfrom pyspark.ml import Pipeline\n\n\n\n\n\n\ndata_prep_pipe = Pipeline(stages=[ham_spam_to_num,tokenizer,stopremove,count_vec,idf,clean_up])\n\n\n\n\n\n\ncleaner = data_prep_pipe.fit(data)\n\n\n\n\n\n\nclean_data = cleaner.transform(data)\n\n\n\n\n\n\n\nTraining and Evaluation\n\nclean_data = clean_data.select(['label','features'])\n\n\n\n\n\n\nclean_data.show()\n\n\n+-----+--------------------+\nlabel|            features|\n+-----+--------------------+\n  0.0|(13424,[7,11,31,6...|\n  0.0|(13424,[0,24,297,...|\n  1.0|(13424,[2,13,19,3...|\n  0.0|(13424,[0,70,80,1...|\n  0.0|(13424,[36,134,31...|\n  1.0|(13424,[10,60,139...|\n  0.0|(13424,[10,53,103...|\n  0.0|(13424,[125,184,4...|\n  1.0|(13424,[1,47,118,...|\n  1.0|(13424,[0,1,13,27...|\n  0.0|(13424,[18,43,120...|\n  1.0|(13424,[8,17,37,8...|\n  1.0|(13424,[13,30,47,...|\n  0.0|(13424,[39,96,217...|\n  0.0|(13424,[552,1697,...|\n  1.0|(13424,[30,109,11...|\n  0.0|(13424,[82,214,47...|\n  0.0|(13424,[0,2,49,13...|\n  0.0|(13424,[0,74,105,...|\n  1.0|(13424,[4,30,33,5...|\n+-----+--------------------+\nonly showing top 20 rows\n\n\n\n\n\n(training,testing) = clean_data.randomSplit([0.7,0.3])\n\n\n\n\n\n\nspam_predictor = nb.fit(training)\n\n\n\n\n\n\ndata.printSchema()\n\n\nroot\n-- class: string (nullable = true)\n-- text: string (nullable = true)\n-- length: integer (nullable = true)\n\n\n\n\n\ntest_results = spam_predictor.transform(testing)\n\n\n\n\n\n\ntest_results.show()\n\n\n+-----+--------------------+--------------------+--------------------+----------+\nlabel|            features|       rawPrediction|         probability|prediction|\n+-----+--------------------+--------------------+--------------------+----------+\n  0.0|(13424,[0,1,2,13,...|[-605.26168264963...|[1.0,7.3447866033...|       0.0|\n  0.0|(13424,[0,1,2,41,...|[-1063.2170425771...|[1.0,9.8700382552...|       0.0|\n  0.0|(13424,[0,1,3,9,1...|[-569.95657733189...|[1.0,1.4498595638...|       0.0|\n  0.0|(13424,[0,1,5,15,...|[-998.87457222776...|[1.0,5.4020023412...|       0.0|\n  0.0|(13424,[0,1,7,15,...|[-658.37986687391...|[1.0,2.6912246466...|       0.0|\n  0.0|(13424,[0,1,14,31...|[-217.18809411711...|[1.0,3.3892033063...|       0.0|\n  0.0|(13424,[0,1,14,78...|[-688.50251926938...|[1.0,8.6317783323...|       0.0|\n  0.0|(13424,[0,1,17,19...|[-809.51840544334...|[1.0,1.3686507989...|       0.0|\n  0.0|(13424,[0,1,27,35...|[-1472.6804140726...|[0.99999999999983...|       0.0|\n  0.0|(13424,[0,1,31,43...|[-341.31126583915...|[1.0,3.4983325940...|       0.0|\n  0.0|(13424,[0,1,46,17...|[-1137.4942938439...|[5.99448563047616...|       1.0|\n  0.0|(13424,[0,1,72,10...|[-704.77256939631...|[1.0,1.2592610663...|       0.0|\n  0.0|(13424,[0,1,874,1...|[-96.404593207515...|[0.99999996015865...|       0.0|\n  0.0|(13424,[0,1,874,1...|[-98.086094104500...|[0.99999996999685...|       0.0|\n  0.0|(13424,[0,2,3,4,6...|[-1289.3891411076...|[1.0,1.3408017664...|       0.0|\n  0.0|(13424,[0,2,3,5,6...|[-2561.6651406471...|[1.0,2.6887776075...|       0.0|\n  0.0|(13424,[0,2,3,5,3...|[-490.88944126371...|[1.0,9.6538338828...|       0.0|\n  0.0|(13424,[0,2,4,5,1...|[-2493.1672898653...|[1.0,9.4058507096...|       0.0|\n  0.0|(13424,[0,2,4,7,2...|[-517.23267032348...|[1.0,2.8915589432...|       0.0|\n  0.0|(13424,[0,2,4,8,2...|[-1402.5570102185...|[1.0,6.7531061115...|       0.0|\n+-----+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n\n\n\n\n## Evaluating Model Accuracy\n\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n\n\n\n\n\nacc_eval = MulticlassClassificationEvaluator()\nacc = acc_eval.evaluate(test_results)\nprint(\"Accuracy of model at predicting spam was: {}\".format(acc))\n\n\nAccuracy of model at predicting spam was: 0.9204435112848836"
  },
  {
    "objectID": "posts/2020-08-29-clustering with pyspark.html",
    "href": "posts/2020-08-29-clustering with pyspark.html",
    "title": "Clustering with Pyspark",
    "section": "",
    "text": "This post includes code from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-08-29-clustering with pyspark.html#using-the-standardscaler",
    "href": "posts/2020-08-29-clustering with pyspark.html#using-the-standardscaler",
    "title": "Clustering with Pyspark",
    "section": "Using the StandardScaler",
    "text": "Using the StandardScaler\n\nfrom pyspark.ml.feature import StandardScaler\n\n\n\n\n\n\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)"
  },
  {
    "objectID": "posts/2020-08-29-clustering with pyspark.html#fitting-the-standardscaler",
    "href": "posts/2020-08-29-clustering with pyspark.html#fitting-the-standardscaler",
    "title": "Clustering with Pyspark",
    "section": "Fitting the StandardScaler",
    "text": "Fitting the StandardScaler\n\n# Compute summary statistics by fitting the StandardScaler\nscalerModel = scaler.fit(final_df)\n\n\n\n\n\n\n# Normalize each feature to have unit standard deviation.\ncluster_final_data = scalerModel.transform(final_df)\n\n\n\n\n\n\nkmeans3 = KMeans(featuresCol='scaledFeatures',k=3)\nkmeans2 = KMeans(featuresCol='scaledFeatures',k=2)\n\n\n\n\n\n\nmodel_k3 = kmeans3.fit(cluster_final_data)\nmodel_k2 = kmeans2.fit(cluster_final_data)\n\n\n\n\n\n\nmodel_k3.transform(cluster_final_data).groupBy('prediction').count().show()\n\n\n+----------+-----+\nprediction|count|\n+----------+-----+\n         1|   15|\n         2|   86|\n         0|  259|\n+----------+-----+\n\n\n\n\n\nmodel_k2.transform(cluster_final_data).groupBy('prediction').count().show()\n\n\n+----------+-----+\nprediction|count|\n+----------+-----+\n         1|  308|\n         0|   52|\n+----------+-----+"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "",
    "text": "Spark uses Java Virtual Machine (JVM) objects Resilient Distributed Datasets (RDD) which are calculated and stored in memory.\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructField,StringType,IntegerType,StructType, DoubleType, FloatType\nfrom pyspark.sql.functions import *\n\n\ndata_schema = [\nStructField(\"_c0\", IntegerType(), True)\n,StructField(\"province\", StringType(), True)\n,StructField(\"specific\", DoubleType(), True)\n,StructField(\"general\", DoubleType(), True)\n,StructField(\"year\", IntegerType(), True)\n,StructField(\"gdp\", FloatType(), True)\n,StructField(\"fdi\", FloatType(), True)\n,StructField(\"rnr\", DoubleType(), True)\n,StructField(\"rr\", FloatType(), True)\n,StructField(\"i\", FloatType(), True)\n,StructField(\"fr\", IntegerType(), True)\n,StructField(\"reg\", StringType(), True)\n,StructField(\"it\", IntegerType(), True)\n]\n\nfinal_struc = StructType(fields=data_schema)\n\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").schema(final_struc).option(\"header\", True).load(file_location)\n\n#df.printSchema()\n\ndf.show()\n\n\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|     fdi| rnr|       rr|        i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661.0| 0.0|      0.0|      0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443.0| 0.0|      0.0|      0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673.0| 0.0|      0.0|      0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131.0|null|     null|     null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0|      0.0|      0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672.0| 0.0|      0.0|      0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0|      0.0|      0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0|      0.0|      0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669.0| 0.0|      0.0|      0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000.0| 0.0|      0.0|0.3243243|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354.0| 0.0|      0.0|0.3243243|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892.0| 0.0|      0.0|0.3243243|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290.0|null|     null|     null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286.0| 0.0|      0.0|      0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800.0| 0.0|      0.0|     0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525.0| 0.0|      0.0|     0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368.0| 0.0|      0.0|     0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818.0| 0.0|      0.0|     0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464.0| 0.0|      0.0|     0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126.0| 0.0|0.7948718|      0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#using-topandas-to-look-at-the-data",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#using-topandas-to-look-at-the-data",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Using toPandas to look at the data",
    "text": "Using toPandas to look at the data\n\ndf.limit(10).toPandas()\n\n\n\n\n\n  \n    \n      \n      _c0\n      province\n      specific\n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      reg\n      it\n    \n  \n  \n    \n      0\n      0\n      Anhui\n      147002.0\n      NaN\n      1996\n      2093.300049\n      50661.0\n      0.0\n      0.0\n      0.000000\n      1128873.0\n      East China\n      631930\n    \n    \n      1\n      1\n      Anhui\n      151981.0\n      NaN\n      1997\n      2347.320068\n      43443.0\n      0.0\n      0.0\n      0.000000\n      1356287.0\n      East China\n      657860\n    \n    \n      2\n      2\n      Anhui\n      174930.0\n      NaN\n      1998\n      2542.959961\n      27673.0\n      0.0\n      0.0\n      0.000000\n      1518236.0\n      East China\n      889463\n    \n    \n      3\n      3\n      Anhui\n      285324.0\n      NaN\n      1999\n      2712.340088\n      26131.0\n      NaN\n      NaN\n      NaN\n      1646891.0\n      East China\n      1227364\n    \n    \n      4\n      4\n      Anhui\n      195580.0\n      32100.0\n      2000\n      2902.090088\n      31847.0\n      0.0\n      0.0\n      0.000000\n      1601508.0\n      East China\n      1499110\n    \n    \n      5\n      5\n      Anhui\n      250898.0\n      NaN\n      2001\n      3246.709961\n      33672.0\n      0.0\n      0.0\n      0.000000\n      1672445.0\n      East China\n      2165189\n    \n    \n      6\n      6\n      Anhui\n      434149.0\n      66529.0\n      2002\n      3519.719971\n      38375.0\n      0.0\n      0.0\n      0.000000\n      1677840.0\n      East China\n      2404936\n    \n    \n      7\n      7\n      Anhui\n      619201.0\n      52108.0\n      2003\n      3923.110107\n      36720.0\n      0.0\n      0.0\n      0.000000\n      1896479.0\n      East China\n      2815820\n    \n    \n      8\n      8\n      Anhui\n      898441.0\n      349699.0\n      2004\n      4759.299805\n      54669.0\n      0.0\n      0.0\n      0.000000\n      NaN\n      East China\n      3422176\n    \n    \n      9\n      9\n      Anhui\n      898441.0\n      NaN\n      2005\n      5350.169922\n      69000.0\n      0.0\n      0.0\n      0.324324\n      NaN\n      East China\n      3874846"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#renaming-columns",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#renaming-columns",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Renaming Columns",
    "text": "Renaming Columns\n\ndf = df.withColumnRenamed(\"reg\",\"region\")\n\n\n\n\n\n\ndf.limit(10).toPandas()\n\n\n\n\n\n  \n    \n      \n      _c0\n      province\n      specific\n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      region\n      it\n    \n  \n  \n    \n      0\n      0\n      Anhui\n      147002.0\n      NaN\n      1996\n      2093.300049\n      50661.0\n      0.0\n      0.0\n      0.000000\n      1128873.0\n      East China\n      631930\n    \n    \n      1\n      1\n      Anhui\n      151981.0\n      NaN\n      1997\n      2347.320068\n      43443.0\n      0.0\n      0.0\n      0.000000\n      1356287.0\n      East China\n      657860\n    \n    \n      2\n      2\n      Anhui\n      174930.0\n      NaN\n      1998\n      2542.959961\n      27673.0\n      0.0\n      0.0\n      0.000000\n      1518236.0\n      East China\n      889463\n    \n    \n      3\n      3\n      Anhui\n      285324.0\n      NaN\n      1999\n      2712.340088\n      26131.0\n      NaN\n      NaN\n      NaN\n      1646891.0\n      East China\n      1227364\n    \n    \n      4\n      4\n      Anhui\n      195580.0\n      32100.0\n      2000\n      2902.090088\n      31847.0\n      0.0\n      0.0\n      0.000000\n      1601508.0\n      East China\n      1499110\n    \n    \n      5\n      5\n      Anhui\n      250898.0\n      NaN\n      2001\n      3246.709961\n      33672.0\n      0.0\n      0.0\n      0.000000\n      1672445.0\n      East China\n      2165189\n    \n    \n      6\n      6\n      Anhui\n      434149.0\n      66529.0\n      2002\n      3519.719971\n      38375.0\n      0.0\n      0.0\n      0.000000\n      1677840.0\n      East China\n      2404936\n    \n    \n      7\n      7\n      Anhui\n      619201.0\n      52108.0\n      2003\n      3923.110107\n      36720.0\n      0.0\n      0.0\n      0.000000\n      1896479.0\n      East China\n      2815820\n    \n    \n      8\n      8\n      Anhui\n      898441.0\n      349699.0\n      2004\n      4759.299805\n      54669.0\n      0.0\n      0.0\n      0.000000\n      NaN\n      East China\n      3422176\n    \n    \n      9\n      9\n      Anhui\n      898441.0\n      NaN\n      2005\n      5350.169922\n      69000.0\n      0.0\n      0.0\n      0.324324\n      NaN\n      East China\n      3874846\n    \n  \n\n\n\n\n\n# df = df.toDF(*['year', 'region', 'province', 'gdp', 'fdi', 'specific', 'general', 'it', 'fr', 'rnr', 'rr', 'i', '_c0', 'specific_classification', 'provinceIndex', 'regionIndex'])"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#selecting-columns-of-interest",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#selecting-columns-of-interest",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Selecting Columns of Interest",
    "text": "Selecting Columns of Interest\n\ndf = df.select('year','region','province','gdp', 'fdi')\n\n\n\n\n\n\ndf.sort(\"gdp\").show()\n\n\n+----+---------------+--------+------+-------+\nyear|         region|province|   gdp|    fdi|\n+----+---------------+--------+------+-------+\n1996|Southwest China|   Tibet| 64.98|  679.0|\n1997|Southwest China|   Tibet| 77.24|   63.0|\n1998|Southwest China|   Tibet|  91.5|  481.0|\n1999|Southwest China|   Tibet|105.98|  196.0|\n2000|Southwest China|   Tibet| 117.8|    2.0|\n2001|Southwest China|   Tibet|139.16|  106.0|\n2002|Southwest China|   Tibet|162.04|  293.0|\n1996|Northwest China| Qinghai|184.17|  576.0|\n2003|Southwest China|   Tibet|185.09|  467.0|\n1997|Northwest China| Qinghai|202.79|  247.0|\n1996|Northwest China| Ningxia| 202.9| 2826.0|\n2004|Southwest China|   Tibet|220.34| 2699.0|\n1998|Northwest China| Qinghai|220.92| 1010.0|\n1997|Northwest China| Ningxia|224.59|  671.0|\n1999|Northwest China| Qinghai|239.38|  459.0|\n1998|Northwest China| Ningxia|245.44| 1856.0|\n2005|Southwest China|   Tibet| 248.8| 1151.0|\n2000|Northwest China| Qinghai|263.68|11020.0|\n1999|Northwest China| Ningxia|264.58| 5134.0|\n2006|Southwest China|   Tibet|290.76| 1522.0|\n+----+---------------+--------+------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#sorting-rdds-by-columns",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#sorting-rdds-by-columns",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Sorting RDDs by Columns",
    "text": "Sorting RDDs by Columns\n\nfrom pyspark.sql import functions as F\ndf.sort(F.desc(\"gdp\")).show()\n\n\n+----+-------------------+---------+--------+---------+\nyear|             region| province|     gdp|      fdi|\n+----+-------------------+---------+--------+---------+\n2007|South Central China|Guangdong|31777.01|1712603.0|\n2006|South Central China|Guangdong|26587.76|1451065.0|\n2007|         East China| Shandong|25776.91|1101159.0|\n2005|South Central China|Guangdong|22557.37|1236400.0|\n2006|         East China| Shandong|21900.19|1000069.0|\n2007|         East China|  Jiangsu|21742.05|1743140.0|\n2004|South Central China|Guangdong|18864.62|1001158.0|\n2007|         East China| Zhejiang|18753.73|1036576.0|\n2006|         East China|  Jiangsu|18598.69|1318339.0|\n2005|         East China| Shandong|18366.87| 897000.0|\n2003|South Central China|Guangdong|15844.64| 782294.0|\n2006|         East China| Zhejiang|15718.47| 888935.0|\n2004|         East China| Shandong|15021.84| 870064.0|\n2007|South Central China|    Henan|15012.46| 306162.0|\n2005|         East China|  Jiangsu| 15003.6|1213800.0|\n2007|        North China|    Hebei|13607.32| 241621.0|\n2002|South Central China|Guangdong|13502.42|1133400.0|\n2005|         East China| Zhejiang|13417.68| 772000.0|\n2007|         East China| Shanghai|12494.01| 792000.0|\n2004|         East China|  Jiangsu|12442.87|1056365.0|\n+----+-------------------+---------+--------+---------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#casting-data-types",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#casting-data-types",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Casting Data Types",
    "text": "Casting Data Types\n\nfrom pyspark.sql.types import IntegerType, StringType, DoubleType\ndf = df.withColumn('gdp', F.col('gdp').cast(DoubleType()))\n\n\n\n\n\n\ndf = df.withColumn('province', F.col('province').cast(StringType()))\n\n\n\n\n\n\ndf.filter((df.gdp>10000) & (df.region=='East China')).show()\n\n\n+----+----------+--------+----------------+---------+\nyear|    region|province|             gdp|      fdi|\n+----+----------+--------+----------------+---------+\n2003|East China| Jiangsu| 10606.849609375|1018960.0|\n2004|East China| Jiangsu|12442.8701171875|1056365.0|\n2005|East China| Jiangsu| 15003.599609375|1213800.0|\n2006|East China| Jiangsu| 18598.689453125|1318339.0|\n2007|East China| Jiangsu|  21742.05078125|1743140.0|\n2002|East China|Shandong|         10275.5| 473404.0|\n2003|East China|Shandong| 12078.150390625| 601617.0|\n2004|East China|Shandong|  15021.83984375| 870064.0|\n2005|East China|Shandong| 18366.869140625| 897000.0|\n2006|East China|Shandong| 21900.189453125|1000069.0|\n2007|East China|Shandong|  25776.91015625|1101159.0|\n2006|East China|Shanghai| 10572.240234375| 710700.0|\n2007|East China|Shanghai| 12494.009765625| 792000.0|\n2004|East China|Zhejiang|11648.7001953125| 668128.0|\n2005|East China|Zhejiang|   13417.6796875| 772000.0|\n2006|East China|Zhejiang|15718.4697265625| 888935.0|\n2007|East China|Zhejiang|  18753.73046875|1036576.0|\n+----+----------+--------+----------------+---------+"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#aggregating-using-groupby-.agg-and-summax",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#aggregating-using-groupby-.agg-and-summax",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Aggregating using groupBy, .agg and sum/max",
    "text": "Aggregating using groupBy, .agg and sum/max\n\nfrom pyspark.sql import functions as F\n\ndf.groupBy([\"region\",\"province\"]).agg(F.sum(\"gdp\") ,F.max(\"gdp\")).show()\n\n\n+-------------------+---------+------------------+------------------+\n             region| province|          sum(gdp)|          max(gdp)|\n+-------------------+---------+------------------+------------------+\nSouth Central China|    Hunan| 57190.69970703125|    9439.599609375|\n        North China|  Tianjin|30343.979858398438|    5252.759765625|\n    Northwest China| Xinjiang| 21946.75994873047| 3523.159912109375|\n        North China|  Beijing|56081.439208984375|   9846.8095703125|\nSouth Central China|Guangdong|  184305.376953125|   31777.009765625|\nSouth Central China|    Henan| 86507.60034179688|  15012.4599609375|\n         East China|  Jiangsu|129142.15966796875|    21742.05078125|\n    Northwest China|    Gansu| 16773.99005126953|  2703.97998046875|\n    Southwest China|  Guizhou|17064.130249023438| 2884.110107421875|\n    Southwest China|  Sichuan|64533.479736328125|  10562.3896484375|\nSouth Central China|   Hainan| 8240.570068359375|1254.1700439453125|\n         East China| Shandong| 147888.0283203125|    25776.91015625|\n    Southwest China|Chongqing|29732.549926757812|   4676.1298828125|\n    Northwest China|  Shaanxi|31896.409790039062|   5757.2900390625|\n         East China| Shanghai|  77189.4501953125|   12494.009765625|\n    Southwest China|    Tibet| 2045.120002746582|341.42999267578125|\n        North China|    Hebei|  83241.8994140625|     13607.3203125|\n    Northeast China|    Jilin|27298.250366210938|   4275.1201171875|\n         East China| Zhejiang|109657.81884765625|    18753.73046875|\n        North China|   Shanxi| 33806.52990722656|   6024.4501953125|\n+-------------------+---------+------------------+------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.groupBy([\"region\",\"province\"]).agg(F.sum(\"gdp\").alias(\"SumGDP\"),F.max(\"gdp\").alias(\"MaxGDP\")).show()\n\n\n+-------------------+---------+------------------+------------------+\n             region| province|               GDP|            MaxGDP|\n+-------------------+---------+------------------+------------------+\nSouth Central China|    Hunan| 57190.69970703125|    9439.599609375|\n        North China|  Tianjin|30343.979858398438|    5252.759765625|\n    Northwest China| Xinjiang| 21946.75994873047| 3523.159912109375|\n        North China|  Beijing|56081.439208984375|   9846.8095703125|\nSouth Central China|Guangdong|  184305.376953125|   31777.009765625|\nSouth Central China|    Henan| 86507.60034179688|  15012.4599609375|\n         East China|  Jiangsu|129142.15966796875|    21742.05078125|\n    Northwest China|    Gansu| 16773.99005126953|  2703.97998046875|\n    Southwest China|  Guizhou|17064.130249023438| 2884.110107421875|\n    Southwest China|  Sichuan|64533.479736328125|  10562.3896484375|\nSouth Central China|   Hainan| 8240.570068359375|1254.1700439453125|\n         East China| Shandong| 147888.0283203125|    25776.91015625|\n    Southwest China|Chongqing|29732.549926757812|   4676.1298828125|\n    Northwest China|  Shaanxi|31896.409790039062|   5757.2900390625|\n         East China| Shanghai|  77189.4501953125|   12494.009765625|\n    Southwest China|    Tibet| 2045.120002746582|341.42999267578125|\n        North China|    Hebei|  83241.8994140625|     13607.3203125|\n    Northeast China|    Jilin|27298.250366210938|   4275.1201171875|\n         East China| Zhejiang|109657.81884765625|    18753.73046875|\n        North China|   Shanxi| 33806.52990722656|   6024.4501953125|\n+-------------------+---------+------------------+------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.groupBy([\"region\",\"province\"]).agg(\n    F.sum(\"gdp\").alias(\"SumGDP\"),\\\n    F.max(\"gdp\").alias(\"MaxGDP\")\\\n    ).show()\n\n\n+-------------------+---------+------------------+------------------+\n             region| province|            SumGDP|            MaxGDP|\n+-------------------+---------+------------------+------------------+\nSouth Central China|    Hunan| 57190.69970703125|    9439.599609375|\n        North China|  Tianjin|30343.979858398438|    5252.759765625|\n    Northwest China| Xinjiang| 21946.75994873047| 3523.159912109375|\n        North China|  Beijing|56081.439208984375|   9846.8095703125|\nSouth Central China|Guangdong|  184305.376953125|   31777.009765625|\nSouth Central China|    Henan| 86507.60034179688|  15012.4599609375|\n         East China|  Jiangsu|129142.15966796875|    21742.05078125|\n    Northwest China|    Gansu| 16773.99005126953|  2703.97998046875|\n    Southwest China|  Guizhou|17064.130249023438| 2884.110107421875|\n    Southwest China|  Sichuan|64533.479736328125|  10562.3896484375|\nSouth Central China|   Hainan| 8240.570068359375|1254.1700439453125|\n         East China| Shandong| 147888.0283203125|    25776.91015625|\n    Southwest China|Chongqing|29732.549926757812|   4676.1298828125|\n    Northwest China|  Shaanxi|31896.409790039062|   5757.2900390625|\n         East China| Shanghai|  77189.4501953125|   12494.009765625|\n    Southwest China|    Tibet| 2045.120002746582|341.42999267578125|\n        North China|    Hebei|  83241.8994140625|     13607.3203125|\n    Northeast China|    Jilin|27298.250366210938|   4275.1201171875|\n         East China| Zhejiang|109657.81884765625|    18753.73046875|\n        North China|   Shanxi| 33806.52990722656|   6024.4501953125|\n+-------------------+---------+------------------+------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.limit(10).toPandas()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.300049\n      50661.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.320068\n      43443.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.959961\n      27673.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.340088\n      26131.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.090088\n      31847.0\n    \n    \n      5\n      2001\n      East China\n      Anhui\n      3246.709961\n      33672.0\n    \n    \n      6\n      2002\n      East China\n      Anhui\n      3519.719971\n      38375.0\n    \n    \n      7\n      2003\n      East China\n      Anhui\n      3923.110107\n      36720.0\n    \n    \n      8\n      2004\n      East China\n      Anhui\n      4759.299805\n      54669.0\n    \n    \n      9\n      2005\n      East China\n      Anhui\n      5350.169922\n      69000.0"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#exponentials-using-exp",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#exponentials-using-exp",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Exponentials using exp",
    "text": "Exponentials using exp\n\ndf = df.withColumn(\"Exp_GDP\", F.exp(\"gdp\"))\ndf.show()\n\n\n+----+-----------+--------+-----------------+--------+--------+\nyear|     region|province|              gdp|     fdi| Exp_GDP|\n+----+-----------+--------+-----------------+--------+--------+\n1996| East China|   Anhui|2093.300048828125| 50661.0|Infinity|\n1997| East China|   Anhui|2347.320068359375| 43443.0|Infinity|\n1998| East China|   Anhui|  2542.9599609375| 27673.0|Infinity|\n1999| East China|   Anhui|2712.340087890625| 26131.0|Infinity|\n2000| East China|   Anhui|2902.090087890625| 31847.0|Infinity|\n2001| East China|   Anhui|  3246.7099609375| 33672.0|Infinity|\n2002| East China|   Anhui|3519.719970703125| 38375.0|Infinity|\n2003| East China|   Anhui|3923.110107421875| 36720.0|Infinity|\n2004| East China|   Anhui|  4759.2998046875| 54669.0|Infinity|\n2005| East China|   Anhui|   5350.169921875| 69000.0|Infinity|\n2006| East China|   Anhui|           6112.5|139354.0|Infinity|\n2007| East China|   Anhui|   7360.919921875|299892.0|Infinity|\n1996|North China| Beijing|1789.199951171875|155290.0|Infinity|\n1997|North China| Beijing|2077.090087890625|159286.0|Infinity|\n1998|North China| Beijing|2377.179931640625|216800.0|Infinity|\n1999|North China| Beijing|2678.820068359375|197525.0|Infinity|\n2000|North China| Beijing|3161.659912109375|168368.0|Infinity|\n2001|North China| Beijing|  3707.9599609375|176818.0|Infinity|\n2002|North China| Beijing|           4315.0|172464.0|Infinity|\n2003|North China| Beijing|  5007.2099609375|219126.0|Infinity|\n+----+-----------+--------+-----------------+--------+--------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#window-functions",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#window-functions",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Window functions",
    "text": "Window functions\n\n\n\n\n\n\nNote\n\n\n\nWindow functions\n\n\n\n# Window functions\n\nfrom pyspark.sql.window import Window\nwindowSpec = Window().partitionBy(['province']).orderBy(F.desc('gdp'))\ndf.withColumn(\"rank\",F.rank().over(windowSpec)).show()\n\n\n+----+-------------------+---------+-----------------+---------+--------+----+\nyear|             region| province|              gdp|      fdi| Exp_GDP|rank|\n+----+-------------------+---------+-----------------+---------+--------+----+\n2007|South Central China|Guangdong|  31777.009765625|1712603.0|Infinity|   1|\n2006|South Central China|Guangdong|  26587.759765625|1451065.0|Infinity|   2|\n2005|South Central China|Guangdong|  22557.369140625|1236400.0|Infinity|   3|\n2004|South Central China|Guangdong|  18864.619140625|1001158.0|Infinity|   4|\n2003|South Central China|Guangdong| 15844.6396484375| 782294.0|Infinity|   5|\n2002|South Central China|Guangdong|  13502.419921875|1133400.0|Infinity|   6|\n2001|South Central China|Guangdong|         12039.25|1193203.0|Infinity|   7|\n2000|South Central China|Guangdong|         10741.25|1128091.0|Infinity|   8|\n1999|South Central China|Guangdong|     9250.6796875|1165750.0|Infinity|   9|\n1998|South Central China|Guangdong|  8530.8798828125|1201994.0|Infinity|  10|\n1997|South Central China|Guangdong| 7774.52978515625|1171083.0|Infinity|  11|\n1996|South Central China|Guangdong| 6834.97021484375|1162362.0|Infinity|  12|\n2007|South Central China|    Hunan|   9439.599609375| 327051.0|Infinity|   1|\n2006|South Central China|    Hunan|   7688.669921875| 259335.0|Infinity|   2|\n2005|South Central China|    Hunan| 6596.10009765625| 207200.0|Infinity|   3|\n2004|South Central China|    Hunan| 5641.93994140625| 141800.0|Infinity|   4|\n2003|South Central China|    Hunan|   4659.990234375| 101835.0|Infinity|   5|\n2002|South Central China|    Hunan|  4151.5400390625|  90022.0|Infinity|   6|\n2001|South Central China|    Hunan| 3831.89990234375|  81011.0|Infinity|   7|\n2000|South Central China|    Hunan|3551.489990234375|  67833.0|Infinity|   8|\n+----+-------------------+---------+-----------------+---------+--------+----+\nonly showing top 20 rows\n\n\n\n\n\nfrom pyspark.sql.window import Window\nwindowSpec = Window().partitionBy(['province']).orderBy('year')"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#lagging-variables",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#lagging-variables",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Lagging Variables",
    "text": "Lagging Variables\n\ndfWithLag = df.withColumn(\"lag_7\",F.lag(\"gdp\", 7).over(windowSpec))\n\n\n\n\n\n\ndf.filter(df.year>'2000').show()\n\n\n+----+---------------+---------+------------------+--------+--------+\nyear|         region| province|               gdp|     fdi| Exp_GDP|\n+----+---------------+---------+------------------+--------+--------+\n2001|     East China|    Anhui|   3246.7099609375| 33672.0|Infinity|\n2002|     East China|    Anhui| 3519.719970703125| 38375.0|Infinity|\n2003|     East China|    Anhui| 3923.110107421875| 36720.0|Infinity|\n2004|     East China|    Anhui|   4759.2998046875| 54669.0|Infinity|\n2005|     East China|    Anhui|    5350.169921875| 69000.0|Infinity|\n2006|     East China|    Anhui|            6112.5|139354.0|Infinity|\n2007|     East China|    Anhui|    7360.919921875|299892.0|Infinity|\n2001|    North China|  Beijing|   3707.9599609375|176818.0|Infinity|\n2002|    North China|  Beijing|            4315.0|172464.0|Infinity|\n2003|    North China|  Beijing|   5007.2099609375|219126.0|Infinity|\n2004|    North China|  Beijing|   6033.2099609375|308354.0|Infinity|\n2005|    North China|  Beijing|  6969.52001953125|352638.0|Infinity|\n2006|    North China|  Beijing|  8117.77978515625|455191.0|Infinity|\n2007|    North China|  Beijing|   9846.8095703125|506572.0|Infinity|\n2001|Southwest China|Chongqing|1976.8599853515625| 25649.0|Infinity|\n2002|Southwest China|Chongqing| 2232.860107421875| 19576.0|Infinity|\n2003|Southwest China|Chongqing| 2555.719970703125| 26083.0|Infinity|\n2004|Southwest China|Chongqing|    3034.580078125| 40508.0|Infinity|\n2005|Southwest China|Chongqing| 3467.719970703125| 51600.0|Infinity|\n2006|Southwest China|Chongqing|  3907.22998046875| 69595.0|Infinity|\n+----+---------------+---------+------------------+--------+--------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-22-window functions and pivot tables with pyspark.html#looking-at-windows-within-the-data",
    "href": "posts/2020-08-22-window functions and pivot tables with pyspark.html#looking-at-windows-within-the-data",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Looking at windows within the data",
    "text": "Looking at windows within the data\n\nfrom pyspark.sql.window import Window\n\nwindowSpec = Window().partitionBy(['province']).orderBy('year').rowsBetween(-6,0)\n\n\n\n\n\n\ndfWithRoll = df.withColumn(\"roll_7_confirmed\",F.mean(\"gdp\").over(windowSpec))\n\n\n\n\n\n\ndfWithRoll.filter(dfWithLag.year>'2001').show()\n\n\n+----+-------------------+---------+------------------+---------+--------------------+------------------+\nyear|             region| province|               gdp|      fdi|             Exp_GDP|  roll_7_confirmed|\n+----+-------------------+---------+------------------+---------+--------------------+------------------+\n2002|South Central China|Guangdong|   13502.419921875|1133400.0|            Infinity|  9810.56849888393|\n2003|South Central China|Guangdong|  15844.6396484375| 782294.0|            Infinity|11097.664132254464|\n2004|South Central China|Guangdong|   18864.619140625|1001158.0|            Infinity|12681.962611607143|\n2005|South Central China|Guangdong|   22557.369140625|1236400.0|            Infinity|14685.746791294643|\n2006|South Central China|Guangdong|   26587.759765625|1451065.0|            Infinity|17162.472516741072|\n2007|South Central China|Guangdong|   31777.009765625|1712603.0|            Infinity|  20167.5810546875|\n2002|South Central China|    Hunan|   4151.5400390625|  90022.0|            Infinity|3309.1999860491073|\n2003|South Central China|    Hunan|    4659.990234375| 101835.0|            Infinity| 3612.037179129464|\n2004|South Central China|    Hunan|  5641.93994140625| 141800.0|            Infinity|4010.9900251116073|\n2005|South Central China|    Hunan|  6596.10009765625| 207200.0|            Infinity|  4521.07146344866|\n2006|South Central China|    Hunan|    7688.669921875| 259335.0|            Infinity| 5160.232875279018|\n2007|South Central China|    Hunan|    9439.599609375| 327051.0|            Infinity| 6001.391392299107|\n2002|        North China|   Shanxi| 2324.800048828125|  21164.0|            Infinity|1749.4771379743304|\n2003|        North China|   Shanxi|  2855.22998046875|  21361.0|            Infinity| 1972.779994419643|\n2004|        North China|   Shanxi|   3571.3701171875|  62184.0|            Infinity| 2272.118582589286|\n2005|        North China|   Shanxi|  4230.52978515625|  27516.0|            Infinity| 2646.325701032366|\n2006|        North China|   Shanxi|  4878.60986328125|  47199.0|            Infinity|3105.1128278459823|\n2007|        North China|   Shanxi|   6024.4501953125| 134283.0|            Infinity| 3702.074288504464|\n2002|    Southwest China|    Tibet| 162.0399932861328|    293.0|2.360885537826244E70|108.38571493966239|\n2003|    Southwest China|    Tibet|185.08999633789062|    467.0|2.418600091901801E80|125.54428536551339|\n+----+-------------------+---------+------------------+---------+--------------------+------------------+\nonly showing top 20 rows\n\n\n\n\n\nfrom pyspark.sql.window import Window\nwindowSpec = Window().partitionBy(['province']).orderBy('year').rowsBetween(Window.unboundedPreceding,Window.currentRow)\n\n\n\n\n\n\ndfWithRoll = df.withColumn(\"cumulative_gdp\",F.sum(\"gdp\").over(windowSpec))\n\n\n\n\n\n\ndfWithRoll.filter(dfWithLag.year>'1999').show()\n\n\n+----+-------------------+---------+-----------------+---------+--------+------------------+\nyear|             region| province|              gdp|      fdi| Exp_GDP|    cumulative_gdp|\n+----+-------------------+---------+-----------------+---------+--------+------------------+\n2000|South Central China|Guangdong|         10741.25|1128091.0|Infinity|  43132.3095703125|\n2001|South Central China|Guangdong|         12039.25|1193203.0|Infinity|  55171.5595703125|\n2002|South Central China|Guangdong|  13502.419921875|1133400.0|Infinity|  68673.9794921875|\n2003|South Central China|Guangdong| 15844.6396484375| 782294.0|Infinity|   84518.619140625|\n2004|South Central China|Guangdong|  18864.619140625|1001158.0|Infinity|   103383.23828125|\n2005|South Central China|Guangdong|  22557.369140625|1236400.0|Infinity|  125940.607421875|\n2006|South Central China|Guangdong|  26587.759765625|1451065.0|Infinity|    152528.3671875|\n2007|South Central China|Guangdong|  31777.009765625|1712603.0|Infinity|  184305.376953125|\n2000|South Central China|    Hunan|3551.489990234375|  67833.0|Infinity|  15180.9599609375|\n2001|South Central China|    Hunan| 3831.89990234375|  81011.0|Infinity| 19012.85986328125|\n2002|South Central China|    Hunan|  4151.5400390625|  90022.0|Infinity| 23164.39990234375|\n2003|South Central China|    Hunan|   4659.990234375| 101835.0|Infinity| 27824.39013671875|\n2004|South Central China|    Hunan| 5641.93994140625| 141800.0|Infinity|   33466.330078125|\n2005|South Central China|    Hunan| 6596.10009765625| 207200.0|Infinity| 40062.43017578125|\n2006|South Central China|    Hunan|   7688.669921875| 259335.0|Infinity| 47751.10009765625|\n2007|South Central China|    Hunan|   9439.599609375| 327051.0|Infinity| 57190.69970703125|\n2000|        North China|   Shanxi|1845.719970703125|  22472.0|Infinity|7892.0098876953125|\n2001|        North China|   Shanxi|2029.530029296875|  23393.0|Infinity| 9921.539916992188|\n2002|        North China|   Shanxi|2324.800048828125|  21164.0|Infinity|12246.339965820312|\n2003|        North China|   Shanxi| 2855.22998046875|  21361.0|Infinity|15101.569946289062|\n+----+-------------------+---------+-----------------+---------+--------+------------------+\nonly showing top 20 rows\n\n\n\n\n\nPivot Dataframes\n\n\n\n\n\n\nNote\n\n\n\nPivot Dataframes\n\n\n\npivoted_df = df.groupBy('year').pivot('province') \\\n                      .agg(F.sum('gdp').alias('gdp') , F.sum('fdi').alias('fdi'))\npivoted_df.limit(10).toPandas()\n\n\n\n\n\n  \n    \n      \n      year\n      Anhui_gdp\n      Anhui_fdi\n      Beijing_gdp\n      Beijing_fdi\n      Chongqing_gdp\n      Chongqing_fdi\n      Fujian_gdp\n      Fujian_fdi\n      Gansu_gdp\n      Gansu_fdi\n      Guangdong_gdp\n      Guangdong_fdi\n      Guangxi_gdp\n      Guangxi_fdi\n      Guizhou_gdp\n      Guizhou_fdi\n      Hainan_gdp\n      Hainan_fdi\n      Hebei_gdp\n      Hebei_fdi\n      Heilongjiang_gdp\n      Heilongjiang_fdi\n      Henan_gdp\n      Henan_fdi\n      Hubei_gdp\n      Hubei_fdi\n      Hunan_gdp\n      Hunan_fdi\n      Jiangsu_gdp\n      Jiangsu_fdi\n      Jiangxi_gdp\n      Jiangxi_fdi\n      Jilin_gdp\n      Jilin_fdi\n      Liaoning_gdp\n      Liaoning_fdi\n      Ningxia_gdp\n      Ningxia_fdi\n      Qinghai_gdp\n      Qinghai_fdi\n      Shaanxi_gdp\n      Shaanxi_fdi\n      Shandong_gdp\n      Shandong_fdi\n      Shanghai_gdp\n      Shanghai_fdi\n      Shanxi_gdp\n      Shanxi_fdi\n      Sichuan_gdp\n      Sichuan_fdi\n      Tianjin_gdp\n      Tianjin_fdi\n      Tibet_gdp\n      Tibet_fdi\n      Xinjiang_gdp\n      Xinjiang_fdi\n      Yunnan_gdp\n      Yunnan_fdi\n      Zhejiang_gdp\n      Zhejiang_fdi\n    \n  \n  \n    \n      0\n      2003\n      3923.110107\n      36720.0\n      5007.209961\n      219126.0\n      2555.719971\n      26083.0\n      4983.669922\n      259903.0\n      1399.829956\n      2342.0\n      15844.639648\n      782294.0\n      2821.110107\n      41856.0\n      1426.339966\n      4521.0\n      713.960022\n      42125.0\n      6921.290039\n      96405.0\n      4057.399902\n      32180.0\n      6867.700195\n      53903.0\n      4757.450195\n      156886.0\n      4659.990234\n      101835.0\n      10606.849609\n      1018960.0\n      2450.479980\n      108197.0\n      2348.540039\n      24468.0\n      5458.220215\n      341168.0\n      445.359985\n      1743.0\n      390.200012\n      2522.0\n      2587.719971\n      33190.0\n      12078.150391\n      601617.0\n      6694.229980\n      546849.0\n      2855.229980\n      21361.0\n      5333.089844\n      41231.0\n      2578.030029\n      153473.0\n      185.089996\n      467.0\n      1886.349976\n      1534.0\n      2556.020020\n      8384.0\n      9705.019531\n      498055.0\n    \n    \n      1\n      2007\n      7360.919922\n      299892.0\n      9846.809570\n      506572.0\n      4676.129883\n      108534.0\n      9248.530273\n      406058.0\n      2703.979980\n      11802.0\n      31777.009766\n      1712603.0\n      5823.410156\n      68396.0\n      2884.110107\n      12651.0\n      1254.170044\n      112001.0\n      13607.320312\n      241621.0\n      7104.000000\n      208508.0\n      15012.459961\n      306162.0\n      9333.400391\n      276622.0\n      9439.599609\n      327051.0\n      21742.050781\n      1743140.0\n      4820.529785\n      280657.0\n      4275.120117\n      76064.0\n      9304.519531\n      598554.0\n      919.109985\n      5047.0\n      797.349976\n      31000.0\n      5757.290039\n      119516.0\n      25776.910156\n      1101159.0\n      12494.009766\n      792000.0\n      6024.450195\n      134283.0\n      10562.389648\n      149322.0\n      5252.759766\n      527776.0\n      341.429993\n      2418.0\n      3523.159912\n      12484.0\n      4772.520020\n      39453.0\n      18753.730469\n      1036576.0\n    \n    \n      2\n      2006\n      6112.500000\n      139354.0\n      8117.779785\n      455191.0\n      3907.229980\n      69595.0\n      7583.850098\n      322047.0\n      2277.350098\n      2954.0\n      26587.759766\n      1451065.0\n      4746.160156\n      44740.0\n      2338.979980\n      9384.0\n      1065.670044\n      74878.0\n      11467.599609\n      201434.0\n      6211.799805\n      170801.0\n      12362.790039\n      184526.0\n      7617.470215\n      244853.0\n      7688.669922\n      259335.0\n      18598.689453\n      1318339.0\n      4056.760010\n      242000.0\n      3620.270020\n      66100.0\n      8047.259766\n      359000.0\n      725.900024\n      3718.0\n      648.500000\n      27500.0\n      4743.609863\n      92489.0\n      21900.189453\n      1000069.0\n      10572.240234\n      710700.0\n      4878.609863\n      47199.0\n      8690.240234\n      120819.0\n      4462.740234\n      413077.0\n      290.760010\n      1522.0\n      3045.260010\n      10366.0\n      3988.139893\n      30234.0\n      15718.469727\n      888935.0\n    \n    \n      3\n      1997\n      2347.320068\n      43443.0\n      2077.090088\n      159286.0\n      1509.750000\n      38675.0\n      2870.899902\n      419666.0\n      793.570007\n      4144.0\n      7774.529785\n      1171083.0\n      1817.250000\n      87986.0\n      805.789978\n      4977.0\n      411.160004\n      70554.0\n      3953.780029\n      110064.0\n      2667.500000\n      73485.0\n      4041.090088\n      69204.0\n      2856.469971\n      79019.0\n      2849.270020\n      91702.0\n      6004.209961\n      507208.0\n      1409.739990\n      30068.0\n      1346.790039\n      45155.0\n      3157.689941\n      167142.0\n      224.589996\n      671.0\n      202.789993\n      247.0\n      1363.599976\n      62816.0\n      6537.069824\n      249294.0\n      3438.790039\n      422536.0\n      1476.000000\n      26592.0\n      3241.469971\n      24846.0\n      1264.630005\n      251135.0\n      77.239998\n      63.0\n      1039.849976\n      2472.0\n      1676.170044\n      16566.0\n      4686.109863\n      150345.0\n    \n    \n      4\n      2004\n      4759.299805\n      54669.0\n      6033.209961\n      308354.0\n      3034.580078\n      40508.0\n      5763.350098\n      474801.0\n      1688.489990\n      3539.0\n      18864.619141\n      1001158.0\n      3433.500000\n      29579.0\n      1677.800049\n      6533.0\n      819.659973\n      64343.0\n      8477.629883\n      162341.0\n      4750.600098\n      123639.0\n      8553.790039\n      87367.0\n      5633.240234\n      207126.0\n      5641.939941\n      141800.0\n      12442.870117\n      1056365.0\n      2807.409912\n      161202.0\n      2662.080078\n      19059.0\n      6002.540039\n      282410.0\n      537.109985\n      6689.0\n      466.100006\n      22500.0\n      3175.580078\n      52664.0\n      15021.839844\n      870064.0\n      8072.830078\n      654100.0\n      3571.370117\n      62184.0\n      6379.629883\n      70129.0\n      3110.969971\n      247243.0\n      220.339996\n      2699.0\n      2209.090088\n      4586.0\n      3081.909912\n      14200.0\n      11648.700195\n      668128.0\n    \n    \n      5\n      1996\n      2093.300049\n      50661.0\n      1789.199951\n      155290.0\n      1315.119995\n      21878.0\n      2484.250000\n      407876.0\n      722.520020\n      9002.0\n      6834.970215\n      1162362.0\n      1697.900024\n      66618.0\n      723.179993\n      3138.0\n      389.679993\n      78960.0\n      3452.969971\n      123652.0\n      2370.500000\n      54841.0\n      3634.689941\n      52566.0\n      2499.770020\n      68878.0\n      2540.129883\n      70344.0\n      5155.250000\n      478058.0\n      1169.729980\n      28818.0\n      1137.229980\n      39876.0\n      2793.370117\n      140405.0\n      202.899994\n      2826.0\n      184.169998\n      576.0\n      1215.839966\n      33008.0\n      5883.799805\n      259041.0\n      2957.550049\n      471578.0\n      1292.109985\n      13802.0\n      2871.649902\n      22519.0\n      1121.930054\n      200587.0\n      64.980003\n      679.0\n      900.929993\n      6639.0\n      1517.689941\n      18000.0\n      4188.529785\n      152021.0\n    \n    \n      6\n      1998\n      2542.959961\n      27673.0\n      2377.179932\n      216800.0\n      1602.380005\n      43107.0\n      3159.909912\n      421211.0\n      887.669983\n      3864.0\n      8530.879883\n      1201994.0\n      1911.300049\n      88613.0\n      858.390015\n      4535.0\n      442.130005\n      71715.0\n      4256.009766\n      142868.0\n      2774.399902\n      52639.0\n      4308.240234\n      61654.0\n      3114.020020\n      97294.0\n      3025.530029\n      81816.0\n      6680.339844\n      543511.0\n      1605.770020\n      47768.0\n      1464.339966\n      40227.0\n      3582.459961\n      220470.0\n      245.440002\n      1856.0\n      220.919998\n      1010.0\n      1458.400024\n      30010.0\n      7021.350098\n      220274.0\n      3801.090088\n      360150.0\n      1611.079956\n      24451.0\n      3474.090088\n      37248.0\n      1374.599976\n      211361.0\n      91.500000\n      481.0\n      1106.949951\n      2167.0\n      1831.329956\n      14568.0\n      5052.620117\n      131802.0\n    \n    \n      7\n      2001\n      3246.709961\n      33672.0\n      3707.959961\n      176818.0\n      1976.859985\n      25649.0\n      4072.850098\n      391804.0\n      1125.369995\n      7439.0\n      12039.250000\n      1193203.0\n      2279.340088\n      38416.0\n      1133.270020\n      2829.0\n      579.169983\n      46691.0\n      5516.759766\n      66989.0\n      3390.100098\n      34114.0\n      5533.009766\n      45729.0\n      3880.530029\n      118860.0\n      3831.899902\n      81011.0\n      8553.690430\n      642550.0\n      2003.069946\n      22724.0\n      1951.510010\n      33701.0\n      4669.060059\n      204446.0\n      337.440002\n      1680.0\n      300.130005\n      3649.0\n      2010.619995\n      35174.0\n      9195.040039\n      352093.0\n      5210.120117\n      429159.0\n      2029.530029\n      23393.0\n      4293.490234\n      58188.0\n      1919.089966\n      213348.0\n      139.160004\n      106.0\n      1491.599976\n      2035.0\n      2138.310059\n      6457.0\n      6898.339844\n      221162.0\n    \n    \n      8\n      2005\n      5350.169922\n      69000.0\n      6969.520020\n      352638.0\n      3467.719971\n      51600.0\n      6554.689941\n      260800.0\n      1933.979980\n      2000.0\n      22557.369141\n      1236400.0\n      3984.100098\n      37866.0\n      2005.420044\n      10768.0\n      918.750000\n      68400.0\n      10012.110352\n      191000.0\n      5513.700195\n      145000.0\n      10587.419922\n      123000.0\n      6590.189941\n      218500.0\n      6596.100098\n      207200.0\n      15003.599609\n      1213800.0\n      3456.699951\n      205238.0\n      3122.010010\n      45266.0\n      6672.000000\n      540679.0\n      612.609985\n      14100.0\n      543.320007\n      26600.0\n      3933.719971\n      62800.0\n      18366.869141\n      897000.0\n      9247.660156\n      685000.0\n      4230.529785\n      27516.0\n      7385.100098\n      88686.0\n      3905.639893\n      332885.0\n      248.800003\n      1151.0\n      2604.189941\n      4700.0\n      3462.729980\n      17352.0\n      13417.679688\n      772000.0\n    \n    \n      9\n      2000\n      2902.090088\n      31847.0\n      3161.659912\n      168368.0\n      1791.000000\n      24436.0\n      3764.540039\n      343191.0\n      1052.880005\n      6235.0\n      10741.250000\n      1128091.0\n      2080.040039\n      52466.0\n      1029.920044\n      2501.0\n      526.820007\n      43080.0\n      5043.959961\n      67923.0\n      3151.399902\n      30086.0\n      5052.990234\n      56403.0\n      3545.389893\n      94368.0\n      3551.489990\n      67833.0\n      7697.819824\n      607756.0\n      1853.650024\n      32080.0\n      1672.959961\n      30120.0\n      4171.689941\n      106173.0\n      295.019989\n      1741.0\n      263.679993\n      11020.0\n      1804.000000\n      28842.0\n      8337.469727\n      297119.0\n      4771.169922\n      316014.0\n      1845.719971\n      22472.0\n      3928.199951\n      43694.0\n      1701.880005\n      116601.0\n      117.800003\n      2.0\n      1363.560059\n      1911.0\n      2011.189941\n      12812.0\n      6141.029785\n      161266.0\n    \n  \n\n\n\n\n\npivoted_df.columns\n\n\nOut[55]: ['year',\n 'Anhui_gdp',\n 'Anhui_fdi',\n 'Beijing_gdp',\n 'Beijing_fdi',\n 'Chongqing_gdp',\n 'Chongqing_fdi',\n 'Fujian_gdp',\n 'Fujian_fdi',\n 'Gansu_gdp',\n 'Gansu_fdi',\n 'Guangdong_gdp',\n 'Guangdong_fdi',\n 'Guangxi_gdp',\n 'Guangxi_fdi',\n 'Guizhou_gdp',\n 'Guizhou_fdi',\n 'Hainan_gdp',\n 'Hainan_fdi',\n 'Hebei_gdp',\n 'Hebei_fdi',\n 'Heilongjiang_gdp',\n 'Heilongjiang_fdi',\n 'Henan_gdp',\n 'Henan_fdi',\n 'Hubei_gdp',\n 'Hubei_fdi',\n 'Hunan_gdp',\n 'Hunan_fdi',\n 'Jiangsu_gdp',\n 'Jiangsu_fdi',\n 'Jiangxi_gdp',\n 'Jiangxi_fdi',\n 'Jilin_gdp',\n 'Jilin_fdi',\n 'Liaoning_gdp',\n 'Liaoning_fdi',\n 'Ningxia_gdp',\n 'Ningxia_fdi',\n 'Qinghai_gdp',\n 'Qinghai_fdi',\n 'Shaanxi_gdp',\n 'Shaanxi_fdi',\n 'Shandong_gdp',\n 'Shandong_fdi',\n 'Shanghai_gdp',\n 'Shanghai_fdi',\n 'Shanxi_gdp',\n 'Shanxi_fdi',\n 'Sichuan_gdp',\n 'Sichuan_fdi',\n 'Tianjin_gdp',\n 'Tianjin_fdi',\n 'Tibet_gdp',\n 'Tibet_fdi',\n 'Xinjiang_gdp',\n 'Xinjiang_fdi',\n 'Yunnan_gdp',\n 'Yunnan_fdi',\n 'Zhejiang_gdp',\n 'Zhejiang_fdi']\n\n\n\nnewColnames = [x.replace(\"-\",\"_\") for x in pivoted_df.columns]\n\n\n\n\n\n\npivoted_df = pivoted_df.toDF(*newColnames)\n\n\n\n\n\n\nexpression = \"\"\ncnt=0\nfor column in pivoted_df.columns:\n    if column!='year':\n        cnt +=1\n        expression += f\"'{column}' , {column},\"\n        \nexpression = f\"stack({cnt}, {expression[:-1]}) as (Type,Value)\"\n\n\n\n\n\n\n\nUnpivoting RDDs\n\nunpivoted_df = pivoted_df.select('year',F.expr(expression))\nunpivoted_df.show()\n\n\n+----+-------------+------------------+\nyear|         Type|             Value|\n+----+-------------+------------------+\n2003|    Anhui_gdp| 3923.110107421875|\n2003|    Anhui_fdi|           36720.0|\n2003|  Beijing_gdp|   5007.2099609375|\n2003|  Beijing_fdi|          219126.0|\n2003|Chongqing_gdp| 2555.719970703125|\n2003|Chongqing_fdi|           26083.0|\n2003|   Fujian_gdp|    4983.669921875|\n2003|   Fujian_fdi|          259903.0|\n2003|    Gansu_gdp|1399.8299560546875|\n2003|    Gansu_fdi|            2342.0|\n2003|Guangdong_gdp|  15844.6396484375|\n2003|Guangdong_fdi|          782294.0|\n2003|  Guangxi_gdp| 2821.110107421875|\n2003|  Guangxi_fdi|           41856.0|\n2003|  Guizhou_gdp|1426.3399658203125|\n2003|  Guizhou_fdi|            4521.0|\n2003|   Hainan_gdp| 713.9600219726562|\n2003|   Hainan_fdi|           42125.0|\n2003|    Hebei_gdp|   6921.2900390625|\n2003|    Hebei_fdi|           96405.0|\n+----+-------------+------------------+\nonly showing top 20 rows\n\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-09-19-stockmarketportfolioanaylsis.html",
    "href": "posts/2020-09-19-stockmarketportfolioanaylsis.html",
    "title": "Stock Market and Portfolio Anaylsis with pandas_datareader and quandl",
    "section": "",
    "text": "import pandas as pd\nfrom pandas_datareader import data, wb\nimport datetime\n\n\nstart = pd.to_datetime('2020-02-04')\nend = pd.to_datetime('today')\n\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nSNOW_stock = web.DataReader('SNOW', 'yahoo', start, end)\nSNOW_stock.head()\n\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Open')\n\nMSFT_stock['Open'].plot(label='Microsoft')\nZOOM_stock['Open'].plot(label='Zoom')\nSNOW_stock['Open'].plot(label='Snowflake')\nplt.legend()\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Volume')\n\nMSFT_stock['Volume'].plot(label='Microsoft')\nZOOM_stock['Volume'].plot(label='Zoom')\nSNOW_stock['Volume'].plot(label='Snowflake')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f557129fa90>\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport quandl\n\n\nstart = pd.to_datetime('2010-01-01')\nend = pd.to_datetime('today')\n\n\n# Grabbing a bunch of tech stocks for our portfolio\nCOST = quandl.get('WIKI/COST.11',\n                  start_date = start,\n                  end_date = end)\nNLSN = quandl.get('WIKI/NLSN.11',\n                   start_date = start,\n                   end_date = end)\nNKE = quandl.get('WIKI/NKE.11',\n                 start_date = start,\n                 end_date = end)\nDIS = quandl.get('WIKI/DIS.11',\n                  start_date = start,\n                  end_date = end)\n\n\n\n# Example\nCOST.iloc[0]['Adj. Close']\n\nfor stock_df in (COST, NLSN, NKE, DIS):\n    stock_df['Normed Return'] = stock_df['Adj. Close'] / stock_df.iloc[0]['Adj. Close']\n\nCOST.head()\n\nCOST.tail()\n\n## Allocations\n\nfor stock_df,allo in zip([COST,NLSN,NKE,DIS],[.2, .1, .4, .3]):\n    stock_df['Allocation'] = stock_df['Normed Return'] * allo\n\nCOST.head()\n\n## Investment\n\nfor stock_df in [COST,NLSN,NKE,DIS]:\n    stock_df['Position Values'] = stock_df['Allocation'] * 1000000\n\n## Total Portfolio Value\n\nportfolio_val = pd.concat([COST['Position Values'],\n                           NLSN['Position Values'],\n                           NKE['Position Values'],\n                           DIS['Position Values']],\n                          axis = 1)\n\nportfolio_val.head()\n\nportfolio_val.columns = ['COST Pos', 'NLSN Pos', 'NKE Pos', 'DIS Pos']\n\nportfolio_val.head()\n\nportfolio_val['Total Pos'] = portfolio_val.sum(axis = 1)\n\nportfolio_val.head()\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nportfolio_val['Total Pos'].plot(figsize = (12, 8))\nplt.title('Total Portfolio Value')\n\nportfolio_val.drop('Total Pos',\n                   axis = 1).plot(kind = 'line', figsize = (12, 8))\n\nportfolio_val.tail()\n\n\n\n\n\n  \n    \n      \n      COST Pos\n      NLSN Pos\n      NKE Pos\n      DIS Pos\n      Total Pos\n    \n    \n      Date\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2018-03-21\n      758153.014553\n      144489.827125\n      1.799185e+06\n      1.054741e+06\n      3.756569e+06\n    \n    \n      2018-03-22\n      744177.280475\n      141728.307617\n      1.746850e+06\n      1.042104e+06\n      3.674859e+06\n    \n    \n      2018-03-23\n      736843.076002\n      140347.547864\n      1.752545e+06\n      1.020764e+06\n      3.650500e+06\n    \n    \n      2018-03-26\n      762838.756299\n      142663.660999\n      1.786983e+06\n      1.042622e+06\n      3.735107e+06\n    \n    \n      2018-03-27\n      746255.305075\n      142930.904822\n      1.794304e+06\n      1.029259e+06\n      3.712749e+06"
  },
  {
    "objectID": "posts/2020-08-21-rdds and schemas and data types with pyspark.html",
    "href": "posts/2020-08-21-rdds and schemas and data types with pyspark.html",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "",
    "text": "from pyspark.sql import SparkSession\n\n# Load data from a CSV\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").option(\"inferSchema\", True).option(\"header\", True).load(file_location)\ndisplay(df.take(5))\n\n\n_c0provincespecificgeneralyeargdpfdirnrrrifrregit0Anhui147002.0null19962093.3506610.00.00.01128873East China6319301Anhui151981.0null19972347.32434430.00.00.01356287East China6578602Anhui174930.0null19982542.96276730.00.00.01518236East China8894633Anhui285324.0null19992712.3426131nullnullnull1646891East China12273644Anhui195580.032100.020002902.09318470.00.00.01601508East China1499110"
  },
  {
    "objectID": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#setting-data-schema-and-data-types",
    "href": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#setting-data-schema-and-data-types",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "Setting Data Schema and Data Types",
    "text": "Setting Data Schema and Data Types\n\nfrom pyspark.sql.types import StructField,StringType,IntegerType,StructType\n\n\n\n\n\n\ndata_schema = [\nStructField(\"_c0\", IntegerType(), True)\n,StructField(\"province\", StringType(), True)\n,StructField(\"specific\", IntegerType(), True)\n,StructField(\"general\", IntegerType(), True)\n,StructField(\"year\", IntegerType(), True)\n,StructField(\"gdp\", IntegerType(), True)\n,StructField(\"fdi\", IntegerType(), True)\n,StructField(\"rnr\", IntegerType(), True)\n,StructField(\"rr\", IntegerType(), True)\n,StructField(\"i\", IntegerType(), True)\n,StructField(\"fr\", IntegerType(), True)\n,StructField(\"reg\", StringType(), True)\n,StructField(\"it\", IntegerType(), True)\n]\n\n\n\n\n\n\nfinal_struc = StructType(fields=data_schema)"
  },
  {
    "objectID": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#applying-the-data-schemadata-types-while-reading-in-a-csv",
    "href": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#applying-the-data-schemadata-types-while-reading-in-a-csv",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "Applying the Data Schema/Data Types while reading in a CSV",
    "text": "Applying the Data Schema/Data Types while reading in a CSV\n\ndf = spark.read.format(\"CSV\").schema(final_struc).load(file_location)\n\n\n\n\n\n\ndf.printSchema()\n\n\nroot\n-- _c0: integer (nullable = true)\n-- province: string (nullable = true)\n-- specific: integer (nullable = true)\n-- general: integer (nullable = true)\n-- year: integer (nullable = true)\n-- gdp: integer (nullable = true)\n-- fdi: integer (nullable = true)\n-- rnr: integer (nullable = true)\n-- rr: integer (nullable = true)\n-- i: integer (nullable = true)\n-- fr: integer (nullable = true)\n-- reg: string (nullable = true)\n-- it: integer (nullable = true)\n\n\n\n\n\ndf.show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf['fr']\n\n\nOut[72]: Column<b'fr'>\n\n\n\ntype(df['fr'])\n\n\nOut[73]: pyspark.sql.column.Column\n\n\n\ndf.select('fr')\n\n\nOut[74]: DataFrame[fr: int]\n\n\n\ntype(df.select('fr'))\n\n\nOut[75]: pyspark.sql.dataframe.DataFrame\n\n\n\ndf.select('fr').show()\n\n\n+-------+\n     fr|\n+-------+\n   null|\n1128873|\n1356287|\n1518236|\n1646891|\n1601508|\n1672445|\n1677840|\n1896479|\n   null|\n   null|\n3434548|\n4468640|\n 634562|\n 634562|\n 938788|\n   null|\n1667114|\n2093925|\n2511249|\n+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.head(2)\n\n\nOut[77]: [Row(_c0=None, province='province', specific=None, general=None, year=None, gdp=None, fdi=None, rnr=None, rr=None, i=None, fr=None, reg='reg', it=None),\n Row(_c0=0, province='Anhui', specific=None, general=None, year=1996, gdp=None, fdi=50661, rnr=None, rr=None, i=None, fr=1128873, reg='East China', it=631930)]\n\n\n\ndf.select(['reg','fr'])\n\n\nOut[78]: DataFrame[reg: string, fr: int]"
  },
  {
    "objectID": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#using-select-with-rdds",
    "href": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#using-select-with-rdds",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "Using select with RDDs",
    "text": "Using select with RDDs\n\ndf.select(['reg','fr']).show()\n\n\n+-----------+-------+\n        reg|     fr|\n+-----------+-------+\n        reg|   null|\n East China|1128873|\n East China|1356287|\n East China|1518236|\n East China|1646891|\n East China|1601508|\n East China|1672445|\n East China|1677840|\n East China|1896479|\n East China|   null|\n East China|   null|\n East China|3434548|\n East China|4468640|\nNorth China| 634562|\nNorth China| 634562|\nNorth China| 938788|\nNorth China|   null|\nNorth China|1667114|\nNorth China|2093925|\nNorth China|2511249|\n+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.withColumn('fiscal_revenue',df['fr']).show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+--------------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|fiscal_revenue|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+--------------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|          null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|       1128873|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|       1356287|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|       1518236|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|       1646891|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|       1601508|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|       1672445|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|       1677840|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|       1896479|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|          null|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|          null|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|       3434548|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|       4468640|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|        634562|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|        634562|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|        938788|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|          null|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|       1667114|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|       2093925|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|       2511249|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+--------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#renaming-columns-using-withcolumnrenamed",
    "href": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#renaming-columns-using-withcolumnrenamed",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "Renaming Columns using withColumnRenamed",
    "text": "Renaming Columns using withColumnRenamed\n\ndf.withColumnRenamed('fr','new_fiscal_revenue').show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+------------------+-----------+-------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|new_fiscal_revenue|        reg|     it|\n+----+--------+--------+-------+----+----+------+----+----+----+------------------+-----------+-------+\nnull|province|    null|   null|null|null|  null|null|null|null|              null|        reg|   null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|           1128873| East China| 631930|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|           1356287| East China| 657860|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|           1518236| East China| 889463|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|           1646891| East China|1227364|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|           1601508| East China|1499110|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|           1672445| East China|2165189|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|           1677840| East China|2404936|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|           1896479| East China|2815820|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|              null| East China|3422176|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|              null| East China|3874846|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|           3434548| East China|5167300|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|           4468640| East China|7040099|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null|            634562|North China| 508135|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null|            634562|North China| 569283|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null|            938788|North China| 695528|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|              null|North China| 944047|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|           1667114|North China| 757990|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|           2093925|North China|1194728|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|           2511249|North China|1078754|\n+----+--------+--------+-------+----+----+------+----+----+----+------------------+-----------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#new-columns-by-transforming-extant-columns-using-withcolumn",
    "href": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#new-columns-by-transforming-extant-columns-using-withcolumn",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "New Columns by Transforming extant Columns using withColumn",
    "text": "New Columns by Transforming extant Columns using withColumn\n\ndf.withColumn('double_fiscal_revenue',df['fr']*2).show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+---------------------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|double_fiscal_revenue|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+---------------------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|                 null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|              2257746|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|              2712574|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|              3036472|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|              3293782|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|              3203016|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|              3344890|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|              3355680|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|              3792958|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|                 null|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|                 null|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|              6869096|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|              8937280|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|              1269124|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|              1269124|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|              1877576|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|                 null|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|              3334228|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|              4187850|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|              5022498|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+---------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.withColumn('add_fiscal_revenue',df['fr']+1).show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+------------------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|add_fiscal_revenue|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+------------------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|              null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|           1128874|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|           1356288|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|           1518237|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|           1646892|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|           1601509|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|           1672446|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|           1677841|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|           1896480|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|              null|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|              null|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|           3434549|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|           4468641|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|            634563|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|            634563|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|            938789|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|              null|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|           1667115|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|           2093926|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|           2511250|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.withColumn('half_fiscal_revenue',df['fr']/2).show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+-------------------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|half_fiscal_revenue|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+-------------------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|               null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|           564436.5|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|           678143.5|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|           759118.0|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|           823445.5|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|           800754.0|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|           836222.5|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|           838920.0|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|           948239.5|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|               null|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|               null|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|          1717274.0|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|          2234320.0|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|           317281.0|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|           317281.0|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|           469394.0|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|               null|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|           833557.0|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|          1046962.5|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|          1255624.5|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+-------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.withColumn('half_fr',df['fr']/2)\n\n\nOut[86]: DataFrame[_c0: int, province: string, specific: int, general: int, year: int, gdp: int, fdi: int, rnr: int, rr: int, i: int, fr: int, reg: string, it: int, half_fr: double]"
  },
  {
    "objectID": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#spark-sql-for-sql-functionality-using-createorreplacetempview",
    "href": "posts/2020-08-21-rdds and schemas and data types with pyspark.html#spark-sql-for-sql-functionality-using-createorreplacetempview",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "Spark SQL for SQL functionality using createOrReplaceTempView",
    "text": "Spark SQL for SQL functionality using createOrReplaceTempView\n\ndf.createOrReplaceTempView(\"economic_data\")\n\n\n\n\n\n\nsql_results = spark.sql(\"SELECT * FROM economic_data\")\n\n\n\n\n\n\nsql_results\n\n\nOut[89]: DataFrame[_c0: int, province: string, specific: int, general: int, year: int, gdp: int, fdi: int, rnr: int, rr: int, i: int, fr: int, reg: string, it: int]\n\n\n\nsql_results.show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\nspark.sql(\"SELECT * FROM economic_data WHERE fr=634562\").show()\n\n\n+---+--------+--------+-------+----+----+------+----+----+----+------+-----------+------+\n_c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|    fr|        reg|    it|\n+---+--------+--------+-------+----+----+------+----+----+----+------+-----------+------+\n 12| Beijing|    null|   null|1996|null|155290|null|null|null|634562|North China|508135|\n 13| Beijing|    null|   null|1997|null|159286|null|null|null|634562|North China|569283|\n+---+--------+--------+-------+----+----+------+----+----+----+------+-----------+------+\n\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-08-16-pandas-stats-fiscal.html",
    "href": "posts/2020-08-16-pandas-stats-fiscal.html",
    "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
    "section": "",
    "text": "# Import required packages\n\nimport numpy as np\nimport pandas as pd\n\n%matplotlib inline\nimport seaborn as sns\n\nimport statsmodels.formula.api as smf\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.lines import Line2D\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')"
  },
  {
    "objectID": "posts/2020-08-16-pandas-stats-fiscal.html#read-required-datasets",
    "href": "posts/2020-08-16-pandas-stats-fiscal.html#read-required-datasets",
    "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
    "section": "Read required datasets",
    "text": "Read required datasets\n\ndf = pd.read_csv('ttb_county_clean.csv')\ndf1 = pd.read_csv('df_panel_fix.csv')"
  },
  {
    "objectID": "posts/2020-08-16-pandas-stats-fiscal.html#figure-1",
    "href": "posts/2020-08-16-pandas-stats-fiscal.html#figure-1",
    "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
    "section": "Figure 1",
    "text": "Figure 1\n\ndf.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n    s=df[\"specific\"]/100, label=\"Specific Purpose Transfers\", figsize=(12,8),\n    c=\"nightlights\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n    sharex=False)\n#save_fig(\"cn-spt-county-heat\")\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f0a4e16b4e0>"
  },
  {
    "objectID": "posts/2020-08-16-pandas-stats-fiscal.html#panel-regression-framework-with-year-and-province-fixed-effects",
    "href": "posts/2020-08-16-pandas-stats-fiscal.html#panel-regression-framework-with-year-and-province-fixed-effects",
    "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
    "section": "Panel regression framework with year and province fixed effects",
    "text": "Panel regression framework with year and province fixed effects\n\nlin_reg = smf.ols('np.log(specific) ~ np.log(gdp) + np.log(fdi) + i + rnr + rr + C(province) + C(year)', data=df1).fit()\n\n\n#lin_reg.summary()"
  },
  {
    "objectID": "posts/2020-08-16-pandas-stats-fiscal.html#figure-2",
    "href": "posts/2020-08-16-pandas-stats-fiscal.html#figure-2",
    "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
    "section": "Figure 2",
    "text": "Figure 2\n\ncoef_df = pd.read_csv('coef.csv')\n\nfig, ax = plt.subplots(figsize=(16, 10))\ncoef_df.plot(x='varname', y='coef', kind='bar', \n             ax=ax, color='none', \n             yerr='err', legend=False)\nax.set_ylabel('Specific Purpose Transfers (ln)')\nax.set_xlabel('Independant Variables')\nax.scatter(x=pd.np.arange(coef_df.shape[0]), \n           marker='s', s=120, \n           y=coef_df['coef'], color='black')\nax.axhline(y=0, linestyle='--', color='blue', linewidth=4)\nax.xaxis.set_ticks_position('none')\n\n_ = ax.set_xticklabels(['GDP', 'FDI', 'Incumbent', 'Non Relevant Rival', 'Relevant Rival'], \n                       rotation=0, fontsize=20)\n\nfs = 16\nax.annotate('Controls', xy=(0.2, -0.2), xytext=(0.2, -0.3), \n            xycoords='axes fraction', \n            textcoords='axes fraction', \n            fontsize=fs, ha='center', va='bottom',\n            bbox=dict(boxstyle='square', fc='white', ec='blue'),\n            arrowprops=dict(arrowstyle='-[, widthB=5.5, lengthB=1.2', lw=2.0, color='blue'))\n\n_ = ax.annotate('Connections', xy=(0.7, -0.2), xytext=(0.7, -0.3), \n                 xycoords='axes fraction', \n                 textcoords='axes fraction', \n                 fontsize=fs, ha='center', va='bottom',\n                 bbox=dict(boxstyle='square', fc='white', ec='red'),\n                 arrowprops=dict(arrowstyle='-[, widthB=10.5, lengthB=1.2', lw=2.0, color='red'))\n\n#save_fig(\"i-coef-plot\")"
  },
  {
    "objectID": "posts/2020-08-16-pandas-stats-fiscal.html#figure-3",
    "href": "posts/2020-08-16-pandas-stats-fiscal.html#figure-3",
    "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
    "section": "Figure 3",
    "text": "Figure 3\n\nimport numpy as np\nfrom plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot,iplot\nprint (__version__) # requires version >= 1.9.0\n\n\n#Always run this the command before at the start of notebook\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n\n\ntrace1 = go.Bar(\n    x=[1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003,\n       2004, 2005, 2006, 2007],\n    y=[188870900000.0, 185182900000.0, 237697500000.0, 347187900000.0, 296716700000.0,\n       397833100000.0, 440204800000.0, 514254300000.0, 686016600000.0, 677746300000.0, 940057900000.0,\n       1881304000000],\n    name='All Other Province Leaders',\n    marker=dict(\n        color='rgb(55, 83, 109)'\n    )\n)\ntrace2 = go.Bar(\n    x=[1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003,\n       2004, 2005, 2006, 2007],\n    y=[260376000000.0, 264934700000.0, 367350200000.0, 463861200000.0, 199068500000.0, 216582600000.0,\n  298631800000.0, 409759300000.0, 830363200000.0, 878158000000.0, 1143745000000.0, 2125891000000.0],\n    name='Incumbent Connected Province Leaders',\n    marker=dict(\n        color='rgb(26, 118, 255)'\n    )\n)\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title='Specific Purpose Transfers',\n    xaxis=dict(\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='RMB',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    legend=dict(\n        x=0,\n        y=1.0,\n        bgcolor='rgba(255, 255, 255, 0)',\n        bordercolor='rgba(255, 255, 255, 0)'\n    ),\n    barmode='group',\n    bargap=0.15,\n    bargroupgap=0.1\n)\n\nfig = go.Figure(data=data, layout=layout)\n\n#iplot(fig, filename='style-bar')\niplot(fig, image='png',filename='spt-i-bar')\n\n4.1.1"
  },
  {
    "objectID": "posts/2020-09-02-nlp_ngram.html",
    "href": "posts/2020-09-02-nlp_ngram.html",
    "title": "NLP ngrams With Python",
    "section": "",
    "text": "import pandas as pd\ndf=pd.read_csv('../../processed_data/nf_complete.csv')"
  },
  {
    "objectID": "posts/2020-09-02-nlp_ngram.html#pre-processing-text",
    "href": "posts/2020-09-02-nlp_ngram.html#pre-processing-text",
    "title": "NLP ngrams With Python",
    "section": "Pre-processing text",
    "text": "Pre-processing text\n\ndef preprocessor(text):\n    text = re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n        ' '.join(emoticons).replace('-', '')\n    return text"
  },
  {
    "objectID": "posts/2020-09-02-nlp_ngram.html#find-total-word-count",
    "href": "posts/2020-09-02-nlp_ngram.html#find-total-word-count",
    "title": "NLP ngrams With Python",
    "section": "Find Total Word Count",
    "text": "Find Total Word Count\n\ntext = \" \".join(review for review in df.abstract)\nprint (\"There are {} words in the combination of all abstracts.\".format(len(text)))\n\nThere are 272025 words in the combination of all abstracts.\n\n\n\nfrom urllib.request import urlopen\nfrom random import randint\n\ndef wordListSum(wordList):\n    sum = 0\n    for word, value in wordList.items():\n        sum += value\n    return sum\n\ndef retrieveRandomWord(wordList):\n    randIndex = randint(1, wordListSum(wordList))\n    for word, value in wordList.items():\n        randIndex -= value\n        if randIndex <= 0:\n            return word\n\ndef buildWordDict(text):\n    # Remove newlines and quotes\n    text = text.replace('\\n', ' ');\n    text = text.replace('\"', '');\n\n    # Make sure punctuation marks are treated as their own \"words,\"\n    # so that they will be included in the Markov chain\n    punctuation = [',','.',';',':']\n    for symbol in punctuation:\n        text = text.replace(symbol, ' {} '.format(symbol));\n\n    words = text.split(' ')\n    # Filter out empty words\n    words = [word for word in words if word != '']\n\n    wordDict = {}\n    for i in range(1, len(words)):\n        if words[i-1] not in wordDict:\n                # Create a new dictionary for this word\n            wordDict[words[i-1]] = {}\n        if words[i] not in wordDict[words[i-1]]:\n            wordDict[words[i-1]][words[i]] = 0\n        wordDict[words[i-1]][words[i]] += 1\n    return wordDict\n\nwordDict = buildWordDict(text)\n\n#Generate a Markov chain of length 100\nlength = 100\nchain = ['Vietnam']\nfor i in range(0, length):\n    newWord = retrieveRandomWord(wordDict[chain[-1]])\n    chain.append(newWord)\n\nprint(' '.join(chain))\n\nVietnam (DRV) hampered the end it? This paper at all) and have on a viable combat jet aircraft into a corps composed overwhelmingly of radical visions of the argument in Iraqi Kurdistan , few reasons : the war experience . The group identified and how elite cues , the emphasis on the ongoing betrayal of 1971-79 under which I argue that expand our knowledge of biological weapons which it . This pushes against Axis material support for any single institutional prerogatives . My work fills an opposition organization at the generalizability of the Cold War strategy to escalate . ” and\n\n\n\ndef getFirstSentenceContaining(ngram, text):\n    #print(ngram)\n    sentences = text.upper().split(\". \")\n    for sentence in sentences: \n        if ngram in sentence:\n            return sentence+'\\n'\n    return \"\"\n\n\nprint(getFirstSentenceContaining('I', text))\n\nCIVIL-MILITARY RELATIONS ARE FREQUENTLY STUDIED AS IF THEY OPERATE ON TWO DISTINCT LEVELS OF ANALYSIS\n\n\n\n\n#text\n\n\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport re\nimport string\nfrom collections import Counter\n\ndef cleanSentence(sentence):\n    sentence = sentence.split(' ')\n    sentence = [word.strip(string.punctuation+string.whitespace) for word in sentence]\n    sentence = [word for word in sentence if len(word) > 1 or (word.lower() == 'a' or word.lower() == 'i')]\n    return sentence\n\ndef cleanInput(content):\n    content = content.upper()\n    content = re.sub('\\n', ' ', content)\n    content = bytes(content, 'UTF-8')\n    content = content.decode('ascii', 'ignore')\n    sentences = content.split('. ')\n    return [cleanSentence(sentence) for sentence in sentences]\n\ndef getNgramsFromSentence(content, n):\n    output = []\n    for i in range(len(content)-n+1):\n        output.append(content[i:i+n])\n    return output\n\ndef getNgrams(content, n):\n    content = cleanInput(content)\n    ngrams = Counter()\n    ngrams_list = []\n    for sentence in content:\n        newNgrams = [' '.join(ngram) for ngram in getNgramsFromSentence(sentence, n)]\n        ngrams_list.extend(newNgrams)\n        ngrams.update(newNgrams)\n    return(ngrams)\n\n\ncontent = str(text)\n\nngrams = getNgrams(content, 3)\n#print(ngrams)\n\n\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport re\nimport string\nfrom collections import Counter\n\ndef isCommon(ngram):\n    commonWords = ['THE', 'BE', 'AND', 'OF', 'A', 'IN', 'TO', 'HAVE', 'IT', 'I', 'THAT', 'FOR', 'YOU', 'HE', 'WITH', 'ON', 'DO', 'SAY', 'THIS', 'THEY', 'IS', 'AN', 'AT', 'BUT', 'WE', 'HIS', 'FROM', 'THAT', 'NOT', 'BY', 'SHE', 'OR', 'AS', 'WHAT', 'GO', 'THEIR', 'CAN', 'WHO', 'GET', 'IF', 'WOULD', 'HER', 'ALL', 'MY', 'MAKE', 'ABOUT', 'KNOW', 'WILL', 'AS', 'UP', 'ONE', 'TIME', 'HAS', 'BEEN', 'THERE', 'YEAR', 'SO', 'THINK', 'WHEN', 'WHICH', 'THEM', 'SOME', 'ME', 'PEOPLE', 'TAKE', 'OUT', 'INTO', 'JUST', 'SEE', 'HIM', 'YOUR', 'COME', 'COULD', 'NOW', 'THAN', 'LIKE', 'OTHER', 'HOW', 'THEN', 'ITS', 'OUR', 'TWO', 'MORE', 'THESE', 'WANT', 'WAY', 'LOOK', 'FIRST', 'ALSO', 'NEW', 'BECAUSE', 'DAY', 'MORE', 'USE', 'NO', 'MAN', 'FIND', 'HERE', 'THING', 'GIVE', 'MANY', 'WELL']\n    for word in ngram:\n        if word in commonWords:\n            return True\n    return False\n\ndef getNgramsFromSentence(content, n):\n    output = []\n    for i in range(len(content)-n+1):\n        if not isCommon(content[i:i+n]):\n            output.append(content[i:i+n])\n    return output\n\nngrams = getNgrams(content, 3)\n#print(ngrams)\n\n\ndef getFirstSentenceContaining(ngram, content):\n    #print(ngram)\n    sentences = content.upper().split(\". \")\n    for sentence in sentences: \n        if ngram in sentence:\n            return sentence+'\\n'\n    return \"\"\n\n\nprint(getFirstSentenceContaining('SINO-JAPANESE WAR 1894-1895', content))\nprint(getFirstSentenceContaining('2ND VIETNAM WAR', content))\nprint(getFirstSentenceContaining('COLD WAR ARMY', content))\nprint(getFirstSentenceContaining('WORLD WAR II', content))\nprint(getFirstSentenceContaining('ARMS CONTROL AGREEMENTS', content))\n\n\n IN THE INTERNATIONAL SITUATION, THE GERMANS PROVIDED SUBSTANTIAL ADVANCES TO TECHNOLOGICAL DEVELOPMENT IN THE IMMEDIATE POST-WAR PERIOD.\n   THE HISTORIOGRAPHY ON THE 2ND VIETNAM WAR HAS FOCUSED MOSTLY ON THE AMERICAN SIDE, WHILE THE ‘OTHER SIDE,’ ESPECIALLY FOR THE EARLY VIETNAM WAR, 1964-1966, HAS NOT ATTRACTED MUCH ATTENTION\n\nCOLD WAR ARMY DURING THE PERIOD 1949 AND 1953 BY EXAMINING HOW SENIOR ARMY LEADERS WERE ABLE TO FUNDAMENTALLY BROADEN THE INSTITUTION’S INTELLECTUAL AND HISTORICAL FRAMEWORK OF “PREPAREDNESS” TO DESIGN A BLUEPRINT FOR A NEW TYPE OF GROUND FORCE THAT WOULD BE MORE ADEPT TO MEET THE CHALLENGES OF THE NEW NATURE OF WAR IMPOSED BY THE COLD WAR\n\n I ARGUE THAT A NORM PROTECTING STATES’ TERRITORIAL SOVEREIGNTY IS ONLY ENTRENCHED AFTER WORLD WAR II, ALTHOUGH IT CAN BE TRACED AT LEAST AS FAR BACK AS THE FOUNDING OF THE LEAGUE OF NATIONS\n\n IN EACH CASE I USE RIGOROUS ANALYSIS ON ORIGINAL DATA TO EXPLAIN THE WHY, WHEN, AND HOW OF THEIR DECISIONS ON THE BOMB, AS WELL AS OF THEIR DECISIONS ON RELATED ISSUES SUCH AS WHETHER TO BUILD UP NUCLEAR TECHNOLOGY, TO SEEK NUCLEAR SECURITY GUARANTEES, AND TO SIGN INTERNATIONAL NUCLEAR ARMS CONTROL AGREEMENTS.\n         THE OVERALL APPROACH INTRODUCED HERE HAS WIDE POTENTIAL APPLICABILITY\n\n\n\n\nfrom urllib.request import urlopen\nfrom random import randint\n\ndef wordListSum(wordList):\n    sum = 0\n    for word, value in wordList.items():\n        sum += value\n    return sum\n\ndef retrieveRandomWord(wordList):\n    randIndex = randint(1, wordListSum(wordList))\n    for word, value in wordList.items():\n        randIndex -= value\n        if randIndex <= 0:\n            return word\n\ndef buildWordDict(text):\n    # Remove newlines and quotes\n    text = text.replace('\\n', ' ');\n    text = text.replace('\"', '');\n\n    # Make sure punctuation marks are treated as their own \"words,\"\n    # so that they will be included in the Markov chain\n    punctuation = [',','.',';',':']\n    for symbol in punctuation:\n        text = text.replace(symbol, ' {} '.format(symbol));\n\n    words = text.split(' ')\n    # Filter out empty words\n    words = [word for word in words if word != '']\n\n    wordDict = {}\n    for i in range(1, len(words)):\n        if words[i-1] not in wordDict:\n                # Create a new dictionary for this word\n            wordDict[words[i-1]] = {}\n        if words[i] not in wordDict[words[i-1]]:\n            wordDict[words[i-1]][words[i]] = 0\n        wordDict[words[i-1]][words[i]] += 1\n    return wordDict\n\nwordDict = buildWordDict(text)\n\n#Generate a Markov chain of length 100\nlength = 100\nchain = ['I']\nfor i in range(0, length):\n    newWord = retrieveRandomWord(wordDict[chain[-1]])\n    chain.append(newWord)\n\n#print(' '.join(chain))\n\n\nimport re\n\ndef getNgrams(content, n):\n    content = re.sub('\\n|[[\\d+\\]]', ' ', content)\n    content = bytes(content, 'UTF-8')\n    content = content.decode('ascii', 'ignore')\n    content = content.split(' ')\n    content = [word for word in content if word != '']\n    output = []\n    for i in range(len(content)-n+1):\n        output.append(content[i:i+n])\n    return output\n\n\nfrom collections import Counter\n\ndef getNgrams(content, n):\n    content = cleanInput(content)\n    ngrams = Counter()\n    ngrams_list = []\n    for sentence in content:\n        newNgrams = [' '.join(ngram) for ngram in getNgramsFromSentence(sentence, n)]\n        ngrams_list.extend(newNgrams)\n        ngrams.update(newNgrams)\n    return(ngrams)\n\n\n#print(getNgrams(content, 2))\n\n\ndef isCommon(ngram):\n    commonWords = ['THE', 'BE', 'AND', 'OF', 'A', 'IN', 'TO', 'HAVE', 'IT', 'I', 'THAT', 'FOR', 'YOU', 'HE', 'WITH', 'ON', 'DO', 'SAY', 'THIS', 'THEY', 'IS', 'AN', 'AT', 'BUT', 'WE', 'HIS', 'FROM', 'THAT', 'NOT', 'BY', 'SHE', 'OR', 'AS', 'WHAT', 'GO', 'THEIR', 'CAN', 'WHO', 'GET', 'IF', 'WOULD', 'HER', 'ALL', 'MY', 'MAKE', 'ABOUT', 'KNOW', 'WILL', 'AS', 'UP', 'ONE', 'TIME', 'HAS', 'BEEN', 'THERE', 'YEAR', 'SO', 'THINK', 'WHEN', 'WHICH', 'THEM', 'SOME', 'ME', 'PEOPLE', 'TAKE', 'OUT', 'INTO', 'JUST', 'SEE', 'HIM', 'YOUR', 'COME', 'COULD', 'NOW', 'THAN', 'LIKE', 'OTHER', 'HOW', 'THEN', 'ITS', 'OUR', 'TWO', 'MORE', 'THESE', 'WANT', 'WAY', 'LOOK', 'FIRST', 'ALSO', 'NEW', 'BECAUSE', 'DAY', 'MORE', 'USE', 'NO', 'MAN', 'FIND', 'HERE', 'THING', 'GIVE', 'MANY', 'WELL']\n    for word in ngram:\n        if word in commonWords:\n            return True\n    return False\n\ndef getNgramsFromSentence(text, n):\n    output = []\n    for i in range(len(text)-n+1):\n        if not isCommon(text[i:i+n]):\n            output.append(text[i:i+n])\n    return output\n\nngrams = getNgrams(text, 3)\n#print(ngrams)"
  },
  {
    "objectID": "posts/2020-08-18-pyspark-nas.html",
    "href": "posts/2020-08-18-pyspark-nas.html",
    "title": "Handling Missing Data with Pyspark",
    "section": "",
    "text": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import countDistinct, avg,stddev\n\n# Load data from a CSV\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").option(\"inferSchema\", True).option(\"header\", True).load(file_location)\ndisplay(df.take(5))\n\n\n_c0provincespecificgeneralyeargdpfdirnrrrifrregit0Anhui147002.0null19962093.3506610.00.00.01128873East China6319301Anhui151981.0null19972347.32434430.00.00.01356287East China6578602Anhui174930.0null19982542.96276730.00.00.01518236East China8894633Anhui285324.0null19992712.3426131nullnullnull1646891East China12273644Anhui195580.032100.020002902.09318470.00.00.01601508East China1499110"
  },
  {
    "objectID": "posts/2020-08-18-pyspark-nas.html#dropping-columns-without-non-null-values",
    "href": "posts/2020-08-18-pyspark-nas.html#dropping-columns-without-non-null-values",
    "title": "Handling Missing Data with Pyspark",
    "section": "Dropping Columns without non-null values",
    "text": "Dropping Columns without non-null values\n\n# Has to have at least 2 NON-null values\ndf.na.drop(thresh=2).show()\n\n\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000| 0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525| 0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-18-pyspark-nas.html#dropping-any-row-that-contains-missing-data",
    "href": "posts/2020-08-18-pyspark-nas.html#dropping-any-row-that-contains-missing-data",
    "title": "Handling Missing Data with Pyspark",
    "section": "Dropping any row that contains missing data",
    "text": "Dropping any row that contains missing data\n\ndf.na.drop().show()\n\n\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\n_c0| province| specific|  general|year|    gdp|   fdi|                rnr|         rr|          i|      fr|            reg|     it|\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\n  4|    Anhui| 195580.0|  32100.0|2000|2902.09| 31847|                0.0|        0.0|        0.0| 1601508|     East China|1499110|\n  6|    Anhui| 434149.0|  66529.0|2002|3519.72| 38375|                0.0|        0.0|        0.0| 1677840|     East China|2404936|\n  7|    Anhui| 619201.0|  52108.0|2003|3923.11| 36720|                0.0|        0.0|        0.0| 1896479|     East China|2815820|\n 10|    Anhui|1457872.0| 279052.0|2006| 6112.5|139354|                0.0|        0.0|0.324324324| 3434548|     East China|5167300|\n 11|    Anhui|2213991.0| 178705.0|2007|7360.92|299892|                0.0|        0.0|0.324324324| 4468640|     East China|7040099|\n 16|  Beijing| 281769.0| 188633.0|2000|3161.66|168368|                0.0|        0.0|       0.53| 1667114|    North China| 757990|\n 18|  Beijing| 558569.0| 280277.0|2002| 4315.0|172464|                0.0|        0.0|       0.53| 2511249|    North China|1078754|\n 19|  Beijing| 642581.0| 269596.0|2003|5007.21|219126|                0.0|0.794871795|        0.0| 2823366|    North China|1426600|\n 22|  Beijing|1315102.0| 405966.0|2006|8117.78|455191|                0.0|0.794871795|        0.0| 4830392|    North China|1782317|\n 23|  Beijing| 752279.0|1023453.0|2007|9846.81|506572|                0.0|0.794871795|        0.0|14926380|    North China|1962192|\n 34|Chongqing| 833423.0| 150000.0|2006|3907.23| 69595|0.09677419400000001|        0.0|        0.0| 1762409|Southwest China|3124234|\n 35|Chongqing|1791403.0| 447900.0|2007|4676.13|108534|0.09677419400000001|        0.0|        0.0| 4427000|Southwest China|3923569|\n 40|   Fujian| 142650.0|  71807.0|2000|3764.54|343191|                0.0|        0.0|        0.0| 2110577|     East China| 819028|\n 42|   Fujian| 137190.0|  59263.0|2002|4467.55|383837|               0.22|        0.3|        0.0| 2373047|     East China|1184990|\n 43|   Fujian| 148812.0|  68142.0|2003|4983.67|259903|                0.0|        0.0|        0.3| 2648861|     East China|1364980|\n 46|   Fujian| 397517.0| 149549.0|2006|7583.85|322047|                0.4|        0.0|        0.0| 4830320|     East China|2135224|\n 47|   Fujian| 753552.0| 317700.0|2007|9248.53|406058|                0.4|        0.0|        0.0| 6994577|     East China|2649011|\n 52|    Gansu| 223984.0|  58533.0|2000|1052.88|  6235|                0.0|0.153846154|        0.0|  505196|Northwest China|1258100|\n 54|    Gansu| 337894.0| 129791.0|2002|1232.03|  6121|                0.0|       0.13|        0.0|  597159|Northwest China|1898911|\n 58|    Gansu| 833430.0| 516342.0|2006|2277.35|  2954|                0.0|        0.0|0.128205128|  924080|Northwest China|3847158|\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.na.drop(subset=[\"general\"]).show()\n\n\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\n_c0| province| specific|  general|year|    gdp|   fdi|                rnr|         rr|          i|      fr|            reg|     it|\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\n  4|    Anhui| 195580.0|  32100.0|2000|2902.09| 31847|                0.0|        0.0|        0.0| 1601508|     East China|1499110|\n  6|    Anhui| 434149.0|  66529.0|2002|3519.72| 38375|                0.0|        0.0|        0.0| 1677840|     East China|2404936|\n  7|    Anhui| 619201.0|  52108.0|2003|3923.11| 36720|                0.0|        0.0|        0.0| 1896479|     East China|2815820|\n  8|    Anhui| 898441.0| 349699.0|2004| 4759.3| 54669|                0.0|        0.0|        0.0|    null|     East China|3422176|\n 10|    Anhui|1457872.0| 279052.0|2006| 6112.5|139354|                0.0|        0.0|0.324324324| 3434548|     East China|5167300|\n 11|    Anhui|2213991.0| 178705.0|2007|7360.92|299892|                0.0|        0.0|0.324324324| 4468640|     East China|7040099|\n 16|  Beijing| 281769.0| 188633.0|2000|3161.66|168368|                0.0|        0.0|       0.53| 1667114|    North China| 757990|\n 18|  Beijing| 558569.0| 280277.0|2002| 4315.0|172464|                0.0|        0.0|       0.53| 2511249|    North China|1078754|\n 19|  Beijing| 642581.0| 269596.0|2003|5007.21|219126|                0.0|0.794871795|        0.0| 2823366|    North China|1426600|\n 20|  Beijing|1009936.0| 309025.0|2004|6033.21|308354|                0.0|0.794871795|        0.0|    null|    North China|1644601|\n 22|  Beijing|1315102.0| 405966.0|2006|8117.78|455191|                0.0|0.794871795|        0.0| 4830392|    North China|1782317|\n 23|  Beijing| 752279.0|1023453.0|2007|9846.81|506572|                0.0|0.794871795|        0.0|14926380|    North China|1962192|\n 30|Chongqing| 311770.0|  41907.0|2002|2232.86| 19576|               null|       null|       null|  762806|Southwest China|1906968|\n 31|Chongqing| 335715.0|  18700.0|2003|2555.72| 26083|               null|       null|       null|  929935|Southwest China|1778125|\n 32|Chongqing| 568835.0|  97500.0|2004|3034.58| 40508|               null|       null|       null|    null|Southwest China|2197948|\n 34|Chongqing| 833423.0| 150000.0|2006|3907.23| 69595|0.09677419400000001|        0.0|        0.0| 1762409|Southwest China|3124234|\n 35|Chongqing|1791403.0| 447900.0|2007|4676.13|108534|0.09677419400000001|        0.0|        0.0| 4427000|Southwest China|3923569|\n 40|   Fujian| 142650.0|  71807.0|2000|3764.54|343191|                0.0|        0.0|        0.0| 2110577|     East China| 819028|\n 42|   Fujian| 137190.0|  59263.0|2002|4467.55|383837|               0.22|        0.3|        0.0| 2373047|     East China|1184990|\n 43|   Fujian| 148812.0|  68142.0|2003|4983.67|259903|                0.0|        0.0|        0.3| 2648861|     East China|1364980|\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.na.drop(how='any').show()\n\n\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\n_c0| province| specific|  general|year|    gdp|   fdi|                rnr|         rr|          i|      fr|            reg|     it|\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\n  4|    Anhui| 195580.0|  32100.0|2000|2902.09| 31847|                0.0|        0.0|        0.0| 1601508|     East China|1499110|\n  6|    Anhui| 434149.0|  66529.0|2002|3519.72| 38375|                0.0|        0.0|        0.0| 1677840|     East China|2404936|\n  7|    Anhui| 619201.0|  52108.0|2003|3923.11| 36720|                0.0|        0.0|        0.0| 1896479|     East China|2815820|\n 10|    Anhui|1457872.0| 279052.0|2006| 6112.5|139354|                0.0|        0.0|0.324324324| 3434548|     East China|5167300|\n 11|    Anhui|2213991.0| 178705.0|2007|7360.92|299892|                0.0|        0.0|0.324324324| 4468640|     East China|7040099|\n 16|  Beijing| 281769.0| 188633.0|2000|3161.66|168368|                0.0|        0.0|       0.53| 1667114|    North China| 757990|\n 18|  Beijing| 558569.0| 280277.0|2002| 4315.0|172464|                0.0|        0.0|       0.53| 2511249|    North China|1078754|\n 19|  Beijing| 642581.0| 269596.0|2003|5007.21|219126|                0.0|0.794871795|        0.0| 2823366|    North China|1426600|\n 22|  Beijing|1315102.0| 405966.0|2006|8117.78|455191|                0.0|0.794871795|        0.0| 4830392|    North China|1782317|\n 23|  Beijing| 752279.0|1023453.0|2007|9846.81|506572|                0.0|0.794871795|        0.0|14926380|    North China|1962192|\n 34|Chongqing| 833423.0| 150000.0|2006|3907.23| 69595|0.09677419400000001|        0.0|        0.0| 1762409|Southwest China|3124234|\n 35|Chongqing|1791403.0| 447900.0|2007|4676.13|108534|0.09677419400000001|        0.0|        0.0| 4427000|Southwest China|3923569|\n 40|   Fujian| 142650.0|  71807.0|2000|3764.54|343191|                0.0|        0.0|        0.0| 2110577|     East China| 819028|\n 42|   Fujian| 137190.0|  59263.0|2002|4467.55|383837|               0.22|        0.3|        0.0| 2373047|     East China|1184990|\n 43|   Fujian| 148812.0|  68142.0|2003|4983.67|259903|                0.0|        0.0|        0.3| 2648861|     East China|1364980|\n 46|   Fujian| 397517.0| 149549.0|2006|7583.85|322047|                0.4|        0.0|        0.0| 4830320|     East China|2135224|\n 47|   Fujian| 753552.0| 317700.0|2007|9248.53|406058|                0.4|        0.0|        0.0| 6994577|     East China|2649011|\n 52|    Gansu| 223984.0|  58533.0|2000|1052.88|  6235|                0.0|0.153846154|        0.0|  505196|Northwest China|1258100|\n 54|    Gansu| 337894.0| 129791.0|2002|1232.03|  6121|                0.0|       0.13|        0.0|  597159|Northwest China|1898911|\n 58|    Gansu| 833430.0| 516342.0|2006|2277.35|  2954|                0.0|        0.0|0.128205128|  924080|Northwest China|3847158|\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.na.drop(how='all').show()\n\n\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000| 0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525| 0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-18-pyspark-nas.html#imputation-of-null-values",
    "href": "posts/2020-08-18-pyspark-nas.html#imputation-of-null-values",
    "title": "Handling Missing Data with Pyspark",
    "section": "Imputation of Null Values",
    "text": "Imputation of Null Values\n\ndf.na.fill('example').show()\n\n\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|example| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000| 0.0|        0.0|0.324324324|example| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525| 0.0|        0.0|       0.53|example|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\nImputation of 0\n\ndf.na.fill(0).show()\n\n\n+---+--------+---------+--------+----+-------+------+---+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi|rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+---+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|     0.0|1996| 2093.3| 50661|0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|     0.0|1997|2347.32| 43443|0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|     0.0|1998|2542.96| 27673|0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|     0.0|1999|2712.34| 26131|0.0|        0.0|        0.0|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847|0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|     0.0|2001|3246.71| 33672|0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375|0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720|0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669|0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|     0.0|2005|5350.17| 69000|0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354|0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892|0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|     0.0|1996| 1789.2|155290|0.0|        0.0|        0.0| 634562|North China| 508135|\n 13| Beijing| 165957.0|     0.0|1997|2077.09|159286|0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|     0.0|1998|2377.18|216800|0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|     0.0|1999|2678.82|197525|0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368|0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|     0.0|2001|3707.96|176818|0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464|0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126|0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+---+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.na.fill('example',subset=['fr']).show()\n\n\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|example| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000| 0.0|        0.0|0.324324324|example| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525| 0.0|        0.0|       0.53|example|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.na.fill(0,subset=['general']).show()\n\n\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|     0.0|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|     0.0|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|     0.0|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|     0.0|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|     0.0|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|     0.0|2005|5350.17| 69000| 0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|     0.0|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|     0.0|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|     0.0|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|     0.0|1999|2678.82|197525| 0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|     0.0|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\n\nImputation of the Mean\n\nfrom pyspark.sql.functions import mean\nmean_val = df.select(mean(df['general'])).collect()\n\n\n\n\n\n\nmean_val[0][0]\n\n\nOut[19]: 309127.53846153844\n\n\n\nmean_gen = mean_val[0][0]\n\n\n\n\n\n\ndf.na.fill(mean_gen,[\"general\"]).show()\n\n\n+---+--------+---------+------------------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific|           general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+------------------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|309127.53846153844|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|309127.53846153844|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|309127.53846153844|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|309127.53846153844|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0|           32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|309127.53846153844|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0|           66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0|           52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|          349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|309127.53846153844|2005|5350.17| 69000| 0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|          279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|          178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|309127.53846153844|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|309127.53846153844|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|309127.53846153844|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|309127.53846153844|1999|2678.82|197525| 0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|          188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|309127.53846153844|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|          280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|          269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+------------------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.na.fill(df.select(mean(df['general'])).collect()[0][0],['general']).show()\n\n\n+---+--------+---------+------------------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific|           general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+------------------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|309127.53846153844|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|309127.53846153844|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|309127.53846153844|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|309127.53846153844|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0|           32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|309127.53846153844|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0|           66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0|           52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|          349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|309127.53846153844|2005|5350.17| 69000| 0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|          279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|          178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|309127.53846153844|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|309127.53846153844|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|309127.53846153844|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|309127.53846153844|1999|2678.82|197525| 0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|          188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|309127.53846153844|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|          280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|          269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+------------------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-09-30-stockmarketportfolioanaylsis_snp.html",
    "href": "posts/2020-09-30-stockmarketportfolioanaylsis_snp.html",
    "title": "Stock Market and Portfolio Anaylsis of the S&P 500 with pandas and quandl",
    "section": "",
    "text": "import pandas as pd\nimport quandl\n\n\nstart = pd.to_datetime('2010-01-01')\nend = pd.to_datetime('today')\n\n\nSP500 = quandl.get(\"MULTPL/SP500_REAL_PRICE_MONTH\",start_date = start,end_date = end)\n\n\nSP500.head()\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-01-01\n      1123.58\n    \n    \n      2010-02-01\n      1089.16\n    \n    \n      2010-03-01\n      1152.05\n    \n    \n      2010-04-01\n      1197.32\n    \n    \n      2010-05-01\n      1125.06\n    \n  \n\n\n\n\n\nSP500.tail()\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2020-07-01\n      3207.62\n    \n    \n      2020-07-31\n      3271.12\n    \n    \n      2020-08-01\n      3391.71\n    \n    \n      2020-08-31\n      3500.31\n    \n    \n      2020-09-01\n      3526.65\n    \n  \n\n\n\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nSP500['Value'].plot(figsize = (12, 8))\nplt.title('Total S&P 500 from 2010 to 2020 Value')\n\nText(0.5, 1.0, 'Total S&P 500 from 2010 to 2020 Value')"
  },
  {
    "objectID": "posts/2020-10-01-stockmarketportfolioanaylsis_snp+.html",
    "href": "posts/2020-10-01-stockmarketportfolioanaylsis_snp+.html",
    "title": "Stock Market and Portfolio Anaylsis of the S&P 500 various metrics with pandas and quandl",
    "section": "",
    "text": "import pandas as pd\nimport quandl\n\n\nstart = pd.to_datetime('2010-01-01')\nend = pd.to_datetime('today')\n\n\nSP500 = quandl.get(\"MULTPL/SP500_REAL_PRICE_MONTH\",start_date = start,end_date = end)\n\n\nSP500 = quandl.get(\"MULTPL/SP500_DIV_YIELD_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-01-31\n      1.98\n    \n    \n      2010-02-28\n      2.03\n    \n    \n      2010-03-31\n      1.90\n    \n    \n      2010-04-30\n      1.83\n    \n    \n      2010-05-31\n      1.95\n    \n    \n      ...\n      ...\n    \n    \n      2020-07-01\n      1.91\n    \n    \n      2020-07-31\n      1.86\n    \n    \n      2020-08-31\n      1.76\n    \n    \n      2020-09-01\n      1.69\n    \n    \n      2020-09-30\n      1.69\n    \n  \n\n136 rows × 1 columns\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_PE_RATIO_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-01-01\n      20.70\n    \n    \n      2010-02-01\n      18.91\n    \n    \n      2010-03-01\n      18.91\n    \n    \n      2010-04-01\n      19.01\n    \n    \n      2010-05-01\n      17.30\n    \n    \n      ...\n      ...\n    \n    \n      2020-07-01\n      27.57\n    \n    \n      2020-07-31\n      28.12\n    \n    \n      2020-08-01\n      29.16\n    \n    \n      2020-08-31\n      30.09\n    \n    \n      2020-09-01\n      30.32\n    \n  \n\n138 rows × 1 columns\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_PE_RATIO_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-01-01\n      20.70\n    \n    \n      2010-02-01\n      18.91\n    \n    \n      2010-03-01\n      18.91\n    \n    \n      2010-04-01\n      19.01\n    \n    \n      2010-05-01\n      17.30\n    \n    \n      ...\n      ...\n    \n    \n      2020-07-01\n      27.57\n    \n    \n      2020-07-31\n      28.12\n    \n    \n      2020-08-01\n      29.16\n    \n    \n      2020-08-31\n      30.09\n    \n    \n      2020-09-01\n      30.32\n    \n  \n\n138 rows × 1 columns\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_EARNINGS_YIELD_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-01-01\n      4.83\n    \n    \n      2010-02-01\n      5.29\n    \n    \n      2010-03-01\n      5.29\n    \n    \n      2010-04-01\n      5.26\n    \n    \n      2010-05-01\n      5.78\n    \n    \n      ...\n      ...\n    \n    \n      2020-07-01\n      3.63\n    \n    \n      2020-07-31\n      3.56\n    \n    \n      2020-08-01\n      3.43\n    \n    \n      2020-08-31\n      3.32\n    \n    \n      2020-09-01\n      3.30\n    \n  \n\n138 rows × 1 columns\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_INFLADJ_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-01-01\n      1343.51\n    \n    \n      2010-02-01\n      1302.03\n    \n    \n      2010-03-01\n      1371.58\n    \n    \n      2010-04-01\n      1423.00\n    \n    \n      2010-05-01\n      1336.08\n    \n    \n      ...\n      ...\n    \n    \n      2020-07-01\n      3207.62\n    \n    \n      2020-07-31\n      3271.12\n    \n    \n      2020-08-01\n      3391.71\n    \n    \n      2020-08-31\n      3500.31\n    \n    \n      2020-09-01\n      3526.65\n    \n  \n\n138 rows × 1 columns\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_DIV_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-01-31\n      26.59\n    \n    \n      2010-02-28\n      26.38\n    \n    \n      2010-03-31\n      26.08\n    \n    \n      2010-04-30\n      26.09\n    \n    \n      2010-05-31\n      26.12\n    \n    \n      ...\n      ...\n    \n    \n      2020-02-29\n      59.23\n    \n    \n      2020-03-31\n      59.81\n    \n    \n      2020-04-30\n      60.25\n    \n    \n      2020-05-31\n      60.28\n    \n    \n      2020-06-30\n      59.99\n    \n  \n\n126 rows × 1 columns\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_DIV_YEAR\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-12-31\n      26.87\n    \n    \n      2011-12-31\n      30.34\n    \n    \n      2012-12-31\n      35.26\n    \n    \n      2013-12-31\n      38.90\n    \n    \n      2014-12-31\n      43.52\n    \n    \n      2015-12-31\n      47.53\n    \n    \n      2016-12-31\n      49.05\n    \n    \n      2017-12-31\n      51.43\n    \n    \n      2018-12-31\n      55.43\n    \n    \n      2019-03-31\n      55.35\n    \n    \n      2019-06-30\n      56.17\n    \n    \n      2019-09-30\n      57.32\n    \n    \n      2019-12-31\n      58.72\n    \n    \n      2020-03-31\n      59.18\n    \n    \n      2020-06-30\n      59.99\n    \n  \n\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_DIV_GROWTH_YEAR\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-12-31\n      1.45\n    \n    \n      2011-12-31\n      16.26\n    \n    \n      2012-12-31\n      18.25\n    \n    \n      2013-12-31\n      11.99\n    \n    \n      2014-12-31\n      12.72\n    \n    \n      2015-12-31\n      10.00\n    \n    \n      2016-12-31\n      5.33\n    \n    \n      2017-12-31\n      7.07\n    \n    \n      2018-12-31\n      9.84\n    \n    \n      2019-03-31\n      9.87\n    \n    \n      2019-06-30\n      9.98\n    \n    \n      2019-09-30\n      9.32\n    \n    \n      2019-12-31\n      8.36\n    \n    \n      2020-03-31\n      8.45\n    \n    \n      2020-06-30\n      6.43\n    \n  \n\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_DIV_GROWTH_QUARTER\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-03-31\n      -19.63\n    \n    \n      2010-06-30\n      -13.90\n    \n    \n      2010-09-30\n      -6.48\n    \n    \n      2010-12-31\n      1.45\n    \n    \n      2011-03-31\n      6.97\n    \n    \n      2011-06-30\n      10.46\n    \n    \n      2011-09-30\n      12.65\n    \n    \n      2011-12-31\n      16.26\n    \n    \n      2012-03-31\n      16.74\n    \n    \n      2012-06-30\n      16.35\n    \n    \n      2012-09-30\n      17.51\n    \n    \n      2012-12-31\n      18.25\n    \n    \n      2013-03-31\n      17.40\n    \n    \n      2013-06-30\n      17.47\n    \n    \n      2013-09-30\n      16.27\n    \n    \n      2013-12-31\n      11.99\n    \n    \n      2014-03-31\n      12.82\n    \n    \n      2014-06-30\n      12.37\n    \n    \n      2014-09-30\n      11.89\n    \n    \n      2014-12-31\n      12.72\n    \n    \n      2015-03-31\n      12.64\n    \n    \n      2015-06-30\n      11.67\n    \n    \n      2015-09-30\n      10.43\n    \n    \n      2015-12-31\n      10.00\n    \n    \n      2016-03-31\n      7.52\n    \n    \n      2016-06-30\n      6.51\n    \n    \n      2016-09-30\n      5.92\n    \n    \n      2016-12-31\n      5.33\n    \n    \n      2017-03-31\n      5.71\n    \n    \n      2017-06-30\n      6.21\n    \n    \n      2017-09-30\n      6.99\n    \n    \n      2017-12-31\n      7.07\n    \n    \n      2018-03-31\n      7.81\n    \n    \n      2018-06-30\n      7.99\n    \n    \n      2018-09-30\n      8.65\n    \n    \n      2018-12-31\n      9.84\n    \n    \n      2019-03-31\n      9.87\n    \n    \n      2019-06-30\n      9.98\n    \n    \n      2019-09-30\n      9.32\n    \n    \n      2019-12-31\n      8.36\n    \n    \n      2020-03-31\n      8.45\n    \n    \n      2020-06-30\n      6.43\n    \n  \n\n\n\n\n\nSP500.head()\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2010-01-01\n      1123.58\n    \n    \n      2010-02-01\n      1089.16\n    \n    \n      2010-03-01\n      1152.05\n    \n    \n      2010-04-01\n      1197.32\n    \n    \n      2010-05-01\n      1125.06\n    \n  \n\n\n\n\n\nSP500.tail()\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2020-07-01\n      3207.62\n    \n    \n      2020-07-31\n      3271.12\n    \n    \n      2020-08-01\n      3391.71\n    \n    \n      2020-08-31\n      3500.31\n    \n    \n      2020-09-01\n      3526.65\n    \n  \n\n\n\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nSP500['Value'].plot(figsize = (12, 8))\nplt.title('Total S&P 500 from 2010 to 2020 Value')\n\nText(0.5, 1.0, 'Total S&P 500 from 2010 to 2020 Value')"
  },
  {
    "objectID": "posts/2020-10-04-stockmarketportfolioanaylsis_snp_pandas_datareader_sqlite-copy1.html",
    "href": "posts/2020-10-04-stockmarketportfolioanaylsis_snp_pandas_datareader_sqlite-copy1.html",
    "title": "Stock Market and Portfolio Anaylsis Tech Stocks and the S&P 500 in 2020 with pandas_datareader and writing to at sqlite database",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport pandas_datareader.data as web\nimport datetime\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n# # start = datetime.datetime(2016, 1, 1)\n# # end = datetime.datetime(2017, 5, 17)\n\n# start = datetime.datetime(2010, 1, 1)\n# end = datetime.datetime(2020, 1, 1)\n\n\nstart = pd.to_datetime('2020-01-01')\nend = pd.to_datetime('today')\n\n\nFXAIX_stock = web.DataReader('FXAIX', 'yahoo', start, end)\nFXAIX_stock.head()\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nSNOW_stock = web.DataReader('SNOW', 'yahoo', start, end)\nSNOW_stock.head()\n\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Open')\n\nMSFT_stock['Open'].plot(label='Microsoft')\nZOOM_stock['Open'].plot(label='Zoom')\nSNOW_stock['Open'].plot(label='Snowflake')\nFXAIX_stock['Open'].plot(label='SNP_500')\nplt.legend()\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Volume')\n\nMSFT_stock['Volume'].plot(label='Microsoft')\nZOOM_stock['Volume'].plot(label='Zoom')\nSNOW_stock['Volume'].plot(label='Snowflake')\nFXAIX_stock['Volume'].plot(label='SNP_500')\n\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7fae549b8ba8>\n\n\n\n\n\n\n\n\n\nFXAIX_stock = web.DataReader('FXAIX', 'yahoo', start, end)\nFXAIX_stock.head()\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nSNOW_stock = web.DataReader('SNOW', 'yahoo', start, end)\nSNOW_stock.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-09-16\n      319.0\n      231.110001\n      245.000000\n      253.929993\n      36099700\n      253.929993\n    \n    \n      2020-09-17\n      241.5\n      215.240005\n      230.759995\n      227.539993\n      11907500\n      227.539993\n    \n    \n      2020-09-18\n      249.0\n      218.589996\n      235.000000\n      240.000000\n      7475400\n      240.000000\n    \n    \n      2020-09-21\n      241.5\n      218.600006\n      230.000000\n      228.850006\n      5524900\n      228.850006\n    \n    \n      2020-09-22\n      239.0\n      225.149994\n      238.500000\n      235.160004\n      3889100\n      235.160004\n    \n  \n\n\n\n\n\nstocks = pd.concat([MSFT_stock['Open'], ZOOM_stock['Open'], SNOW_stock['Open'], FXAIX_stock['Open']],\n                   axis = 1)\n\n\nstocks\n\n\n\n\n\n  \n    \n      \n      Open\n      Open\n      Open\n      Open\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-02\n      158.779999\n      68.800003\n      NaN\n      112.980003\n    \n    \n      2020-01-03\n      158.320007\n      67.620003\n      NaN\n      112.190002\n    \n    \n      2020-01-06\n      157.080002\n      66.629997\n      NaN\n      112.589996\n    \n    \n      2020-01-07\n      159.320007\n      70.290001\n      NaN\n      112.290001\n    \n    \n      2020-01-08\n      158.929993\n      71.809998\n      NaN\n      112.839996\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2020-09-28\n      210.880005\n      502.410004\n      235.929993\n      116.650002\n    \n    \n      2020-09-29\n      209.350006\n      488.130005\n      255.000000\n      116.099998\n    \n    \n      2020-09-30\n      207.729996\n      464.209991\n      261.500000\n      117.070000\n    \n    \n      2020-10-01\n      213.490005\n      477.000000\n      255.250000\n      117.699997\n    \n    \n      2020-10-02\n      208.000000\n      485.005005\n      232.440002\n      116.120003\n    \n  \n\n191 rows × 4 columns\n\n\n\n\nstocks.columns = ['MSFT_stock','ZOOM_stock','SNOW_stock','FXAIX_stock']\n\n\nstocks\n\n\n\n\n\n  \n    \n      \n      MSFT_stock\n      ZOOM_stock\n      SNOW_stock\n      FXAIX_stock\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-02\n      158.779999\n      68.800003\n      NaN\n      112.980003\n    \n    \n      2020-01-03\n      158.320007\n      67.620003\n      NaN\n      112.190002\n    \n    \n      2020-01-06\n      157.080002\n      66.629997\n      NaN\n      112.589996\n    \n    \n      2020-01-07\n      159.320007\n      70.290001\n      NaN\n      112.290001\n    \n    \n      2020-01-08\n      158.929993\n      71.809998\n      NaN\n      112.839996\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2020-09-28\n      210.880005\n      502.410004\n      235.929993\n      116.650002\n    \n    \n      2020-09-29\n      209.350006\n      488.130005\n      255.000000\n      116.099998\n    \n    \n      2020-09-30\n      207.729996\n      464.209991\n      261.500000\n      117.070000\n    \n    \n      2020-10-01\n      213.490005\n      477.000000\n      255.250000\n      117.699997\n    \n    \n      2020-10-02\n      208.000000\n      485.005005\n      232.440002\n      116.120003\n    \n  \n\n191 rows × 4 columns\n\n\n\n\nmean_daily_ret = stocks.pct_change(1).mean()\nmean_daily_ret\n\nMSFT_stock     0.001751\nZOOM_stock     0.011973\nSNOW_stock    -0.002546\nFXAIX_stock    0.000440\ndtype: float64\n\n\n\nstocks.pct_change(1).corr()\n\n\n\n\n\n  \n    \n      \n      MSFT_stock\n      ZOOM_stock\n      SNOW_stock\n      FXAIX_stock\n    \n  \n  \n    \n      MSFT_stock\n      1.000000\n      0.209041\n      0.661827\n      0.382807\n    \n    \n      ZOOM_stock\n      0.209041\n      1.000000\n      0.095052\n      0.127526\n    \n    \n      SNOW_stock\n      0.661827\n      0.095052\n      1.000000\n      0.292117\n    \n    \n      FXAIX_stock\n      0.382807\n      0.127526\n      0.292117\n      1.000000\n    \n  \n\n\n\n\n\nstock_normed = stocks/stocks.iloc[0]\nstock_normed.plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7fae54a74a90>\n\n\n\n\n\n\nstock_daily_ret = stocks.pct_change(1)\nstock_daily_ret.head()\n\n\n\n\n\n  \n    \n      \n      MSFT_stock\n      ZOOM_stock\n      SNOW_stock\n      FXAIX_stock\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-02\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2020-01-03\n      -0.002897\n      -0.017151\n      NaN\n      -0.006992\n    \n    \n      2020-01-06\n      -0.007832\n      -0.014641\n      NaN\n      0.003565\n    \n    \n      2020-01-07\n      0.014260\n      0.054930\n      NaN\n      -0.002664\n    \n    \n      2020-01-08\n      -0.002448\n      0.021625\n      NaN\n      0.004898\n    \n  \n\n\n\n\n\nlog_ret = np.log(stocks / stocks.shift(1))\nlog_ret.head()\n\n\n\n\n\n  \n    \n      \n      MSFT_stock\n      ZOOM_stock\n      SNOW_stock\n      FXAIX_stock\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-02\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2020-01-03\n      -0.002901\n      -0.017300\n      NaN\n      -0.007017\n    \n    \n      2020-01-06\n      -0.007863\n      -0.014749\n      NaN\n      0.003559\n    \n    \n      2020-01-07\n      0.014160\n      0.053475\n      NaN\n      -0.002668\n    \n    \n      2020-01-08\n      -0.002451\n      0.021394\n      NaN\n      0.004886\n    \n  \n\n\n\n\n\nlog_ret.hist(bins = 100,\n             figsize = (12, 6));\nplt.tight_layout()\n\n\n\n\n\nlog_ret.describe().transpose()\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n  \n  \n    \n      MSFT_stock\n      190.0\n      0.001421\n      0.025752\n      -0.087821\n      -0.012115\n      0.004000\n      0.016980\n      0.081248\n    \n    \n      ZOOM_stock\n      190.0\n      0.010279\n      0.056461\n      -0.142569\n      -0.017014\n      0.011119\n      0.035968\n      0.368600\n    \n    \n      SNOW_stock\n      12.0\n      -0.004386\n      0.063753\n      -0.131433\n      -0.033113\n      0.019477\n      0.034320\n      0.077728\n    \n    \n      FXAIX_stock\n      190.0\n      0.000144\n      0.024461\n      -0.127150\n      -0.007774\n      0.002806\n      0.010082\n      0.089894\n    \n  \n\n\n\n\n\nlog_ret.mean() * 252\n\nMSFT_stock     0.358130\nZOOM_stock     2.590236\nSNOW_stock    -1.105148\nFXAIX_stock    0.036359\ndtype: float64\n\n\n\nlog_ret.cov()\n\n\n\n\n\n  \n    \n      \n      MSFT_stock\n      ZOOM_stock\n      SNOW_stock\n      FXAIX_stock\n    \n  \n  \n    \n      MSFT_stock\n      0.000663\n      0.000323\n      0.001291\n      0.000245\n    \n    \n      ZOOM_stock\n      0.000323\n      0.003188\n      0.000290\n      0.000184\n    \n    \n      SNOW_stock\n      0.001291\n      0.000290\n      0.004064\n      0.000231\n    \n    \n      FXAIX_stock\n      0.000245\n      0.000184\n      0.000231\n      0.000598\n    \n  \n\n\n\n\n\n# Set seed (optional)\nnp.random.seed(101)\n\n# Stock Columns\nprint('Stocks')\nprint(stocks.columns)\nprint('\\n')\n\n# Create Random Weights\nprint('Creating Random Weights')\nweights = np.array(np.random.random(4))\nprint(weights)\nprint('\\n')\n\n# Rebalance Weights\nprint('Rebalance to sum to 1.0')\nweights = weights / np.sum(weights)\nprint(weights)\nprint('\\n')\n\n# Expected Return\nprint('Expected Portfolio Return')\nexp_ret = np.sum(log_ret.mean() * weights) *252\nprint(exp_ret)\nprint('\\n')\n\n# Expected Variance\nprint('Expected Volatility')\nexp_vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\nprint(exp_vol)\nprint('\\n')\n\n# Sharpe Ratio\nSR = exp_ret/exp_vol\nprint('Sharpe Ratio')\nprint(SR)\n\nStocks\nIndex(['MSFT_stock', 'ZOOM_stock', 'SNOW_stock', 'FXAIX_stock'], dtype='object')\n\n\nCreating Random Weights\n[0.51639863 0.57066759 0.02847423 0.17152166]\n\n\nRebalance to sum to 1.0\n[0.40122278 0.44338777 0.02212343 0.13326603]\n\n\nExpected Portfolio Return\n1.272564336318203\n\n\nExpected Volatility\n0.4864366288684257\n\n\nSharpe Ratio\n2.6160948020680697\n\n\n\nnum_ports = 15000\n\nall_weights = np.zeros((num_ports, len(stocks.columns)))\nret_arr = np.zeros(num_ports)\nvol_arr = np.zeros(num_ports)\nsharpe_arr = np.zeros(num_ports)\n\nfor ind in range(num_ports):\n\n    # Create Random Weights\n    weights = np.array(np.random.random(4))\n\n    # Rebalance Weights\n    weights = weights / np.sum(weights)\n    \n    # Save Weights\n    all_weights[ind,:] = weights\n\n    # Expected Return\n    ret_arr[ind] = np.sum((log_ret.mean() * weights) *252)\n\n    # Expected Variance\n    vol_arr[ind] = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n\n    # Sharpe Ratio\n    sharpe_arr[ind] = ret_arr[ind] / vol_arr[ind]\n\n\nsharpe_arr.max()\n\n2.8667995807841824\n\n\n\nsharpe_arr.argmax()\n\n5483\n\n\n\nall_weights[10619,:]\n\narray([5.06395348e-01, 4.67772019e-04, 2.64242193e-01, 2.28894687e-01])\n\n\n\nmax_sr_ret = ret_arr[1419]\nmax_sr_vol = vol_arr[1419]\n\n\nplt.figure(figsize = (12, 8))\nplt.scatter(vol_arr,\n            ret_arr,\n            c = sharpe_arr,\n            cmap = 'plasma')\nplt.colorbar(label = 'Sharpe Ratio')\nplt.xlabel('Volatility')\nplt.ylabel('Return')\n\n# Add red dot for max SR\nplt.scatter(max_sr_vol,\n            max_sr_ret,\n            c = 'red',\n            s = 50,\n            edgecolors = 'black')\n\n<matplotlib.collections.PathCollection at 0x7fae54366048>\n\n\n\n\n\n\ndef get_ret_vol_sr(weights):\n    \"\"\"\n    Takes in weights, returns array or return,volatility, sharpe ratio\n    \"\"\"\n    weights = np.array(weights)\n    ret = np.sum(log_ret.mean() * weights) * 252\n    vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n    sr = ret/vol\n    return np.array([ret, vol, sr])\n\nfrom scipy.optimize import minimize\nimport numpy as np\n\ndef neg_sharpe(weights):\n    return  get_ret_vol_sr(weights)[2] * -1\n\n# Contraints\ndef check_sum(weights):\n    '''\n    Returns 0 if sum of weights is 1.0\n    '''\n    return np.sum(weights) - 1\n\n# By convention of minimize function it should be a function that returns zero for conditions\ncons = ({'type' : 'eq', 'fun': check_sum})\n\n# 0-1 bounds for each weight\nbounds = ((0, 1), (0, 1), (0, 1), (0, 1))\n\n# Initial Guess (equal distribution)\ninit_guess = [0.25, 0.25, 0.25, 0.25]\n\n# Sequential Least Squares \nopt_results = minimize(neg_sharpe,\n                       init_guess,\n                       method = 'SLSQP',\n                       bounds = bounds,\n                       constraints = cons)\n\nopt_results\n\n     fun: -2.8998675936504807\n     jac: array([-3.57061625e-04,  6.75618649e-05,  1.98669076e+00,  1.90789163e-01])\n message: 'Optimization terminated successfully.'\n    nfev: 42\n     nit: 7\n    njev: 7\n  status: 0\n success: True\n       x: array([1.59222977e-01, 8.40777023e-01, 7.68699340e-16, 0.00000000e+00])\n\n\n\nopt_results.x\n\nget_ret_vol_sr(opt_results.x)\n\narray([2.23483308, 0.77066728, 2.89986759])\n\n\n\nfrontier_y = np.linspace(0, 0.3, 100)\n\n\ndef minimize_volatility(weights):\n    return  get_ret_vol_sr(weights)[1] \n\nfrontier_volatility = []\n\nfor possible_return in frontier_y:\n    # function for return\n    cons = ({'type':'eq','fun': check_sum},\n            {'type':'eq','fun': lambda w: get_ret_vol_sr(w)[0] - possible_return})\n    \n    result = minimize(minimize_volatility,\n                      init_guess,\n                      method = 'SLSQP',\n                      bounds = bounds,\n                      constraints = cons)\n    \n    frontier_volatility.append(result['fun'])\n\n\nplt.figure(figsize = (12, 8))\nplt.scatter(vol_arr,\n            ret_arr,\n            c = sharpe_arr,\n            cmap = 'plasma')\nplt.colorbar(label = 'Sharpe Ratio')\nplt.xlabel('Volatility')\nplt.ylabel('Return')\n\n\n\n# Add frontier line\nplt.plot(frontier_volatility,\n         frontier_y,\n         'g--',\n         linewidth = 3)\n\n\n\n\n\nstocks['FXAIX_stock'].plot(figsize = (12, 8))\nplt.title('Total S&P 500 in 2020 Value')\n\nText(0.5, 1.0, 'Total S&P 500 in 2020 Value')\n\n\n\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nstocks\n\n\n\n\n\n  \n    \n      \n      MSFT_stock\n      ZOOM_stock\n      SNOW_stock\n      FXAIX_stock\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2020-01-02\n      158.779999\n      68.800003\n      NaN\n      112.980003\n    \n    \n      2020-01-03\n      158.320007\n      67.620003\n      NaN\n      112.190002\n    \n    \n      2020-01-06\n      157.080002\n      66.629997\n      NaN\n      112.589996\n    \n    \n      2020-01-07\n      159.320007\n      70.290001\n      NaN\n      112.290001\n    \n    \n      2020-01-08\n      158.929993\n      71.809998\n      NaN\n      112.839996\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2020-09-28\n      210.880005\n      502.410004\n      235.929993\n      116.650002\n    \n    \n      2020-09-29\n      209.350006\n      488.130005\n      255.000000\n      116.099998\n    \n    \n      2020-09-30\n      207.729996\n      464.209991\n      261.500000\n      117.070000\n    \n    \n      2020-10-01\n      213.490005\n      477.000000\n      255.250000\n      117.699997\n    \n    \n      2020-10-02\n      208.000000\n      485.005005\n      232.440002\n      116.120003\n    \n  \n\n191 rows × 4 columns\n\n\n\n\nengine = db.create_engine('sqlite:///stocks.sqlite')\n\n\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nstocks.to_sql('stocks', con=engine, if_exists='append', index=True)\n\n\nengine.execute(\"SELECT * FROM stocks LIMIT 10\").fetchall()\n\n[(158.77999877929688, 68.80000305175781, None, 112.9800033569336),\n (158.32000732421875, 67.62000274658203, None, 112.19000244140625),\n (157.0800018310547, 66.62999725341797, None, 112.58999633789062),\n (159.32000732421875, 70.29000091552734, None, 112.29000091552734),\n (158.92999267578125, 71.80999755859375, None, 112.83999633789062),\n (161.83999633789062, 73.98999786376953, None, 113.62000274658203),\n (162.82000732421875, 73.08000183105469, None, 113.30000305175781),\n (161.75999450683594, 73.88999938964844, None, 114.08999633789062),\n (163.38999938964844, 74.31999969482422, None, 113.93000030517578),\n (162.6199951171875, 73.27999877929688, None, 114.13999938964844)]\n\n\n\nengine.execute(\"SELECT FXAIX_stock FROM stocks LIMIT 10\").fetchall()\n\n[(112.9800033569336,),\n (112.19000244140625,),\n (112.58999633789062,),\n (112.29000091552734,),\n (112.83999633789062,),\n (113.62000274658203,),\n (113.30000305175781,),\n (114.08999633789062,),\n (113.93000030517578,),\n (114.13999938964844,)]\n\n\n\n# df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n# df\n\n# df.to_sql('users', con=engine)\n\n# engine.execute(\"SELECT * FROM users\").fetchall()"
  },
  {
    "objectID": "posts/2020-08-17-pyspark-group-by.html",
    "href": "posts/2020-08-17-pyspark-group-by.html",
    "title": "Group By and Aggregation with Pyspark",
    "section": "",
    "text": "“Group By and Aggregation with Pyspark”"
  },
  {
    "objectID": "posts/2020-08-17-pyspark-group-by.html#read-csv-and-inferschema",
    "href": "posts/2020-08-17-pyspark-group-by.html#read-csv-and-inferschema",
    "title": "Group By and Aggregation with Pyspark",
    "section": "Read CSV and inferSchema",
    "text": "Read CSV and inferSchema\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import countDistinct, avg,stddev\n\n\n# Load data from a CSV\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").option(\"inferSchema\", True).option(\"header\", True).load(file_location)\ndisplay(df.take(5))\n\n\n_c0provincespecificgeneralyeargdpfdirnrrrifrregit0Anhui147002.0null19962093.3506610.00.00.01128873East China6319301Anhui151981.0null19972347.32434430.00.00.01356287East China6578602Anhui174930.0null19982542.96276730.00.00.01518236East China8894633Anhui285324.0null19992712.3426131nullnullnull1646891East China12273644Anhui195580.032100.020002902.09318470.00.00.01601508East China1499110\n\n\n\ndf.printSchema()\n\n\nroot\n-- _c0: integer (nullable = true)\n-- province: string (nullable = true)\n-- specific: double (nullable = true)\n-- general: double (nullable = true)\n-- year: integer (nullable = true)\n-- gdp: double (nullable = true)\n-- fdi: integer (nullable = true)\n-- rnr: double (nullable = true)\n-- rr: double (nullable = true)\n-- i: double (nullable = true)\n-- fr: string (nullable = true)\n-- reg: string (nullable = true)\n-- it: integer (nullable = true)"
  },
  {
    "objectID": "posts/2020-08-17-pyspark-group-by.html#using-groupby-for-averages-and-counts",
    "href": "posts/2020-08-17-pyspark-group-by.html#using-groupby-for-averages-and-counts",
    "title": "Group By and Aggregation with Pyspark",
    "section": "Using groupBy for Averages and Counts",
    "text": "Using groupBy for Averages and Counts\n\ndf.groupBy(\"province\")\n\n\nOut[8]: <pyspark.sql.group.GroupedData at 0x7f939a0aada0>\n\n\n\ndf.groupBy(\"province\").mean().show()\n\n\n+------------+--------+------------------+------------------+---------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\n    province|avg(_c0)|     avg(specific)|      avg(general)|avg(year)|          avg(gdp)|          avg(fdi)|            avg(rnr)|             avg(rr)|              avg(i)|           avg(it)|\n+------------+--------+------------------+------------------+---------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\n   Guangdong|    65.5|1123328.0833333333|          312308.0|   2001.5|15358.781666666668|        1194950.25|0.011261261250000001|                 0.0|                 0.0|        3099014.25|\n       Hunan|   161.5| 824676.9166666666| 480788.3333333333|   2001.5| 4765.891666666666|         132110.25|                 0.0| 0.07291666666666667|                 0.0|         3215128.5|\n      Shanxi|   281.5| 577540.4166666666|          351680.0|   2001.5| 2817.210833333333|38628.833333333336|                 0.0|                 0.0|                 0.0|1983718.3333333333|\n       Tibet|   317.5|189219.91666666666|165365.33333333334|   2001.5|170.42666666666665|            839.75| 0.03030303033333333| 0.15583333333333335| 0.20278090583333333|1174175.5833333333|\n       Hubei|   149.5|         595463.25|          391326.5|   2001.5| 4772.503333333333|         149713.25|         0.045045045| 0.11386386375000002| 0.06230392158333333|        2904659.75|\n     Tianjin|   305.5| 76884.16666666667|          126636.0|   2001.5|2528.6650000000004|250173.33333333334|                 0.0|                 0.0|                 0.0| 831028.4166666666|\n     Beijing|    17.5| 581440.8333333334|          412825.0|   2001.5| 4673.453333333333|257369.33333333334|                 0.0|  0.3613053613636364| 0.29545454545454547|1175965.4166666667|\nHeilongjiang|   125.5|1037878.1666666666| 315925.3333333333|   2001.5| 4041.241666666667| 82719.33333333333|                 0.0|                 0.0| 0.03931203927272728|3230451.1666666665|\n    Liaoning|   209.5|        1111002.75|185280.83333333334|   2001.5| 5231.135000000001| 285925.3333333333| 0.11469534044444446|                 0.0|                null|2628358.4166666665|\n       Henan|   137.5| 955407.4166666666|          673392.5|   2001.5| 7208.966666666667|           94426.0|                 0.0|                0.04| 0.08602150533333335|3671970.6666666665|\n       Anhui|     5.5| 643984.1666666666|159698.83333333334|   2001.5|3905.8700000000003| 70953.08333333333|                 0.0|                 0.0| 0.08845208836363637|2649674.4166666665|\n    Xinjiang|   329.5| 345334.3333333333|          412906.0|   2001.5|1828.8966666666665| 4433.083333333333|                 0.0|                 0.0|                 0.0|         2251012.0|\n      Fujian|    41.5|246144.16666666666|140619.33333333334|   2001.5|4864.0233333333335| 374466.4166666667|  0.1366666666666667|0.049999999999999996| 0.09999999999999999|        1274116.75|\n     Jiangxi|   185.5| 592906.3333333334| 458268.6666666667|   2001.5|         2460.7825|         103735.25|                 0.0|  0.1491841490909091|0.042727272727272725|        1760613.25|\n       Jilin|   197.5|         711132.25|          348186.0|   2001.5|2274.8541666666665|41226.583333333336|                 0.0|                 0.0|                 0.0|2136634.9166666665|\n   Chongqing|    29.5| 561854.1111111111|          151201.4|   2001.5|         2477.7125|41127.833333333336| 0.09677419400000001|                 0.0|                 0.0|1636146.4166666667|\n     Shaanxi|   245.5| 387167.1666666667|          386760.5|   2001.5| 2658.034166666667|50892.583333333336|0.002840909090909091|                 0.0| 0.07386363636363637|2474031.4166666665|\n     Sichuan|   293.5|         1194640.5| 707032.8333333334|   2001.5|           5377.79|62197.166666666664| 0.00818181818181818| 0.00818181818181818|                 0.2|4016479.5833333335|\n      Yunnan|   341.5| 802151.1666666666|          200426.0|   2001.5| 2604.054166666667|17048.333333333332|                 0.0|                 0.0|                 0.0|3165418.9166666665|\n       Gansu|    53.5| 498930.9166666667| 382092.6666666667|   2001.5|1397.8325000000002|            5295.5| 0.11111111120000002|         0.088974359| 0.13038461533333334|         2045347.0|\n+------------+--------+------------------+------------------+---------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.groupBy(\"reg\").mean().show()\n\n\n+-------------------+------------------+------------------+------------------+---------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\n                reg|          avg(_c0)|     avg(specific)|      avg(general)|avg(year)|          avg(gdp)|          avg(fdi)|            avg(rnr)|             avg(rr)|              avg(i)|           avg(it)|\n+-------------------+------------------+------------------+------------------+---------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\n    Southwest China|             214.3| 648086.8070175438|          327627.0|   2001.5|2410.3988333333336|25405.083333333332| 0.01764440930612245|0.053185448081632655| 0.13679739081632653|         2424971.4|\n         East China|183.78571428571428|517524.90476190473|230217.37142857144|   2001.5| 7126.732976190476|414659.03571428574| 0.08284508739240506| 0.05701117448101268| 0.09036240282278483|1949130.4761904762|\n    Northeast China|             177.5| 953337.7222222222|283130.72222222225|   2001.5| 3849.076944444444|         136623.75| 0.03686635942857143|                 0.0| 0.02275960168421053|2665148.1666666665|\n        North China|             179.5|506433.57446808513|334689.14285714284|   2001.5| 4239.038541666667|169600.58333333334|                 0.0| 0.15428824051724138| 0.11206896551724138|1733718.7291666667|\n    Northwest China|             216.7|324849.06666666665|293066.73333333334|   2001.5|1340.0261666666668|15111.133333333333|0.022847222240000003|0.033887245249999996|0.048179240615384616|        1703537.75|\nSouth Central China|             115.5| 690125.8333333334| 382414.8888888889|   2001.5| 5952.826944444445|281785.59722222225|0.014928879322033899| 0.07324349771186443| 0.06797753142372882|       2626299.875|\n+-------------------+------------------+------------------+------------------+---------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\n\n\n\n\n\n# Count\ndf.groupBy(\"reg\").count().show()\n\n\n+-------------------+-----+\n                reg|count|\n+-------------------+-----+\n    Southwest China|   60|\n         East China|   84|\n    Northeast China|   36|\n        North China|   48|\n    Northwest China|   60|\nSouth Central China|   72|\n+-------------------+-----+\n\n\n\n\n\n# Max\ndf.groupBy(\"reg\").max().show()\n\n\n+-------------------+--------+-------------+------------+---------+--------+--------+------------------+-----------+-------------------+--------+\n                reg|max(_c0)|max(specific)|max(general)|max(year)|max(gdp)|max(fdi)|          max(rnr)|    max(rr)|             max(i)| max(it)|\n+-------------------+--------+-------------+------------+---------+--------+--------+------------------+-----------+-------------------+--------+\n    Southwest China|     347|    3937966.0|   1725100.0|     2007|10562.39|  149322|       0.181818182|       0.84|               0.75|10384846|\n         East China|     359|    2213991.0|   1272600.0|     2007|25776.91| 1743140|       1.214285714|       0.53|                0.6| 7040099|\n    Northeast China|     215|    3847672.0|   1046700.0|     2007| 9304.52|  598554|       0.516129032|        0.0|0.21621621600000002| 7968319|\n        North China|     311|    2981235.0|   1023453.0|     2007|13607.32|  527776|               0.0|0.794871795|                0.6| 7537692|\n    Northwest China|     335|    2669238.0|   1197400.0|     2007| 5757.29|  119516|0.5555555560000001|        0.5|               1.05| 6308151|\nSouth Central China|     167|    3860764.0|   1737800.0|     2007|31777.01| 1712603|        0.27027027|     0.4375| 0.6176470589999999|10533312|\n+-------------------+--------+-------------+------------+---------+--------+--------+------------------+-----------+-------------------+--------+\n\n\n\n\n\n# Min\ndf.groupBy(\"reg\").min().show()\n\n\n+-------------------+--------+-------------+------------+---------+--------+--------+--------+-------+------+-------+\n                reg|min(_c0)|min(specific)|min(general)|min(year)|min(gdp)|min(fdi)|min(rnr)|min(rr)|min(i)|min(it)|\n+-------------------+--------+-------------+------------+---------+--------+--------+--------+-------+------+-------+\n    Southwest China|      24|      18829.0|     18700.0|     1996|   64.98|       2|     0.0|    0.0|   0.0| 176802|\n         East China|       0|       8964.0|         0.0|     1996| 1169.73|   22724|     0.0|    0.0|   0.0| 489132|\n    Northeast China|     120|      80595.0|     19360.0|     1996| 1137.23|   19059|     0.0|    0.0|   0.0| 625471|\n        North China|      12|      35084.0|     32119.0|     1996| 1121.93|   13802|     0.0|    0.0|   0.0| 303992|\n    Northwest China|      48|      32088.0|      2990.0|     1996|  184.17|     247|     0.0|    0.0|   0.0| 178668|\nSouth Central China|      60|      54462.0|         0.0|     1996|  389.68|   29579|     0.0|    0.0|   0.0| 147897|\n+-------------------+--------+-------------+------------+---------+--------+--------+--------+-------+------+-------+\n\n\n\n\n\n# Sum\ndf.groupBy(\"reg\").sum().show()\n\n\n+-------------------+--------+-------------+------------+---------+------------------+--------+------------------+------------------+-------------------+---------+\n                reg|sum(_c0)|sum(specific)|sum(general)|sum(year)|          sum(gdp)|sum(fdi)|          sum(rnr)|           sum(rr)|             sum(i)|  sum(it)|\n+-------------------+--------+-------------+------------+---------+------------------+--------+------------------+------------------+-------------------+---------+\n    Southwest China|   12858|  3.6940948E7|   9501183.0|   120090|144623.93000000002| 1524305|       0.864576056|       2.606086956|         6.70307215|145498284|\n         East China|   15438|  4.3472092E7|   8057608.0|   168126|         598645.57|34831359|       6.544761904| 4.503882784000002|  7.138629823000002|163726960|\n    Northeast China|    6390|  3.4320158E7|   5096353.0|    72054|         138566.77| 4918455|       1.032258064|               0.0|0.43243243200000003| 95945334|\n        North China|    8616|  2.3802378E7|   7028472.0|    96072|         203473.85| 8140828|               0.0|       4.474358975|               3.25| 83218499|\n    Northwest China|   13002|  1.9490944E7|   8792002.0|   120090|          80401.57|  906668|1.1423611120000001|1.7621367529999998|        2.505320512|102212265|\nSouth Central China|    8316|   4.968906E7| 1.3766936E7|   144108|428603.54000000004|20288563|        0.88080388| 4.321366365000001|  4.010674354000001|189093591|\n+-------------------+--------+-------------+------------+---------+------------------+--------+------------------+------------------+-------------------+---------+\n\n\n\n\n\n# Max it across everything\ndf.agg({'specific':'max'}).show()\n\n\n+-------------+\nmax(specific)|\n+-------------+\n    3937966.0|\n+-------------+\n\n\n\n\n\ngrouped = df.groupBy(\"reg\")\ngrouped.agg({\"it\":'max'}).show()\n\n\n+-------------------+--------+\n                reg| max(it)|\n+-------------------+--------+\n    Southwest China|10384846|\n         East China| 7040099|\n    Northeast China| 7968319|\n        North China| 7537692|\n    Northwest China| 6308151|\nSouth Central China|10533312|\n+-------------------+--------+\n\n\n\n\n\ndf.select(countDistinct(\"reg\")).show()\n\n\n+-------------------+\ncount(DISTINCT reg)|\n+-------------------+\n                  6|\n+-------------------+\n\n\n\n\n\ndf.select(countDistinct(\"reg\").alias(\"Distinct Region\")).show()\n\n\n+---------------+\nDistinct Region|\n+---------------+\n              6|\n+---------------+\n\n\n\n\n\ndf.select(avg('specific')).show()\n\n\n+-----------------+\n    avg(specific)|\n+-----------------+\n583470.7303370787|\n+-----------------+\n\n\n\n\n\ndf.select(stddev(\"specific\")).show()\n\n\n+---------------------+\nstddev_samp(specific)|\n+---------------------+\n    654055.3290782663|\n+---------------------+"
  },
  {
    "objectID": "posts/2020-08-17-pyspark-group-by.html#choosing-significant-digits-with-format_number",
    "href": "posts/2020-08-17-pyspark-group-by.html#choosing-significant-digits-with-format_number",
    "title": "Group By and Aggregation with Pyspark",
    "section": "Choosing Significant Digits with format_number",
    "text": "Choosing Significant Digits with format_number\n\nfrom pyspark.sql.functions import format_number\n\n\nspecific_std = df.select(stddev(\"specific\").alias('std'))\nspecific_std.show()\n\n\n+-----------------+\n              std|\n+-----------------+\n654055.3290782663|\n+-----------------+\n\n\n\n\n\nspecific_std.select(format_number('std',0)).show()\n\n\n+---------------------+\nformat_number(std, 0)|\n+---------------------+\n              654,055|\n+---------------------+"
  },
  {
    "objectID": "posts/2020-08-17-pyspark-group-by.html#using-orderby",
    "href": "posts/2020-08-17-pyspark-group-by.html#using-orderby",
    "title": "Group By and Aggregation with Pyspark",
    "section": "Using orderBy",
    "text": "Using orderBy\n\ndf.orderBy(\"specific\").show()\n\n\n+---+---------+--------+-------+----+--------+------+-----------+----+----+-------+---------------+-------+\n_c0| province|specific|general|year|     gdp|   fdi|        rnr|  rr|   i|     fr|            reg|     it|\n+---+---------+--------+-------+----+--------+------+-----------+----+----+-------+---------------+-------+\n 28|Chongqing|    null|   null|2000|  1791.0| 24436|       null|null|null|   null|Southwest China|1022148|\n109|    Hebei|    null|   null|1997| 3953.78|110064|       null|null|null|   null|    North China| 826734|\n 24|Chongqing|    null|   null|1996| 1315.12| 21878|       null|null|null|   null|Southwest China| 176802|\n 25|Chongqing|    null|   null|1997| 1509.75| 38675|       null|null|null|   null|Southwest China| 383402|\n268| Shanghai|  8964.0|   null|2000| 4771.17|316014|        0.0| 0.0|0.44|2224124|     East China|1212473|\n269| Shanghai|  9834.0|   null|2001| 5210.12|429159|        0.0| 0.0|0.44|2947285|     East China|1053917|\n312|    Tibet| 18829.0|   null|1996|   64.98|   679|0.181818182| 0.0| 0.0|  27801|Southwest China| 306114|\n270| Shanghai| 19985.0|   null|2002| 5741.03|427229|        0.0| 0.0|0.44|3380397|     East China|1572208|\n271| Shanghai| 23547.0|   null|2003| 6694.23|546849|        0.0|0.53| 0.0|4461153|     East China|2031496|\n313|    Tibet| 25185.0|   null|1997|   77.24|    63|0.181818182| 0.0| 0.0|  33787|Southwest China| 346368|\n273| Shanghai| 29943.0|   null|2005| 9247.66|685000|        0.0|0.53| 0.0|   null|     East China|2140461|\n272| Shanghai| 29943.0|   null|2004| 8072.83|654100|        0.0|0.53| 0.0|   null|     East China|2703643|\n216|  Ningxia| 32088.0|   null|1996|   202.9|  2826|       null|null|null|  90805|Northwest China| 178668|\n305|  Tianjin| 35084.0|   null|2001| 1919.09|213348|        0.0| 0.0| 0.0| 942763|    North China| 688810|\n228|  Qinghai| 37976.0|   null|1996|  184.17|   576|       null|null|null|  73260|Northwest China| 218361|\n302|  Tianjin| 39364.0|   null|1998|  1374.6|211361|       null|null|null| 540178|    North China| 361723|\n274| Shanghai| 42928.0|   null|2006|10572.24|710700|        0.0|0.53| 0.0|8175966|     East China|2239987|\n217|  Ningxia| 44267.0|   null|1997|  224.59|   671|       null|null|null| 102083|Northwest China| 195295|\n303|  Tianjin| 45463.0|   null|1999| 1500.95|176399|        0.0| 0.0| 0.0| 605662|    North China| 422522|\n314|    Tibet| 48197.0|   null|1998|    91.5|   481|        0.0|0.24| 0.0|   3810|Southwest China| 415547|\n+---+---------+--------+-------+----+--------+------+-----------+----+----+-------+---------------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.orderBy(df[\"specific\"].desc()).show()\n\n\n+---+------------+---------+---------+----+--------+-------+-----------+-----------+-------------------+--------+-------------------+--------+\n_c0|    province| specific|  general|year|     gdp|    fdi|        rnr|         rr|                  i|      fr|                reg|      it|\n+---+------------+---------+---------+----+--------+-------+-----------+-----------+-------------------+--------+-------------------+--------+\n299|     Sichuan|3937966.0|1725100.0|2007|10562.39| 149322|       null|       null|               null| 8508606|    Southwest China|10384846|\n143|       Henan|3860764.0|1737800.0|2007|15012.46| 306162|        0.0|        0.0|                0.0| 8620804|South Central China|10533312|\n131|Heilongjiang|3847672.0|1046700.0|2007|  7104.0| 208508|        0.0|        0.0|0.21621621600000002| 4404689|    Northeast China| 7968319|\n215|    Liaoning|3396397.0| 599600.0|2007| 9304.52| 598554|0.516129032|        0.0|               null|10826948|    Northeast China| 5502192|\n167|       Hunan|3156087.0|1329200.0|2007|  9439.6| 327051|        0.0|     0.4375|                0.0| 6065508|South Central China| 8340692|\n119|       Hebei|2981235.0| 694400.0|2007|13607.32| 241621|        0.0|        0.5|                0.0| 7891198|        North China| 7537692|\n155|       Hubei|2922784.0|1263500.0|2007|  9333.4| 276622|        0.0|0.111111111|                0.0| 5903552|South Central China| 7666512|\n251|     Shaanxi|2669238.0|1081000.0|2007| 5757.29| 119516|    0.03125|        0.0|             0.8125| 4752398|    Northwest China| 6308151|\n203|       Jilin|2663667.0|1016400.0|2007| 4275.12|  76064|        0.0|        0.0|                0.0| 3206892|    Northeast China| 4607955|\n347|      Yunnan|2482173.0| 564400.0|2007| 4772.52|  39453|        0.0|        0.0|                0.0| 4867146|    Southwest China| 6832541|\n298|     Sichuan|2225220.0|1187958.0|2006| 8690.24| 120819|        0.0|        0.0|               0.55| 4247403|    Southwest China| 7646885|\n 11|       Anhui|2213991.0| 178705.0|2007| 7360.92| 299892|        0.0|        0.0|        0.324324324| 4468640|         East China| 7040099|\n287|      Shanxi|2189020.0| 661200.0|2007| 6024.45| 134283|       null|       null|               null| 5978870|        North China| 5070166|\n263|    Shandong|2121243.0| 581800.0|2007|25776.91|1101159|        0.0|        0.0|                0.0|16753980|         East China| 6357869|\n191|     Jiangxi|2045869.0|1272600.0|2007| 4820.53| 280657|        0.0| 0.41025641|                0.0| 3898510|         East China| 4229821|\n 83|     Guangxi|2022957.0|1214100.0|2007| 5823.41|  68396|0.205128205|        0.0|0.23076923100000002| 4188265|South Central China| 6185600|\n142|       Henan|2018158.0|1131615.0|2006|12362.79| 184526|        0.0|        0.0|                0.0| 6212824|South Central China| 7601825|\n 59|       Gansu|2010553.0|1039400.0|2007| 2703.98|  11802|       null|        0.0|               1.05| 1909107|    Northwest China| 5111059|\n 95|     Guizhou|1956261.0|1239200.0|2007| 2884.11|  12651|        0.0|        0.0| 0.7105263159999999| 2851375|    Southwest China| 5639838|\n214|    Liaoning|1947031.0| 179893.0|2006| 8047.26| 359000|0.516129032|        0.0|               null| 6530236|    Northeast China| 4605917|\n+---+------------+---------+---------+----+--------+-------+-----------+-----------+-------------------+--------+-------------------+--------+\nonly showing top 20 rows\n\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-10-02-databases_sqllite_sqlalchemy_27-copy1.html",
    "href": "posts/2020-10-02-databases_sqllite_sqlalchemy_27-copy1.html",
    "title": "Creating and accessing SQL databases with python using sqlalchemy and sqlite",
    "section": "",
    "text": "import sqlalchemy as db\nimport sqlite3\nimport pandas as pd\n\n\nfrom sqlalchemy import Column, Integer, String, ForeignKey, create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import relationship, backref, sessionmaker, joinedload\n\n# For this example we will use an in-memory sqlite DB.\n# Let's also configure it to echo everything it does to the screen.\nengine = create_engine('sqlite:///:memory:', echo=True)\n\n\n# The base class which our objects will be defined on.\nBase = declarative_base()\n\n# Our User object, mapped to the 'users' table\nclass User(Base):\n    __tablename__ = 'users'\n\n    # Every SQLAlchemy table should have a primary key named 'id'\n    id = Column(Integer, primary_key=True)\n\n    name = Column(String)\n    fullname = Column(String)\n    password = Column(String)\n\n    # Lets us print out a user object conveniently.\n    def __repr__(self):\n       return \"<User(name='%s', fullname='%s', password'%s')>\" % (\n                               self.name, self.fullname, self.password)\n\n\n# The Address object stores the addresses \n# of a user in the 'adressess' table.\nclass Address(Base):\n    __tablename__ = 'addresses'\n    id = Column(Integer, primary_key=True)\n    email_address = Column(String, nullable=False)\n\n    # Since we have a 1:n relationship, we need to store a foreign key \n    # to the users table.\n    user_id = Column(Integer, ForeignKey('users.id'))\n\n    # Defines the 1:n relationship between users and addresses.\n    # Also creates a backreference which is accessible from a User object.\n    user = relationship(\"User\", backref=backref('addresses'))\n\n    # Lets us print out an address object conveniently.\n    def __repr__(self):\n        return \"<Address(email_address='%s')>\" % self.email_address\n\n\n\n# Create all tables by issuing CREATE TABLE commands to the DB.\nBase.metadata.create_all(engine) \n\n# Creates a new session to the database by using the engine we described.\nSession = sessionmaker(bind=engine)\nsession = Session()\n\n# Let's create a user and add two e-mail addresses to that user.\nexample_user = User(name='example', fullname='example last_name_example', password='examplepassword')\nexample_user.addresses = [Address(email_address='example@gmail.com'), Address(email_address='example@yahoo.com')]\n\n# Let's add the user and its addresses we've created to the DB and commit.\nsession.add(example_user)\nsession.commit()\n\n# Now let's query the user that has the e-mail address ed@google.com\n# SQLAlchemy will construct a JOIN query automatically.\nuser_by_email = session.query(User)\\\n    .filter(Address.email_address=='example@gmail.com')\\\n    .first()\n\n2020-10-02 08:55:48,507 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info(\"users\")\n2020-10-02 08:55:48,508 INFO sqlalchemy.engine.base.Engine ()\n2020-10-02 08:55:48,510 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info(\"addresses\")\n2020-10-02 08:55:48,510 INFO sqlalchemy.engine.base.Engine ()\n2020-10-02 08:55:48,513 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)\n2020-10-02 08:55:48,514 INFO sqlalchemy.engine.base.Engine INSERT INTO users (name, fullname, password) VALUES (?, ?, ?)\n2020-10-02 08:55:48,515 INFO sqlalchemy.engine.base.Engine ('example', 'example last_name_example', 'examplepassword')\n2020-10-02 08:55:48,517 INFO sqlalchemy.engine.base.Engine INSERT INTO addresses (email_address, user_id) VALUES (?, ?)\n2020-10-02 08:55:48,517 INFO sqlalchemy.engine.base.Engine ('example@gmail.com', 1)\n2020-10-02 08:55:48,519 INFO sqlalchemy.engine.base.Engine INSERT INTO addresses (email_address, user_id) VALUES (?, ?)\n2020-10-02 08:55:48,519 INFO sqlalchemy.engine.base.Engine ('example@yahoo.com', 1)\n2020-10-02 08:55:48,520 INFO sqlalchemy.engine.base.Engine COMMIT\n2020-10-02 08:55:48,522 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)\n2020-10-02 08:55:48,523 INFO sqlalchemy.engine.base.Engine SELECT users.id AS users_id, users.name AS users_name, users.fullname AS users_fullname, users.password AS users_password \nFROM users, addresses \nWHERE addresses.email_address = ?\n LIMIT ? OFFSET ?\n2020-10-02 08:55:48,524 INFO sqlalchemy.engine.base.Engine ('example@gmail.com', 1, 0)\n\n\n\nprint(user_by_email)\n\n<User(name='example', fullname='example last_name_example', password'examplepassword')>\n\n\n\n# This will cause an additional query by lazy loading from the DB.\nprint(user_by_email.addresses)\n\n2020-10-02 08:55:53,081 INFO sqlalchemy.engine.base.Engine SELECT addresses.id AS addresses_id, addresses.email_address AS addresses_email_address, addresses.user_id AS addresses_user_id \nFROM addresses \nWHERE ? = addresses.user_id\n2020-10-02 08:55:53,083 INFO sqlalchemy.engine.base.Engine (1,)\n[<Address(email_address='example@gmail.com')>, <Address(email_address='example@yahoo.com')>]\n\n\n\n# To avoid querying again when getting all addresses of a user,\n# we use the joinedload option. SQLAlchemy will load all results and hide\n# the duplicate entries from us, so we can then get for\n# the user's addressess without an additional query to the DB.\nuser_by_email = session.query(User)\\\n    .filter(Address.email_address=='example@gmail.com')\\\n    .options(joinedload(User.addresses))\\\n    .first()\n\n2020-10-02 08:56:04,305 INFO sqlalchemy.engine.base.Engine SELECT anon_1.users_id AS anon_1_users_id, anon_1.users_name AS anon_1_users_name, anon_1.users_fullname AS anon_1_users_fullname, anon_1.users_password AS anon_1_users_password, addresses_1.id AS addresses_1_id, addresses_1.email_address AS addresses_1_email_address, addresses_1.user_id AS addresses_1_user_id \nFROM (SELECT users.id AS users_id, users.name AS users_name, users.fullname AS users_fullname, users.password AS users_password \nFROM users, addresses \nWHERE addresses.email_address = ?\n LIMIT ? OFFSET ?) AS anon_1 LEFT OUTER JOIN addresses AS addresses_1 ON anon_1.users_id = addresses_1.user_id\n2020-10-02 08:56:04,306 INFO sqlalchemy.engine.base.Engine ('example@gmail.com', 1, 0)\n\n\n\nprint(user_by_email)\n\n<User(name='example', fullname='example last_name_example', password'examplepassword')>\n\n\n\nprint(user_by_email.addresses)\n\n[<Address(email_address='example@gmail.com')>, <Address(email_address='example@yahoo.com')>]"
  },
  {
    "objectID": "posts/2020-06-07-kwargs-decorators.html",
    "href": "posts/2020-06-07-kwargs-decorators.html",
    "title": "A timer for ML functions",
    "section": "",
    "text": "toc: true- branch: master- badges: true\ncomments: true\nauthor: David Kearney\ncategories: [timer, jupyter]\ndescription: A timer for ML functions\ntitle: A timer for ML functions\n\n\n\nCode\nfrom functools import wraps\nimport time\n\n\ndef timer(func):\n    \"\"\"[This decorator is a timer for functions]\n\n    Args:\n        func ([function]): [This decorator takes a function as argument]\n\n    Returns:\n        [string]: [states the duration of time between the function begining and ending]\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        print(f\"{func.__name__!r} begins\")\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        print(f\"{func.__name__!r} ends in {time.time()-start_time}  secs\")\n        return result\n    return wrapper\n\n\n\n@timer\ndef model_metrics(*args, **kwargs):\n    \"\"\"[This is a function to print model metrics of interest]\n    \"\"\"\n    print(\"Model ID Number:\", args)\n    print(\"Metric of Interest:\", kwargs)\n\n\nmodel_metrics(1, 2, 10, key=\"word\", key2=\"word2\", numtrees=\"200\")\n\n\nfrom collections import Counter\nimport math, random\n\n#\n# data splitting\n#\n\ndef split_data(data, prob):\n    \"\"\"split data into fractions [prob, 1 - prob]\"\"\"\n    results = [], []\n    for row in data:\n        results[0 if random.random() < prob else 1].append(row)\n    return results\n\ndef train_test_split(x, y, test_pct):\n    data = list(zip(x, y))                        # pair corresponding values\n    train, test = split_data(data, 1 - test_pct)  # split the dataset of pairs\n    x_train, y_train = list(zip(*train))          # magical un-zip trick\n    x_test, y_test = list(zip(*test))\n    return x_train, x_test, y_train, y_test\n\n#\n# correctness\n#\n\ndef accuracy(tp, fp, fn, tn):\n    correct = tp + tn\n    total = tp + fp + fn + tn\n    return correct / total\n\ndef precision(tp, fp, fn, tn):\n    return tp / (tp + fp)\n\ndef recall(tp, fp, fn, tn):\n    return tp / (tp + fn)\n\ndef f1_score(tp, fp, fn, tn):\n    p = precision(tp, fp, fn, tn)\n    r = recall(tp, fp, fn, tn)\n\n    return 2 * p * r / (p + r)\n\nif __name__ == \"__main__\":\n\n    print(\"accuracy(70, 4930, 13930, 981070)\", accuracy(70, 4930, 13930, 981070))\n    print(\"precision(70, 4930, 13930, 981070)\", precision(70, 4930, 13930, 981070))\n    print(\"recall(70, 4930, 13930, 981070)\", recall(70, 4930, 13930, 981070))\n    print(\"f1_score(70, 4930, 13930, 981070)\", f1_score(70, 4930, 13930, 981070))\n\n\nfavorite_number = 7\n\n\ndef add(a, b):\n    return a + b\n\n\ndef sub(a, b):\n    return a - b\n\n\ndef multiply(a, b):\n    return a * b\n\n\ndef divide(a, b):\n    return a / b\n\n\ndef count_vowels(word):\n    count = 0\n    for letter in word.lower():\n        count += letter in 'aeiou'\n\n    return count\n\n\n# import example_module as sm\n\n# print(sm.favorite_number)\n\n# # add two numbers together\n# print(sm.add(3, 8))\n\n# # count the number of vowels in a string\n# print(sm.count_vowels('Testing'))\n\n\nimport pandas as pd\nfrom alive_progress import alive_bar, showtime, show_bars, show_spinners, config_handler\nconfig_handler.set_global(theme='ascii', spinner='notes', bar='solid')\n\nwith alive_bar(3) as bar:\n    df = pd.read_csv('https://gist.githubusercontent.com/davidrkearney/bb461ba351da484336a19bd00a2612e2/raw/18dd90b57fec46a247248d161ffd8085de2a00db/china_province_economicdata_1996_2007.csv')\n    bar('file read, printing file')\n    print(df.head)\n    bar('data printed ok, printing methods of data')\n    print(dir(df))\n    bar('process complete')\n\n\nfrom functools import wraps\nimport time\n\n\ndef timer(func):\n    \"\"\"[This decorator is a timer for functions]\n\n    Args:\n        func ([function]): [This decorator takes a function as argument]\n\n    Returns:\n        [string]: [states the duration of time between the function begining and ending]\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        print(f\"{func.__name__!r} begins\")\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        print(f\"{func.__name__!r} ends in {time.time()-start_time}  secs\")\n        return result\n    return wrapper\n\n\n\n@timer\ndef model_metrics(*args, **kwargs):\n    \"\"\"[This is a function to print model metrics of interest]\n    \"\"\"\n    print(\"Model ID Number:\", args)\n    print(\"Metric of Interest:\", kwargs)\n\n\nmodel_metrics(1, 2, 10, key=\"word\", key2=\"word2\", numtrees=\"200\")\n\nThis post includes code adapted from Data Science from Scratch"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html",
    "title": "Regression and Classification with Pyspark ML",
    "section": "",
    "text": "from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructField,StringType,IntegerType,StructType, DoubleType, FloatType\nfrom pyspark.sql.functions import *\n\ndata_schema = [\nStructField(\"_c0\", IntegerType(), True)\n,StructField(\"province\", StringType(), True)\n,StructField(\"specific\", DoubleType(), True)\n,StructField(\"general\", DoubleType(), True)\n,StructField(\"year\", IntegerType(), True)\n,StructField(\"gdp\", FloatType(), True)\n,StructField(\"fdi\", FloatType(), True)\n,StructField(\"rnr\", DoubleType(), True)\n,StructField(\"rr\", FloatType(), True)\n,StructField(\"i\", FloatType(), True)\n,StructField(\"fr\", IntegerType(), True)\n,StructField(\"reg\", StringType(), True)\n,StructField(\"it\", IntegerType(), True)\n]\n\nfinal_struc = StructType(fields=data_schema)\n\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").schema(final_struc).option(\"header\", True).load(file_location)\n\n#df.printSchema()\n\ndf.show()\n\n\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|     fdi| rnr|       rr|        i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661.0| 0.0|      0.0|      0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443.0| 0.0|      0.0|      0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673.0| 0.0|      0.0|      0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131.0|null|     null|     null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0|      0.0|      0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672.0| 0.0|      0.0|      0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0|      0.0|      0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0|      0.0|      0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669.0| 0.0|      0.0|      0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000.0| 0.0|      0.0|0.3243243|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354.0| 0.0|      0.0|0.3243243|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892.0| 0.0|      0.0|0.3243243|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290.0|null|     null|     null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286.0| 0.0|      0.0|      0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800.0| 0.0|      0.0|     0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525.0| 0.0|      0.0|     0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368.0| 0.0|      0.0|     0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818.0| 0.0|      0.0|     0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464.0| 0.0|      0.0|     0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126.0| 0.0|0.7948718|      0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.groupBy('province').count().show()\n\n\n+------------+-----+\n    province|count|\n+------------+-----+\n   Guangdong|   12|\n       Hunan|   12|\n      Shanxi|   12|\n       Tibet|   12|\n       Hubei|   12|\n     Tianjin|   12|\n     Beijing|   12|\nHeilongjiang|   12|\n    Liaoning|   12|\n       Henan|   12|\n       Anhui|   12|\n    Xinjiang|   12|\n      Fujian|   12|\n     Jiangxi|   12|\n       Jilin|   12|\n   Chongqing|   12|\n     Shaanxi|   12|\n     Sichuan|   12|\n      Yunnan|   12|\n       Gansu|   12|\n+------------+-----+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#imputation-of-mean-values-to-prepare-the-data",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#imputation-of-mean-values-to-prepare-the-data",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Imputation of mean values to prepare the data",
    "text": "Imputation of mean values to prepare the data\n\nmean_val = df.select(mean(df['general'])).collect()\nmean_val[0][0]\nmean_gen = mean_val[0][0]\ndf = df.na.fill(mean_gen,[\"general\"])\n\n\n\n\n\n\nmean_val = df.select(mean(df['specific'])).collect()\nmean_val[0][0]\nmean_gen = mean_val[0][0]\ndf = df.na.fill(mean_gen,[\"specific\"])\n\n\n\n\n\n\nmean_val = df.select(mean(df['rr'])).collect()\nmean_val[0][0]\nmean_gen = mean_val[0][0]\ndf = df.na.fill(mean_gen,[\"rr\"])\n\n\n\n\n\n\nmean_val = df.select(mean(df['fr'])).collect()\nmean_val[0][0]\nmean_gen = mean_val[0][0]\ndf = df.na.fill(mean_gen,[\"fr\"])\n\n\n\n\n\n\nmean_val = df.select(mean(df['rnr'])).collect()\nmean_val[0][0]\nmean_gen = mean_val[0][0]\ndf = df.na.fill(mean_gen,[\"rnr\"])\n\n\n\n\n\n\nmean_val = df.select(mean(df['i'])).collect()\nmean_val[0][0]\nmean_gen = mean_val[0][0]\ndf = df.na.fill(mean_gen,[\"i\"])"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#creating-binary-target-feature-from-extant-column-for-classification",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#creating-binary-target-feature-from-extant-column-for-classification",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Creating binary target feature from extant column for classification",
    "text": "Creating binary target feature from extant column for classification\n\nfrom pyspark.sql.functions import *\ndf = df.withColumn('specific_classification',when(df.specific >= 583470.7303370787,1).otherwise(0))"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#using-stringindexer-for-categorical-encoding-of-string-type-columns",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#using-stringindexer-for-categorical-encoding-of-string-type-columns",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Using StringIndexer for categorical encoding of string type columns",
    "text": "Using StringIndexer for categorical encoding of string type columns\n\nfrom pyspark.ml.feature import StringIndexer\n\n\n\n\n\n\nindexer = StringIndexer(inputCol=\"province\", outputCol=\"provinceIndex\")\ndf = indexer.fit(df).transform(df)\n\n\n\n\n\n\nindexer = StringIndexer(inputCol=\"reg\", outputCol=\"regionIndex\")\ndf = indexer.fit(df).transform(df)\n\n\n\n\n\n\ndf.show()\n\n\n+---+--------+---------+------------------+----+-------+--------+------------------+----------+----------+-------+-----------+-------+-----------------------+-------------+-----------+\n_c0|province| specific|           general|year|    gdp|     fdi|               rnr|        rr|         i|     fr|        reg|     it|specific_classification|provinceIndex|regionIndex|\n+---+--------+---------+------------------+----+-------+--------+------------------+----------+----------+-------+-----------+-------+-----------------------+-------------+-----------+\n  0|   Anhui| 147002.0|309127.53846153844|1996| 2093.3| 50661.0|               0.0|       0.0|       0.0|1128873| East China| 631930|                      0|          0.0|        0.0|\n  1|   Anhui| 151981.0|309127.53846153844|1997|2347.32| 43443.0|               0.0|       0.0|       0.0|1356287| East China| 657860|                      0|          0.0|        0.0|\n  2|   Anhui| 174930.0|309127.53846153844|1998|2542.96| 27673.0|               0.0|       0.0|       0.0|1518236| East China| 889463|                      0|          0.0|        0.0|\n  3|   Anhui| 285324.0|309127.53846153844|1999|2712.34| 26131.0|0.0355944252244898|0.05968862|0.08376352|1646891| East China|1227364|                      0|          0.0|        0.0|\n  4|   Anhui| 195580.0|           32100.0|2000|2902.09| 31847.0|               0.0|       0.0|       0.0|1601508| East China|1499110|                      0|          0.0|        0.0|\n  5|   Anhui| 250898.0|309127.53846153844|2001|3246.71| 33672.0|               0.0|       0.0|       0.0|1672445| East China|2165189|                      0|          0.0|        0.0|\n  6|   Anhui| 434149.0|           66529.0|2002|3519.72| 38375.0|               0.0|       0.0|       0.0|1677840| East China|2404936|                      0|          0.0|        0.0|\n  7|   Anhui| 619201.0|           52108.0|2003|3923.11| 36720.0|               0.0|       0.0|       0.0|1896479| East China|2815820|                      1|          0.0|        0.0|\n  8|   Anhui| 898441.0|          349699.0|2004| 4759.3| 54669.0|               0.0|       0.0|       0.0|2522449| East China|3422176|                      1|          0.0|        0.0|\n  9|   Anhui| 898441.0|309127.53846153844|2005|5350.17| 69000.0|               0.0|       0.0| 0.3243243|2522449| East China|3874846|                      1|          0.0|        0.0|\n 10|   Anhui|1457872.0|          279052.0|2006| 6112.5|139354.0|               0.0|       0.0| 0.3243243|3434548| East China|5167300|                      1|          0.0|        0.0|\n 11|   Anhui|2213991.0|          178705.0|2007|7360.92|299892.0|               0.0|       0.0| 0.3243243|4468640| East China|7040099|                      1|          0.0|        0.0|\n 12| Beijing| 165957.0|309127.53846153844|1996| 1789.2|155290.0|0.0355944252244898|0.05968862|0.08376352| 634562|North China| 508135|                      0|          1.0|        4.0|\n 13| Beijing| 165957.0|309127.53846153844|1997|2077.09|159286.0|               0.0|       0.0|       0.6| 634562|North China| 569283|                      0|          1.0|        4.0|\n 14| Beijing| 245198.0|309127.53846153844|1998|2377.18|216800.0|               0.0|       0.0|      0.53| 938788|North China| 695528|                      0|          1.0|        4.0|\n 15| Beijing| 388083.0|309127.53846153844|1999|2678.82|197525.0|               0.0|       0.0|      0.53|2522449|North China| 944047|                      0|          1.0|        4.0|\n 16| Beijing| 281769.0|          188633.0|2000|3161.66|168368.0|               0.0|       0.0|      0.53|1667114|North China| 757990|                      0|          1.0|        4.0|\n 17| Beijing| 441923.0|309127.53846153844|2001|3707.96|176818.0|               0.0|       0.0|      0.53|2093925|North China|1194728|                      0|          1.0|        4.0|\n 18| Beijing| 558569.0|          280277.0|2002| 4315.0|172464.0|               0.0|       0.0|      0.53|2511249|North China|1078754|                      0|          1.0|        4.0|\n 19| Beijing| 642581.0|          269596.0|2003|5007.21|219126.0|               0.0| 0.7948718|       0.0|2823366|North China|1426600|                      1|          1.0|        4.0|\n+---+--------+---------+------------------+----+-------+--------+------------------+----------+----------+-------+-----------+-------+-----------------------+-------------+-----------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#using-vectorassembler-to-prepare-features-for-machine-learning",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#using-vectorassembler-to-prepare-features-for-machine-learning",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Using VectorAssembler to prepare features for machine learning",
    "text": "Using VectorAssembler to prepare features for machine learning\n\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\n\n\n\n\n\ndf.columns\n\n\nOut[375]: ['_c0',\n 'province',\n 'specific',\n 'general',\n 'year',\n 'gdp',\n 'fdi',\n 'rnr',\n 'rr',\n 'i',\n 'fr',\n 'reg',\n 'it',\n 'specific_classification',\n 'provinceIndex',\n 'regionIndex']\n\n\n\nassembler = VectorAssembler(\n inputCols=[\n 'provinceIndex',\n# 'specific',\n 'general',\n 'year',\n 'gdp',\n 'fdi',\n #'rnr',\n #'rr',\n #'i',\n #'fr',\n 'regionIndex',\n 'it'\n ],\n outputCol=\"features\")\n\n\n\n\n\n\noutput = assembler.transform(df)\n\n\n\n\n\n\nfinal_data = output.select(\"features\", \"specific\")"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#spliting-data-into-train-and-test",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#spliting-data-into-train-and-test",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Spliting data into train and test",
    "text": "Spliting data into train and test\n\ntrain_data,test_data = final_data.randomSplit([0.7,0.3])"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#regression-with-pyspark-ml",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#regression-with-pyspark-ml",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Regression with Pyspark ML",
    "text": "Regression with Pyspark ML\n\nfrom pyspark.ml.regression import LinearRegression\nlr = LinearRegression(labelCol='specific')"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#fitting-the-linear-regression-model-to-the-training-data",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#fitting-the-linear-regression-model-to-the-training-data",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Fitting the linear regression model to the training data",
    "text": "Fitting the linear regression model to the training data\n\nlrModel = lr.fit(train_data)"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#coefficients-and-intercept-of-the-linear-regression-model",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#coefficients-and-intercept-of-the-linear-regression-model",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Coefficients and Intercept of the linear regression model",
    "text": "Coefficients and Intercept of the linear regression model\n\nprint(\"Coefficients: {} Intercept: {}\".format(lrModel.coefficients,lrModel.intercept))\n\n\nCoefficients: [-4936.461707001148,0.8007702471080539,-3994.683052325085,-7.5033201950338,0.42095493334994133,50994.51222529955,0.2531915644818595] Intercept: 7695214.561654471"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#evaluating-trained-linear-regression-model-on-the-test-data",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#evaluating-trained-linear-regression-model-on-the-test-data",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Evaluating trained linear regression model on the test data",
    "text": "Evaluating trained linear regression model on the test data\n\ntest_results = lrModel.evaluate(test_data)"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#metrics-of-trained-linear-regression-model-on-the-test-data-rmse-mse-r2",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#metrics-of-trained-linear-regression-model-on-the-test-data-rmse-mse-r2",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Metrics of trained linear regression model on the test data (RMSE, MSE, R2)",
    "text": "Metrics of trained linear regression model on the test data (RMSE, MSE, R2)\n\nprint(\"RMSE: {}\".format(test_results.rootMeanSquaredError))\nprint(\"MSE: {}\".format(test_results.meanSquaredError))\nprint(\"R2: {}\".format(test_results.r2))\n\n\nRMSE: 292695.0825058327\nMSE: 85670411323.0962\nR2: 0.7853651103073853"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#looking-at-correlations-with-corr",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#looking-at-correlations-with-corr",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Looking at correlations with corr",
    "text": "Looking at correlations with corr\n\nfrom pyspark.sql.functions import corr\n\n\n\n\n\n\ndf.select(corr('specific','gdp')).show()\n\n\n+-------------------+\ncorr(specific, gdp)|\n+-------------------+\n 0.5141876884991972|\n+-------------------+"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#classification-with-pyspark-ml",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#classification-with-pyspark-ml",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Classification with Pyspark ML",
    "text": "Classification with Pyspark ML\n\nfrom pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\nfrom pyspark.ml import Pipeline"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#decisiontreeclassifier-randomforestclassifier-and-gbtclassifier",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#decisiontreeclassifier-randomforestclassifier-and-gbtclassifier",
    "title": "Regression and Classification with Pyspark ML",
    "section": "DecisionTreeClassifier, RandomForestClassifier and GBTClassifier",
    "text": "DecisionTreeClassifier, RandomForestClassifier and GBTClassifier\n\ndtc = DecisionTreeClassifier(labelCol='specific_classification',featuresCol='features')\nrfc = RandomForestClassifier(labelCol='specific_classification',featuresCol='features')\ngbt = GBTClassifier(labelCol='specific_classification',featuresCol='features')"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#selecting-features-and-binary-target",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#selecting-features-and-binary-target",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Selecting features and binary target",
    "text": "Selecting features and binary target\n\nfinal_data = output.select(\"features\", \"specific_classification\")\ntrain_data,test_data = final_data.randomSplit([0.7,0.3])"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#fitting-the-classifiers-to-the-training-data",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#fitting-the-classifiers-to-the-training-data",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Fitting the Classifiers to the Training Data",
    "text": "Fitting the Classifiers to the Training Data\n\nrfc_model = rfc.fit(train_data)\ngbt_model = gbt.fit(train_data)\ndtc_model = dtc.fit(train_data)"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#classifier-predictions-on-test-data",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#classifier-predictions-on-test-data",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Classifier predictions on test data",
    "text": "Classifier predictions on test data\n\ndtc_predictions = dtc_model.transform(test_data)\nrfc_predictions = rfc_model.transform(test_data)\ngbt_predictions = gbt_model.transform(test_data)"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#evaluating-classifiers-using-pyspark.ml.evaluation-and-multiclassclassificationevaluator",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#evaluating-classifiers-using-pyspark.ml.evaluation-and-multiclassclassificationevaluator",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Evaluating Classifiers using pyspark.ml.evaluation and MulticlassClassificationEvaluator",
    "text": "Evaluating Classifiers using pyspark.ml.evaluation and MulticlassClassificationEvaluator\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n\n\n\n\n\nClassifier Accuracy\n\nacc_evaluator = MulticlassClassificationEvaluator(labelCol=\"specific_classification\", predictionCol=\"prediction\", metricName=\"accuracy\")"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#classifier-accuracy-metrics",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#classifier-accuracy-metrics",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Classifier Accuracy Metrics",
    "text": "Classifier Accuracy Metrics\n\ndtc_acc = acc_evaluator.evaluate(dtc_predictions)\nrfc_acc = acc_evaluator.evaluate(rfc_predictions)\ngbt_acc = acc_evaluator.evaluate(gbt_predictions)\n\n\n\n\n\n\nprint('-'*80)\nprint('Decision tree accuracy: {0:2.2f}%'.format(dtc_acc*100))\nprint('-'*80)\nprint('Random forest ensemble accuracy: {0:2.2f}%'.format(rfc_acc*100))\nprint('-'*80)\nprint('GBT accuracy: {0:2.2f}%'.format(gbt_acc*100))\nprint('-'*80)\n\n\n--------------------------------------------------------------------------------\nDecision tree accuracy: 81.98%\n--------------------------------------------------------------------------------\nRandom forest ensemble accuracy: 88.29%\n--------------------------------------------------------------------------------\nGBT accuracy: 81.08%\n--------------------------------------------------------------------------------"
  },
  {
    "objectID": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#classification-correlation-with-corr",
    "href": "posts/2020-08-25-linear regression and random forest_gbt classification with pyspark.html#classification-correlation-with-corr",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Classification Correlation with Corr",
    "text": "Classification Correlation with Corr\n\ndf.select(corr('specific_classification','fdi')).show()\n\n\n+----------------------------------+\ncorr(specific_classification, fdi)|\n+----------------------------------+\n                 0.307429849493392|\n+----------------------------------+\n\n\n\n\n\ndf.select(corr('specific_classification','gdp')).show()\n\n\n+----------------------------------+\ncorr(specific_classification, gdp)|\n+----------------------------------+\n                 0.492176921599151|\n+----------------------------------+\n\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-09-28-databases_sqllite_sqlalchemy_27.html",
    "href": "posts/2020-09-28-databases_sqllite_sqlalchemy_27.html",
    "title": "Creating and accessing SQL databases with python using sqlalchemy and sqlite3",
    "section": "",
    "text": "import sqlalchemy as db\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///music.sqlite')\n\n\nconnection = engine.connect()\nmetadata = db.MetaData()\n\nmusic = db.Table('music', metadata,\n              db.Column('Id', db.Integer()),\n              db.Column('song', db.String(255), nullable=False),\n              db.Column('album', db.String(255), nullable=False),\n              db.Column('artist', db.String(255), nullable=False)\n              )\n\nmetadata.create_all(engine) \n\n\n#Inserting one record\nquery = db.insert(music).values(Id=1, song='song3', album='album3', artist='artist3') \nResultProxy = connection.execute(query)\n\n\n#Inserting many records\nquery = db.insert(music) \nvalues_list = [{'Id':'2', 'song':'song1', 'album':'album1', 'artist':'artist1'},\n               {'Id':'3', 'song':'song2', 'album':'album2', 'artist':'artist2'}]\n\nResultProxy = connection.execute(query,values_list)\n\nresults = connection.execute(db.select([music])).fetchall()\ndf = pd.DataFrame(results)\ndf.columns = results[0].keys()\ndf.head(10)\n\n\n\n\n\n  \n    \n      \n      Id\n      song\n      album\n      artist\n    \n  \n  \n    \n      0\n      1\n      song3\n      album3\n      artist3\n    \n    \n      1\n      2\n      song1\n      album1\n      artist1\n    \n    \n      2\n      3\n      song2\n      album2\n      artist2\n    \n    \n      3\n      2\n      song1\n      album1\n      artist1\n    \n    \n      4\n      3\n      song2\n      album2\n      artist2\n    \n  \n\n\n\n\n\nresults = connection.execute(db.select([music])).fetchall()\ndf = pd.DataFrame(results)\ndf.columns = results[0].keys()\ndf.head(4)\n\n\nquery = db.select([music]).where(db.and_(music.columns.song == 'song3', music.columns.artist == 'artist3'))\nresult = connection.execute(query).fetchall()\nresult[:3]\n\n[(1, 'song3', 'album3', 'artist3')]\n\n\n\nconn = sqlite3.connect('music.sqlite')\n\n\nc = conn.cursor()\n\n# Create table\nc.execute('''CREATE TABLE stockmarket\n             (date text, trans text, symbol text, qty real, price real)''')\n\n# Insert a row of data\nc.execute(\"INSERT INTO stockmarket VALUES ('2006-01-05','BUY','RHAT',100,35.14)\")\n\n# Save (commit) the changes\nconn.commit()\n\n# We can also close the connection if we are done with it.\n# Just be sure any changes have been committed or they will be lost.\nconn.close()\n\n\nconn = sqlite3.connect('music.sqlite')\nc = conn.cursor()\n\n\nsymbol = 'RHAT'\nc.execute(\"SELECT * FROM stockmarket WHERE symbol = '%s'\" % symbol)\n\n<sqlite3.Cursor at 0x7f0098ae0180>\n\n\n\nt = ('RHAT',)\nc.execute('SELECT * FROM stockmarket WHERE symbol=?', t)\nprint(c.fetchone())\n\n('2006-01-05', 'BUY', 'RHAT', 100.0, 35.14)\n\n\n\n# Larger example that inserts many records at a time\npurchases = [('2006-03-28', 'BUY', 'IBM', 1000, 45.00),\n             ('2006-04-05', 'BUY', 'MSFT', 1000, 72.00),\n             ('2006-04-06', 'SELL', 'IBM', 500, 53.00),\n            ]\nc.executemany('INSERT INTO stockmarket VALUES (?,?,?,?,?)', purchases)\n\n<sqlite3.Cursor at 0x7f0098ae0180>\n\n\n\nfor row in c.execute('SELECT * FROM stockmarket ORDER BY price'):\n        print(row)\n\n('2006-01-05', 'BUY', 'RHAT', 100.0, 35.14)\n('2006-03-28', 'BUY', 'IBM', 1000.0, 45.0)\n('2006-04-06', 'SELL', 'IBM', 500.0, 53.0)\n('2006-04-05', 'BUY', 'MSFT', 1000.0, 72.0)\n\n\n\n# Use dbeaver to examine"
  },
  {
    "objectID": "posts/2020-08-19-pyspark-filtering.html",
    "href": "posts/2020-08-19-pyspark-filtering.html",
    "title": "Dataframe Filitering and Operations with Pyspark",
    "section": "",
    "text": "from pyspark.sql import SparkSession\n\n# Load data from a CSV\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").option(\"inferSchema\", True).option(\"header\", True).load(file_location)\ndisplay(df.take(5))\n\n\n_c0provincespecificgeneralyeargdpfdirnrrrifrregit0Anhui147002.0null19962093.3506610.00.00.01128873East China6319301Anhui151981.0null19972347.32434430.00.00.01356287East China6578602Anhui174930.0null19982542.96276730.00.00.01518236East China8894633Anhui285324.0null19992712.3426131nullnullnull1646891East China12273644Anhui195580.032100.020002902.09318470.00.00.01601508East China1499110"
  },
  {
    "objectID": "posts/2020-08-19-pyspark-filtering.html#filtering-on-values-in-a-column",
    "href": "posts/2020-08-19-pyspark-filtering.html#filtering-on-values-in-a-column",
    "title": "Dataframe Filitering and Operations with Pyspark",
    "section": "Filtering on values in a column",
    "text": "Filtering on values in a column\n\ndf.filter(\"specific<10000\").show()\n\n\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n_c0|province|specific|general|year|    gdp|   fdi|rnr| rr|   i|     fr|       reg|     it|\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n268|Shanghai|  8964.0|   null|2000|4771.17|316014|0.0|0.0|0.44|2224124|East China|1212473|\n269|Shanghai|  9834.0|   null|2001|5210.12|429159|0.0|0.0|0.44|2947285|East China|1053917|\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n\n\n\n\n\ndf.filter(\"specific<10000\").select('province').show()\n\n\n+--------+\nprovince|\n+--------+\nShanghai|\nShanghai|\n+--------+\n\n\n\n\n\ndf.filter(\"specific<10000\").select(['province','year']).show()\n\n\n+--------+----+\nprovince|year|\n+--------+----+\nShanghai|2000|\nShanghai|2001|\n+--------+----+\n\n\n\n\n\ndf.filter(df[\"specific\"] < 10000).show()\n\n\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n_c0|province|specific|general|year|    gdp|   fdi|rnr| rr|   i|     fr|       reg|     it|\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n268|Shanghai|  8964.0|   null|2000|4771.17|316014|0.0|0.0|0.44|2224124|East China|1212473|\n269|Shanghai|  9834.0|   null|2001|5210.12|429159|0.0|0.0|0.44|2947285|East China|1053917|\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+"
  },
  {
    "objectID": "posts/2020-08-19-pyspark-filtering.html#filtering-on-values-in-2-columns",
    "href": "posts/2020-08-19-pyspark-filtering.html#filtering-on-values-in-2-columns",
    "title": "Dataframe Filitering and Operations with Pyspark",
    "section": "Filtering on values in 2+ columns",
    "text": "Filtering on values in 2+ columns\n\ndf.filter((df[\"specific\"] < 55000) & (df['gdp'] > 200) ).show()\n\n\n+---+--------+--------+-------+----+--------+------+----+----+----+-------+-------------------+-------+\n_c0|province|specific|general|year|     gdp|   fdi| rnr|  rr|   i|     fr|                reg|     it|\n+---+--------+--------+-------+----+--------+------+----+----+----+-------+-------------------+-------+\n 98|  Hainan| 54462.0|   null|1998|  442.13| 71715|null|null|null| 236461|South Central China| 177748|\n216| Ningxia| 32088.0|   null|1996|   202.9|  2826|null|null|null|  90805|    Northwest China| 178668|\n217| Ningxia| 44267.0|   null|1997|  224.59|   671|null|null|null| 102083|    Northwest China| 195295|\n268|Shanghai|  8964.0|   null|2000| 4771.17|316014| 0.0| 0.0|0.44|2224124|         East China|1212473|\n269|Shanghai|  9834.0|   null|2001| 5210.12|429159| 0.0| 0.0|0.44|2947285|         East China|1053917|\n270|Shanghai| 19985.0|   null|2002| 5741.03|427229| 0.0| 0.0|0.44|3380397|         East China|1572208|\n271|Shanghai| 23547.0|   null|2003| 6694.23|546849| 0.0|0.53| 0.0|4461153|         East China|2031496|\n272|Shanghai| 29943.0|   null|2004| 8072.83|654100| 0.0|0.53| 0.0|   null|         East China|2703643|\n273|Shanghai| 29943.0|   null|2005| 9247.66|685000| 0.0|0.53| 0.0|   null|         East China|2140461|\n274|Shanghai| 42928.0|   null|2006|10572.24|710700| 0.0|0.53| 0.0|8175966|         East China|2239987|\n302| Tianjin| 39364.0|   null|1998|  1374.6|211361|null|null|null| 540178|        North China| 361723|\n303| Tianjin| 45463.0|   null|1999| 1500.95|176399| 0.0| 0.0| 0.0| 605662|        North China| 422522|\n304| Tianjin| 51821.0|   null|2000| 1701.88|116601| 0.0| 0.0| 0.0| 757464|        North China| 547120|\n305| Tianjin| 35084.0|   null|2001| 1919.09|213348| 0.0| 0.0| 0.0| 942763|        North China| 688810|\n+---+--------+--------+-------+----+--------+------+----+----+----+-------+-------------------+-------+\n\n\n\n\n\ndf.filter((df[\"specific\"] < 55000) | (df['gdp'] > 20000) ).show()\n\n\n+---+---------+---------+--------+----+--------+-------+--------------------+----+-----------+--------+-------------------+-------+\n_c0| province| specific| general|year|     gdp|    fdi|                 rnr|  rr|          i|      fr|                reg|     it|\n+---+---------+---------+--------+----+--------+-------+--------------------+----+-----------+--------+-------------------+-------+\n 69|Guangdong|1491588.0|    null|2005|22557.37|1236400|0.027027027000000002| 0.0|        0.0|    null|South Central China|4327217|\n 70|Guangdong|1897575.0|498913.0|2006|26587.76|1451065|0.027027027000000002| 0.0|        0.0|16804703|South Central China|4559252|\n 71|Guangdong| 859482.0|     0.0|2007|31777.01|1712603|0.027027027000000002| 0.0|        0.0|27858007|South Central China|4947824|\n 98|   Hainan|  54462.0|    null|1998|  442.13|  71715|                null|null|       null|  236461|South Central China| 177748|\n179|  Jiangsu|1188989.0|     0.0|2007|21742.05|1743140|                 0.0| 0.0|0.275862069|22377276|         East China|3557071|\n216|  Ningxia|  32088.0|    null|1996|   202.9|   2826|                null|null|       null|   90805|    Northwest China| 178668|\n217|  Ningxia|  44267.0|    null|1997|  224.59|    671|                null|null|       null|  102083|    Northwest China| 195295|\n228|  Qinghai|  37976.0|    null|1996|  184.17|    576|                null|null|       null|   73260|    Northwest China| 218361|\n262| Shandong|1204547.0|112137.0|2006|21900.19|1000069|                 0.0| 0.0|        0.0|11673659|         East China|5304833|\n263| Shandong|2121243.0|581800.0|2007|25776.91|1101159|                 0.0| 0.0|        0.0|16753980|         East China|6357869|\n268| Shanghai|   8964.0|    null|2000| 4771.17| 316014|                 0.0| 0.0|       0.44| 2224124|         East China|1212473|\n269| Shanghai|   9834.0|    null|2001| 5210.12| 429159|                 0.0| 0.0|       0.44| 2947285|         East China|1053917|\n270| Shanghai|  19985.0|    null|2002| 5741.03| 427229|                 0.0| 0.0|       0.44| 3380397|         East China|1572208|\n271| Shanghai|  23547.0|    null|2003| 6694.23| 546849|                 0.0|0.53|        0.0| 4461153|         East China|2031496|\n272| Shanghai|  29943.0|    null|2004| 8072.83| 654100|                 0.0|0.53|        0.0|    null|         East China|2703643|\n273| Shanghai|  29943.0|    null|2005| 9247.66| 685000|                 0.0|0.53|        0.0|    null|         East China|2140461|\n274| Shanghai|  42928.0|    null|2006|10572.24| 710700|                 0.0|0.53|        0.0| 8175966|         East China|2239987|\n302|  Tianjin|  39364.0|    null|1998|  1374.6| 211361|                null|null|       null|  540178|        North China| 361723|\n303|  Tianjin|  45463.0|    null|1999| 1500.95| 176399|                 0.0| 0.0|        0.0|  605662|        North China| 422522|\n304|  Tianjin|  51821.0|    null|2000| 1701.88| 116601|                 0.0| 0.0|        0.0|  757464|        North China| 547120|\n+---+---------+---------+--------+----+--------+-------+--------------------+----+-----------+--------+-------------------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.filter((df[\"specific\"] < 55000) & ~(df['gdp'] > 20000) ).show()\n\n\n+---+--------+--------+-------+----+--------+------+-----------+----+----+-------+-------------------+-------+\n_c0|province|specific|general|year|     gdp|   fdi|        rnr|  rr|   i|     fr|                reg|     it|\n+---+--------+--------+-------+----+--------+------+-----------+----+----+-------+-------------------+-------+\n 98|  Hainan| 54462.0|   null|1998|  442.13| 71715|       null|null|null| 236461|South Central China| 177748|\n216| Ningxia| 32088.0|   null|1996|   202.9|  2826|       null|null|null|  90805|    Northwest China| 178668|\n217| Ningxia| 44267.0|   null|1997|  224.59|   671|       null|null|null| 102083|    Northwest China| 195295|\n228| Qinghai| 37976.0|   null|1996|  184.17|   576|       null|null|null|  73260|    Northwest China| 218361|\n268|Shanghai|  8964.0|   null|2000| 4771.17|316014|        0.0| 0.0|0.44|2224124|         East China|1212473|\n269|Shanghai|  9834.0|   null|2001| 5210.12|429159|        0.0| 0.0|0.44|2947285|         East China|1053917|\n270|Shanghai| 19985.0|   null|2002| 5741.03|427229|        0.0| 0.0|0.44|3380397|         East China|1572208|\n271|Shanghai| 23547.0|   null|2003| 6694.23|546849|        0.0|0.53| 0.0|4461153|         East China|2031496|\n272|Shanghai| 29943.0|   null|2004| 8072.83|654100|        0.0|0.53| 0.0|   null|         East China|2703643|\n273|Shanghai| 29943.0|   null|2005| 9247.66|685000|        0.0|0.53| 0.0|   null|         East China|2140461|\n274|Shanghai| 42928.0|   null|2006|10572.24|710700|        0.0|0.53| 0.0|8175966|         East China|2239987|\n302| Tianjin| 39364.0|   null|1998|  1374.6|211361|       null|null|null| 540178|        North China| 361723|\n303| Tianjin| 45463.0|   null|1999| 1500.95|176399|        0.0| 0.0| 0.0| 605662|        North China| 422522|\n304| Tianjin| 51821.0|   null|2000| 1701.88|116601|        0.0| 0.0| 0.0| 757464|        North China| 547120|\n305| Tianjin| 35084.0|   null|2001| 1919.09|213348|        0.0| 0.0| 0.0| 942763|        North China| 688810|\n312|   Tibet| 18829.0|   null|1996|   64.98|   679|0.181818182| 0.0| 0.0|  27801|    Southwest China| 306114|\n313|   Tibet| 25185.0|   null|1997|   77.24|    63|0.181818182| 0.0| 0.0|  33787|    Southwest China| 346368|\n314|   Tibet| 48197.0|   null|1998|    91.5|   481|        0.0|0.24| 0.0|   3810|    Southwest China| 415547|\n+---+--------+--------+-------+----+--------+------+-----------+----+----+-------+-------------------+-------+\n\n\n\n\n\ndf.filter(df[\"specific\"] == 8964.0).show()\n\n\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n_c0|province|specific|general|year|    gdp|   fdi|rnr| rr|   i|     fr|       reg|     it|\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n268|Shanghai|  8964.0|   null|2000|4771.17|316014|0.0|0.0|0.44|2224124|East China|1212473|\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n\n\n\n\n\ndf.filter(df[\"province\"] == \"Zhejiang\").show()\n\n\n+---+--------+---------+--------+----+--------+-------+-----------+-----------+-----------+--------+----------+-------+\n_c0|province| specific| general|year|     gdp|    fdi|        rnr|         rr|          i|      fr|       reg|     it|\n+---+--------+---------+--------+----+--------+-------+-----------+-----------+-----------+--------+----------+-------+\n348|Zhejiang| 273253.0|    null|1996| 4188.53| 152021|        0.0|        0.0|        0.0| 1291252|East China| 740327|\n349|Zhejiang| 330558.0|    null|1997| 4686.11| 150345|        0.0|        0.0|        0.0| 1432453|East China| 814253|\n350|Zhejiang| 426756.0|    null|1998| 5052.62| 131802|        0.0|        0.0|        0.0| 1761084|East China| 923455|\n351|Zhejiang| 586457.0|    null|1999| 5443.92| 123262|        0.0|        0.0|        0.0| 2146200|East China|1001703|\n352|Zhejiang| 408151.0|    null|2000| 6141.03| 161266|        0.0|        0.0|        0.0| 2955508|East China|1135215|\n353|Zhejiang| 358714.0|    null|2001| 6898.34| 221162|        0.0|        0.0|        0.0| 4436868|East China|1203372|\n354|Zhejiang| 365437.0|321686.0|2002| 8003.67| 307610|        0.0|        0.0|        0.0| 4958329|East China|1962633|\n355|Zhejiang| 391292.0|260313.0|2003| 9705.02| 498055|1.214285714|0.035714286|0.035714286| 6217715|East China|2261631|\n356|Zhejiang| 656175.0|276652.0|2004| 11648.7| 668128|1.214285714|0.035714286|0.035714286|    null|East China|3162299|\n357|Zhejiang| 656175.0|    null|2005|13417.68| 772000|1.214285714|0.035714286|0.035714286|    null|East China|2370200|\n358|Zhejiang|1017303.0|394795.0|2006|15718.47| 888935|1.214285714|0.035714286|0.035714286|11537149|East China|2553268|\n359|Zhejiang| 844647.0|     0.0|2007|18753.73|1036576|0.047619048|        0.0|        0.0|16494981|East China|2939778|\n+---+--------+---------+--------+----+--------+-------+-----------+-----------+-----------+--------+----------+-------+\n\n\n\n\n\ndf.filter(df[\"specific\"] == 8964.0).collect()\n\n\nOut[15]: [Row(_c0=268, province='Shanghai', specific=8964.0, general=None, year=2000, gdp=4771.17, fdi=316014, rnr=0.0, rr=0.0, i=0.44, fr='2224124', reg='East China', it=1212473)]\n\n\n\nresult = df.filter(df[\"specific\"] == 8964.0).collect()\n\n\n\n\n\n\ntype(result[0])\n\n\nOut[17]: pyspark.sql.types.Row\n\n\n\nrow = result[0]\n\n\n\n\n\n\nrow.asDict()\n\n\nOut[19]: {'_c0': 268,\n 'province': 'Shanghai',\n 'specific': 8964.0,\n 'general': None,\n 'year': 2000,\n 'gdp': 4771.17,\n 'fdi': 316014,\n 'rnr': 0.0,\n 'rr': 0.0,\n 'i': 0.44,\n 'fr': '2224124',\n 'reg': 'East China',\n 'it': 1212473}\n\n\n\nfor item in result[0]:\n    print(item)\n\n\n268\nShanghai\n8964.0\nNone\n2000\n4771.17\n316014\n0.0\n0.0\n0.44\n2224124\nEast China\n1212473\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-09-23-analyzingusinflation.html",
    "href": "posts/2020-09-23-analyzingusinflation.html",
    "title": "Analyzing US Inflation From 1959 - 2009 with statsmodels",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n%matplotlib inline\n\ndf = sm.datasets.macrodata.load_pandas().data\n\nprint(sm.datasets.macrodata.NOTE)\n\n::\n    Number of Observations - 203\n\n    Number of Variables - 14\n\n    Variable name definitions::\n\n        year      - 1959q1 - 2009q3\n        quarter   - 1-4\n        realgdp   - Real gross domestic product (Bil. of chained 2005 US$,\n                    seasonally adjusted annual rate)\n        realcons  - Real personal consumption expenditures (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realinv   - Real gross private domestic investment (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realgovt  - Real federal consumption expenditures & gross investment\n                    (Bil. of chained 2005 US$, seasonally adjusted annual rate)\n        realdpi   - Real private disposable income (Bil. of chained 2005\n                    US$, seasonally adjusted annual rate)\n        cpi       - End of the quarter consumer price index for all urban\n                    consumers: all items (1982-84 = 100, seasonally adjusted).\n        m1        - End of the quarter M1 nominal money stock (Seasonally\n                    adjusted)\n        tbilrate  - Quarterly monthly average of the monthly 3-month\n                    treasury bill: secondary market rate\n        unemp     - Seasonally adjusted unemployment rate (%)\n        pop       - End of the quarter total population: all ages incl. armed\n                    forces over seas\n        infl      - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400)\n        realint   - Real interest rate (tbilrate - infl)\n\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      quarter\n      realgdp\n      realcons\n      realinv\n      realgovt\n      realdpi\n      cpi\n      m1\n      tbilrate\n      unemp\n      pop\n      infl\n      realint\n    \n  \n  \n    \n      0\n      1959.0\n      1.0\n      2710.349\n      1707.4\n      286.898\n      470.045\n      1886.9\n      28.98\n      139.7\n      2.82\n      5.8\n      177.146\n      0.00\n      0.00\n    \n    \n      1\n      1959.0\n      2.0\n      2778.801\n      1733.7\n      310.859\n      481.301\n      1919.7\n      29.15\n      141.7\n      3.08\n      5.1\n      177.830\n      2.34\n      0.74\n    \n    \n      2\n      1959.0\n      3.0\n      2775.488\n      1751.8\n      289.226\n      491.260\n      1916.4\n      29.35\n      140.5\n      3.82\n      5.3\n      178.657\n      2.74\n      1.09\n    \n    \n      3\n      1959.0\n      4.0\n      2785.204\n      1753.7\n      299.356\n      484.052\n      1931.3\n      29.37\n      140.0\n      4.33\n      5.6\n      179.386\n      0.27\n      4.06\n    \n    \n      4\n      1960.0\n      1.0\n      2847.699\n      1770.5\n      331.722\n      462.199\n      1955.5\n      29.54\n      139.6\n      3.50\n      5.2\n      180.007\n      2.31\n      1.19\n    \n  \n\n\n\n\n\nindex = pd.Index(sm.tsa.datetools.dates_from_range('1959Q1', '2009Q3'))\ndf.index = index\ndf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      quarter\n      realgdp\n      realcons\n      realinv\n      realgovt\n      realdpi\n      cpi\n      m1\n      tbilrate\n      unemp\n      pop\n      infl\n      realint\n    \n  \n  \n    \n      1959-03-31\n      1959.0\n      1.0\n      2710.349\n      1707.4\n      286.898\n      470.045\n      1886.9\n      28.98\n      139.7\n      2.82\n      5.8\n      177.146\n      0.00\n      0.00\n    \n    \n      1959-06-30\n      1959.0\n      2.0\n      2778.801\n      1733.7\n      310.859\n      481.301\n      1919.7\n      29.15\n      141.7\n      3.08\n      5.1\n      177.830\n      2.34\n      0.74\n    \n    \n      1959-09-30\n      1959.0\n      3.0\n      2775.488\n      1751.8\n      289.226\n      491.260\n      1916.4\n      29.35\n      140.5\n      3.82\n      5.3\n      178.657\n      2.74\n      1.09\n    \n    \n      1959-12-31\n      1959.0\n      4.0\n      2785.204\n      1753.7\n      299.356\n      484.052\n      1931.3\n      29.37\n      140.0\n      4.33\n      5.6\n      179.386\n      0.27\n      4.06\n    \n    \n      1960-03-31\n      1960.0\n      1.0\n      2847.699\n      1770.5\n      331.722\n      462.199\n      1955.5\n      29.54\n      139.6\n      3.50\n      5.2\n      180.007\n      2.31\n      1.19\n    \n  \n\n\n\n\n\ndf['infl'].plot()\nplt.ylabel(\"infl\")\n\nText(0, 0.5, 'infl')\n\n\n\n\n\n\\(\\min_{\\\\{ \\tau_{t}\\\\} }\\sum_{t}^{T}\\zeta_{t}^{2}+\\lambda\\sum_{t=1}^{T}\\left[\\left(\\tau_{t}-\\tau_{t-1}\\right)-\\left(\\tau_{t-1}-\\tau_{t-2}\\right)\\right]^{2}\\)\n\n# unpacking\ninfl_cycle, infl_trend = sm.tsa.filters.hpfilter(df.infl)\n\ninfl_cycle\n\n1959-03-31    -1.206811\n1959-06-30     1.141499\n1959-09-30     1.550564\n1959-12-31    -0.909577\n1960-03-31     1.140149\n                ...    \n2008-09-30    -5.064733\n2008-12-31   -10.550048\n2009-03-31    -0.681429\n2009-06-30     1.883255\n2009-09-30     2.206560\nName: infl, Length: 203, dtype: float64\n\n\n\ntype(infl_cycle)\n\npandas.core.series.Series\n\n\n\ndf[\"trend\"] = infl_trend\n\ndf[['trend','infl']].plot(figsize = (12, 8))\n\ndf[['trend','infl']][\"2000-03-31\":].plot(figsize = (12, 8))\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7fec38616be0>"
  },
  {
    "objectID": "posts/2020-09-22-analyzingusunemployment.html",
    "href": "posts/2020-09-22-analyzingusunemployment.html",
    "title": "Analyzing US Unemployment From 1959 - 2009 with statsmodels",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n%matplotlib inline\n\ndf = sm.datasets.macrodata.load_pandas().data\n\nprint(sm.datasets.macrodata.NOTE)\n\n::\n    Number of Observations - 203\n\n    Number of Variables - 14\n\n    Variable name definitions::\n\n        year      - 1959q1 - 2009q3\n        quarter   - 1-4\n        realgdp   - Real gross domestic product (Bil. of chained 2005 US$,\n                    seasonally adjusted annual rate)\n        realcons  - Real personal consumption expenditures (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realinv   - Real gross private domestic investment (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realgovt  - Real federal consumption expenditures & gross investment\n                    (Bil. of chained 2005 US$, seasonally adjusted annual rate)\n        realdpi   - Real private disposable income (Bil. of chained 2005\n                    US$, seasonally adjusted annual rate)\n        cpi       - End of the quarter consumer price index for all urban\n                    consumers: all items (1982-84 = 100, seasonally adjusted).\n        m1        - End of the quarter M1 nominal money stock (Seasonally\n                    adjusted)\n        tbilrate  - Quarterly monthly average of the monthly 3-month\n                    treasury bill: secondary market rate\n        unemp     - Seasonally adjusted unemployment rate (%)\n        pop       - End of the quarter total population: all ages incl. armed\n                    forces over seas\n        infl      - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400)\n        realint   - Real interest rate (tbilrate - infl)\n\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      quarter\n      realgdp\n      realcons\n      realinv\n      realgovt\n      realdpi\n      cpi\n      m1\n      tbilrate\n      unemp\n      pop\n      infl\n      realint\n    \n  \n  \n    \n      0\n      1959.0\n      1.0\n      2710.349\n      1707.4\n      286.898\n      470.045\n      1886.9\n      28.98\n      139.7\n      2.82\n      5.8\n      177.146\n      0.00\n      0.00\n    \n    \n      1\n      1959.0\n      2.0\n      2778.801\n      1733.7\n      310.859\n      481.301\n      1919.7\n      29.15\n      141.7\n      3.08\n      5.1\n      177.830\n      2.34\n      0.74\n    \n    \n      2\n      1959.0\n      3.0\n      2775.488\n      1751.8\n      289.226\n      491.260\n      1916.4\n      29.35\n      140.5\n      3.82\n      5.3\n      178.657\n      2.74\n      1.09\n    \n    \n      3\n      1959.0\n      4.0\n      2785.204\n      1753.7\n      299.356\n      484.052\n      1931.3\n      29.37\n      140.0\n      4.33\n      5.6\n      179.386\n      0.27\n      4.06\n    \n    \n      4\n      1960.0\n      1.0\n      2847.699\n      1770.5\n      331.722\n      462.199\n      1955.5\n      29.54\n      139.6\n      3.50\n      5.2\n      180.007\n      2.31\n      1.19\n    \n  \n\n\n\n\n\nindex = pd.Index(sm.tsa.datetools.dates_from_range('1959Q1', '2009Q3'))\ndf.index = index\ndf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      quarter\n      realgdp\n      realcons\n      realinv\n      realgovt\n      realdpi\n      cpi\n      m1\n      tbilrate\n      unemp\n      pop\n      infl\n      realint\n    \n  \n  \n    \n      1959-03-31\n      1959.0\n      1.0\n      2710.349\n      1707.4\n      286.898\n      470.045\n      1886.9\n      28.98\n      139.7\n      2.82\n      5.8\n      177.146\n      0.00\n      0.00\n    \n    \n      1959-06-30\n      1959.0\n      2.0\n      2778.801\n      1733.7\n      310.859\n      481.301\n      1919.7\n      29.15\n      141.7\n      3.08\n      5.1\n      177.830\n      2.34\n      0.74\n    \n    \n      1959-09-30\n      1959.0\n      3.0\n      2775.488\n      1751.8\n      289.226\n      491.260\n      1916.4\n      29.35\n      140.5\n      3.82\n      5.3\n      178.657\n      2.74\n      1.09\n    \n    \n      1959-12-31\n      1959.0\n      4.0\n      2785.204\n      1753.7\n      299.356\n      484.052\n      1931.3\n      29.37\n      140.0\n      4.33\n      5.6\n      179.386\n      0.27\n      4.06\n    \n    \n      1960-03-31\n      1960.0\n      1.0\n      2847.699\n      1770.5\n      331.722\n      462.199\n      1955.5\n      29.54\n      139.6\n      3.50\n      5.2\n      180.007\n      2.31\n      1.19\n    \n  \n\n\n\n\n\ndf['unemp'].plot()\nplt.ylabel(\"unemp\")\n\nText(0, 0.5, 'unemp')\n\n\n\n\n\n\\(\\min_{\\\\{ \\tau_{t}\\\\} }\\sum_{t}^{T}\\zeta_{t}^{2}+\\lambda\\sum_{t=1}^{T}\\left[\\left(\\tau_{t}-\\tau_{t-1}\\right)-\\left(\\tau_{t-1}-\\tau_{t-2}\\right)\\right]^{2}\\)\n\n# unpacking\nunemp_cycle, unemp_trend = sm.tsa.filters.hpfilter(df.unemp)\n\nunemp_cycle\n\n1959-03-31    0.011338\n1959-06-30   -0.702548\n1959-09-30   -0.516441\n1959-12-31   -0.229910\n1960-03-31   -0.642198\n                ...   \n2008-09-30   -0.481666\n2008-12-31    0.198598\n2009-03-31    1.171440\n2009-06-30    2.040247\n2009-09-30    2.207674\nName: unemp, Length: 203, dtype: float64\n\n\n\ntype(unemp_cycle)\n\npandas.core.series.Series\n\n\n\ndf[\"trend\"] = unemp_trend\n\ndf[['trend','unemp']].plot(figsize = (12, 8))\n\ndf[['trend','unemp']][\"2000-03-31\":].plot(figsize = (12, 8))\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7fdb5c2ecdd8>"
  },
  {
    "objectID": "posts/2020-08-20-pyspark-dataframes-data-types.html",
    "href": "posts/2020-08-20-pyspark-dataframes-data-types.html",
    "title": "Dataframes, Formatting, Casting Data Type and Correlation with Pyspark",
    "section": "",
    "text": "from pyspark.sql import SparkSession\n\n# Load data from a CSV\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").option(\"inferSchema\", True).option(\"header\", True).load(file_location)\ndisplay(df.take(5))\n\n\n_c0provincespecificgeneralyeargdpfdirnrrrifrregit0Anhui147002.0null19962093.3506610.00.00.01128873East China6319301Anhui151981.0null19972347.32434430.00.00.01356287East China6578602Anhui174930.0null19982542.96276730.00.00.01518236East China8894633Anhui285324.0null19992712.3426131nullnullnull1646891East China12273644Anhui195580.032100.020002902.09318470.00.00.01601508East China1499110"
  },
  {
    "objectID": "posts/2020-08-20-pyspark-dataframes-data-types.html#casting-data-types-and-formatting-significant-digits",
    "href": "posts/2020-08-20-pyspark-dataframes-data-types.html#casting-data-types-and-formatting-significant-digits",
    "title": "Dataframes, Formatting, Casting Data Type and Correlation with Pyspark",
    "section": "Casting Data Types and Formatting Significant Digits",
    "text": "Casting Data Types and Formatting Significant Digits\n\nfrom pyspark.sql.functions import format_number\n\n\n\n\n\n\nresult = df.describe()\nresult.select(result['province']\n,format_number(result['specific'].cast('float'),2).alias('specific')\n,format_number(result['general'].cast('float'),2).alias('general')\n,format_number(result['year'].cast('int'),2).alias('year'),format_number(result['gdp'].cast('float'),2).alias('gdp')\n,format_number(result['rnr'].cast('int'),2).alias('rnr'),format_number(result['rr'].cast('float'),2).alias('rr')\n,format_number(result['fdi'].cast('int'),2).alias('fdi'),format_number(result['it'].cast('float'),2).alias('it')\n,result['reg'].cast('string').alias('reg')\n             ).show()\n\n\n+--------+------------+------------+--------+---------+------+------+------------+-------------+---------------+\nprovince|    specific|     general|    year|      gdp|   rnr|    rr|         fdi|           it|            reg|\n+--------+------------+------------+--------+---------+------+------+------------+-------------+---------------+\n     360|      356.00|      169.00|  360.00|   360.00|294.00|296.00|      360.00|       360.00|            360|\n    null|  583,470.75|  309,127.53|2,001.00| 4,428.65|  0.00|  0.06|  196,139.00| 2,165,819.25|           null|\n    null|  654,055.31|  355,423.56|    3.00| 4,484.67|  0.00|  0.16|  303,043.00| 1,769,294.25|           null|\n   Anhui|    8,964.00|        0.00|1,996.00|    64.98|  0.00|  0.00|        2.00|   147,897.00|     East China|\nZhejiang|3,937,966.00|1,737,800.00|2,007.00|31,777.01|  1.00|  0.84|1,743,140.00|10,533,312.00|Southwest China|\n+--------+------------+------------+--------+---------+------+------+------------+-------------+---------------+"
  },
  {
    "objectID": "posts/2020-08-20-pyspark-dataframes-data-types.html#new-columns-generated-from-extant-columns-using-withcolumn",
    "href": "posts/2020-08-20-pyspark-dataframes-data-types.html#new-columns-generated-from-extant-columns-using-withcolumn",
    "title": "Dataframes, Formatting, Casting Data Type and Correlation with Pyspark",
    "section": "New Columns generated from extant columns using withColumn",
    "text": "New Columns generated from extant columns using withColumn\n\ndf2 = df.withColumn(\"specific_gdp_ratio\",df[\"specific\"]/(df[\"gdp\"]*100))#.show()\n\n\n\n\n\n\ndf2.select('specific_gdp_ratio').show()\n\n\n+------------------+\nspecific_gdp_ratio|\n+------------------+\n0.7022500358285959|\n0.6474660463848132|\n0.6878991411583352|\n1.0519477646607727|\n 0.673928100093381|\n0.7727761333780966|\n 1.233475958314866|\n1.5783421826051272|\n1.8877587040110941|\n1.6792756118029895|\n2.3850666666666664|\n3.0077639751552794|\n0.9275486250838364|\n0.7989880072601573|\n1.0314658544998698|\n 1.448708759827088|\n0.8912058855158366|\n1.1918224576316896|\n1.2944820393974508|\n 1.283311464867661|\n+------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.orderBy(df[\"specific\"].asc()).head(1)[0][0]\n\n\nOut[94]: 24"
  },
  {
    "objectID": "posts/2020-08-20-pyspark-dataframes-data-types.html#finding-the-mean-max-and-min",
    "href": "posts/2020-08-20-pyspark-dataframes-data-types.html#finding-the-mean-max-and-min",
    "title": "Dataframes, Formatting, Casting Data Type and Correlation with Pyspark",
    "section": "Finding the Mean, Max, and Min",
    "text": "Finding the Mean, Max, and Min\n\nfrom pyspark.sql.functions import mean\ndf.select(mean(\"specific\")).show()\n\n\n+-----------------+\n    avg(specific)|\n+-----------------+\n583470.7303370787|\n+-----------------+\n\n\n\n\n\nfrom pyspark.sql.functions import max,min\n\n\n\n\n\n\ndf.select(max(\"specific\"),min(\"specific\")).show()\n\n\n+-------------+-------------+\nmax(specific)|min(specific)|\n+-------------+-------------+\n    3937966.0|       8964.0|\n+-------------+-------------+\n\n\n\n\n\ndf.filter(\"specific < 60000\").count()\n\n\nOut[98]: 23\n\n\n\ndf.filter(df['specific'] < 60000).count()\n\n\nOut[99]: 23\n\n\n\nfrom pyspark.sql.functions import count\nresult = df.filter(df['specific'] < 60000)\nresult.select(count('specific')).show()\n\n\n+---------------+\ncount(specific)|\n+---------------+\n             23|\n+---------------+\n\n\n\n\n\n(df.filter(df[\"gdp\"]>8000).count()*1.0/df.count())*100\n\n\nOut[101]: 14.444444444444443\n\n\n\nfrom pyspark.sql.functions import corr\ndf.select(corr(\"gdp\",\"fdi\")).show()\n\n\n+------------------+\n    corr(gdp, fdi)|\n+------------------+\n0.8366328478935896|\n+------------------+"
  },
  {
    "objectID": "posts/2020-08-20-pyspark-dataframes-data-types.html#finding-the-max-value-by-year",
    "href": "posts/2020-08-20-pyspark-dataframes-data-types.html#finding-the-max-value-by-year",
    "title": "Dataframes, Formatting, Casting Data Type and Correlation with Pyspark",
    "section": "Finding the max value by Year",
    "text": "Finding the max value by Year\n\nfrom pyspark.sql.functions import year\n#yeardf = df.withColumn(\"Year\",year(df[\"year\"]))\n\n\n\n\n\n\nmax_df = df.groupBy('year').max()\n\n\n\n\n\n\nmax_df.select('year','max(gdp)').show()\n\n\n+----+--------+\nyear|max(gdp)|\n+----+--------+\n2003|15844.64|\n2007|31777.01|\n2006|26587.76|\n1997| 7774.53|\n2004|18864.62|\n1996| 6834.97|\n1998| 8530.88|\n2001|12039.25|\n2005|22557.37|\n2000|10741.25|\n1999| 9250.68|\n2002|13502.42|\n+----+--------+\n\n\n\n\n\nfrom pyspark.sql.functions import month\n\n\n\n\n\n\n#df.select(\"year\",\"avg(gdp)\").orderBy('year').show()\n\n\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-09-25-analyzingsizeofarmedforces.html",
    "href": "posts/2020-09-25-analyzingsizeofarmedforces.html",
    "title": "Analyzing Size of Armed Forces From 1947 - 1963 with statsmodels",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n%matplotlib inline\n\n\ndf = sm.datasets.longley.load_pandas().data\n\n#print(sm.datasets.longley.NOTE)\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      TOTEMP\n      GNPDEFL\n      GNP\n      UNEMP\n      ARMED\n      POP\n      YEAR\n    \n  \n  \n    \n      0\n      60323.0\n      83.0\n      234289.0\n      2356.0\n      1590.0\n      107608.0\n      1947.0\n    \n    \n      1\n      61122.0\n      88.5\n      259426.0\n      2325.0\n      1456.0\n      108632.0\n      1948.0\n    \n    \n      2\n      60171.0\n      88.2\n      258054.0\n      3682.0\n      1616.0\n      109773.0\n      1949.0\n    \n    \n      3\n      61187.0\n      89.5\n      284599.0\n      3351.0\n      1650.0\n      110929.0\n      1950.0\n    \n    \n      4\n      63221.0\n      96.2\n      328975.0\n      2099.0\n      3099.0\n      112075.0\n      1951.0\n    \n  \n\n\n\n\n\nindex = pd.Index(sm.tsa.datetools.dates_from_range('1947', '1962'))\ndf.index = index\ndf.head()\n\n\n\n\n\n  \n    \n      \n      TOTEMP\n      GNPDEFL\n      GNP\n      UNEMP\n      ARMED\n      POP\n      YEAR\n    \n  \n  \n    \n      1947-12-31\n      60323.0\n      83.0\n      234289.0\n      2356.0\n      1590.0\n      107608.0\n      1947.0\n    \n    \n      1948-12-31\n      61122.0\n      88.5\n      259426.0\n      2325.0\n      1456.0\n      108632.0\n      1948.0\n    \n    \n      1949-12-31\n      60171.0\n      88.2\n      258054.0\n      3682.0\n      1616.0\n      109773.0\n      1949.0\n    \n    \n      1950-12-31\n      61187.0\n      89.5\n      284599.0\n      3351.0\n      1650.0\n      110929.0\n      1950.0\n    \n    \n      1951-12-31\n      63221.0\n      96.2\n      328975.0\n      2099.0\n      3099.0\n      112075.0\n      1951.0\n    \n  \n\n\n\n\n\ndf['ARMED'].plot()\nplt.ylabel(\"ARMED\")\n\nText(0, 0.5, 'ARMED')\n\n\n\n\n\n\n# unpacking\ncycle, trend = sm.tsa.filters.hpfilter(df.ARMED)\n\ncycle\n\n1947-12-31    -497.642333\n1948-12-31    -713.661033\n1949-12-31    -635.368706\n1950-12-31    -682.008289\n1951-12-31     688.574390\n1952-12-31    1108.959755\n1953-12-31     992.297873\n1954-12-31     731.045710\n1955-12-31     370.040046\n1956-12-31     124.660757\n1957-12-31      15.056446\n1958-12-31    -193.702199\n1959-12-31    -324.553899\n1960-12-31    -407.316313\n1961-12-31    -393.604252\n1962-12-31    -182.777954\nName: ARMED, dtype: float64\n\n\n\ntype(cycle)\n\npandas.core.series.Series\n\n\n\ndf[\"trend\"] = trend\n\ndf[['trend','ARMED']].plot(figsize = (12, 8))\n\ndf[['trend','ARMED']][\"1950-01-01\":\"1955-01-01\"].plot(figsize = (12, 8))\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f2d5af13940>"
  },
  {
    "objectID": "posts/2020-09-24-analyzingusrealinterestrate.html",
    "href": "posts/2020-09-24-analyzingusrealinterestrate.html",
    "title": "Analyzing US Real Interest Rate From 1959 - 2009 with statsmodels",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n%matplotlib inline\n\ndf = sm.datasets.macrodata.load_pandas().data\n\nprint(sm.datasets.macrodata.NOTE)\n\n/home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n  import pandas.util.testing as tm\n\n\n::\n    Number of Observations - 203\n\n    Number of Variables - 14\n\n    Variable name definitions::\n\n        year      - 1959q1 - 2009q3\n        quarter   - 1-4\n        realgdp   - Real gross domestic product (Bil. of chained 2005 US$,\n                    seasonally adjusted annual rate)\n        realcons  - Real personal consumption expenditures (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realinv   - Real gross private domestic investment (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realgovt  - Real federal consumption expenditures & gross investment\n                    (Bil. of chained 2005 US$, seasonally adjusted annual rate)\n        realdpi   - Real private disposable income (Bil. of chained 2005\n                    US$, seasonally adjusted annual rate)\n        cpi       - End of the quarter consumer price index for all urban\n                    consumers: all items (1982-84 = 100, seasonally adjusted).\n        m1        - End of the quarter M1 nominal money stock (Seasonally\n                    adjusted)\n        tbilrate  - Quarterly monthly average of the monthly 3-month\n                    treasury bill: secondary market rate\n        unemp     - Seasonally adjusted unemployment rate (%)\n        pop       - End of the quarter total population: all ages incl. armed\n                    forces over seas\n        infl      - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400)\n        realint   - Real interest rate (tbilrate - infl)\n\n\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      quarter\n      realgdp\n      realcons\n      realinv\n      realgovt\n      realdpi\n      cpi\n      m1\n      tbilrate\n      unemp\n      pop\n      infl\n      realint\n    \n  \n  \n    \n      0\n      1959.0\n      1.0\n      2710.349\n      1707.4\n      286.898\n      470.045\n      1886.9\n      28.98\n      139.7\n      2.82\n      5.8\n      177.146\n      0.00\n      0.00\n    \n    \n      1\n      1959.0\n      2.0\n      2778.801\n      1733.7\n      310.859\n      481.301\n      1919.7\n      29.15\n      141.7\n      3.08\n      5.1\n      177.830\n      2.34\n      0.74\n    \n    \n      2\n      1959.0\n      3.0\n      2775.488\n      1751.8\n      289.226\n      491.260\n      1916.4\n      29.35\n      140.5\n      3.82\n      5.3\n      178.657\n      2.74\n      1.09\n    \n    \n      3\n      1959.0\n      4.0\n      2785.204\n      1753.7\n      299.356\n      484.052\n      1931.3\n      29.37\n      140.0\n      4.33\n      5.6\n      179.386\n      0.27\n      4.06\n    \n    \n      4\n      1960.0\n      1.0\n      2847.699\n      1770.5\n      331.722\n      462.199\n      1955.5\n      29.54\n      139.6\n      3.50\n      5.2\n      180.007\n      2.31\n      1.19\n    \n  \n\n\n\n\n\nindex = pd.Index(sm.tsa.datetools.dates_from_range('1959Q1', '2009Q3'))\ndf.index = index\ndf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      quarter\n      realgdp\n      realcons\n      realinv\n      realgovt\n      realdpi\n      cpi\n      m1\n      tbilrate\n      unemp\n      pop\n      infl\n      realint\n    \n  \n  \n    \n      1959-03-31\n      1959.0\n      1.0\n      2710.349\n      1707.4\n      286.898\n      470.045\n      1886.9\n      28.98\n      139.7\n      2.82\n      5.8\n      177.146\n      0.00\n      0.00\n    \n    \n      1959-06-30\n      1959.0\n      2.0\n      2778.801\n      1733.7\n      310.859\n      481.301\n      1919.7\n      29.15\n      141.7\n      3.08\n      5.1\n      177.830\n      2.34\n      0.74\n    \n    \n      1959-09-30\n      1959.0\n      3.0\n      2775.488\n      1751.8\n      289.226\n      491.260\n      1916.4\n      29.35\n      140.5\n      3.82\n      5.3\n      178.657\n      2.74\n      1.09\n    \n    \n      1959-12-31\n      1959.0\n      4.0\n      2785.204\n      1753.7\n      299.356\n      484.052\n      1931.3\n      29.37\n      140.0\n      4.33\n      5.6\n      179.386\n      0.27\n      4.06\n    \n    \n      1960-03-31\n      1960.0\n      1.0\n      2847.699\n      1770.5\n      331.722\n      462.199\n      1955.5\n      29.54\n      139.6\n      3.50\n      5.2\n      180.007\n      2.31\n      1.19\n    \n  \n\n\n\n\n\ndf['realint'].plot()\nplt.ylabel(\"realint\")\n\nText(0, 0.5, 'realint')\n\n\n\n\n\n\\(\\min_{\\\\{ \\tau_{t}\\\\} }\\sum_{t}^{T}\\zeta_{t}^{2}+\\lambda\\sum_{t=1}^{T}\\left[\\left(\\tau_{t}-\\tau_{t-1}\\right)-\\left(\\tau_{t-1}-\\tau_{t-2}\\right)\\right]^{2}\\)\n\n# unpacking\ncycle, trend = sm.tsa.filters.hpfilter(df.realint)\n\ncycle\n\n1959-03-31   -1.195751\n1959-06-30   -0.505792\n1959-09-30   -0.205086\n1959-12-31    2.717430\n1960-03-31   -0.197051\n                ...   \n2008-09-30    4.330269\n2008-12-31    8.961987\n2009-03-31   -0.596183\n2009-06-30   -3.008487\n2009-09-30   -3.188797\nName: realint, Length: 203, dtype: float64\n\n\n\ntype(cycle)\n\npandas.core.series.Series\n\n\n\ndf[\"trend\"] = trend\n\ndf[['trend','realint']].plot(figsize = (12, 8))\n\ndf[['trend','realint']][\"2000-03-31\":].plot(figsize = (12, 8))\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7fd2a8100438>"
  },
  {
    "objectID": "posts/2020-09-21-stockmarketportfolioanaylsis2.html",
    "href": "posts/2020-09-21-stockmarketportfolioanaylsis2.html",
    "title": "Stock Market and Optimal Portfolio Anaylsis scipy and quandl",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport quandl\n%matplotlib inline\n\n\nstart = pd.to_datetime('2010-01-01')\nend = pd.to_datetime('today')\n\n\n# Grabbing a bunch of tech stocks for our portfolio\nCOST = quandl.get('WIKI/COST.11',\n                  start_date = start,\n                  end_date = end)\nNLSN = quandl.get('WIKI/NLSN.11',\n                   start_date = start,\n                   end_date = end)\nNKE = quandl.get('WIKI/NKE.11',\n                 start_date = start,\n                 end_date = end)\nDIS = quandl.get('WIKI/DIS.11',\n                  start_date = start,\n                  end_date = end)\n\n\nstocks = pd.concat([COST, NLSN, NKE, DIS],\n                   axis = 1)\nstocks.columns = ['COST','NLSN','NKE','DIS']\n\n\nstocks\n\n\n\n\n\n  \n    \n      \n      COST\n      NLSN\n      NKE\n      DIS\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2010-01-04\n      49.085078\n      NaN\n      14.751122\n      28.960651\n    \n    \n      2010-01-05\n      48.936361\n      NaN\n      14.809811\n      28.888407\n    \n    \n      2010-01-06\n      49.572542\n      NaN\n      14.719521\n      28.734890\n    \n    \n      2010-01-07\n      49.332941\n      NaN\n      14.863985\n      28.743920\n    \n    \n      2010-01-08\n      48.977671\n      NaN\n      14.834641\n      28.789072\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      2018-03-21\n      186.070000\n      32.44\n      66.350000\n      101.820000\n    \n    \n      2018-03-22\n      182.640000\n      31.82\n      64.420000\n      100.600000\n    \n    \n      2018-03-23\n      180.840000\n      31.51\n      64.630000\n      98.540000\n    \n    \n      2018-03-26\n      187.220000\n      32.03\n      65.900000\n      100.650000\n    \n    \n      2018-03-27\n      183.150000\n      32.09\n      66.170000\n      99.360000\n    \n  \n\n2071 rows × 4 columns\n\n\n\n\nmean_daily_ret = stocks.pct_change(1).mean()\nmean_daily_ret\n\nCOST    0.000699\nNLSN    0.000312\nNKE     0.000833\nDIS     0.000683\ndtype: float64\n\n\n\nstocks.pct_change(1).corr()\n\n\n\n\n\n  \n    \n      \n      COST\n      NLSN\n      NKE\n      DIS\n    \n  \n  \n    \n      COST\n      1.000000\n      0.265003\n      0.370978\n      0.415377\n    \n    \n      NLSN\n      0.265003\n      1.000000\n      0.312192\n      0.392808\n    \n    \n      NKE\n      0.370978\n      0.312192\n      1.000000\n      0.446150\n    \n    \n      DIS\n      0.415377\n      0.392808\n      0.446150\n      1.000000\n    \n  \n\n\n\n\n\nstocks.head()\n\n\n\n\n\n  \n    \n      \n      COST\n      NLSN\n      NKE\n      DIS\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2010-01-04\n      49.085078\n      NaN\n      14.751122\n      28.960651\n    \n    \n      2010-01-05\n      48.936361\n      NaN\n      14.809811\n      28.888407\n    \n    \n      2010-01-06\n      49.572542\n      NaN\n      14.719521\n      28.734890\n    \n    \n      2010-01-07\n      49.332941\n      NaN\n      14.863985\n      28.743920\n    \n    \n      2010-01-08\n      48.977671\n      NaN\n      14.834641\n      28.789072\n    \n  \n\n\n\n\n\nstock_normed = stocks/stocks.iloc[0]\nstock_normed.plot()\n\n<AxesSubplot:xlabel='Date'>\n\n\n\n\n\n\nstock_daily_ret = stocks.pct_change(1)\nstock_daily_ret.head()\n\n\n\n\n\n  \n    \n      \n      COST\n      NLSN\n      NKE\n      DIS\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2010-01-04\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2010-01-05\n      -0.003030\n      NaN\n      0.003979\n      -0.002495\n    \n    \n      2010-01-06\n      0.013000\n      NaN\n      -0.006097\n      -0.005314\n    \n    \n      2010-01-07\n      -0.004833\n      NaN\n      0.009814\n      0.000314\n    \n    \n      2010-01-08\n      -0.007201\n      NaN\n      -0.001974\n      0.001571\n    \n  \n\n\n\n\n\nlog_ret = np.log(stocks / stocks.shift(1))\nlog_ret.head()\n\n\n\n\n\n  \n    \n      \n      COST\n      NLSN\n      NKE\n      DIS\n    \n    \n      Date\n      \n      \n      \n      \n    \n  \n  \n    \n      2010-01-04\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2010-01-05\n      -0.003034\n      NaN\n      0.003971\n      -0.002498\n    \n    \n      2010-01-06\n      0.012916\n      NaN\n      -0.006115\n      -0.005328\n    \n    \n      2010-01-07\n      -0.004845\n      NaN\n      0.009767\n      0.000314\n    \n    \n      2010-01-08\n      -0.007228\n      NaN\n      -0.001976\n      0.001570\n    \n  \n\n\n\n\n\nlog_ret.hist(bins = 100,\n             figsize = (12, 6));\nplt.tight_layout()\n\n\n\n\n\nlog_ret.describe().transpose()\n\n\n\n\n\n  \n    \n      \n      count\n      mean\n      std\n      min\n      25%\n      50%\n      75%\n      max\n    \n  \n  \n    \n      COST\n      2068.0\n      0.000633\n      0.011172\n      -0.083110\n      -0.005293\n      0.000413\n      0.006618\n      0.060996\n    \n    \n      NLSN\n      1801.0\n      0.000198\n      0.015121\n      -0.185056\n      -0.007131\n      0.000000\n      0.008051\n      0.095201\n    \n    \n      NKE\n      2070.0\n      0.000725\n      0.014682\n      -0.098743\n      -0.006602\n      0.000656\n      0.008155\n      0.115342\n    \n    \n      DIS\n      2070.0\n      0.000596\n      0.013220\n      -0.096190\n      -0.005710\n      0.000776\n      0.007453\n      0.073531\n    \n  \n\n\n\n\n\nlog_ret.mean() * 252\n\nCOST    0.159439\nNLSN    0.049979\nNKE     0.182719\nDIS     0.150081\ndtype: float64\n\n\n\n# Compute pairwise covariance of columns\nlog_ret.cov()\n\n\n\n\n\n  \n    \n      \n      COST\n      NLSN\n      NKE\n      DIS\n    \n  \n  \n    \n      COST\n      0.000125\n      0.000045\n      0.000061\n      0.000061\n    \n    \n      NLSN\n      0.000045\n      0.000229\n      0.000070\n      0.000077\n    \n    \n      NKE\n      0.000061\n      0.000070\n      0.000216\n      0.000087\n    \n    \n      DIS\n      0.000061\n      0.000077\n      0.000087\n      0.000175\n    \n  \n\n\n\n\n\n# Set seed (optional)\nnp.random.seed(101)\n\n# Stock Columns\nprint('Stocks')\nprint(stocks.columns)\nprint('\\n')\n\n# Create Random Weights\nprint('Creating Random Weights')\nweights = np.array(np.random.random(4))\nprint(weights)\nprint('\\n')\n\n# Rebalance Weights\nprint('Rebalance to sum to 1.0')\nweights = weights / np.sum(weights)\nprint(weights)\nprint('\\n')\n\n# Expected Return\nprint('Expected Portfolio Return')\nexp_ret = np.sum(log_ret.mean() * weights) *252\nprint(exp_ret)\nprint('\\n')\n\n# Expected Variance\nprint('Expected Volatility')\nexp_vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\nprint(exp_vol)\nprint('\\n')\n\n# Sharpe Ratio\nSR = exp_ret/exp_vol\nprint('Sharpe Ratio')\nprint(SR)\n\nStocks\nIndex(['COST', 'NLSN', 'NKE', 'DIS'], dtype='object')\n\n\nCreating Random Weights\n[0.51639863 0.57066759 0.02847423 0.17152166]\n\n\nRebalance to sum to 1.0\n[0.40122278 0.44338777 0.02212343 0.13326603]\n\n\nExpected Portfolio Return\n0.11017373023155777\n\n\nExpected Volatility\n0.16110487214223854\n\n\nSharpe Ratio\n0.6838634286260817\n\n\n\nnum_ports = 15000\n\nall_weights = np.zeros((num_ports, len(stocks.columns)))\nret_arr = np.zeros(num_ports)\nvol_arr = np.zeros(num_ports)\nsharpe_arr = np.zeros(num_ports)\n\nfor ind in range(num_ports):\n\n    # Create Random Weights\n    weights = np.array(np.random.random(4))\n\n    # Rebalance Weights\n    weights = weights / np.sum(weights)\n    \n    # Save Weights\n    all_weights[ind,:] = weights\n\n    # Expected Return\n    ret_arr[ind] = np.sum((log_ret.mean() * weights) *252)\n\n    # Expected Variance\n    vol_arr[ind] = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n\n    # Sharpe Ratio\n    sharpe_arr[ind] = ret_arr[ind] / vol_arr[ind]\n\n\nsharpe_arr.max()\n\n1.042687299617254\n\n\n\nsharpe_arr.argmax()\n\n10619\n\n\n\nall_weights[10619,:]\n\narray([5.06395348e-01, 4.67772019e-04, 2.64242193e-01, 2.28894687e-01])\n\n\n\nmax_sr_ret = ret_arr[1419]\nmax_sr_vol = vol_arr[1419]\n\n\nplt.figure(figsize = (12, 8))\nplt.scatter(vol_arr,\n            ret_arr,\n            c = sharpe_arr,\n            cmap = 'plasma')\nplt.colorbar(label = 'Sharpe Ratio')\nplt.xlabel('Volatility')\nplt.ylabel('Return')\n\n# Add red dot for max SR\nplt.scatter(max_sr_vol,\n            max_sr_ret,\n            c = 'red',\n            s = 50,\n            edgecolors = 'black')\n\n<matplotlib.collections.PathCollection at 0x7f703b26fd00>\n\n\n\n\n\n\ndef get_ret_vol_sr(weights):\n    \"\"\"\n    Takes in weights, returns array or return,volatility, sharpe ratio\n    \"\"\"\n    weights = np.array(weights)\n    ret = np.sum(log_ret.mean() * weights) * 252\n    vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n    sr = ret/vol\n    return np.array([ret, vol, sr])\n\nfrom scipy.optimize import minimize\nimport numpy as np\n\ndef neg_sharpe(weights):\n    return  get_ret_vol_sr(weights)[2] * -1\n\n# Contraints\ndef check_sum(weights):\n    '''\n    Returns 0 if sum of weights is 1.0\n    '''\n    return np.sum(weights) - 1\n\n# By convention of minimize function it should be a function that returns zero for conditions\ncons = ({'type' : 'eq', 'fun': check_sum})\n\n# 0-1 bounds for each weight\nbounds = ((0, 1), (0, 1), (0, 1), (0, 1))\n\n# Initial Guess (equal distribution)\ninit_guess = [0.25, 0.25, 0.25, 0.25]\n\n# Sequential Least Squares \nopt_results = minimize(neg_sharpe,\n                       init_guess,\n                       method = 'SLSQP',\n                       bounds = bounds,\n                       constraints = cons)\n\nopt_results\n\n     fun: -1.0442236428192482\n     jac: array([-1.85623765e-04,  3.00063133e-01,  3.43203545e-04,  1.72853470e-05])\n message: 'Optimization terminated successfully'\n    nfev: 20\n     nit: 4\n    njev: 4\n  status: 0\n success: True\n       x: array([0.53438392, 0.        , 0.27969302, 0.18592306])\n\n\n\nopt_results.x\n\nget_ret_vol_sr(opt_results.x)\n\narray([0.16421049, 0.15725605, 1.04422364])\n\n\n\nfrontier_y = np.linspace(0, 0.3, 100)\n\n\ndef minimize_volatility(weights):\n    return  get_ret_vol_sr(weights)[1] \n\nfrontier_volatility = []\n\nfor possible_return in frontier_y:\n    # function for return\n    cons = ({'type':'eq','fun': check_sum},\n            {'type':'eq','fun': lambda w: get_ret_vol_sr(w)[0] - possible_return})\n    \n    result = minimize(minimize_volatility,\n                      init_guess,\n                      method = 'SLSQP',\n                      bounds = bounds,\n                      constraints = cons)\n    \n    frontier_volatility.append(result['fun'])\n\n\nplt.figure(figsize = (12, 8))\nplt.scatter(vol_arr,\n            ret_arr,\n            c = sharpe_arr,\n            cmap = 'plasma')\nplt.colorbar(label = 'Sharpe Ratio')\nplt.xlabel('Volatility')\nplt.ylabel('Return')\n\n\n\n# Add frontier line\nplt.plot(frontier_volatility,\n         frontier_y,\n         'g--',\n         linewidth = 3)"
  },
  {
    "objectID": "posts/2020-10-05-stockmarketportfolioanaylsis_snp_pandas_datareader_sqlite-copy1.html",
    "href": "posts/2020-10-05-stockmarketportfolioanaylsis_snp_pandas_datareader_sqlite-copy1.html",
    "title": "Using sqlalchemy and pandas to read and write from and to a local sqlite database for Stock Market and Portfolio Anaylsis Tech Stocks and the S&P 500 in 2020",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport pandas_datareader.data as web\nimport datetime\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndf = pd.read_csv('https://stocks-snp-500.herokuapp.com/stocks/stocks_table.csv?_size=max')\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      rowid\n      Date\n      MSFT_stock\n      ZOOM_stock\n      SNOW_stock\n      FXAIX_stock\n    \n  \n  \n    \n      0\n      1\n      2020-01-02 00:00:00.000000\n      158.779999\n      68.800003\n      NaN\n      112.980003\n    \n    \n      1\n      2\n      2020-01-03 00:00:00.000000\n      158.320007\n      67.620003\n      NaN\n      112.190002\n    \n    \n      2\n      3\n      2020-01-06 00:00:00.000000\n      157.080002\n      66.629997\n      NaN\n      112.589996\n    \n    \n      3\n      4\n      2020-01-07 00:00:00.000000\n      159.320007\n      70.290001\n      NaN\n      112.290001\n    \n    \n      4\n      5\n      2020-01-08 00:00:00.000000\n      158.929993\n      71.809998\n      NaN\n      112.839996\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      258\n      259\n      2021-01-11 00:00:00.000000\n      218.470001\n      344.980011\n      295.000000\n      131.750000\n    \n    \n      259\n      260\n      2021-01-12 00:00:00.000000\n      216.500000\n      333.200012\n      298.000000\n      131.800003\n    \n    \n      260\n      261\n      2021-01-13 00:00:00.000000\n      214.020004\n      360.000000\n      295.000000\n      132.100006\n    \n    \n      261\n      262\n      2021-01-14 00:00:00.000000\n      215.910004\n      371.000000\n      305.000000\n      131.619995\n    \n    \n      262\n      263\n      2021-01-15 00:00:00.000000\n      213.520004\n      397.709991\n      306.820007\n      130.679993\n    \n  \n\n263 rows × 6 columns\n\n\n\n\nstart = pd.to_datetime('2020-01-01')\nend = pd.to_datetime('today')\n\n\nFXAIX_stock = web.DataReader('FXAIX', 'yahoo', start, end)\nFXAIX_stock.head()\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nSNOW_stock = web.DataReader('SNOW', 'yahoo', start, end)\nSNOW_stock.head()\n\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Open')\n\nMSFT_stock['Open'].plot(label='Microsoft')\nZOOM_stock['Open'].plot(label='Zoom')\nSNOW_stock['Open'].plot(label='Snowflake')\nFXAIX_stock['Open'].plot(label='SNP_500')\nplt.legend()\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Volume')\n\nMSFT_stock['Volume'].plot(label='Microsoft')\nZOOM_stock['Volume'].plot(label='Zoom')\nSNOW_stock['Volume'].plot(label='Snowflake')\nFXAIX_stock['Volume'].plot(label='SNP_500')\n\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f7f95b65f40>\n\n\n\n\n\n\n\n\n\nFXAIX_stock = web.DataReader('FXAIX', 'yahoo', start, end)\nFXAIX_stock.head()\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nSNOW_stock = web.DataReader('SNOW', 'yahoo', start, end)\nSNOW_stock.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-09-16\n      319.0\n      231.110001\n      245.000000\n      253.929993\n      36099700\n      253.929993\n    \n    \n      2020-09-17\n      241.5\n      215.240005\n      230.759995\n      227.539993\n      11907500\n      227.539993\n    \n    \n      2020-09-18\n      249.0\n      218.589996\n      235.000000\n      240.000000\n      7475400\n      240.000000\n    \n    \n      2020-09-21\n      241.5\n      218.600006\n      230.000000\n      228.850006\n      5524900\n      228.850006\n    \n    \n      2020-09-22\n      239.0\n      225.149994\n      238.500000\n      235.160004\n      3889100\n      235.160004\n    \n  \n\n\n\n\n\nstocks = pd.concat([MSFT_stock['Open'], ZOOM_stock['Open'], SNOW_stock['Open'], FXAIX_stock['Open']],\n                   axis = 1)\n\n\nstocks.reset_index(level=0, inplace=True)\n\n\nstocks\n\n\n\n\n\n  \n    \n      \n      Date\n      Open\n      Open\n      Open\n      Open\n    \n  \n  \n    \n      0\n      2020-01-02\n      158.779999\n      68.800003\n      NaN\n      112.980003\n    \n    \n      1\n      2020-01-03\n      158.320007\n      67.620003\n      NaN\n      112.190002\n    \n    \n      2\n      2020-01-06\n      157.080002\n      66.629997\n      NaN\n      112.589996\n    \n    \n      3\n      2020-01-07\n      159.320007\n      70.290001\n      NaN\n      112.290001\n    \n    \n      4\n      2020-01-08\n      158.929993\n      71.809998\n      NaN\n      112.839996\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      258\n      2021-01-11\n      218.470001\n      344.980011\n      295.000000\n      131.750000\n    \n    \n      259\n      2021-01-12\n      216.500000\n      333.200012\n      298.000000\n      131.800003\n    \n    \n      260\n      2021-01-13\n      214.020004\n      360.000000\n      295.000000\n      132.100006\n    \n    \n      261\n      2021-01-14\n      215.910004\n      371.000000\n      305.000000\n      131.619995\n    \n    \n      262\n      2021-01-15\n      213.520004\n      397.709991\n      306.820007\n      130.679993\n    \n  \n\n263 rows × 5 columns\n\n\n\n\nstocks.columns = ['Date','MSFT_stock','ZOOM_stock','SNOW_stock','FXAIX_stock']\n\n\nstocks\n\n\n\n\n\n  \n    \n      \n      Date\n      MSFT_stock\n      ZOOM_stock\n      SNOW_stock\n      FXAIX_stock\n    \n  \n  \n    \n      0\n      2020-01-02\n      158.779999\n      68.800003\n      NaN\n      112.980003\n    \n    \n      1\n      2020-01-03\n      158.320007\n      67.620003\n      NaN\n      112.190002\n    \n    \n      2\n      2020-01-06\n      157.080002\n      66.629997\n      NaN\n      112.589996\n    \n    \n      3\n      2020-01-07\n      159.320007\n      70.290001\n      NaN\n      112.290001\n    \n    \n      4\n      2020-01-08\n      158.929993\n      71.809998\n      NaN\n      112.839996\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      258\n      2021-01-11\n      218.470001\n      344.980011\n      295.000000\n      131.750000\n    \n    \n      259\n      2021-01-12\n      216.500000\n      333.200012\n      298.000000\n      131.800003\n    \n    \n      260\n      2021-01-13\n      214.020004\n      360.000000\n      295.000000\n      132.100006\n    \n    \n      261\n      2021-01-14\n      215.910004\n      371.000000\n      305.000000\n      131.619995\n    \n    \n      262\n      2021-01-15\n      213.520004\n      397.709991\n      306.820007\n      130.679993\n    \n  \n\n263 rows × 5 columns\n\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///stocks.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nstocks_table = db.Table('stocks_table', metadata, \n    db.Column('Date',db.Integer, nullable=True, index=False),\n    db.Column('MSFT_stock',db.Integer, nullable=True),\n    db.Column('ZOOM_stock',db.Integer, nullable=True),\n    db.Column('SNOW_stock',db.Integer, nullable=True),\n    db.Column('FXAIX_stock', db.Numeric, nullable=True)\n)\n\n\nmetadata.create_all(engine) #Creates the table\n\n\nstocks_table\n\nTable('stocks_table', MetaData(bind=None), Column('Date', Integer(), table=<stocks_table>), Column('MSFT_stock', Integer(), table=<stocks_table>), Column('ZOOM_stock', Integer(), table=<stocks_table>), Column('SNOW_stock', Integer(), table=<stocks_table>), Column('FXAIX_stock', Numeric(), table=<stocks_table>), schema=None)\n\n\n\nstocks.to_sql('stocks_table', con=engine, if_exists='append', index=False)\n\n\nengine.execute(\"SELECT * FROM stocks_table LIMIT 10\").fetchall()\n\n[('2020-01-02 00:00:00.000000', 158.77999877929688, 68.80000305175781, None, 112.9800033569336),\n ('2020-01-03 00:00:00.000000', 158.32000732421875, 67.62000274658203, None, 112.19000244140625),\n ('2020-01-06 00:00:00.000000', 157.0800018310547, 66.62999725341797, None, 112.58999633789062),\n ('2020-01-07 00:00:00.000000', 159.32000732421875, 70.29000091552734, None, 112.29000091552734),\n ('2020-01-08 00:00:00.000000', 158.92999267578125, 71.80999755859375, None, 112.83999633789062),\n ('2020-01-09 00:00:00.000000', 161.83999633789062, 73.98999786376953, None, 113.62000274658203),\n ('2020-01-10 00:00:00.000000', 162.82000732421875, 73.08000183105469, None, 113.30000305175781),\n ('2020-01-13 00:00:00.000000', 161.75999450683594, 73.88999938964844, None, 114.08999633789062),\n ('2020-01-14 00:00:00.000000', 163.38999938964844, 74.31999969482422, None, 113.93000030517578),\n ('2020-01-15 00:00:00.000000', 162.6199951171875, 73.27999877929688, None, 114.13999938964844)]\n\n\n\nsql = \"\"\"\nSELECT\n  DATE(date) AS DATE\n, FXAIX_stock\n, MSFT_stock\n, SNOW_stock\n, row_number() OVER (PARTITION BY Date, MSFT_stock ORDER BY Date) AS REC_NBR\n, COUNT(*) OVER (PARTITION BY Date, MSFT_stock ORDER BY Date) AS REC_CNT\n, CASE WHEN FXAIX_stock >= 120 THEN 'SNP_High' ELSE 'SNP_low' END AS SNP_HIGH_LOW\nFROM stocks_table\n--WHERE FXAIX_stock >= 120\n\"\"\"\n\ncnxn = connection\n\n\nstocks = pd.read_sql(sql, cnxn)\n\n\nstocks.tail(30)\n\n\n\n\n\n  \n    \n      \n      DATE\n      FXAIX_stock\n      MSFT_stock\n      SNOW_stock\n      REC_NBR\n      REC_CNT\n      SNP_HIGH_LOW\n    \n  \n  \n    \n      233\n      2020-12-03\n      127.540001\n      214.610001\n      290.540009\n      1\n      1\n      SNP_High\n    \n    \n      234\n      2020-12-04\n      128.669998\n      214.220001\n      335.399994\n      1\n      1\n      SNP_High\n    \n    \n      235\n      2020-12-07\n      128.419998\n      214.369995\n      393.500000\n      1\n      1\n      SNP_High\n    \n    \n      236\n      2020-12-08\n      128.789993\n      213.970001\n      388.799988\n      1\n      1\n      SNP_High\n    \n    \n      237\n      2020-12-09\n      127.769997\n      215.160004\n      393.399994\n      1\n      1\n      SNP_High\n    \n    \n      238\n      2020-12-10\n      127.610001\n      211.770004\n      362.000000\n      1\n      1\n      SNP_High\n    \n    \n      239\n      2020-12-11\n      126.870003\n      210.050003\n      360.399994\n      1\n      1\n      SNP_High\n    \n    \n      240\n      2020-12-14\n      126.339996\n      213.100006\n      352.489990\n      1\n      1\n      SNP_High\n    \n    \n      241\n      2020-12-15\n      127.970001\n      215.169998\n      308.980011\n      1\n      1\n      SNP_High\n    \n    \n      242\n      2020-12-16\n      128.199997\n      214.750000\n      328.429993\n      1\n      1\n      SNP_High\n    \n    \n      243\n      2020-12-17\n      128.940002\n      219.869995\n      333.820007\n      1\n      1\n      SNP_High\n    \n    \n      244\n      2020-12-18\n      128.500000\n      218.589996\n      332.769989\n      1\n      1\n      SNP_High\n    \n    \n      245\n      2020-12-21\n      128.000000\n      217.550003\n      329.000000\n      1\n      1\n      SNP_High\n    \n    \n      246\n      2020-12-22\n      127.750000\n      222.690002\n      349.890015\n      1\n      1\n      SNP_High\n    \n    \n      247\n      2020-12-23\n      127.839996\n      223.110001\n      341.160004\n      1\n      1\n      SNP_High\n    \n    \n      248\n      2020-12-24\n      128.309998\n      221.419998\n      334.100006\n      1\n      1\n      SNP_High\n    \n    \n      249\n      2020-12-28\n      129.429993\n      224.449997\n      324.869995\n      1\n      1\n      SNP_High\n    \n    \n      250\n      2020-12-29\n      129.139999\n      226.309998\n      305.250000\n      1\n      1\n      SNP_High\n    \n    \n      251\n      2020-12-30\n      129.330002\n      225.229996\n      304.000000\n      1\n      1\n      SNP_High\n    \n    \n      252\n      2020-12-31\n      130.169998\n      221.699997\n      299.700012\n      1\n      1\n      SNP_High\n    \n    \n      253\n      2021-01-04\n      128.259995\n      222.529999\n      285.410004\n      1\n      1\n      SNP_High\n    \n    \n      254\n      2021-01-05\n      129.179993\n      217.259995\n      280.619995\n      1\n      1\n      SNP_High\n    \n    \n      255\n      2021-01-06\n      129.919998\n      212.169998\n      279.989990\n      1\n      1\n      SNP_High\n    \n    \n      256\n      2021-01-07\n      131.880005\n      214.039993\n      272.589996\n      1\n      1\n      SNP_High\n    \n    \n      257\n      2021-01-08\n      132.619995\n      218.679993\n      315.000000\n      1\n      1\n      SNP_High\n    \n    \n      258\n      2021-01-11\n      131.750000\n      218.470001\n      295.000000\n      1\n      1\n      SNP_High\n    \n    \n      259\n      2021-01-12\n      131.800003\n      216.500000\n      298.000000\n      1\n      1\n      SNP_High\n    \n    \n      260\n      2021-01-13\n      132.100006\n      214.020004\n      295.000000\n      1\n      1\n      SNP_High\n    \n    \n      261\n      2021-01-14\n      131.619995\n      215.910004\n      305.000000\n      1\n      1\n      SNP_High\n    \n    \n      262\n      2021-01-15\n      130.679993\n      213.520004\n      306.820007\n      1\n      1\n      SNP_High\n    \n  \n\n\n\n\n\nstocks['FXAIX_stock'].plot(figsize = (12, 8))\nplt.title('Total S&P 500 in 2020 Value')\n\nText(0.5, 1.0, 'Total S&P 500 in 2020 Value')"
  },
  {
    "objectID": "posts/2020-08-15-pyspark-fiscal-data-regression.html",
    "href": "posts/2020-08-15-pyspark-fiscal-data-regression.html",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "",
    "text": "“A minimal example of using Pyspark for Linear Regression”"
  },
  {
    "objectID": "posts/2020-08-15-pyspark-fiscal-data-regression.html#bring-in-needed-imports",
    "href": "posts/2020-08-15-pyspark-fiscal-data-regression.html#bring-in-needed-imports",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "Bring in needed imports",
    "text": "Bring in needed imports\n\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import StringType,BooleanType,DateType,IntegerType\nfrom pyspark.sql.functions import *"
  },
  {
    "objectID": "posts/2020-08-15-pyspark-fiscal-data-regression.html#load-data-from-csv",
    "href": "posts/2020-08-15-pyspark-fiscal-data-regression.html#load-data-from-csv",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "Load data from CSV",
    "text": "Load data from CSV\n\n\nCode\n# Load data from a CSV\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").option(\"inferSchema\", True).option(\"header\", True).load(file_location)\ndisplay(df.take(5))\n\n\n\n_c0provincespecificgeneralyeargdpfdirnrrrifrregit0Anhui147002.0null19962093.3506610.00.00.01128873East China6319301Anhui151981.0null19972347.32434430.00.00.01356287East China6578602Anhui174930.0null19982542.96276730.00.00.01518236East China8894633Anhui285324.0null19992712.3426131nullnullnull1646891East China12273644Anhui195580.032100.020002902.09318470.00.00.01601508East China1499110\n\n\n\ndf.createOrReplaceTempView(\"fiscal_stats\")\n\nsums = spark.sql(\"\"\"\nselect year, sum(it) as total_yearly_it, sum(fr) as total_yearly_fr\nfrom fiscal_stats\ngroup by 1\norder by year asc\n\"\"\")\n\nsums.show()\n\n\n+----+---------------+---------------+\nyear|total_yearly_it|total_yearly_fr|\n+----+---------------+---------------+\n1996|       19825341|    2.9579215E7|\n1997|       21391321|    2.9110765E7|\n1998|       25511453|    3.8154711E7|\n1999|       31922107|    4.2128627E7|\n2000|       38721293|    4.8288092E7|\n2001|       50754944|    5.8910649E7|\n2002|       62375881|    6.2071474E7|\n2003|       69316709|    7.2479293E7|\n2004|       88626786|           null|\n2005|       98263665|           null|\n2006|      119517822|    1.3349148E8|\n2007|      153467611|   2.27385701E8|\n+----+---------------+---------------+"
  },
  {
    "objectID": "posts/2020-08-15-pyspark-fiscal-data-regression.html#describing-the-data",
    "href": "posts/2020-08-15-pyspark-fiscal-data-regression.html#describing-the-data",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "Describing the Data",
    "text": "Describing the Data\n\ndf.describe().toPandas().transpose()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n    \n  \n  \n    \n      summary\n      count\n      mean\n      stddev\n      min\n      max\n    \n    \n      _c0\n      360\n      179.5\n      104.06728592598157\n      0\n      359\n    \n    \n      province\n      360\n      None\n      None\n      Anhui\n      Zhejiang\n    \n    \n      specific\n      356\n      583470.7303370787\n      654055.3290782663\n      8964.0\n      3937966.0\n    \n    \n      general\n      169\n      309127.53846153844\n      355423.5760674793\n      0.0\n      1737800.0\n    \n    \n      year\n      360\n      2001.5\n      3.4568570586927794\n      1996\n      2007\n    \n    \n      gdp\n      360\n      4428.653416666667\n      4484.668659976412\n      64.98\n      31777.01\n    \n    \n      fdi\n      360\n      196139.38333333333\n      303043.97011891654\n      2\n      1743140\n    \n    \n      rnr\n      294\n      0.0355944252244898\n      0.16061503029299648\n      0.0\n      1.214285714\n    \n    \n      rr\n      296\n      0.059688621057432424\n      0.15673351824073453\n      0.0\n      0.84\n    \n    \n      i\n      287\n      0.08376351662369343\n      0.1838933104683607\n      0.0\n      1.05\n    \n    \n      fr\n      295\n      2522449.0034013605\n      3491329.8613106664\n      #REF!\n      9898522\n    \n    \n      reg\n      360\n      None\n      None\n      East China\n      Southwest China\n    \n    \n      it\n      360\n      2165819.2583333333\n      1769294.2935487411\n      147897\n      10533312"
  },
  {
    "objectID": "posts/2020-08-15-pyspark-fiscal-data-regression.html#cast-data-type",
    "href": "posts/2020-08-15-pyspark-fiscal-data-regression.html#cast-data-type",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "Cast Data Type",
    "text": "Cast Data Type\n\ndf2 = df.withColumn(\"gdp\",col(\"gdp\").cast(IntegerType())) \\\n.withColumn(\"specific\",col(\"specific\").cast(IntegerType())) \\\n.withColumn(\"general\",col(\"general\").cast(IntegerType())) \\\n.withColumn(\"year\",col(\"year\").cast(IntegerType())) \\\n.withColumn(\"fdi\",col(\"fdi\").cast(IntegerType())) \\\n.withColumn(\"rnr\",col(\"rnr\").cast(IntegerType())) \\\n.withColumn(\"rr\",col(\"rr\").cast(IntegerType())) \\\n.withColumn(\"i\",col(\"i\").cast(IntegerType())) \\\n.withColumn(\"fr\",col(\"fr\").cast(IntegerType()))"
  },
  {
    "objectID": "posts/2020-08-15-pyspark-fiscal-data-regression.html#printschema",
    "href": "posts/2020-08-15-pyspark-fiscal-data-regression.html#printschema",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "printSchema",
    "text": "printSchema\n\ndf2.printSchema()\n\n\nroot\n-- _c0: integer (nullable = true)\n-- province: string (nullable = true)\n-- specific: integer (nullable = true)\n-- general: integer (nullable = true)\n-- year: integer (nullable = true)\n-- gdp: integer (nullable = true)\n-- fdi: integer (nullable = true)\n-- rnr: integer (nullable = true)\n-- rr: integer (nullable = true)\n-- i: integer (nullable = true)\n-- fr: integer (nullable = true)\n-- reg: string (nullable = true)\n-- it: integer (nullable = true)\n\n\n\n\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nassembler = VectorAssembler(inputCols=['gdp', 'fdi'], outputCol=\"features\")\ntrain_df = assembler.transform(df2) \n\n\n\n\n\n\ntrain_df.select(\"specific\", \"year\").show()\n\n\n+--------+----+\nspecific|year|\n+--------+----+\n  147002|1996|\n  151981|1997|\n  174930|1998|\n  285324|1999|\n  195580|2000|\n  250898|2001|\n  434149|2002|\n  619201|2003|\n  898441|2004|\n  898441|2005|\n 1457872|2006|\n 2213991|2007|\n  165957|1996|\n  165957|1997|\n  245198|1998|\n  388083|1999|\n  281769|2000|\n  441923|2001|\n  558569|2002|\n  642581|2003|\n+--------+----+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-15-pyspark-fiscal-data-regression.html#linear-regression-in-pyspark",
    "href": "posts/2020-08-15-pyspark-fiscal-data-regression.html#linear-regression-in-pyspark",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "Linear Regression in Pyspark",
    "text": "Linear Regression in Pyspark\n\nlr = LinearRegression(featuresCol = 'features', labelCol='it')\nlr_model = lr.fit(train_df)\n\ntrainingSummary = lr_model.summary\nprint(\"Coefficients: \" + str(lr_model.coefficients))\nprint(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\nprint(\"R2: %f\" % trainingSummary.r2)\n\n\nCoefficients: [495.05888709337756,-4.968141828763066]\nRMSE: 1234228.673087\nR2: 0.512023\n\n\n\n\nlr_predictions = lr_model.transform(train_df)\nlr_predictions.select(\"prediction\",\"it\",\"features\").show(5)\nfrom pyspark.ml.evaluation import RegressionEvaluator\nlr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n                 labelCol=\"it\",metricName=\"r2\")\n\n\n+------------------+-------+----------------+\n        prediction|     it|        features|\n+------------------+-------+----------------+\n1732528.7382477913| 631930|[2093.0,50661.0]|\n1894133.7432895212| 657860|[2347.0,43443.0]|\n2069017.8229123235| 889463|[2542.0,27673.0]|\n2160838.7084181504|1227364|[2712.0,26131.0]|\n2226501.9982726825|1499110|[2902.0,31847.0]|\n+------------------+-------+----------------+\nonly showing top 5 rows\n\n\n\n\n\nprint(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))\n\n\nR Squared (R2) on test data = 0.512023\n\n\n\n\nprint(\"numIterations: %d\" % trainingSummary.totalIterations)\nprint(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\ntrainingSummary.residuals.show()\n\n\nnumIterations: 1\nobjectiveHistory: [0.0]\n+-------------------+\n          residuals|\n+-------------------+\n-1100598.7382477913|\n-1236273.7432895212|\n-1179554.8229123235|\n -933474.7084181504|\n -727391.9982726825|\n-222546.39659531135|\n -94585.30175113119|\n 108072.63313654158|\n 389732.58121094666|\n  621021.2194867637|\n  1885768.997742407|\n  3938310.059555837|\n  -554084.125169754|\n -615660.3899049093|\n -352195.3468934437|\n-348450.00565795833|\n -918476.5594253046|\n -710059.9133252408|\n-1148661.0062004486|\n  -911572.322055324|\n+-------------------+\nonly showing top 20 rows\n\n\n\n\n\npredictions = lr_model.transform(test_df)\npredictions.select(\"prediction\",\"it\",\"features\").show()\n\n\n+------------------+-------+---------------+\n        prediction|     it|       features|\n+------------------+-------+---------------+\n 976371.9212205639| 306114|   [64.0,679.0]|\n 990722.2032541803| 415547|   [91.0,481.0]|\n1016348.0830204486| 983251|  [139.0,106.0]|\n1036290.7062801318| 218361|  [184.0,576.0]|\n1034023.4471330958| 178668| [202.0,2826.0]|\n1060130.0768520113| 274994| [245.0,1856.0]|\n1023513.0851009073| 546541|[263.0,11020.0]|\n   1053250.6267921| 361358| [264.0,5134.0]|\n1123768.8091592425| 866691| [377.0,2200.0]|\n1128604.8330225947| 948521| [390.0,2522.0]|\n 810587.2575938476| 177748|[442.0,71715.0]|\n 1159703.254297337| 736165| [445.0,1743.0]|\n 1066975.770986663|1260633|[466.0,22500.0]|\n1288507.6625716756|1423771| [725.0,3718.0]|\n 1320055.238474972| 573905| [793.0,4144.0]|\n1188611.0570700848|2347862|[797.0,31000.0]|\n 1321857.482976733| 582711| [805.0,4977.0]|\n1033849.5995896922| 746784|[819.0,64343.0]|\n 1445051.792853667|1216605|[1029.0,2501.0]|\n1437887.1056682135|1258100|[1052.0,6235.0]|\n+------------------+-------+---------------+\nonly showing top 20 rows\n\n\n\n\n\nfrom pyspark.ml.regression import DecisionTreeRegressor\ndt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'it')\ndt_model = dt.fit(train_df)\ndt_predictions = dt_model.transform(train_df)\ndt_evaluator = RegressionEvaluator(\n    labelCol=\"it\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = dt_evaluator.evaluate(dt_predictions)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n\n\nRoot Mean Squared Error (RMSE) on test data = 1.01114e+06\n\n\n\n\nfrom pyspark.ml.regression import GBTRegressor\ngbt = GBTRegressor(featuresCol = 'features', labelCol = 'it', maxIter=10)\ngbt_model = gbt.fit(train_df)\ngbt_predictions = gbt_model.transform(train_df)\ngbt_predictions.select('prediction', 'it', 'features').show(5)\n\n\ngbt_evaluator = RegressionEvaluator(\n    labelCol=\"it\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = gbt_evaluator.evaluate(gbt_predictions)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n\n\n+------------------+-------+----------------+\n        prediction|     it|        features|\n+------------------+-------+----------------+\n 1388898.308543053| 631930|[2093.0,50661.0]|\n 1388898.308543053| 657860|[2347.0,43443.0]|\n1649083.6277172007| 889463|[2542.0,27673.0]|\n1649083.6277172007|1227364|[2712.0,26131.0]|\n1649083.6277172007|1499110|[2902.0,31847.0]|\n+------------------+-------+----------------+\nonly showing top 5 rows\n\nRoot Mean Squared Error (RMSE) on test data = 778728\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-09-03-nlp_seaborn_heatmaps.html",
    "href": "posts/2020-09-03-nlp_seaborn_heatmaps.html",
    "title": "NLP Heatmaps with Seaborn",
    "section": "",
    "text": "df=pd.read_csv('../processed_data/nf_complete.csv')\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 126 entries, 0 to 125\nData columns (total 23 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   Unnamed: 0             126 non-null    int64 \n 1   year                   126 non-null    int64 \n 2   title                  126 non-null    object\n 3   abstract               126 non-null    object\n 4   theme                  126 non-null    object\n 5   China                  126 non-null    int64 \n 6   Russia                 126 non-null    int64 \n 7   War                    126 non-null    int64 \n 8   President              126 non-null    int64 \n 9   US                     126 non-null    int64 \n 10  Vietnam                126 non-null    int64 \n 11  Cold War               126 non-null    int64 \n 12  World War              126 non-null    int64 \n 13  Vietnam War            126 non-null    int64 \n 14  Korean War             126 non-null    int64 \n 15  Survey                 126 non-null    int64 \n 16  Case Study             126 non-null    int64 \n 17  Trade                  126 non-null    int64 \n 18  Humanitarian           126 non-null    int64 \n 19  fixed_effects          126 non-null    int64 \n 20  instrumental_variable  126 non-null    int64 \n 21  regression             126 non-null    int64 \n 22  experimental           126 non-null    int64 \ndtypes: int64(20), object(3)\nmemory usage: 22.8+ KB\n\n\n\nimport plotly_express as ple\nple.histogram(df.sort_values('year').groupby(['year','theme'])['Cold War'].sum().reset_index(), x=\"year\", y=\"Cold War\", histfunc=\"sum\", color=\"theme\")\n\n\n\n        \n        \n            \n            \n        \n\n\n\n# ple.lidifferences(dfm_regional.sort_values('year').groupby(['year','theme'])['Cold War'].sum().reset_index(),\n#          x='year',\n#          y='Cold War',\n#          line_group='theme',\n#          color='theme'\n#         )\n\n\n# Create the crosstab DataFrame\npd_crosstab = pd.crosstab(df[\"theme\"], df[\"year\"])\n\n# Plot a heatmap of the table with no color bar and using the BuGn palette\nsns.heatmap(pd_crosstab, cbar=False, cmap=\"GnBu\", linewidths=0.3)\n\n# Rotate tick marks for visibility\nplt.yticks(rotation=0)\nplt.xticks(rotation=90)\n\nplt.tight_layout()\n#plt.savefig('./img/theme_heat_1.png', bbox_inches='tight', dpi=500)\n\n\n#Show the plot\nplt.show()\nplt.clf()\n\n\n\n\n<Figure size 720x648 with 0 Axes>\n\n\n\nsns.clustermap(pd_crosstab, cmap='Greens', robust=True)\n\n# plot using a color palette\n#sns.heatmap(df, cmap=\"YlGnBu\")\n#sns.heatmap(df, cmap=\"Blues\")\n#sns.heatmap(df, cmap=\"BuPu\")\n#sns.heatmap(df, cmap=\"Greens\")\n\n<seaborn.matrix.ClusterGrid at 0x7f978c07a470>\n\n\n\n\n\n\n# Import seaborn library\nimport seaborn as sns\n\n# Get correlation matrix of the meat DataFrame\ncorr_meat = df.corr(method='pearson')\n\n# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\nfig = sns.clustermap(pd_crosstab,\n                     row_cluster=True,\n                     col_cluster=True,\n                     figsize=(10, 10))\n\nplt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\nplt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\nplt.show()\n\n\n\n\n\ndata_normalized = pd_crosstab\n\n# Standardize the mean and variance within a stat, so different stats can be comparable\n# (This is the same as changing all the columns to Z-scores)\ndata_normalized = (data_normalized - data_normalized.mean())/data_normalized.var()\n\n# Normalize these values to range from -1 to 1\ndata_normalized = (data_normalized)/(data_normalized.max() - data_normalized.min())\n\ndata_normalized = data_normalized.T\n\n# Can use a semicolon after the command to suppress output of the row_dendrogram and col_dendrogram.\nsns.clustermap(data_normalized, cmap='Blues');\n\n\n\n\n\ndata_normalized = pd_crosstab\n\n# Standardize the mean and variance within a stat, so different stats can be comparable\n# (This is the same as changing all the columns to Z-scores)\ndata_normalized = (data_normalized - data_normalized.mean())/data_normalized.var()\n\n# Normalize these values to range from -1 to 1\ndata_normalized = (data_normalized)/(data_normalized.max() - data_normalized.min())\n\n#data_normalized = data_normalized.T\n\n# Can use a semicolon after the command to suppress output of the row_dendrogram and col_dendrogram.\nsns.clustermap(data_normalized, cmap='BuPu');\n\n\n\n\n\nimport matplotlib.pyplot as plt\nsns.clustermap(data_normalized);\nfig = plt.gcf()\nfig.savefig('clusteredheatmap_bbox_tight.png', bbox_inches='tight')\n\n\n\n\n\ntidy_df = pd.melt(df.reset_index(), id_vars='index')\ndf.T.head()\n\n\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      116\n      117\n      118\n      119\n      120\n      121\n      122\n      123\n      124\n      125\n    \n  \n  \n    \n      Unnamed: 0\n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      14\n      13\n      ...\n      128\n      130\n      123\n      125\n      131\n      132\n      133\n      134\n      135\n      136\n    \n    \n      year\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2000\n      2001\n      2001\n      ...\n      2017\n      2017\n      2017\n      2017\n      2018\n      2018\n      2018\n      2018\n      2018\n      2018\n    \n    \n      title\n      \"Institutions at the Domestic/International Ne...\n      Born to Lose and Doomed to Survive: State Deat...\n      The significance of “allegiance” in internatio...\n      The significance of “allegiance” in internatio...\n      Truth-Telling and Mythmaking in Post-Soviet Ru...\n      Building a Cape Fear Metropolis: Fort Bragg, F...\n      The Glories and the Sadness: Shaping the natio...\n      What leads longstanding adversaries to engage ...\n      A School for the Nation: Military  Institution...\n      The 'American Century' Army:  The Origins of t...\n      ...\n      Fully Committed? Religiously Committed State P...\n      Straddling the Threshold of Two Worlds: Soldie...\n      U.S. Army’s Investigation and Adjudication of ...\n      Grand Strategic Crucibles: The Lasting Effects...\n      Trust in International Politics: The Role of L...\n      Planning for the Short Haul: Trade Among Belli...\n      Clinging to the Anti-Imperial Mantle: The Repu...\n      The New Navy's Pacific Wars:  Peripheral Confl...\n      Stop or I'll Shoot, Comply and I Won't: The Di...\n      Unexpected Humanitarians: Albania, the U.S. Mi...\n    \n    \n      abstract\n      Civil-military relations are frequently studie...\n      Under what conditions do states die, or exit t...\n      My dissertation employs original and secondary...\n      \\nThis study revises prevailing interpretation...\n      Can distorted and pernicious ideas r histo...\n      My dissertation examines the cultural and econ...\n      In my dissertation I compare the ways in whic...\n      This dissertation develops a socio-psychoanal...\n      Beginning in Europe in the latter half of the ...\n      This dissertation covers the period 1949-1959 ...\n      ...\n      This dissertation argues that the higher the l...\n      This dissertation explores how American soldie...\n      This dissertation examines the U.S. Army’s res...\n      When and how do military interventions shape g...\n      In my dissertation, I focus on how leader rela...\n      In times of war, why do belligerents continue ...\n      My dissertation project, Clinging to the Anti-...\n      Using a transnational methodology and sources ...\n      There is a dilemma at the heart of coercion. S...\n      Using archives and oral history, this disserta...\n    \n    \n      theme\n      IR scholarship\n      IR scholarship\n      IR scholarship\n      Conflit Between States\n      Conflict Between States\n      Domestic Military History\n      Culture\n      Culture / Peace Process\n      Military History\n      Military History\n      ...\n      IR Scholarship\n      Military History\n      Military History\n      IR Scholarship\n      Nuclear Weapons\n      Conflict between states\n      Cold War\n      Military History\n      IR Scholarship\n      Military History\n    \n  \n\n5 rows × 126 columns"
  },
  {
    "objectID": "posts/2020-10-10-dask_fiscal-db-to-dask-dataframe.html",
    "href": "posts/2020-10-10-dask_fiscal-db-to-dask-dataframe.html",
    "title": "Working with dask data frames. Reading Fiscal Data from a sqlite db to a dask dataframe. Computing, visualizing and groupby with dask dataframes. Using dask.distributed locally.",
    "section": "",
    "text": "import numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///fiscal_data.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nengine.execute(\"SELECT * FROM fiscal_data LIMIT 1\").fetchall()\n\n[(1996, 'East China', 'Anhui', '2093.3', 50661, 631930, 147002)]\n\n\n\nsql = \"\"\"\nSELECT year\n, region\n, province\n, gdp\n, fdi\n, it\n, specific\nFROM fiscal_data\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.3\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.7\n      668128\n      3162299\n      656175.0\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      18753.73\n      1036576\n      2939778\n      844647.0\n    \n  \n\n360 rows × 7 columns\n\n\n\n\ndf.columns\n\nIndex(['year', 'region', 'province', 'gdp', 'fdi', 'it', 'specific'], dtype='object')\n\n\n\nfrom dask import dataframe as dd \n\n\nddf = dd.from_pandas(df, npartitions=5)\n\n\nprint(ddf)\n\nDask DataFrame Structure:\n                year  region province     gdp    fdi     it specific\nnpartitions=5                                                       \n0              int64  object   object  object  int64  int64  float64\n72               ...     ...      ...     ...    ...    ...      ...\n...              ...     ...      ...     ...    ...    ...      ...\n288              ...     ...      ...     ...    ...    ...      ...\n359              ...     ...      ...     ...    ...    ...      ...\nDask Name: from_pandas, 5 tasks\n\n\n\nddf.npartitions\n\n5\n\n\n\nddf.npartitions\n\n5\n\n\n\nlen(ddf)\n\n360\n\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n\n\n\n\nClient\n\n  Scheduler: inproc://192.168.1.71/12451/1\n  Dashboard: http://localhost:8787/status\n\n\n\nCluster\n\n  Workers: 3\n  Cores: 6\n  Memory: 12.00 GB\n\n\n\n\n\n\n\nddf.describe().compute()\n\n\n\n\n\n  \n    \n      \n      year\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      count\n      360.000000\n      3.600000e+02\n      3.600000e+02\n      3.560000e+02\n    \n    \n      mean\n      2001.500000\n      1.961394e+05\n      2.165819e+06\n      5.834707e+05\n    \n    \n      std\n      3.456857\n      3.030440e+05\n      1.769294e+06\n      6.540553e+05\n    \n    \n      min\n      1996.000000\n      2.000000e+00\n      1.478970e+05\n      8.964000e+03\n    \n    \n      25%\n      1998.750000\n      3.309900e+04\n      1.077466e+06\n      2.237530e+05\n    \n    \n      50%\n      2001.500000\n      1.411025e+05\n      2.020634e+06\n      4.243700e+05\n    \n    \n      75%\n      2004.250000\n      4.065125e+05\n      3.375492e+06\n      1.011846e+06\n    \n    \n      max\n      2007.000000\n      1.743140e+06\n      1.053331e+07\n      3.937966e+06\n    \n  \n\n\n\n\n\nddf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.3\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n  \n\n\n\n\n\ngroupby_yr = ddf.groupby('year').count()\n\n\ngroupby_yr.compute()\n\n\n\n\n\n  \n    \n      \n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1996\n      30\n      30\n      30\n      30\n      30\n      29\n    \n    \n      1997\n      30\n      30\n      30\n      30\n      30\n      28\n    \n    \n      1998\n      30\n      30\n      30\n      30\n      30\n      30\n    \n    \n      1999\n      30\n      30\n      30\n      30\n      30\n      30\n    \n    \n      2000\n      30\n      30\n      30\n      30\n      30\n      29\n    \n    \n      2001\n      30\n      30\n      30\n      30\n      30\n      30\n    \n    \n      2002\n      30\n      30\n      30\n      30\n      30\n      30\n    \n    \n      2003\n      30\n      30\n      30\n      30\n      30\n      30\n    \n    \n      2004\n      30\n      30\n      30\n      30\n      30\n      30\n    \n    \n      2005\n      30\n      30\n      30\n      30\n      30\n      30\n    \n    \n      2006\n      30\n      30\n      30\n      30\n      30\n      30\n    \n    \n      2007\n      30\n      30\n      30\n      30\n      30\n      30\n    \n  \n\n\n\n\n\ngroup_region = ddf.groupby('region')['gdp'].sum()\n\n\ngroup_region.compute()\n\nregion\nEast China             2093.32347.322542.962712.342902.093246.713519....\nNorth China            1789.22077.092377.182678.823161.663707.964315....\nNorthwest China        722.52793.57887.67956.321052.881125.371232.031...\nSouth Central China    6834.977774.538530.889250.6810741.2512039.2513...\nSouthwest China        1315.121509.751602.381663.21791.01976.862232.8...\nNortheast China        2370.52667.52774.42866.33151.43390.13637.24057...\nName: gdp, dtype: object\n\n\n\nddf.nlargest(5, 'fdi').compute()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      179\n      2007\n      East China\n      Jiangsu\n      21742.05\n      1743140\n      3557071\n      1188989.0\n    \n    \n      71\n      2007\n      South Central China\n      Guangdong\n      31777.01\n      1712603\n      4947824\n      859482.0\n    \n    \n      70\n      2006\n      South Central China\n      Guangdong\n      26587.76\n      1451065\n      4559252\n      1897575.0\n    \n    \n      178\n      2006\n      East China\n      Jiangsu\n      18598.69\n      1318339\n      2926542\n      1388043.0\n    \n    \n      69\n      2005\n      South Central China\n      Guangdong\n      22557.37\n      1236400\n      4327217\n      1491588.0\n    \n  \n\n\n\n\n\nddf.sum().visualize()\n\n\n\n\n\nddf.sum().visualize(rankdir=\"LR\")\n\n\n\n\n\n(ddf).visualize(rankdir=\"LR\")\n\n\n\n\n\nddf.visualize(rankdir=\"LR\")\n\n\n\n\n\nclient.close()"
  },
  {
    "objectID": "posts/2020-10-26-working_with_sqlitedbs_in_jupyter_soccer_pred.html",
    "href": "posts/2020-10-26-working_with_sqlitedbs_in_jupyter_soccer_pred.html",
    "title": "Working with sqlite databases in Jupyter for European Soccer Match Data",
    "section": "",
    "text": "import sqlalchemy as db\nimport sqlite3\nimport pandas as pd\nimport numpy as np\n\n\nengine = db.create_engine('sqlite:///database.sqlite')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nconnection\n\n<sqlalchemy.engine.base.Connection at 0x7fb9c3356780>\n\n\n\nengine.execute(\"SELECT * FROM Country LIMIT 10\").fetchall()\n\n[(1, 'Belgium'),\n (1729, 'England'),\n (4769, 'France'),\n (7809, 'Germany'),\n (10257, 'Italy'),\n (13274, 'Netherlands'),\n (15722, 'Poland'),\n (17642, 'Portugal'),\n (19694, 'Scotland'),\n (21518, 'Spain')]\n\n\n\n%load_ext sql\n\nThe sql extension is already loaded. To reload it, use:\n  %reload_ext sql\n\n\n\n%sql sqlite:///database.sqlite\n\n\n%%sql\nSELECT *\nFROM Country\nLIMIT 10\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n    \n        id\n        name\n    \n    \n        1\n        Belgium\n    \n    \n        1729\n        England\n    \n    \n        4769\n        France\n    \n    \n        7809\n        Germany\n    \n    \n        10257\n        Italy\n    \n    \n        13274\n        Netherlands\n    \n    \n        15722\n        Poland\n    \n    \n        17642\n        Portugal\n    \n    \n        19694\n        Scotland\n    \n    \n        21518\n        Spain\n    \n\n\n\n\n%%sql\nSELECT id\n,name\nFROM Country\nWHERE name = \"England\"\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n    \n        id\n        name\n    \n    \n        1729\n        England\n    \n\n\n\n\n%%sql\nSELECT * FROM League LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n    \n        id\n        country_id\n        name\n    \n    \n        1\n        1\n        Belgium Jupiler League\n    \n    \n        1729\n        1729\n        England Premier League\n    \n    \n        4769\n        4769\n        France Ligue 1\n    \n    \n        7809\n        7809\n        Germany 1. Bundesliga\n    \n    \n        10257\n        10257\n        Italy Serie A\n    \n    \n        13274\n        13274\n        Netherlands Eredivisie\n    \n    \n        15722\n        15722\n        Poland Ekstraklasa\n    \n    \n        17642\n        17642\n        Portugal Liga ZON Sagres\n    \n    \n        19694\n        19694\n        Scotland Premier League\n    \n    \n        21518\n        21518\n        Spain LIGA BBVA\n    \n\n\n\n\n%%sql\nSELECT * FROM Match LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n    \n        id\n        country_id\n        league_id\n        season\n        stage\n        date\n        match_api_id\n        home_team_api_id\n        away_team_api_id\n        home_team_goal\n        away_team_goal\n        home_player_X1\n        home_player_X2\n        home_player_X3\n        home_player_X4\n        home_player_X5\n        home_player_X6\n        home_player_X7\n        home_player_X8\n        home_player_X9\n        home_player_X10\n        home_player_X11\n        away_player_X1\n        away_player_X2\n        away_player_X3\n        away_player_X4\n        away_player_X5\n        away_player_X6\n        away_player_X7\n        away_player_X8\n        away_player_X9\n        away_player_X10\n        away_player_X11\n        home_player_Y1\n        home_player_Y2\n        home_player_Y3\n        home_player_Y4\n        home_player_Y5\n        home_player_Y6\n        home_player_Y7\n        home_player_Y8\n        home_player_Y9\n        home_player_Y10\n        home_player_Y11\n        away_player_Y1\n        away_player_Y2\n        away_player_Y3\n        away_player_Y4\n        away_player_Y5\n        away_player_Y6\n        away_player_Y7\n        away_player_Y8\n        away_player_Y9\n        away_player_Y10\n        away_player_Y11\n        home_player_1\n        home_player_2\n        home_player_3\n        home_player_4\n        home_player_5\n        home_player_6\n        home_player_7\n        home_player_8\n        home_player_9\n        home_player_10\n        home_player_11\n        away_player_1\n        away_player_2\n        away_player_3\n        away_player_4\n        away_player_5\n        away_player_6\n        away_player_7\n        away_player_8\n        away_player_9\n        away_player_10\n        away_player_11\n        goal\n        shoton\n        shotoff\n        foulcommit\n        card\n        cross\n        corner\n        possession\n        B365H\n        B365D\n        B365A\n        BWH\n        BWD\n        BWA\n        IWH\n        IWD\n        IWA\n        LBH\n        LBD\n        LBA\n        PSH\n        PSD\n        PSA\n        WHH\n        WHD\n        WHA\n        SJH\n        SJD\n        SJA\n        VCH\n        VCD\n        VCA\n        GBH\n        GBD\n        GBA\n        BSH\n        BSD\n        BSA\n    \n    \n        1\n        1\n        1\n        2008/2009\n        1\n        2008-08-17 00:00:00\n        492473\n        9987\n        9993\n        1\n        1\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        1.73\n        3.4\n        5\n        1.75\n        3.35\n        4.2\n        1.85\n        3.2\n        3.5\n        1.8\n        3.3\n        3.75\n        None\n        None\n        None\n        1.7\n        3.3\n        4.33\n        1.9\n        3.3\n        4\n        1.65\n        3.4\n        4.5\n        1.78\n        3.25\n        4\n        1.73\n        3.4\n        4.2\n    \n    \n        2\n        1\n        1\n        2008/2009\n        1\n        2008-08-16 00:00:00\n        492474\n        10000\n        9994\n        0\n        0\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        1.95\n        3.2\n        3.6\n        1.8\n        3.3\n        3.95\n        1.9\n        3.2\n        3.5\n        1.9\n        3.2\n        3.5\n        None\n        None\n        None\n        1.83\n        3.3\n        3.6\n        1.95\n        3.3\n        3.8\n        2\n        3.25\n        3.25\n        1.85\n        3.25\n        3.75\n        1.91\n        3.25\n        3.6\n    \n    \n        3\n        1\n        1\n        2008/2009\n        1\n        2008-08-16 00:00:00\n        492475\n        9984\n        8635\n        0\n        3\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        2.38\n        3.3\n        2.75\n        2.4\n        3.3\n        2.55\n        2.6\n        3.1\n        2.3\n        2.5\n        3.2\n        2.5\n        None\n        None\n        None\n        2.5\n        3.25\n        2.4\n        2.63\n        3.3\n        2.5\n        2.35\n        3.25\n        2.65\n        2.5\n        3.2\n        2.5\n        2.3\n        3.2\n        2.75\n    \n    \n        4\n        1\n        1\n        2008/2009\n        1\n        2008-08-17 00:00:00\n        492476\n        9991\n        9998\n        5\n        0\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        1.44\n        3.75\n        7.5\n        1.4\n        4\n        6.8\n        1.4\n        3.9\n        6\n        1.44\n        3.6\n        6.5\n        None\n        None\n        None\n        1.44\n        3.75\n        6\n        1.44\n        4\n        7.5\n        1.45\n        3.75\n        6.5\n        1.5\n        3.75\n        5.5\n        1.44\n        3.75\n        6.5\n    \n    \n        5\n        1\n        1\n        2008/2009\n        1\n        2008-08-16 00:00:00\n        492477\n        7947\n        9985\n        1\n        3\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        5\n        3.5\n        1.65\n        5\n        3.5\n        1.6\n        4\n        3.3\n        1.7\n        4\n        3.4\n        1.72\n        None\n        None\n        None\n        4.2\n        3.4\n        1.7\n        4.5\n        3.5\n        1.73\n        4.5\n        3.4\n        1.65\n        4.5\n        3.5\n        1.65\n        4.75\n        3.3\n        1.67\n    \n    \n        6\n        1\n        1\n        2008/2009\n        1\n        2008-09-24 00:00:00\n        492478\n        8203\n        8342\n        1\n        1\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        4.75\n        3.4\n        1.67\n        4.85\n        3.4\n        1.65\n        3.7\n        3.2\n        1.8\n        5\n        3.25\n        1.62\n        None\n        None\n        None\n        4.2\n        3.4\n        1.7\n        5.5\n        3.75\n        1.67\n        4.35\n        3.4\n        1.7\n        4.5\n        3.4\n        1.7\n        None\n        None\n        None\n    \n    \n        7\n        1\n        1\n        2008/2009\n        1\n        2008-08-16 00:00:00\n        492479\n        9999\n        8571\n        2\n        2\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        2.1\n        3.2\n        3.3\n        2.05\n        3.25\n        3.15\n        1.85\n        3.2\n        3.5\n        1.83\n        3.3\n        3.6\n        None\n        None\n        None\n        1.83\n        3.3\n        3.6\n        1.91\n        3.4\n        3.6\n        2.1\n        3.25\n        3\n        1.85\n        3.25\n        3.75\n        2.1\n        3.25\n        3.1\n    \n    \n        8\n        1\n        1\n        2008/2009\n        1\n        2008-08-16 00:00:00\n        492480\n        4049\n        9996\n        1\n        2\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        3.2\n        3.4\n        2.2\n        2.55\n        3.3\n        2.4\n        2.4\n        3.2\n        2.4\n        2.5\n        3.2\n        2.5\n        None\n        None\n        None\n        2.7\n        3.25\n        2.25\n        2.6\n        3.4\n        2.4\n        2.8\n        3.25\n        2.25\n        2.8\n        3.2\n        2.25\n        2.88\n        3.25\n        2.2\n    \n    \n        9\n        1\n        1\n        2008/2009\n        1\n        2008-08-16 00:00:00\n        492481\n        10001\n        9986\n        1\n        0\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        2.25\n        3.25\n        2.88\n        2.3\n        3.25\n        2.7\n        2.1\n        3.1\n        3\n        2.25\n        3.2\n        2.75\n        None\n        None\n        None\n        2.2\n        3.25\n        2.75\n        2.2\n        3.3\n        3.1\n        2.25\n        3.25\n        2.8\n        2.2\n        3.3\n        2.8\n        2.25\n        3.2\n        2.8\n    \n    \n        10\n        1\n        1\n        2008/2009\n        10\n        2008-11-01 00:00:00\n        492564\n        8342\n        8571\n        4\n        1\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        None\n        1.3\n        5.25\n        9.5\n        1.25\n        5\n        10\n        1.3\n        4.2\n        8\n        1.25\n        4.5\n        10\n        None\n        None\n        None\n        1.35\n        4.2\n        7\n        1.27\n        5\n        10\n        1.3\n        4.35\n        8.5\n        1.25\n        5\n        10\n        1.29\n        4.5\n        9\n    \n\n\n\n\n%%sql\nSELECT * FROM Player LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n    \n        id\n        player_api_id\n        player_name\n        player_fifa_api_id\n        birthday\n        height\n        weight\n    \n    \n        1\n        505942\n        Aaron Appindangoye\n        218353\n        1992-02-29 00:00:00\n        182.88\n        187\n    \n    \n        2\n        155782\n        Aaron Cresswell\n        189615\n        1989-12-15 00:00:00\n        170.18\n        146\n    \n    \n        3\n        162549\n        Aaron Doran\n        186170\n        1991-05-13 00:00:00\n        170.18\n        163\n    \n    \n        4\n        30572\n        Aaron Galindo\n        140161\n        1982-05-08 00:00:00\n        182.88\n        198\n    \n    \n        5\n        23780\n        Aaron Hughes\n        17725\n        1979-11-08 00:00:00\n        182.88\n        154\n    \n    \n        6\n        27316\n        Aaron Hunt\n        158138\n        1986-09-04 00:00:00\n        182.88\n        161\n    \n    \n        7\n        564793\n        Aaron Kuhl\n        221280\n        1996-01-30 00:00:00\n        172.72\n        146\n    \n    \n        8\n        30895\n        Aaron Lennon\n        152747\n        1987-04-16 00:00:00\n        165.1\n        139\n    \n    \n        9\n        528212\n        Aaron Lennox\n        206592\n        1993-02-19 00:00:00\n        190.5\n        181\n    \n    \n        10\n        101042\n        Aaron Meijers\n        188621\n        1987-10-28 00:00:00\n        175.26\n        170\n    \n\n\n\n\n%%sql\nSELECT * FROM Player_Attributes LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n    \n        id\n        player_fifa_api_id\n        player_api_id\n        date\n        overall_rating\n        potential\n        preferred_foot\n        attacking_work_rate\n        defensive_work_rate\n        crossing\n        finishing\n        heading_accuracy\n        short_passing\n        volleys\n        dribbling\n        curve\n        free_kick_accuracy\n        long_passing\n        ball_control\n        acceleration\n        sprint_speed\n        agility\n        reactions\n        balance\n        shot_power\n        jumping\n        stamina\n        strength\n        long_shots\n        aggression\n        interceptions\n        positioning\n        vision\n        penalties\n        marking\n        standing_tackle\n        sliding_tackle\n        gk_diving\n        gk_handling\n        gk_kicking\n        gk_positioning\n        gk_reflexes\n    \n    \n        1\n        218353\n        505942\n        2016-02-18 00:00:00\n        67\n        71\n        right\n        medium\n        medium\n        49\n        44\n        71\n        61\n        44\n        51\n        45\n        39\n        64\n        49\n        60\n        64\n        59\n        47\n        65\n        55\n        58\n        54\n        76\n        35\n        71\n        70\n        45\n        54\n        48\n        65\n        69\n        69\n        6\n        11\n        10\n        8\n        8\n    \n    \n        2\n        218353\n        505942\n        2015-11-19 00:00:00\n        67\n        71\n        right\n        medium\n        medium\n        49\n        44\n        71\n        61\n        44\n        51\n        45\n        39\n        64\n        49\n        60\n        64\n        59\n        47\n        65\n        55\n        58\n        54\n        76\n        35\n        71\n        70\n        45\n        54\n        48\n        65\n        69\n        69\n        6\n        11\n        10\n        8\n        8\n    \n    \n        3\n        218353\n        505942\n        2015-09-21 00:00:00\n        62\n        66\n        right\n        medium\n        medium\n        49\n        44\n        71\n        61\n        44\n        51\n        45\n        39\n        64\n        49\n        60\n        64\n        59\n        47\n        65\n        55\n        58\n        54\n        76\n        35\n        63\n        41\n        45\n        54\n        48\n        65\n        66\n        69\n        6\n        11\n        10\n        8\n        8\n    \n    \n        4\n        218353\n        505942\n        2015-03-20 00:00:00\n        61\n        65\n        right\n        medium\n        medium\n        48\n        43\n        70\n        60\n        43\n        50\n        44\n        38\n        63\n        48\n        60\n        64\n        59\n        46\n        65\n        54\n        58\n        54\n        76\n        34\n        62\n        40\n        44\n        53\n        47\n        62\n        63\n        66\n        5\n        10\n        9\n        7\n        7\n    \n    \n        5\n        218353\n        505942\n        2007-02-22 00:00:00\n        61\n        65\n        right\n        medium\n        medium\n        48\n        43\n        70\n        60\n        43\n        50\n        44\n        38\n        63\n        48\n        60\n        64\n        59\n        46\n        65\n        54\n        58\n        54\n        76\n        34\n        62\n        40\n        44\n        53\n        47\n        62\n        63\n        66\n        5\n        10\n        9\n        7\n        7\n    \n    \n        6\n        189615\n        155782\n        2016-04-21 00:00:00\n        74\n        76\n        left\n        high\n        medium\n        80\n        53\n        58\n        71\n        40\n        73\n        70\n        69\n        68\n        71\n        79\n        78\n        78\n        67\n        90\n        71\n        85\n        79\n        56\n        62\n        68\n        67\n        60\n        66\n        59\n        76\n        75\n        78\n        14\n        7\n        9\n        9\n        12\n    \n    \n        7\n        189615\n        155782\n        2016-04-07 00:00:00\n        74\n        76\n        left\n        high\n        medium\n        80\n        53\n        58\n        71\n        32\n        73\n        70\n        69\n        68\n        71\n        79\n        78\n        78\n        67\n        90\n        71\n        85\n        79\n        56\n        60\n        68\n        67\n        60\n        66\n        59\n        76\n        75\n        78\n        14\n        7\n        9\n        9\n        12\n    \n    \n        8\n        189615\n        155782\n        2016-01-07 00:00:00\n        73\n        75\n        left\n        high\n        medium\n        79\n        52\n        57\n        70\n        29\n        71\n        68\n        69\n        68\n        70\n        79\n        78\n        78\n        67\n        90\n        71\n        84\n        79\n        56\n        59\n        67\n        66\n        58\n        65\n        59\n        76\n        75\n        78\n        14\n        7\n        9\n        9\n        12\n    \n    \n        9\n        189615\n        155782\n        2015-12-24 00:00:00\n        73\n        75\n        left\n        high\n        medium\n        79\n        51\n        57\n        70\n        29\n        71\n        68\n        69\n        68\n        70\n        79\n        78\n        78\n        67\n        90\n        71\n        84\n        79\n        56\n        58\n        67\n        66\n        58\n        65\n        59\n        76\n        75\n        78\n        14\n        7\n        9\n        9\n        12\n    \n    \n        10\n        189615\n        155782\n        2015-12-17 00:00:00\n        73\n        75\n        left\n        high\n        medium\n        79\n        51\n        57\n        70\n        29\n        71\n        68\n        69\n        68\n        70\n        79\n        78\n        78\n        67\n        90\n        71\n        84\n        79\n        56\n        58\n        67\n        66\n        58\n        65\n        59\n        76\n        75\n        78\n        14\n        7\n        9\n        9\n        12\n    \n\n\n\n\n%%sql\nSELECT * FROM Team LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n    \n        id\n        team_api_id\n        team_fifa_api_id\n        team_long_name\n        team_short_name\n    \n    \n        1\n        9987\n        673\n        KRC Genk\n        GEN\n    \n    \n        2\n        9993\n        675\n        Beerschot AC\n        BAC\n    \n    \n        3\n        10000\n        15005\n        SV Zulte-Waregem\n        ZUL\n    \n    \n        4\n        9994\n        2007\n        Sporting Lokeren\n        LOK\n    \n    \n        5\n        9984\n        1750\n        KSV Cercle Brugge\n        CEB\n    \n    \n        6\n        8635\n        229\n        RSC Anderlecht\n        AND\n    \n    \n        7\n        9991\n        674\n        KAA Gent\n        GEN\n    \n    \n        8\n        9998\n        1747\n        RAEC Mons\n        MON\n    \n    \n        9\n        7947\n        None\n        FCV Dender EH\n        DEN\n    \n    \n        10\n        9985\n        232\n        Standard de Liège\n        STL\n    \n\n\n\n\n%%sql\nSELECT * FROM Team_Attributes LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n    \n        id\n        team_fifa_api_id\n        team_api_id\n        date\n        buildUpPlaySpeed\n        buildUpPlaySpeedClass\n        buildUpPlayDribbling\n        buildUpPlayDribblingClass\n        buildUpPlayPassing\n        buildUpPlayPassingClass\n        buildUpPlayPositioningClass\n        chanceCreationPassing\n        chanceCreationPassingClass\n        chanceCreationCrossing\n        chanceCreationCrossingClass\n        chanceCreationShooting\n        chanceCreationShootingClass\n        chanceCreationPositioningClass\n        defencePressure\n        defencePressureClass\n        defenceAggression\n        defenceAggressionClass\n        defenceTeamWidth\n        defenceTeamWidthClass\n        defenceDefenderLineClass\n    \n    \n        1\n        434\n        9930\n        2010-02-22 00:00:00\n        60\n        Balanced\n        None\n        Little\n        50\n        Mixed\n        Organised\n        60\n        Normal\n        65\n        Normal\n        55\n        Normal\n        Organised\n        50\n        Medium\n        55\n        Press\n        45\n        Normal\n        Cover\n    \n    \n        2\n        434\n        9930\n        2014-09-19 00:00:00\n        52\n        Balanced\n        48\n        Normal\n        56\n        Mixed\n        Organised\n        54\n        Normal\n        63\n        Normal\n        64\n        Normal\n        Organised\n        47\n        Medium\n        44\n        Press\n        54\n        Normal\n        Cover\n    \n    \n        3\n        434\n        9930\n        2015-09-10 00:00:00\n        47\n        Balanced\n        41\n        Normal\n        54\n        Mixed\n        Organised\n        54\n        Normal\n        63\n        Normal\n        64\n        Normal\n        Organised\n        47\n        Medium\n        44\n        Press\n        54\n        Normal\n        Cover\n    \n    \n        4\n        77\n        8485\n        2010-02-22 00:00:00\n        70\n        Fast\n        None\n        Little\n        70\n        Long\n        Organised\n        70\n        Risky\n        70\n        Lots\n        70\n        Lots\n        Organised\n        60\n        Medium\n        70\n        Double\n        70\n        Wide\n        Cover\n    \n    \n        5\n        77\n        8485\n        2011-02-22 00:00:00\n        47\n        Balanced\n        None\n        Little\n        52\n        Mixed\n        Organised\n        53\n        Normal\n        48\n        Normal\n        52\n        Normal\n        Organised\n        47\n        Medium\n        47\n        Press\n        52\n        Normal\n        Cover\n    \n    \n        6\n        77\n        8485\n        2012-02-22 00:00:00\n        58\n        Balanced\n        None\n        Little\n        62\n        Mixed\n        Organised\n        45\n        Normal\n        70\n        Lots\n        55\n        Normal\n        Organised\n        40\n        Medium\n        40\n        Press\n        60\n        Normal\n        Cover\n    \n    \n        7\n        77\n        8485\n        2013-09-20 00:00:00\n        62\n        Balanced\n        None\n        Little\n        45\n        Mixed\n        Organised\n        40\n        Normal\n        50\n        Normal\n        55\n        Normal\n        Organised\n        42\n        Medium\n        42\n        Press\n        60\n        Normal\n        Cover\n    \n    \n        8\n        77\n        8485\n        2014-09-19 00:00:00\n        58\n        Balanced\n        64\n        Normal\n        62\n        Mixed\n        Organised\n        56\n        Normal\n        68\n        Lots\n        57\n        Normal\n        Organised\n        41\n        Medium\n        42\n        Press\n        60\n        Normal\n        Cover\n    \n    \n        9\n        77\n        8485\n        2015-09-10 00:00:00\n        59\n        Balanced\n        64\n        Normal\n        53\n        Mixed\n        Organised\n        51\n        Normal\n        72\n        Lots\n        63\n        Normal\n        Free Form\n        49\n        Medium\n        45\n        Press\n        63\n        Normal\n        Cover\n    \n    \n        10\n        614\n        8576\n        2010-02-22 00:00:00\n        60\n        Balanced\n        None\n        Little\n        40\n        Mixed\n        Organised\n        45\n        Normal\n        35\n        Normal\n        55\n        Normal\n        Organised\n        30\n        Deep\n        70\n        Double\n        30\n        Narrow\n        Offside Trap\n    \n\n\n\n\n%%sql\nCREATE TABLE Team_table AS\nSELECT * FROM Team_Attributes LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n[]\n\n\n\n%%sql\nDROP TABLE IF EXISTS Team_table\n\n * sqlite:///database.sqlite\nDone.\n\n\n[]\n\n\n\nsql_query = %sql SELECT * FROM Team LIMIT 10\ndf = sql_query.DataFrame()\n\n * sqlite:///database.sqlite\nDone.\n\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      id\n      team_api_id\n      team_fifa_api_id\n      team_long_name\n      team_short_name\n    \n  \n  \n    \n      0\n      1\n      9987\n      673.0\n      KRC Genk\n      GEN\n    \n    \n      1\n      2\n      9993\n      675.0\n      Beerschot AC\n      BAC\n    \n    \n      2\n      3\n      10000\n      15005.0\n      SV Zulte-Waregem\n      ZUL\n    \n    \n      3\n      4\n      9994\n      2007.0\n      Sporting Lokeren\n      LOK\n    \n    \n      4\n      5\n      9984\n      1750.0\n      KSV Cercle Brugge\n      CEB\n    \n    \n      5\n      6\n      8635\n      229.0\n      RSC Anderlecht\n      AND\n    \n    \n      6\n      7\n      9991\n      674.0\n      KAA Gent\n      GEN\n    \n    \n      7\n      8\n      9998\n      1747.0\n      RAEC Mons\n      MON\n    \n    \n      8\n      9\n      7947\n      NaN\n      FCV Dender EH\n      DEN\n    \n    \n      9\n      10\n      9985\n      232.0\n      Standard de Liège\n      STL\n    \n  \n\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,7))\n\nplot = %sql SELECT team_short_name, count(*) FROM Team GROUP BY team_short_name ORDER BY team_short_name\nplot.bar();\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\n\nplot.pie();\n\n\n\n\n\ntype(plot)\n\nsql.run.ResultSet"
  },
  {
    "objectID": "posts/2020-10-17-dask-nlp-gutenberg-books.html",
    "href": "posts/2020-10-17-dask-nlp-gutenberg-books.html",
    "title": "Using Dask with dask.bag and regex to parse Notes from the Underground from project gutenberg",
    "section": "",
    "text": "import dask.bag as db\nimport re\n\n\nbook_bag = db.from_url('https://www.gutenberg.org/cache/epub/600/pg600.txt')\n\n\nbook_bag.take(5)\n\n(b\"\\xef\\xbb\\xbfProject Gutenberg's Notes from the Underground, by Feodor Dostoevsky\\r\\n\",\n b'\\r\\n',\n b'This eBook is for the use of anyone anywhere at no cost and with\\r\\n',\n b'almost no restrictions whatsoever.  You may copy it, give it away or\\r\\n',\n b're-use it under the terms of the Project Gutenberg License included\\r\\n')\n\n\n\nremove_spaces = book_bag.map(lambda x:x.strip())\n\n\nremove_spaces.take(10)\n\n(b\"\\xef\\xbb\\xbfProject Gutenberg's Notes from the Underground, by Feodor Dostoevsky\",\n b'',\n b'This eBook is for the use of anyone anywhere at no cost and with',\n b'almost no restrictions whatsoever.  You may copy it, give it away or',\n b're-use it under the terms of the Project Gutenberg License included',\n b'with this eBook or online at www.gutenberg.net',\n b'',\n b'',\n b'Title: Notes from the Underground',\n b'')\n\n\n\ndef decode_to_ascii(x):\n    return x.decode(\"ascii\",\"ignore\") \n\n\nascii_text = remove_spaces.map(decode_to_ascii)\n\n\nascii_text.take(10)\n\n(\"Project Gutenberg's Notes from the Underground, by Feodor Dostoevsky\",\n '',\n 'This eBook is for the use of anyone anywhere at no cost and with',\n 'almost no restrictions whatsoever.  You may copy it, give it away or',\n 're-use it under the terms of the Project Gutenberg License included',\n 'with this eBook or online at www.gutenberg.net',\n '',\n '',\n 'Title: Notes from the Underground',\n '')\n\n\n\ndef remove_punctuation(x):\n    return re.sub(r'[^\\w\\s]','',x)\n\n\nremove_punctuation = ascii_text.map(remove_punctuation)\n\n\nremove_punctuation.take(10)\n\n('Project Gutenbergs Notes from the Underground by Feodor Dostoevsky',\n '',\n 'This eBook is for the use of anyone anywhere at no cost and with',\n 'almost no restrictions whatsoever  You may copy it give it away or',\n 'reuse it under the terms of the Project Gutenberg License included',\n 'with this eBook or online at wwwgutenbergnet',\n '',\n '',\n 'Title Notes from the Underground',\n '')\n\n\n\nlower_text = remove_punctuation.map(str.lower)\n\n\nlower_text.take(10)\n\n('project gutenbergs notes from the underground by feodor dostoevsky',\n '',\n 'this ebook is for the use of anyone anywhere at no cost and with',\n 'almost no restrictions whatsoever  you may copy it give it away or',\n 'reuse it under the terms of the project gutenberg license included',\n 'with this ebook or online at wwwgutenbergnet',\n '',\n '',\n 'title notes from the underground',\n '')\n\n\n\nsplit_word_list = lower_text.map(lambda x: x.split(' '))\n\n\nsplit_word_list.take(10)\n\n(['project',\n  'gutenbergs',\n  'notes',\n  'from',\n  'the',\n  'underground',\n  'by',\n  'feodor',\n  'dostoevsky'],\n [''],\n ['this',\n  'ebook',\n  'is',\n  'for',\n  'the',\n  'use',\n  'of',\n  'anyone',\n  'anywhere',\n  'at',\n  'no',\n  'cost',\n  'and',\n  'with'],\n ['almost',\n  'no',\n  'restrictions',\n  'whatsoever',\n  '',\n  'you',\n  'may',\n  'copy',\n  'it',\n  'give',\n  'it',\n  'away',\n  'or'],\n ['reuse',\n  'it',\n  'under',\n  'the',\n  'terms',\n  'of',\n  'the',\n  'project',\n  'gutenberg',\n  'license',\n  'included'],\n ['with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergnet'],\n [''],\n [''],\n ['title', 'notes', 'from', 'the', 'underground'],\n [''])\n\n\n\ndef remove_empty_words(word_list):\n    return list(filter(lambda a: a != '', word_list))\n\nnon_empty_words = split_word_list.filter(remove_empty_words)\n\n\nnon_empty_words.take(10)\n\n(['project',\n  'gutenbergs',\n  'notes',\n  'from',\n  'the',\n  'underground',\n  'by',\n  'feodor',\n  'dostoevsky'],\n ['this',\n  'ebook',\n  'is',\n  'for',\n  'the',\n  'use',\n  'of',\n  'anyone',\n  'anywhere',\n  'at',\n  'no',\n  'cost',\n  'and',\n  'with'],\n ['almost',\n  'no',\n  'restrictions',\n  'whatsoever',\n  '',\n  'you',\n  'may',\n  'copy',\n  'it',\n  'give',\n  'it',\n  'away',\n  'or'],\n ['reuse',\n  'it',\n  'under',\n  'the',\n  'terms',\n  'of',\n  'the',\n  'project',\n  'gutenberg',\n  'license',\n  'included'],\n ['with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergnet'],\n ['title', 'notes', 'from', 'the', 'underground'],\n ['author', 'feodor', 'dostoevsky'],\n ['posting', 'date', 'september', '13', '2008', 'ebook', '600'],\n ['release', 'date', 'july', '1996'],\n ['language', 'english'])\n\n\n\nall_words = non_empty_words.flatten()\n\n\ntype(all_words)\n\ndask.bag.core.Bag\n\n\n\nall_words.take(30)\n\n('project',\n 'gutenbergs',\n 'notes',\n 'from',\n 'the',\n 'underground',\n 'by',\n 'feodor',\n 'dostoevsky',\n 'this',\n 'ebook',\n 'is',\n 'for',\n 'the',\n 'use',\n 'of',\n 'anyone',\n 'anywhere',\n 'at',\n 'no',\n 'cost',\n 'and',\n 'with',\n 'almost',\n 'no',\n 'restrictions',\n 'whatsoever',\n '',\n 'you',\n 'may')\n\n\n\nchange_to_key_value = all_words.map(lambda x: (x, 1))\n\n\nchange_to_key_value.take(4)\n\n(('project', 1), ('gutenbergs', 1), ('notes', 1), ('from', 1))\n\n\n\ngrouped_words = all_words.groupby(lambda x:x)\n\n\ngrouped_words.take(1)\n\n(('project',\n  ['project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project']),)\n\n\n\nword_count = grouped_words.map(lambda x: (x[0], len(x[1])))\n\n\nword_count.take(10)\n\n(('project', 87),\n ('gutenbergs', 2),\n ('notes', 11),\n ('from', 186),\n ('the', 1555),\n ('underground', 26),\n ('by', 153),\n ('feodor', 3),\n ('dostoevsky', 3),\n ('this', 237))\n\n\n\nchange_to_key_value.take(10)\n\n(('project', 1),\n ('gutenbergs', 1),\n ('notes', 1),\n ('from', 1),\n ('the', 1),\n ('underground', 1),\n ('by', 1),\n ('feodor', 1),\n ('dostoevsky', 1),\n ('this', 1))\n\n\n\n# Take a running count of a word\n# In this case, the default value of \n# count needs to be provided\ndef add_bin_op(count, x):\n    return count + x[1]\n\n# Take the output from multiple bin_op(s)\n# and add them to get the total count of\n# a word\ndef add_combine_op(x, y):\n    return x + y\n\nword_count = change_to_key_value.foldby(lambda x: x[0],\n                                       add_bin_op, 0,\n                                       add_combine_op)\n\n\nword_count.take(10)\n\n(('project', 87),\n ('gutenbergs', 2),\n ('notes', 11),\n ('from', 186),\n ('the', 1555),\n ('underground', 26),\n ('by', 153),\n ('feodor', 3),\n ('dostoevsky', 3),\n ('this', 237))\n\n\n\nmuch_easier = all_words.frequencies()\n\n\nmuch_easier.take(10)\n\n(('project', 87),\n ('gutenbergs', 2),\n ('notes', 11),\n ('from', 186),\n ('the', 1555),\n ('underground', 26),\n ('by', 153),\n ('feodor', 3),\n ('dostoevsky', 3),\n ('this', 237))\n\n\n\nRemoving stop words in top word frequency counts\n\nfrom spacy.lang.en import STOP_WORDS\n\n\nwithout_stopwords = all_words.filter(lambda x: x not in STOP_WORDS)\n\n\nnew_freq = without_stopwords.frequencies()\n\n\nnew_freq.take(20)\n\n(('project', 87),\n ('gutenbergs', 2),\n ('notes', 11),\n ('underground', 26),\n ('feodor', 3),\n ('dostoevsky', 3),\n ('ebook', 9),\n ('use', 18),\n ('cost', 5),\n ('restrictions', 3),\n ('whatsoever', 2),\n ('', 1896),\n ('copy', 12),\n ('away', 59),\n ('reuse', 2),\n ('terms', 24),\n ('gutenberg', 28),\n ('license', 15),\n ('included', 6),\n ('online', 4))\n\n\n\nnew_freq.topk(10)\n\ndask.bag<topk-aggregate, npartitions=1>\n\n\n\nnew_freq.topk(10, key=lambda x: x[1]).compute()\n\n[('', 1896),\n ('man', 122),\n ('know', 90),\n ('project', 87),\n ('time', 83),\n ('like', 82),\n ('come', 74),\n ('course', 73),\n ('love', 72),\n ('life', 69)]"
  },
  {
    "objectID": "posts/2020-10-07-fiscal_data-sqlitedb-copy1.html",
    "href": "posts/2020-10-07-fiscal_data-sqlitedb-copy1.html",
    "title": "Using sqlalchemy and pandas to read and write from and to a local sqlite database for Fiscal Data",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport pandas_datareader.data as web\nimport datetime\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndf=pd.read_csv('df_panel_fix.csv')\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      province\n      specific\n      general\n      year\n      gdp\n      fdi\n      rnr\n      rr\n      i\n      fr\n      reg\n      it\n    \n  \n  \n    \n      0\n      0\n      Anhui\n      147002.0\n      NaN\n      1996\n      2093.30\n      50661\n      0.000000\n      0.000000\n      0.000000\n      1128873\n      East China\n      631930\n    \n    \n      1\n      1\n      Anhui\n      151981.0\n      NaN\n      1997\n      2347.32\n      43443\n      0.000000\n      0.000000\n      0.000000\n      1356287\n      East China\n      657860\n    \n    \n      2\n      2\n      Anhui\n      174930.0\n      NaN\n      1998\n      2542.96\n      27673\n      0.000000\n      0.000000\n      0.000000\n      1518236\n      East China\n      889463\n    \n    \n      3\n      3\n      Anhui\n      285324.0\n      NaN\n      1999\n      2712.34\n      26131\n      NaN\n      NaN\n      NaN\n      1646891\n      East China\n      1227364\n    \n    \n      4\n      4\n      Anhui\n      195580.0\n      32100.0\n      2000\n      2902.09\n      31847\n      0.000000\n      0.000000\n      0.000000\n      1601508\n      East China\n      1499110\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      355\n      Zhejiang\n      391292.0\n      260313.0\n      2003\n      9705.02\n      498055\n      1.214286\n      0.035714\n      0.035714\n      6217715\n      East China\n      2261631\n    \n    \n      356\n      356\n      Zhejiang\n      656175.0\n      276652.0\n      2004\n      11648.70\n      668128\n      1.214286\n      0.035714\n      0.035714\n      NaN\n      East China\n      3162299\n    \n    \n      357\n      357\n      Zhejiang\n      656175.0\n      NaN\n      2005\n      13417.68\n      772000\n      1.214286\n      0.035714\n      0.035714\n      NaN\n      East China\n      2370200\n    \n    \n      358\n      358\n      Zhejiang\n      1017303.0\n      394795.0\n      2006\n      15718.47\n      888935\n      1.214286\n      0.035714\n      0.035714\n      11537149\n      East China\n      2553268\n    \n    \n      359\n      359\n      Zhejiang\n      844647.0\n      0.0\n      2007\n      18753.73\n      1036576\n      0.047619\n      0.000000\n      0.000000\n      16494981\n      East China\n      2939778\n    \n  \n\n360 rows × 13 columns\n\n\n\n\ndf_subset = df[[\"year\", \"reg\", \"province\", \"it\", \"specific\", 'gdp',\"fdi\"]]\ndf_subset\n\n\n\n\n\n  \n    \n      \n      year\n      reg\n      province\n      it\n      specific\n      gdp\n      fdi\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      631930\n      147002.0\n      2093.30\n      50661\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      657860\n      151981.0\n      2347.32\n      43443\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      889463\n      174930.0\n      2542.96\n      27673\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      1227364\n      285324.0\n      2712.34\n      26131\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      1499110\n      195580.0\n      2902.09\n      31847\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      2261631\n      391292.0\n      9705.02\n      498055\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      3162299\n      656175.0\n      11648.70\n      668128\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      2370200\n      656175.0\n      13417.68\n      772000\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      2553268\n      1017303.0\n      15718.47\n      888935\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      2939778\n      844647.0\n      18753.73\n      1036576\n    \n  \n\n360 rows × 7 columns\n\n\n\n\ndf_subset.columns = [\"year\", \"region\", \"province\", \"it\", \"specific\", 'gdp',\"fdi\"]\n\n\ndf_subset\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      it\n      specific\n      gdp\n      fdi\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      631930\n      147002.0\n      2093.30\n      50661\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      657860\n      151981.0\n      2347.32\n      43443\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      889463\n      174930.0\n      2542.96\n      27673\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      1227364\n      285324.0\n      2712.34\n      26131\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      1499110\n      195580.0\n      2902.09\n      31847\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      2261631\n      391292.0\n      9705.02\n      498055\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      3162299\n      656175.0\n      11648.70\n      668128\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      2370200\n      656175.0\n      13417.68\n      772000\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      2553268\n      1017303.0\n      15718.47\n      888935\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      2939778\n      844647.0\n      18753.73\n      1036576\n    \n  \n\n360 rows × 7 columns\n\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///fiscal.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nfiscal_table = db.Table('fiscal_table', metadata, \n    db.Column('year',db.Integer, nullable=True, index=False),\n    db.Column('region',db.Integer, nullable=True),\n    db.Column('province',db.Integer, nullable=True),\n    db.Column('it',db.Integer, nullable=True),\n    db.Column('specific',db.Integer, nullable=True),\n    db.Column('gdp',db.Integer, nullable=True),\n    db.Column('fdi', db.Numeric, nullable=True)\n)\n\n\nmetadata.create_all(engine) #Creates the table\n\n\ndf_subset.to_sql('fiscal_table', con=engine, if_exists='append', index=False)\n\n\nengine.execute(\"SELECT * FROM fiscal_table LIMIT 10\").fetchall()\n\n[(1996, 'East China', 'Anhui', 631930, 147002, 2093.3, 50661),\n (1997, 'East China', 'Anhui', 657860, 151981, 2347.32, 43443),\n (1998, 'East China', 'Anhui', 889463, 174930, 2542.96, 27673),\n (1999, 'East China', 'Anhui', 1227364, 285324, 2712.34, 26131),\n (2000, 'East China', 'Anhui', 1499110, 195580, 2902.09, 31847),\n (2001, 'East China', 'Anhui', 2165189, 250898, 3246.71, 33672),\n (2002, 'East China', 'Anhui', 2404936, 434149, 3519.72, 38375),\n (2003, 'East China', 'Anhui', 2815820, 619201, 3923.11, 36720),\n (2004, 'East China', 'Anhui', 3422176, 898441, 4759.3, 54669),\n (2005, 'East China', 'Anhui', 3874846, 898441, 5350.17, 69000)]\n\n\n\nsql = \"\"\"\nSELECT\n  year\n, region\n, province\n, it\n--, CURRENT_DATE()\nFROM fiscal_table\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf.tail(30)\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      it\n    \n  \n  \n    \n      330\n      2002\n      Northwest China\n      Xinjiang\n      2150325\n    \n    \n      331\n      2003\n      Northwest China\n      Xinjiang\n      2355164\n    \n    \n      332\n      2004\n      Northwest China\n      Xinjiang\n      2838346\n    \n    \n      333\n      2005\n      Northwest China\n      Xinjiang\n      3421743\n    \n    \n      334\n      2006\n      Northwest China\n      Xinjiang\n      4686125\n    \n    \n      335\n      2007\n      Northwest China\n      Xinjiang\n      5502470\n    \n    \n      336\n      1996\n      Southwest China\n      Yunnan\n      1374111\n    \n    \n      337\n      1997\n      Southwest China\n      Yunnan\n      1452425\n    \n    \n      338\n      1998\n      Southwest China\n      Yunnan\n      1617463\n    \n    \n      339\n      1999\n      Southwest China\n      Yunnan\n      1888666\n    \n    \n      340\n      2000\n      Southwest China\n      Yunnan\n      2254281\n    \n    \n      341\n      2001\n      Southwest China\n      Yunnan\n      2856307\n    \n    \n      342\n      2002\n      Southwest China\n      Yunnan\n      3035767\n    \n    \n      343\n      2003\n      Southwest China\n      Yunnan\n      3388449\n    \n    \n      344\n      2004\n      Southwest China\n      Yunnan\n      3957158\n    \n    \n      345\n      2005\n      Southwest China\n      Yunnan\n      4280994\n    \n    \n      346\n      2006\n      Southwest China\n      Yunnan\n      5046865\n    \n    \n      347\n      2007\n      Southwest China\n      Yunnan\n      6832541\n    \n    \n      348\n      1996\n      East China\n      Zhejiang\n      740327\n    \n    \n      349\n      1997\n      East China\n      Zhejiang\n      814253\n    \n    \n      350\n      1998\n      East China\n      Zhejiang\n      923455\n    \n    \n      351\n      1999\n      East China\n      Zhejiang\n      1001703\n    \n    \n      352\n      2000\n      East China\n      Zhejiang\n      1135215\n    \n    \n      353\n      2001\n      East China\n      Zhejiang\n      1203372\n    \n    \n      354\n      2002\n      East China\n      Zhejiang\n      1962633\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      2261631\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      3162299\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      2370200\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      2553268\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      2939778\n    \n  \n\n\n\n\n\n#df['it'].plot(figsize = (12, 8))"
  },
  {
    "objectID": "posts/2020-10-06-nlp-sqlitedb.html",
    "href": "posts/2020-10-06-nlp-sqlitedb.html",
    "title": "Using sqlalchemy and pandas to read and write from and to a local sqlite database for NLP",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport pandas_datareader.data as web\nimport datetime\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndf=pd.read_csv('nf_complete.csv')\n\n\ndf.columns\n\nIndex(['Unnamed: 0', 'year', 'title', 'abstract', 'theme', 'China', 'Russia',\n       'War', 'President', 'US', 'Vietnam', 'Cold War', 'World War',\n       'Vietnam War', 'Korean War', 'Survey', 'Case Study', 'Trade',\n       'Humanitarian', 'fixed_effects', 'instrumental_variable', 'regression',\n       'experimental'],\n      dtype='object')\n\n\n\ndf[[\"year\",\"title\"]]\n\n\n\n\n\n  \n    \n      \n      year\n      title\n    \n  \n  \n    \n      0\n      2000\n      \"Institutions at the Domestic/International Ne...\n    \n    \n      1\n      2000\n      Born to Lose and Doomed to Survive: State Deat...\n    \n    \n      2\n      2000\n      The significance of “allegiance” in internatio...\n    \n    \n      3\n      2000\n      The significance of “allegiance” in internatio...\n    \n    \n      4\n      2000\n      Truth-Telling and Mythmaking in Post-Soviet Ru...\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      121\n      2018\n      Planning for the Short Haul: Trade Among Belli...\n    \n    \n      122\n      2018\n      Clinging to the Anti-Imperial Mantle: The Repu...\n    \n    \n      123\n      2018\n      The New Navy's Pacific Wars:  Peripheral Confl...\n    \n    \n      124\n      2018\n      Stop or I'll Shoot, Comply and I Won't: The Di...\n    \n    \n      125\n      2018\n      Unexpected Humanitarians: Albania, the U.S. Mi...\n    \n  \n\n126 rows × 2 columns\n\n\n\n\ndf_subset = df[[\"year\", \"title\", \"abstract\", \"theme\", \"War\", 'Cold War',\"Trade\"]]\ndf_subset\n\n\n\n\n\n  \n    \n      \n      year\n      title\n      abstract\n      theme\n      War\n      Cold War\n      Trade\n    \n  \n  \n    \n      0\n      2000\n      \"Institutions at the Domestic/International Ne...\n      Civil-military relations are frequently studie...\n      IR scholarship\n      1\n      0\n      0\n    \n    \n      1\n      2000\n      Born to Lose and Doomed to Survive: State Deat...\n      Under what conditions do states die, or exit t...\n      IR scholarship\n      1\n      1\n      0\n    \n    \n      2\n      2000\n      The significance of “allegiance” in internatio...\n      My dissertation employs original and secondary...\n      IR scholarship\n      1\n      0\n      0\n    \n    \n      3\n      2000\n      The significance of “allegiance” in internatio...\n      \\nThis study revises prevailing interpretation...\n      Conflit Between States\n      0\n      1\n      0\n    \n    \n      4\n      2000\n      Truth-Telling and Mythmaking in Post-Soviet Ru...\n      Can distorted and pernicious ideas about histo...\n      Conflict Between States\n      1\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      121\n      2018\n      Planning for the Short Haul: Trade Among Belli...\n      In times of war, why do belligerents continue ...\n      Conflict between states\n      1\n      0\n      1\n    \n    \n      122\n      2018\n      Clinging to the Anti-Imperial Mantle: The Repu...\n      My dissertation project, Clinging to the Anti-...\n      Cold War\n      0\n      1\n      0\n    \n    \n      123\n      2018\n      The New Navy's Pacific Wars:  Peripheral Confl...\n      Using a transnational methodology and sources ...\n      Military History\n      1\n      0\n      0\n    \n    \n      124\n      2018\n      Stop or I'll Shoot, Comply and I Won't: The Di...\n      There is a dilemma at the heart of coercion. S...\n      IR Scholarship\n      0\n      0\n      1\n    \n    \n      125\n      2018\n      Unexpected Humanitarians: Albania, the U.S. Mi...\n      Using archives and oral history, this disserta...\n      Military History\n      0\n      0\n      0\n    \n  \n\n126 rows × 7 columns\n\n\n\n\ndf_subset.columns = [\"year\", \"title\", \"abstract\", \"theme\", \"War\", 'Cold War',\"Trade\"]\n\n\ndf_subset\n\n\n\n\n\n  \n    \n      \n      year\n      title\n      abstract\n      theme\n      War\n      Cold War\n      Trade\n    \n  \n  \n    \n      0\n      2000\n      \"Institutions at the Domestic/International Ne...\n      Civil-military relations are frequently studie...\n      IR scholarship\n      1\n      0\n      0\n    \n    \n      1\n      2000\n      Born to Lose and Doomed to Survive: State Deat...\n      Under what conditions do states die, or exit t...\n      IR scholarship\n      1\n      1\n      0\n    \n    \n      2\n      2000\n      The significance of “allegiance” in internatio...\n      My dissertation employs original and secondary...\n      IR scholarship\n      1\n      0\n      0\n    \n    \n      3\n      2000\n      The significance of “allegiance” in internatio...\n      \\nThis study revises prevailing interpretation...\n      Conflit Between States\n      0\n      1\n      0\n    \n    \n      4\n      2000\n      Truth-Telling and Mythmaking in Post-Soviet Ru...\n      Can distorted and pernicious ideas about histo...\n      Conflict Between States\n      1\n      0\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      121\n      2018\n      Planning for the Short Haul: Trade Among Belli...\n      In times of war, why do belligerents continue ...\n      Conflict between states\n      1\n      0\n      1\n    \n    \n      122\n      2018\n      Clinging to the Anti-Imperial Mantle: The Repu...\n      My dissertation project, Clinging to the Anti-...\n      Cold War\n      0\n      1\n      0\n    \n    \n      123\n      2018\n      The New Navy's Pacific Wars:  Peripheral Confl...\n      Using a transnational methodology and sources ...\n      Military History\n      1\n      0\n      0\n    \n    \n      124\n      2018\n      Stop or I'll Shoot, Comply and I Won't: The Di...\n      There is a dilemma at the heart of coercion. S...\n      IR Scholarship\n      0\n      0\n      1\n    \n    \n      125\n      2018\n      Unexpected Humanitarians: Albania, the U.S. Mi...\n      Using archives and oral history, this disserta...\n      Military History\n      0\n      0\n      0\n    \n  \n\n126 rows × 7 columns\n\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///nf_nlp.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nnf_nlp_table = db.Table('nf_nlp_table', metadata, \n    db.Column('year',db.Integer, nullable=True, index=False),\n    db.Column('title',db.String, nullable=True),\n    db.Column('abstract',db.String, nullable=True),\n    db.Column('theme',db.String, nullable=True),\n    db.Column('War',db.Integer, nullable=True),\n    db.Column('Cold War',db.Integer, nullable=True),\n    db.Column('Trade', db.Integer, nullable=True)\n)\n\n\nmetadata.create_all(engine) #Creates the table\n\n\nnf_nlp_table\n\nTable('nf_nlp_table', MetaData(bind=None), Column('year', Integer(), table=<nf_nlp_table>), Column('title', String(), table=<nf_nlp_table>), Column('abstract', String(), table=<nf_nlp_table>), Column('theme', String(), table=<nf_nlp_table>), Column('War', Integer(), table=<nf_nlp_table>), Column('Cold War', Integer(), table=<nf_nlp_table>), Column('Trade', Integer(), table=<nf_nlp_table>), schema=None)\n\n\n\ndf_subset.to_sql('nf_nlp_table', con=engine, if_exists='append', index=False)\n\n\nengine.execute(\"SELECT year, theme, title FROM nf_nlp_table LIMIT 10\").fetchall()\n\n[(2000, 'IR scholarship', '\"Institutions at the Domestic/International Nexus: the political-military  origins of military effectiveness, strategic integration and war'),\n (2000, 'IR scholarship', 'Born to Lose and Doomed to Survive: State Death and Survival in the International System'),\n (2000, 'IR scholarship', 'The significance of “allegiance” in international relations'),\n (2000, 'Conflit Between States', 'The significance of “allegiance” in international relations'),\n (2000, 'Conflict Between States', 'Truth-Telling and Mythmaking in Post-Soviet Russia: Historical Ideas, Mass Education, and Interstate Conflict'),\n (2000, 'Domestic Military History', 'Building a Cape Fear Metropolis: Fort Bragg, Fayetteville, and the  Sandhills of North Carolina'),\n (2000, 'Culture', 'The Glories and the Sadness: Shaping the national Memory of the First World War in Great Britain, Canada and Australia'),\n (2000, 'Culture / Peace Process', 'What leads longstanding adversaries to engage in conflict resolution'),\n (2001, 'Military History', 'A School for the Nation: Military  Institutions and the Boundaries of Nationality'),\n (2001, 'Military History', \"The 'American Century' Army:  The Origins of the U.S. Cold War Army, 1949-1959\")]\n\n\n\nsql = \"\"\"\nSELECT\n  year\n, theme\n, title\nFROM nf_nlp_table\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf.tail(30)\n\n\n\n\n\n  \n    \n      \n      year\n      theme\n      title\n    \n  \n  \n    \n      96\n      2014\n      IR Scholarship\n      “Multiparty Mediation: Identifying Characteris...\n    \n    \n      97\n      2014\n      IR Scholarship\n      The Justice Dilemma: International Criminal Ac...\n    \n    \n      98\n      2014\n      IR Scholarship\n      Beyond Revolution and Repression: U.S. Foreign...\n    \n    \n      99\n      2014\n      IR Scholarship\n      Protection States Trust?: Major Power Patronag...\n    \n    \n      100\n      2014\n      Nuclear Weapons\n      The Constraining Power of the Nuclear Nonproli...\n    \n    \n      101\n      2015\n      Military History\n      Selling Her the Military: Recruiting Women int...\n    \n    \n      102\n      2015\n      IR Scholarship\n      American Evangelicals, Israel, and Modern Chri...\n    \n    \n      103\n      2015\n      Non-state\n      Who Can Keep the Peace? Insurgent Organization...\n    \n    \n      104\n      2015\n      IR Scholarship\n      Credibility in Crisis: The Role of Leadership ...\n    \n    \n      105\n      2015\n      IR Scholarship\n      Evaluating the Changing of the Guards: Survey ...\n    \n    \n      106\n      2015\n      Soviet Union\n      Extracting the Eagle’s Talons: The Soviet Unio...\n    \n    \n      107\n      2015\n      IR Scholarship\n      The Control War: Communist Revolutionary Warfa...\n    \n    \n      108\n      2015\n      Nuclear Weapons\n      Nuclear Weapons and Foreign Policy\n    \n    \n      109\n      2016\n      Civ-Mil\n      Securing Control and Controlling Security: Civ...\n    \n    \n      110\n      2016\n      Military History\n      Digging for Victory: The Stalinist State’s Mob...\n    \n    \n      111\n      2016\n      Non-state\n      Persuading Power: Insurgent Diplomacy and the ...\n    \n    \n      112\n      2016\n      Conflict between states\n      A Prelude to Violence? The Effect of Nationali...\n    \n    \n      113\n      2016\n      Conflict between states\n      Engaging the ‘Evil Empire’: East – West Relati...\n    \n    \n      114\n      2017\n      IR Scholarship\n      More Talk, Less Action: Why Costless Diplomacy...\n    \n    \n      115\n      2017\n      Cold War\n      Experiments in Peace: Asian Neutralism, Human ...\n    \n    \n      116\n      2017\n      IR Scholarship\n      Fully Committed? Religiously Committed State P...\n    \n    \n      117\n      2017\n      Military History\n      Straddling the Threshold of Two Worlds: Soldie...\n    \n    \n      118\n      2017\n      Military History\n      U.S. Army’s Investigation and Adjudication of ...\n    \n    \n      119\n      2017\n      IR Scholarship\n      Grand Strategic Crucibles: The Lasting Effects...\n    \n    \n      120\n      2018\n      Nuclear Weapons\n      Trust in International Politics: The Role of L...\n    \n    \n      121\n      2018\n      Conflict between states\n      Planning for the Short Haul: Trade Among Belli...\n    \n    \n      122\n      2018\n      Cold War\n      Clinging to the Anti-Imperial Mantle: The Repu...\n    \n    \n      123\n      2018\n      Military History\n      The New Navy's Pacific Wars:  Peripheral Confl...\n    \n    \n      124\n      2018\n      IR Scholarship\n      Stop or I'll Shoot, Comply and I Won't: The Di...\n    \n    \n      125\n      2018\n      Military History\n      Unexpected Humanitarians: Albania, the U.S. Mi..."
  },
  {
    "objectID": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#t-test-comparisons-uses-the-means-counts-and-standard-deviations-of-a-treatment-and-control-in-comparison-to-an-idealized-normal-distribution-to-calculate-a-p-value-which-by-intuition-is-the-likelihood-of-seeing-a-mean-difference-of-the-same-or-more-extreme-magnitude-between-treatment-and-control-as-a-result-of-chance.-this-is-done-through-a-comparison-to-an-idealized-normal-distribution-through-the-calculation-of-a-t-statistic.-while-the-test-statistic-is-assumed-to-follow-an-idealized-normal-distribution-if-the-scaling-term-but-where-the-scaling-term-is-unknown-and-it-is-instead-estimated-based-on-the-data-which-is-assumed-to-follow-the-students-t-distribution.this-process-can-be-thought-of-trying-to-disentangle-the-signal-mean-difference-and-counts-from-the-noise-variability.-here-the-mean-difference-is-the-direction-of-the-signal-and-the-counts-are-the-strength-of-the-signal.",
    "href": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#t-test-comparisons-uses-the-means-counts-and-standard-deviations-of-a-treatment-and-control-in-comparison-to-an-idealized-normal-distribution-to-calculate-a-p-value-which-by-intuition-is-the-likelihood-of-seeing-a-mean-difference-of-the-same-or-more-extreme-magnitude-between-treatment-and-control-as-a-result-of-chance.-this-is-done-through-a-comparison-to-an-idealized-normal-distribution-through-the-calculation-of-a-t-statistic.-while-the-test-statistic-is-assumed-to-follow-an-idealized-normal-distribution-if-the-scaling-term-but-where-the-scaling-term-is-unknown-and-it-is-instead-estimated-based-on-the-data-which-is-assumed-to-follow-the-students-t-distribution.this-process-can-be-thought-of-trying-to-disentangle-the-signal-mean-difference-and-counts-from-the-noise-variability.-here-the-mean-difference-is-the-direction-of-the-signal-and-the-counts-are-the-strength-of-the-signal.",
    "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
    "section": "T test comparisons uses the means, counts and standard deviations of a treatment and control in comparison to an idealized normal distribution to calculate a p value, which by intuition is the likelihood of seeing a mean difference of the same or more extreme magnitude between treatment and control as a result of chance. This is done through a comparison to an idealized normal distribution, through the calculation of a t-statistic. While the test statistic is assumed to follow an idealized normal distribution if the scaling term, but where the scaling term is unknown and it is instead estimated based on the data, which is assumed to follow the student’s t distribution.This process can be thought of trying to disentangle the signal (mean difference and counts) from the noise variability. Here the mean difference is the direction of the signal and the counts are the strength of the signal.",
    "text": "T test comparisons uses the means, counts and standard deviations of a treatment and control in comparison to an idealized normal distribution to calculate a p value, which by intuition is the likelihood of seeing a mean difference of the same or more extreme magnitude between treatment and control as a result of chance. This is done through a comparison to an idealized normal distribution, through the calculation of a t-statistic. While the test statistic is assumed to follow an idealized normal distribution if the scaling term, but where the scaling term is unknown and it is instead estimated based on the data, which is assumed to follow the student’s t distribution.This process can be thought of trying to disentangle the signal (mean difference and counts) from the noise variability. Here the mean difference is the direction of the signal and the counts are the strength of the signal.\n\nimport numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\nimport seaborn as sns\n\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.stats.power import NormalIndPower, TTestIndPower\nfrom scipy.stats import ttest_ind_from_stats\nimport numpy as np\nimport scipy\n\n/home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n  import pandas.util.testing as tm\n\n\n\ndf = pd.read_csv('df_panel_fix.csv')\n\n\ndf_subset = df[[\"year\", \"reg\", \"province\", \"gdp\", \"fdi\", 'it',\"specific\"]]\ndf_subset.columns = [\"year\", \"region\", \"province\", \"gdp\", \"fdi\", 'it',\"specific\"]\n\n\ndf=df_subset\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.30\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.70\n      668128\n      3162299\n      656175.0\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      18753.73\n      1036576\n      2939778\n      844647.0\n    \n  \n\n360 rows × 7 columns\n\n\n\n\n# Add distributions by region\nimport matplotlib.pyplot as plt\n#fig, axes = plt.subplots(nrows=3, ncols=3)\n\ntest_cells = ['East China', 'North China']\nmetrics = ['gdp', 'fdi', 'it']\n\nfor test_cell in test_cells:\n    for metric in metrics:\n        df.loc[df[\"region\"] == test_cell].hist(column=[metric], bins=60)\n        print(test_cell)\n        print(metric)\n\n        \n\nEast China\ngdp\nEast China\nfdi\nEast China\nit\nNorth China\ngdp\nNorth China\nfdi\nNorth China\nit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf.hist(column=['fdi'], bins=60)\n\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec7cbdd8>]],\n      dtype=object)"
  },
  {
    "objectID": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#distributions-of-dependant-variables",
    "href": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#distributions-of-dependant-variables",
    "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
    "section": "Distributions of Dependant Variables",
    "text": "Distributions of Dependant Variables\n\nRight skew\n\ndf.hist(column=['fdi'], bins=60)\n\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec6e00f0>]],\n      dtype=object)\n\n\n\n\n\n\nsns.distplot(df['gdp'])\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec5d6a90>\n\n\n\n\n\n\nsns.distplot(df['fdi'])\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec4a4d30>\n\n\n\n\n\n\nsns.distplot(df['it'])\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec4df278>\n\n\n\n\n\n\nsns.distplot(df['specific'].dropna())\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec3e09e8>\n\n\n\n\n\n\ndf.hist(column=['fdi'], bins=60)\n\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec31ccc0>]],\n      dtype=object)"
  },
  {
    "objectID": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "href": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
    "section": "Removal of GDP value outliers more than 3 standard deviations away from the mean",
    "text": "Removal of GDP value outliers more than 3 standard deviations away from the mean"
  },
  {
    "objectID": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "href": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
    "section": "outlier removal of rows with GDP values that are > 3 standard deviations away form the mean",
    "text": "outlier removal of rows with GDP values that are > 3 standard deviations away form the mean\n\nimport scipy.stats as stats\n\n\ndf['gdp_zscore'] = stats.zscore(df['gdp'])"
  },
  {
    "objectID": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "href": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
    "section": "these are the observations more then > 3 SDs away from the mean of gdp that will be dropped",
    "text": "these are the observations more then > 3 SDs away from the mean of gdp that will be dropped\n\ndf[abs(df['gdp_zscore'])>3].hist(column = ['gdp'])\n\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec873208>]],\n      dtype=object)\n\n\n\n\n\n\ndf_no_gdp_outliers=df[abs(df['gdp_zscore'])<3]\n\n\ndf_no_gdp_outliers\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n      gdp_zscore\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.30\n      50661\n      631930\n      147002.0\n      -0.521466\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n      -0.464746\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n      -0.421061\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n      -0.383239\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n      -0.340870\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      354\n      2002\n      East China\n      Zhejiang\n      8003.67\n      307610\n      1962633\n      365437.0\n      0.798274\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n      1.178172\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.70\n      668128\n      3162299\n      656175.0\n      1.612181\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n      2.007180\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n      2.520929\n    \n  \n\n350 rows × 8 columns\n\n\n\n\ndf_no_gdp_outliers.hist(column=['gdp'], bins=60)\n\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec95e4e0>]],\n      dtype=object)\n\n\n\n\n\n\ncounts_fiscal=df.groupby('region').count()\ncounts_fiscal\n\n\n\n\n\n  \n    \n      \n      year\n      province\n      gdp\n      fdi\n      it\n      specific\n      gdp_zscore\n    \n    \n      region\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      East China\n      84\n      84\n      84\n      84\n      84\n      84\n      84\n    \n    \n      North China\n      48\n      48\n      48\n      48\n      48\n      47\n      48\n    \n    \n      Northeast China\n      36\n      36\n      36\n      36\n      36\n      36\n      36\n    \n    \n      Northwest China\n      60\n      60\n      60\n      60\n      60\n      60\n      60\n    \n    \n      South Central China\n      72\n      72\n      72\n      72\n      72\n      72\n      72\n    \n    \n      Southwest China\n      60\n      60\n      60\n      60\n      60\n      57\n      60\n    \n  \n\n\n\n\n\ncounts_fiscal=df.groupby('province').count()\ncounts_fiscal\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      gdp\n      fdi\n      it\n      specific\n      gdp_zscore\n    \n    \n      province\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Anhui\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Beijing\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Chongqing\n      12\n      12\n      12\n      12\n      12\n      9\n      12\n    \n    \n      Fujian\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Gansu\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Guangdong\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Guangxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Guizhou\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hainan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hebei\n      12\n      12\n      12\n      12\n      12\n      11\n      12\n    \n    \n      Heilongjiang\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Henan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hubei\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hunan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Jiangsu\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Jiangxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Jilin\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Liaoning\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Ningxia\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Qinghai\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shaanxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shandong\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shanghai\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shanxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Sichuan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Tianjin\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Tibet\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Xinjiang\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Yunnan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Zhejiang\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n  \n\n\n\n\n\n#df_no_gdp_outliers.pivot_table(index='grouping column 1', columns='grouping column 2', values='aggregating column', aggfunc='sum')\n\n\n#pd.crosstab(df_no_gdp_outliers, 'year')\n\n\ndf_no_gdp_outliers_subset = df_no_gdp_outliers[['region', 'gdp', 'fdi', 'it']]\ndf_no_gdp_outliers_subset\n\n\n\n\n\n  \n    \n      \n      region\n      gdp\n      fdi\n      it\n    \n  \n  \n    \n      0\n      East China\n      2093.30\n      50661\n      631930\n    \n    \n      1\n      East China\n      2347.32\n      43443\n      657860\n    \n    \n      2\n      East China\n      2542.96\n      27673\n      889463\n    \n    \n      3\n      East China\n      2712.34\n      26131\n      1227364\n    \n    \n      4\n      East China\n      2902.09\n      31847\n      1499110\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      354\n      East China\n      8003.67\n      307610\n      1962633\n    \n    \n      355\n      East China\n      9705.02\n      498055\n      2261631\n    \n    \n      356\n      East China\n      11648.70\n      668128\n      3162299\n    \n    \n      357\n      East China\n      13417.68\n      772000\n      2370200\n    \n    \n      358\n      East China\n      15718.47\n      888935\n      2553268\n    \n  \n\n350 rows × 4 columns\n\n\n\n\ndef aggregate_and_ttest(dataset, groupby_feature='region', alpha=.05, test_cells = [0, 1]):\n    #Imports\n    from tqdm import tqdm\n    from scipy.stats import ttest_ind_from_stats\n\n    \n    metrics = ['gdp', 'fdi', 'it']\n    \n    feature_size = 'size'\n    feature_mean = 'mean'\n    feature_std = 'std'    \n\n    for metric in tqdm(metrics):\n        \n        #print(metric)\n        crosstab = dataset.groupby(groupby_feature, as_index=False)[metric].agg(['size', 'mean', 'std'])\n        print(crosstab)\n        \n        treatment = crosstab.index[test_cells[0]]\n        control = crosstab.index[test_cells[1]]\n        \n        counts_control = crosstab.loc[control, feature_size]\n        counts_treatment = crosstab.loc[treatment, feature_size]\n\n        mean_control = crosstab.loc[control, feature_mean]\n        mean_treatment = crosstab.loc[treatment, feature_mean]\n\n        standard_deviation_control = crosstab.loc[control, feature_std]\n        standard_deviation_treatment = crosstab.loc[treatment, feature_std]\n        \n        t_statistic, p_value = ttest_ind_from_stats(mean1=mean_treatment, std1=standard_deviation_treatment, nobs1=counts_treatment,mean2=mean_control,std2=standard_deviation_control,nobs2=counts_control)\n        \n        #fstring to print the p value and t statistic\n        print(f\"The t statistic of the comparison of the treatment test cell of {treatment} compared to the control test cell of {control} is {t_statistic} and the p value is {p_value}.\")\n        \n        #f string to say of the comparison is significant at a given alpha level\n\n        if p_value < alpha: \n            print(f'The comparison between {treatment} and {control} is statistically significant at the threshold of {alpha}') \n        else: \n            print(f'The comparison between {treatment} and {control} is not statistically significant at the threshold of {alpha}')\n\n\naggregate_and_ttest(df_no_gdp_outliers_subset, test_cells = [0,1])\n\n100%|██████████| 3/3 [00:00<00:00, 115.78it/s]\n\n\n                     size         mean          std\nregion                                             \nEast China             78  6070.604231  3500.372702\nNorth China            48  4239.038542  2866.705149\nNortheast China        36  3849.076944  1948.531835\nNorthwest China        60  1340.026167  1174.399739\nSouth Central China    68  4835.540882  3697.129915\nSouthwest China        60  2410.398833  2144.589994\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 3.0488753833171947 and the p value is 0.002808541335921234.\nThe comparison between East China and North China is statistically significant at the threshold of 0.05\n                     size           mean            std\nregion                                                 \nEast China             78  355577.897436  275635.866746\nNorth China            48  169600.583333  127011.475909\nNortheast China        36  136623.750000  142734.495232\nNorthwest China        60   15111.133333   22954.193559\nSouth Central China    68  218931.426471  339981.399823\nSouthwest China        60   25405.083333   31171.373876\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 4.391461461316698 and the p value is 2.3859390186769955e-05.\nThe comparison between East China and North China is statistically significant at the threshold of 0.05\n                     size          mean           std\nregion                                               \nEast China             78  1.775615e+06  1.153030e+06\nNorth China            48  1.733719e+06  1.548794e+06\nNortheast China        36  2.665148e+06  1.768442e+06\nNorthwest China        60  1.703538e+06  1.446408e+06\nSouth Central China    68  2.500962e+06  2.196436e+06\nSouthwest China        60  2.424971e+06  2.002198e+06\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 0.17339716493934587 and the p value is 0.862621991978372.\nThe comparison between East China and North China is not statistically significant at the threshold of 0.05\n\n\n\n\n\n\nfrom tqdm import tqdm\nfor i in tqdm(range(10000)):\n    ...\n\n100%|██████████| 10000/10000 [00:00<00:00, 2169617.21it/s]\n\n\n\nEastvNorth=pd.DataFrame()\nEastvNorth= aggregate_and_ttest(df_no_gdp_outliers_subset, test_cells = [0,1])\nEastvNorth\n\n100%|██████████| 3/3 [00:00<00:00, 135.00it/s]\n\n\n                     size         mean          std\nregion                                             \nEast China             78  6070.604231  3500.372702\nNorth China            48  4239.038542  2866.705149\nNortheast China        36  3849.076944  1948.531835\nNorthwest China        60  1340.026167  1174.399739\nSouth Central China    68  4835.540882  3697.129915\nSouthwest China        60  2410.398833  2144.589994\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 3.0488753833171947 and the p value is 0.002808541335921234.\nThe comparison between East China and North China is statistically significant at the threshold of 0.05\n                     size           mean            std\nregion                                                 \nEast China             78  355577.897436  275635.866746\nNorth China            48  169600.583333  127011.475909\nNortheast China        36  136623.750000  142734.495232\nNorthwest China        60   15111.133333   22954.193559\nSouth Central China    68  218931.426471  339981.399823\nSouthwest China        60   25405.083333   31171.373876\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 4.391461461316698 and the p value is 2.3859390186769955e-05.\nThe comparison between East China and North China is statistically significant at the threshold of 0.05\n                     size          mean           std\nregion                                               \nEast China             78  1.775615e+06  1.153030e+06\nNorth China            48  1.733719e+06  1.548794e+06\nNortheast China        36  2.665148e+06  1.768442e+06\nNorthwest China        60  1.703538e+06  1.446408e+06\nSouth Central China    68  2.500962e+06  2.196436e+06\nSouthwest China        60  2.424971e+06  2.002198e+06\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 0.17339716493934587 and the p value is 0.862621991978372.\nThe comparison between East China and North China is not statistically significant at the threshold of 0.05"
  },
  {
    "objectID": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#genearate-an-experimental_crosstab-to-be-used-in-statistical-tests",
    "href": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#genearate-an-experimental_crosstab-to-be-used-in-statistical-tests",
    "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
    "section": "Genearate an experimental_crosstab to be used in statistical tests",
    "text": "Genearate an experimental_crosstab to be used in statistical tests\n\nexperimental_crosstab = df_no_gdp_outliers_subset.groupby('region').agg(['size', 'mean', 'std'])\n\n\nexperimental_crosstab.index\n\nIndex(['East China', 'North China', 'Northeast China', 'Northwest China',\n       'South Central China', 'Southwest China'],\n      dtype='object', name='region')\n\n\n\ndf = experimental_crosstab.T\ndf\n\n\n\n\n\n  \n    \n      \n      region\n      East China\n      North China\n      Northeast China\n      Northwest China\n      South Central China\n      Southwest China\n    \n  \n  \n    \n      gdp\n      size\n      7.800000e+01\n      4.800000e+01\n      3.600000e+01\n      6.000000e+01\n      6.800000e+01\n      6.000000e+01\n    \n    \n      mean\n      6.070604e+03\n      4.239039e+03\n      3.849077e+03\n      1.340026e+03\n      4.835541e+03\n      2.410399e+03\n    \n    \n      std\n      3.500373e+03\n      2.866705e+03\n      1.948532e+03\n      1.174400e+03\n      3.697130e+03\n      2.144590e+03\n    \n    \n      fdi\n      size\n      7.800000e+01\n      4.800000e+01\n      3.600000e+01\n      6.000000e+01\n      6.800000e+01\n      6.000000e+01\n    \n    \n      mean\n      3.555779e+05\n      1.696006e+05\n      1.366238e+05\n      1.511113e+04\n      2.189314e+05\n      2.540508e+04\n    \n    \n      std\n      2.756359e+05\n      1.270115e+05\n      1.427345e+05\n      2.295419e+04\n      3.399814e+05\n      3.117137e+04\n    \n    \n      it\n      size\n      7.800000e+01\n      4.800000e+01\n      3.600000e+01\n      6.000000e+01\n      6.800000e+01\n      6.000000e+01\n    \n    \n      mean\n      1.775615e+06\n      1.733719e+06\n      2.665148e+06\n      1.703538e+06\n      2.500962e+06\n      2.424971e+06\n    \n    \n      std\n      1.153030e+06\n      1.548794e+06\n      1.768442e+06\n      1.446408e+06\n      2.196436e+06\n      2.002198e+06\n    \n  \n\n\n\n\n\n#experimental_crosstab.reset_index().unstack()\n\n\nexperimental_crosstab.iloc[0,1]\n\n6070.604230769231\n\n\n\nexperimental_crosstab.index\n\nIndex(['East China', 'North China', 'Northeast China', 'Northwest China',\n       'South Central China', 'Southwest China'],\n      dtype='object', name='region')\n\n\n\nexperimental_crosstab\n\n\n\n\n\n  \n    \n      \n      gdp\n      fdi\n      it\n    \n    \n      \n      size\n      mean\n      std\n      size\n      mean\n      std\n      size\n      mean\n      std\n    \n    \n      region\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      East China\n      78\n      6070.604231\n      3500.372702\n      78\n      355577.897436\n      275635.866746\n      78\n      1.775615e+06\n      1.153030e+06\n    \n    \n      North China\n      48\n      4239.038542\n      2866.705149\n      48\n      169600.583333\n      127011.475909\n      48\n      1.733719e+06\n      1.548794e+06\n    \n    \n      Northeast China\n      36\n      3849.076944\n      1948.531835\n      36\n      136623.750000\n      142734.495232\n      36\n      2.665148e+06\n      1.768442e+06\n    \n    \n      Northwest China\n      60\n      1340.026167\n      1174.399739\n      60\n      15111.133333\n      22954.193559\n      60\n      1.703538e+06\n      1.446408e+06\n    \n    \n      South Central China\n      68\n      4835.540882\n      3697.129915\n      68\n      218931.426471\n      339981.399823\n      68\n      2.500962e+06\n      2.196436e+06\n    \n    \n      Southwest China\n      60\n      2410.398833\n      2144.589994\n      60\n      25405.083333\n      31171.373876\n      60\n      2.424971e+06\n      2.002198e+06\n    \n  \n\n\n\n\n\nexperimental_crosstab.columns = ['_'.join(col) for col in experimental_crosstab.columns.values]\n\n\nexperimental_crosstab\n\n\n\n\n\n  \n    \n      \n      gdp_size\n      gdp_mean\n      gdp_std\n      fdi_size\n      fdi_mean\n      fdi_std\n      it_size\n      it_mean\n      it_std\n    \n    \n      region\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      East China\n      78\n      6070.604231\n      3500.372702\n      78\n      355577.897436\n      275635.866746\n      78\n      1.775615e+06\n      1.153030e+06\n    \n    \n      North China\n      48\n      4239.038542\n      2866.705149\n      48\n      169600.583333\n      127011.475909\n      48\n      1.733719e+06\n      1.548794e+06\n    \n    \n      Northeast China\n      36\n      3849.076944\n      1948.531835\n      36\n      136623.750000\n      142734.495232\n      36\n      2.665148e+06\n      1.768442e+06\n    \n    \n      Northwest China\n      60\n      1340.026167\n      1174.399739\n      60\n      15111.133333\n      22954.193559\n      60\n      1.703538e+06\n      1.446408e+06\n    \n    \n      South Central China\n      68\n      4835.540882\n      3697.129915\n      68\n      218931.426471\n      339981.399823\n      68\n      2.500962e+06\n      2.196436e+06\n    \n    \n      Southwest China\n      60\n      2410.398833\n      2144.589994\n      60\n      25405.083333\n      31171.373876\n      60\n      2.424971e+06\n      2.002198e+06\n    \n  \n\n\n\n\n\nexperimental_crosstab.loc['East China', 'gdp_size']\n\n78\n\n\n\nexperimental_crosstab.to_csv('fiscal_experimental_crosstab.csv')"
  },
  {
    "objectID": "posts/2020-10-11-dask-xgboost-fiscal-data.html",
    "href": "posts/2020-10-11-dask-xgboost-fiscal-data.html",
    "title": "Moving Fiscal Data from a sqlite db to a dask dataframe",
    "section": "",
    "text": "import numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///fiscal_data.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nengine.execute(\"SELECT * FROM fiscal_data LIMIT 1\").fetchall()\n\n[(1996, 'East China', 'Anhui', '2093.3', 50661, 631930, 147002)]\n\n\n\nsql = \"\"\"\nSELECT year\n, region\n, province\n, gdp\n, fdi\n, it\n, specific\nFROM fiscal_data\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.3\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.7\n      668128\n      3162299\n      656175.0\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      18753.73\n      1036576\n      2939778\n      844647.0\n    \n  \n\n360 rows × 7 columns\n\n\n\n\ndf.columns\n\nIndex(['year', 'region', 'province', 'gdp', 'fdi', 'it', 'specific'], dtype='object')\n\n\n\ndf.gdp.hist()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f41255eacc0>\n\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n\nfrom dask import dataframe as dd \n\n\nddf = dd.from_pandas(df, npartitions=5)\n\n\nprint(ddf)\n\n\nddf.npartitions\n\n\nddf.npartitions\n\n\nlen(ddf)\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n\nddf.describe().compute()\n\n\nddf.columns\n\n\nfeat_list = [\"year\", \"fdi\"]\ncat_feat_list = [\"region\", \"province\"]\ntarget = [\"gdp\"]\n\n\nddf[\"year\"] = ddf[\"year\"].astype(float)\nddf[\"fdi\"] = ddf[\"fdi\"].astype(float)\n#ddf[\"province\"] = ddf[\"province\"].astype(float)\n#ddf[\"region\"] = ddf[\"region\"].astype(float)\nddf[\"gdp\"] = ddf[\"gdp\"].astype(float)\nddf[\"it\"] = ddf[\"it\"].astype(float)\n\n\ntype(target)\n\n\nx=ddf[feat_list].persist()\ny=ddf[target].persist()\n\n\nx\n\n\ny.compute()\n\n\nprint(x.shape,y.shape)\n\n\nx.count().compute()\n\n\nfrom dask_ml.xgboost import XGBRegressor\n\n\nXGBR = XGBRegressor()\n\n\n%%time\nXGBR_model = XGBR.fit(x,y)\n\n\nXGBR_model\n\n\nclient.close()"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "",
    "text": "import numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\nimport seaborn as sns"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html#distributions-of-dependant-variables",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html#distributions-of-dependant-variables",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "Distributions of Dependant Variables",
    "text": "Distributions of Dependant Variables\n\nRight skew\n\nsns.distplot(df['gdp'])\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f1043c20588>\n\n\n\n\n\n\nsns.distplot(df['fdi'])\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f10437f74a8>\n\n\n\n\n\n\nsns.distplot(df['it'])\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f1043a09ef0>\n\n\n\n\n\n\nsns.distplot(df['specific'].dropna())\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f10439b7a20>\n\n\n\n\n\n\ndf.hist(column=['fdi'], bins=60)\n\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f10439a19e8>]],\n      dtype=object)"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "Removal of GDP value outliers more than 3 standard deviations away from the mean",
    "text": "Removal of GDP value outliers more than 3 standard deviations away from the mean"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "outlier removal of rows with GDP values that are > 3 standard deviations away form the mean",
    "text": "outlier removal of rows with GDP values that are > 3 standard deviations away form the mean\n\nimport scipy.stats as stats\n\n\ndf['gdp_zscore'] = stats.zscore(df['gdp'])"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "these are the observations more then > 3 SDs away from the mean of gdp that will be dropped",
    "text": "these are the observations more then > 3 SDs away from the mean of gdp that will be dropped\n\ndf[abs(df['gdp_zscore'])>3].hist(column = ['gdp'])\n\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f104364e0f0>]],\n      dtype=object)\n\n\n\n\n\n\ndf_no_gdp_outliers=df[abs(df['gdp_zscore'])<3]\n\n\ndf_no_gdp_outliers\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n      gdp_zscore\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.30\n      50661\n      631930\n      147002.0\n      -0.521466\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n      -0.464746\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n      -0.421061\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n      -0.383239\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n      -0.340870\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      354\n      2002\n      East China\n      Zhejiang\n      8003.67\n      307610\n      1962633\n      365437.0\n      0.798274\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n      1.178172\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.70\n      668128\n      3162299\n      656175.0\n      1.612181\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n      2.007180\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n      2.520929\n    \n  \n\n350 rows × 8 columns\n\n\n\n\ndf_no_gdp_outliers.hist(column=['gdp'], bins=60)\n\narray([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f10429f5ba8>]],\n      dtype=object)\n\n\n\n\n\n\ncounts_fiscal=df.groupby('region').count()\ncounts_fiscal\n\n\n\n\n\n  \n    \n      \n      year\n      province\n      gdp\n      fdi\n      it\n      specific\n      gdp_zscore\n    \n    \n      region\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      East China\n      84\n      84\n      84\n      84\n      84\n      84\n      84\n    \n    \n      North China\n      48\n      48\n      48\n      48\n      48\n      47\n      48\n    \n    \n      Northeast China\n      36\n      36\n      36\n      36\n      36\n      36\n      36\n    \n    \n      Northwest China\n      60\n      60\n      60\n      60\n      60\n      60\n      60\n    \n    \n      South Central China\n      72\n      72\n      72\n      72\n      72\n      72\n      72\n    \n    \n      Southwest China\n      60\n      60\n      60\n      60\n      60\n      57\n      60\n    \n  \n\n\n\n\n\ncounts_fiscal=df.groupby('province').count()\ncounts_fiscal\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      gdp\n      fdi\n      it\n      specific\n      gdp_zscore\n    \n    \n      province\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Anhui\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Beijing\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Chongqing\n      12\n      12\n      12\n      12\n      12\n      9\n      12\n    \n    \n      Fujian\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Gansu\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Guangdong\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Guangxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Guizhou\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hainan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hebei\n      12\n      12\n      12\n      12\n      12\n      11\n      12\n    \n    \n      Heilongjiang\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Henan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hubei\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Hunan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Jiangsu\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Jiangxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Jilin\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Liaoning\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Ningxia\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Qinghai\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shaanxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shandong\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shanghai\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Shanxi\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Sichuan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Tianjin\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Tibet\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Xinjiang\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Yunnan\n      12\n      12\n      12\n      12\n      12\n      12\n      12\n    \n    \n      Zhejiang\n      12\n      12\n      12\n      12\n      12\n      12\n      12"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html#subset-by-needed-columns",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html#subset-by-needed-columns",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "Subset by needed columns",
    "text": "Subset by needed columns\n\ndf_no_gdp_outliers.columns\n\nIndex(['year', 'region', 'province', 'gdp', 'fdi', 'it', 'specific',\n       'gdp_zscore'],\n      dtype='object')\n\n\n\ndf_no_gdp_outliers_subset = df_no_gdp_outliers[['region', 'gdp', 'fdi', 'it']]\ndf_no_gdp_outliers_subset\n\n\n\n\n\n  \n    \n      \n      region\n      gdp\n      fdi\n      it\n    \n  \n  \n    \n      0\n      East China\n      2093.30\n      50661\n      631930\n    \n    \n      1\n      East China\n      2347.32\n      43443\n      657860\n    \n    \n      2\n      East China\n      2542.96\n      27673\n      889463\n    \n    \n      3\n      East China\n      2712.34\n      26131\n      1227364\n    \n    \n      4\n      East China\n      2902.09\n      31847\n      1499110\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      354\n      East China\n      8003.67\n      307610\n      1962633\n    \n    \n      355\n      East China\n      9705.02\n      498055\n      2261631\n    \n    \n      356\n      East China\n      11648.70\n      668128\n      3162299\n    \n    \n      357\n      East China\n      13417.68\n      772000\n      2370200\n    \n    \n      358\n      East China\n      15718.47\n      888935\n      2553268\n    \n  \n\n350 rows × 4 columns"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html#genearate-an-experimental_crosstab-to-be-used-in-statistical-tests",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html#genearate-an-experimental_crosstab-to-be-used-in-statistical-tests",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "Genearate an experimental_crosstab to be used in statistical tests",
    "text": "Genearate an experimental_crosstab to be used in statistical tests\n\nexperimental_crosstab = df_no_gdp_outliers_subset.groupby('region').agg(['size', 'mean', 'std'])\n\n\nexperimental_crosstab.index\n\nIndex(['East China', 'North China', 'Northeast China', 'Northwest China',\n       'South Central China', 'Southwest China'],\n      dtype='object', name='region')\n\n\n\nexperimental_crosstab = experimental_crosstab.reset_index()\n\n\nexperimental_crosstab\n\n\n\n\n\n  \n    \n      \n      region\n      gdp\n      fdi\n      it\n    \n    \n      \n      \n      size\n      mean\n      std\n      size\n      mean\n      std\n      size\n      mean\n      std\n    \n  \n  \n    \n      0\n      East China\n      78\n      6070.604231\n      3500.372702\n      78\n      355577.897436\n      275635.866746\n      78\n      1.775615e+06\n      1.153030e+06\n    \n    \n      1\n      North China\n      48\n      4239.038542\n      2866.705149\n      48\n      169600.583333\n      127011.475909\n      48\n      1.733719e+06\n      1.548794e+06\n    \n    \n      2\n      Northeast China\n      36\n      3849.076944\n      1948.531835\n      36\n      136623.750000\n      142734.495232\n      36\n      2.665148e+06\n      1.768442e+06\n    \n    \n      3\n      Northwest China\n      60\n      1340.026167\n      1174.399739\n      60\n      15111.133333\n      22954.193559\n      60\n      1.703538e+06\n      1.446408e+06\n    \n    \n      4\n      South Central China\n      68\n      4835.540882\n      3697.129915\n      68\n      218931.426471\n      339981.399823\n      68\n      2.500962e+06\n      2.196436e+06\n    \n    \n      5\n      Southwest China\n      60\n      2410.398833\n      2144.589994\n      60\n      25405.083333\n      31171.373876\n      60\n      2.424971e+06\n      2.002198e+06\n    \n  \n\n\n\n\n\nexperimental_crosstab.to_csv('fiscal_experimental_crosstab.csv')"
  },
  {
    "objectID": "posts/2020-10-25-arima-adf-forecasting.html",
    "href": "posts/2020-10-25-arima-adf-forecasting.html",
    "title": "Stock Market Analysis of the S&P 500 Index using ARIMA and Seasonal ARIMA for forecasting",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport pandas_datareader\nimport datetime\nimport pandas_datareader.data as web\n\nimport statsmodels.api as sm\nimport quandl"
  },
  {
    "objectID": "posts/2020-10-25-arima-adf-forecasting.html#step-2-visualize-the-data",
    "href": "posts/2020-10-25-arima-adf-forecasting.html#step-2-visualize-the-data",
    "title": "Stock Market Analysis of the S&P 500 Index using ARIMA and Seasonal ARIMA for forecasting",
    "section": "Step 2: Visualize the Data",
    "text": "Step 2: Visualize the Data\nLet’s visualize this data with a few methods.\n\ndf.plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8873251b38>\n\n\n\n\n\n\ntimeseries = df['Value']\n\n\ntimeseries.rolling(12).mean().plot(label='12 Month Rolling Mean')\ntimeseries.rolling(12).std().plot(label='12 Month Rolling Std')\ntimeseries.plot()\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f88731e22e8>\n\n\n\n\n\n\ntimeseries.rolling(12).mean().plot(label='12 Month Rolling Mean')\ntimeseries.plot()\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f887315f630>"
  },
  {
    "objectID": "posts/2020-10-25-arima-adf-forecasting.html#decomposition",
    "href": "posts/2020-10-25-arima-adf-forecasting.html#decomposition",
    "title": "Stock Market Analysis of the S&P 500 Index using ARIMA and Seasonal ARIMA for forecasting",
    "section": "Decomposition",
    "text": "Decomposition\nETS decomposition allows us to see the individual parts!\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(df['Value'], freq=12)  \nfig = plt.figure()  \nfig = decomposition.plot()  \nfig.set_size_inches(15, 8)\n\n<Figure size 432x288 with 0 Axes>"
  },
  {
    "objectID": "posts/2020-10-25-arima-adf-forecasting.html#testing-for-stationarity",
    "href": "posts/2020-10-25-arima-adf-forecasting.html#testing-for-stationarity",
    "title": "Stock Market Analysis of the S&P 500 Index using ARIMA and Seasonal ARIMA for forecasting",
    "section": "Testing for Stationarity",
    "text": "Testing for Stationarity\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      1960-01-01\n      58.03\n    \n    \n      1960-02-01\n      55.78\n    \n    \n      1960-03-01\n      55.02\n    \n    \n      1960-04-01\n      55.73\n    \n    \n      1960-05-01\n      55.22\n    \n  \n\n\n\n\n\nfrom statsmodels.tsa.stattools import adfuller\n\n\nresult = adfuller(df['Value'])\n\n\nprint('Augmented Dickey-Fuller Test:')\nlabels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n\nfor value,label in zip(result,labels):\n    print(label+' : '+str(value) )\n    \nif result[1] <= 0.05:\n    print(\"strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\")\nelse:\n    print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : 1.7247353245135\np-value : 0.9981874531215522\n#Lags Used : 20\nNumber of Observations Used : 719\nweak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \n\n\n\ndef adf_check(time_series):\n    \"\"\"\n    Pass in a time series, returns ADF report\n    \"\"\"\n    result = adfuller(time_series)\n    print('Augmented Dickey-Fuller Test:')\n    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n\n    for value,label in zip(result,labels):\n        print(label+' : '+str(value) )\n    \n    if result[1] <= 0.05:\n        print(\"strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\")\n    else:\n        print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n\n** First Difference **\n\ndf['Value First Difference'] = df['Value'] - df['Value'].shift(1)\n\n\nadf_check(df['Value First Difference'].dropna())\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : -4.267790128581322\np-value : 0.0005048563860225925\n#Lags Used : 20\nNumber of Observations Used : 718\nstrong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\n\n\n\ndf['Value First Difference'].plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f886d63feb8>\n\n\n\n\n\n** Second Difference **\n\ndf['Value Second Difference'] = df['Value First Difference'] - df['Value First Difference'].shift(1)\n\n\nadf_check(df['Value Second Difference'].dropna())\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : -12.29955077642857\np-value : 7.504260735615441e-23\n#Lags Used : 18\nNumber of Observations Used : 719\nstrong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\n\n\n\ndf['Value Second Difference'].plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8877f726d8>\n\n\n\n\n\n** Seasonal Difference **\n\ndf['Seasonal Difference'] = df['Value'] - df['Value'].shift(12)\ndf['Seasonal Difference'].plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8877fb79e8>\n\n\n\n\n\n\nadf_check(df['Seasonal Difference'].dropna())\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : -5.239903673260254\np-value : 7.284266188346342e-06\n#Lags Used : 20\nNumber of Observations Used : 707\nstrong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\n\n\n** Seasonal First Difference **\n\ndf['Seasonal First Difference'] = df['Value First Difference'] - df['Value First Difference'].shift(12)\ndf['Seasonal First Difference'].plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f88acb0c2b0>\n\n\n\n\n\n\nadf_check(df['Seasonal First Difference'].dropna())\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : -6.196739887980032\np-value : 5.940155101037563e-08\n#Lags Used : 20\nNumber of Observations Used : 706\nstrong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n\n\n# Check out: https://stackoverflow.com/questions/21788593/statsmodels-duplicate-charts\n# https://github.com/statsmodels/statsmodels/issues/1265\nfig_first = plot_acf(df[\"Value First Difference\"].dropna())\n\n\n\n\n\nfig_seasonal_first = plot_acf(df[\"Seasonal First Difference\"].dropna())\n\n\n\n\nPandas also has this functionality built in, but only for ACF, not PACF. So I recommend using statsmodels, as ACF and PACF is more core to its functionality than it is to pandas’ functionality.\n\nfrom pandas.plotting import autocorrelation_plot\nautocorrelation_plot(df['Seasonal First Difference'].dropna())\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f886d660358>\n\n\n\n\n\nWe can then plot this relationship:\n\nresult = plot_pacf(df[\"Seasonal First Difference\"].dropna())\n\n\n\n\n\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(df['Seasonal First Difference'].iloc[13:], lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(df['Seasonal First Difference'].iloc[13:], lags=40, ax=ax2)\n\n\n\n\n\nfrom statsmodels.tsa.arima_model import ARIMA\n\n\nhelp(ARIMA)\n\nHelp on class ARIMA in module statsmodels.tsa.arima_model:\n\nclass ARIMA(ARMA)\n |  ARIMA(endog, order, exog=None, dates=None, freq=None, missing='none')\n |  \n |  Autoregressive Integrated Moving Average ARIMA(p,d,q) Model\n |  \n |  Parameters\n |  ----------\n |  endog : array-like\n |      The endogenous variable.\n |  order : iterable\n |      The (p,d,q) order of the model for the number of AR parameters,\n |      differences, and MA parameters to use.\n |  exog : array-like, optional\n |      An optional array of exogenous variables. This should *not* include a\n |      constant or trend. You can specify this in the `fit` method.\n |  dates : array-like of datetime, optional\n |      An array-like object of datetime objects. If a pandas object is given\n |      for endog or exog, it is assumed to have a DateIndex.\n |  freq : str, optional\n |      The frequency of the time-series. A Pandas offset or 'B', 'D', 'W',\n |      'M', 'A', or 'Q'. This is optional if dates are given.\n |  \n |  \n |  Notes\n |  -----\n |  If exogenous variables are given, then the model that is fit is\n |  \n |  .. math::\n |  \n |     \\phi(L)(y_t - X_t\\beta) = \\theta(L)\\epsilon_t\n |  \n |  where :math:`\\phi` and :math:`\\theta` are polynomials in the lag\n |  operator, :math:`L`. This is the regression model with ARMA errors,\n |  or ARMAX model. This specification is used, whether or not the model\n |  is fit using conditional sum of square or maximum-likelihood, using\n |  the `method` argument in\n |  :meth:`statsmodels.tsa.arima_model.ARIMA.fit`. Therefore, for\n |  now, `css` and `mle` refer to estimation methods only. This may\n |  change for the case of the `css` model in future versions.\n |  \n |  Method resolution order:\n |      ARIMA\n |      ARMA\n |      statsmodels.tsa.base.tsa_model.TimeSeriesModel\n |      statsmodels.base.model.LikelihoodModel\n |      statsmodels.base.model.Model\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __getnewargs__(self)\n |  \n |  __init__(self, endog, order, exog=None, dates=None, freq=None, missing='none')\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  fit(self, start_params=None, trend='c', method='css-mle', transparams=True, solver='lbfgs', maxiter=500, full_output=1, disp=5, callback=None, start_ar_lags=None, **kwargs)\n |      Fits ARIMA(p,d,q) model by exact maximum likelihood via Kalman filter.\n |      \n |      Parameters\n |      ----------\n |      start_params : array-like, optional\n |          Starting parameters for ARMA(p,q).  If None, the default is given\n |          by ARMA._fit_start_params.  See there for more information.\n |      transparams : bool, optional\n |          Whehter or not to transform the parameters to ensure stationarity.\n |          Uses the transformation suggested in Jones (1980).  If False,\n |          no checking for stationarity or invertibility is done.\n |      method : str {'css-mle','mle','css'}\n |          This is the loglikelihood to maximize.  If \"css-mle\", the\n |          conditional sum of squares likelihood is maximized and its values\n |          are used as starting values for the computation of the exact\n |          likelihood via the Kalman filter.  If \"mle\", the exact likelihood\n |          is maximized via the Kalman Filter.  If \"css\" the conditional sum\n |          of squares likelihood is maximized.  All three methods use\n |          `start_params` as starting parameters.  See above for more\n |          information.\n |      trend : str {'c','nc'}\n |          Whether to include a constant or not.  'c' includes constant,\n |          'nc' no constant.\n |      solver : str or None, optional\n |          Solver to be used.  The default is 'lbfgs' (limited memory\n |          Broyden-Fletcher-Goldfarb-Shanno).  Other choices are 'bfgs',\n |          'newton' (Newton-Raphson), 'nm' (Nelder-Mead), 'cg' -\n |          (conjugate gradient), 'ncg' (non-conjugate gradient), and\n |          'powell'. By default, the limited memory BFGS uses m=12 to\n |          approximate the Hessian, projected gradient tolerance of 1e-8 and\n |          factr = 1e2. You can change these by using kwargs.\n |      maxiter : int, optional\n |          The maximum number of function evaluations. Default is 500.\n |      tol : float\n |          The convergence tolerance.  Default is 1e-08.\n |      full_output : bool, optional\n |          If True, all output from solver will be available in\n |          the Results object's mle_retvals attribute.  Output is dependent\n |          on the solver.  See Notes for more information.\n |      disp : int, optional\n |          If True, convergence information is printed.  For the default\n |          l_bfgs_b solver, disp controls the frequency of the output during\n |          the iterations. disp < 0 means no output in this case.\n |      callback : function, optional\n |          Called after each iteration as callback(xk) where xk is the current\n |          parameter vector.\n |      start_ar_lags : int, optional\n |          Parameter for fitting start_params. When fitting start_params,\n |          residuals are obtained from an AR fit, then an ARMA(p,q) model is\n |          fit via OLS using these residuals. If start_ar_lags is None, fit\n |          an AR process according to best BIC. If start_ar_lags is not None,\n |          fits an AR process with a lag length equal to start_ar_lags.\n |          See ARMA._fit_start_params_hr for more information.\n |      kwargs\n |          See Notes for keyword arguments that can be passed to fit.\n |      \n |      Returns\n |      -------\n |      `statsmodels.tsa.arima.ARIMAResults` class\n |      \n |      See Also\n |      --------\n |      statsmodels.base.model.LikelihoodModel.fit : for more information\n |          on using the solvers.\n |      ARIMAResults : results class returned by fit\n |      \n |      Notes\n |      -----\n |      If fit by 'mle', it is assumed for the Kalman Filter that the initial\n |      unknown state is zero, and that the initial variance is\n |      P = dot(inv(identity(m**2)-kron(T,T)),dot(R,R.T).ravel('F')).reshape(r,\n |      r, order = 'F')\n |  \n |  predict(self, params, start=None, end=None, exog=None, typ='linear', dynamic=False)\n |      ARIMA model in-sample and out-of-sample prediction\n |      \n |      Parameters\n |      ----------\n |      params : array-like\n |          The fitted parameters of the model.\n |      start : int, str, or datetime\n |          Zero-indexed observation number at which to start forecasting, ie.,\n |          the first forecast is start. Can also be a date string to\n |          parse or a datetime type.\n |      end : int, str, or datetime\n |          Zero-indexed observation number at which to end forecasting, ie.,\n |          the first forecast is start. Can also be a date string to\n |          parse or a datetime type. However, if the dates index does not\n |          have a fixed frequency, end must be an integer index if you\n |          want out of sample prediction.\n |      exog : array-like, optional\n |          If the model is an ARMAX and out-of-sample forecasting is\n |          requested, exog must be given. Note that you'll need to pass\n |          `k_ar` additional lags for any exogenous variables. E.g., if you\n |          fit an ARMAX(2, q) model and want to predict 5 steps, you need 7\n |          observations to do this.\n |      dynamic : bool, optional\n |          The `dynamic` keyword affects in-sample prediction. If dynamic\n |          is False, then the in-sample lagged values are used for\n |          prediction. If `dynamic` is True, then in-sample forecasts are\n |          used in place of lagged dependent variables. The first forecasted\n |          value is `start`.\n |      typ : str {'linear', 'levels'}\n |      \n |          - 'linear' : Linear prediction in terms of the differenced\n |            endogenous variables.\n |          - 'levels' : Predict the levels of the original endogenous\n |            variables.\n |      \n |      \n |      Returns\n |      -------\n |      predict : array\n |          The predicted values.\n |      \n |      \n |      \n |      Notes\n |      -----\n |      Use the results predict method instead.\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  __new__(cls, endog, order, exog=None, dates=None, freq=None, missing='none')\n |      Create and return a new object.  See help(type) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from ARMA:\n |  \n |  geterrors(self, params)\n |      Get the errors of the ARMA process.\n |      \n |      Parameters\n |      ----------\n |      params : array-like\n |          The fitted ARMA parameters\n |      order : array-like\n |          3 item iterable, with the number of AR, MA, and exogenous\n |          parameters, including the trend\n |  \n |  hessian(self, params)\n |      Compute the Hessian at params,\n |      \n |      Notes\n |      -----\n |      This is a numerical approximation.\n |  \n |  loglike(self, params, set_sigma2=True)\n |      Compute the log-likelihood for ARMA(p,q) model\n |      \n |      Notes\n |      -----\n |      Likelihood used depends on the method set in fit\n |  \n |  loglike_css(self, params, set_sigma2=True)\n |      Conditional Sum of Squares likelihood function.\n |  \n |  loglike_kalman(self, params, set_sigma2=True)\n |      Compute exact loglikelihood for ARMA(p,q) model by the Kalman Filter.\n |  \n |  score(self, params)\n |      Compute the score function at params.\n |      \n |      Notes\n |      -----\n |      This is a numerical approximation.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from ARMA:\n |  \n |  from_formula(formula, data, subset=None, drop_cols=None, *args, **kwargs) from builtins.type\n |      Create a Model from a formula and dataframe.\n |      \n |      Parameters\n |      ----------\n |      formula : str or generic Formula object\n |          The formula specifying the model\n |      data : array-like\n |          The data for the model. See Notes.\n |      subset : array-like\n |          An array-like object of booleans, integers, or index values that\n |          indicate the subset of df to use in the model. Assumes df is a\n |          `pandas.DataFrame`\n |      drop_cols : array-like\n |          Columns to drop from the design matrix.  Cannot be used to\n |          drop terms involving categoricals.\n |      args : extra arguments\n |          These are passed to the model\n |      kwargs : extra keyword arguments\n |          These are passed to the model with one exception. The\n |          ``eval_env`` keyword is passed to patsy. It can be either a\n |          :class:`patsy:patsy.EvalEnvironment` object or an integer\n |          indicating the depth of the namespace to use. For example, the\n |          default ``eval_env=0`` uses the calling namespace. If you wish\n |          to use a \"clean\" environment set ``eval_env=-1``.\n |      \n |      Returns\n |      -------\n |      model : Model instance\n |      \n |      Notes\n |      -----\n |      data must define __getitem__ with the keys in the formula terms\n |      args and kwargs are passed on to the model instantiation. E.g.,\n |      a numpy structured or rec array, a dictionary, or a pandas DataFrame.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from statsmodels.tsa.base.tsa_model.TimeSeriesModel:\n |  \n |  exog_names\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from statsmodels.base.model.LikelihoodModel:\n |  \n |  information(self, params)\n |      Fisher information matrix of model\n |      \n |      Returns -Hessian of loglike evaluated at params.\n |  \n |  initialize(self)\n |      Initialize (possibly re-initialize) a Model instance. For\n |      instance, the design matrix of a linear model may change\n |      and some things must be recomputed.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from statsmodels.base.model.Model:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  endog_names\n |      Names of endogenous variables\n\n\n\n\nmodel = sm.tsa.statespace.SARIMAX(df['Value'],order=(0,1,0), seasonal_order=(1,1,1,12))\nresults = model.fit()\nprint(results.summary())\n\n/home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:219: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  ' ignored when e.g. forecasting.', ValueWarning)\n\n\n                                 Statespace Model Results                                 \n==========================================================================================\nDep. Variable:                              Value   No. Observations:                  740\nModel:             SARIMAX(0, 1, 0)x(1, 1, 1, 12)   Log Likelihood               -3719.108\nDate:                            Fri, 23 Oct 2020   AIC                           7444.215\nTime:                                    09:03:22   BIC                           7457.982\nSample:                                         0   HQIC                          7449.528\n                                            - 740                                         \nCovariance Type:                              opg                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.S.L12       0.0043      0.021      0.201      0.840      -0.037       0.046\nma.S.L12      -0.9513      0.018    -53.297      0.000      -0.986      -0.916\nsigma2      1563.8216     30.663     51.001      0.000    1503.724    1623.919\n===================================================================================\nLjung-Box (Q):                      116.83   Jarque-Bera (JB):              9251.13\nProb(Q):                              0.00   Prob(JB):                         0.00\nHeteroskedasticity (H):             410.23   Skew:                            -1.86\nProb(H) (two-sided):                  0.00   Kurtosis:                        20.08\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\nresults.resid.plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8872daf940>\n\n\n\n\n\n\nresults.resid.plot(kind='kde')\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8872d863c8>"
  },
  {
    "objectID": "posts/2020-10-25-arima-adf-forecasting.html#prediction-of-future-values",
    "href": "posts/2020-10-25-arima-adf-forecasting.html#prediction-of-future-values",
    "title": "Stock Market Analysis of the S&P 500 Index using ARIMA and Seasonal ARIMA for forecasting",
    "section": "Prediction of Future Values",
    "text": "Prediction of Future Values\nFirts we can get an idea of how well our model performs by just predicting for values that we actually already know:\n\ndf['forecast'] = results.predict(start = 1, end= 720, dynamic= True)  \ndf[['Value','forecast']].plot(figsize=(12,8))\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f88ace0d7b8>\n\n\n\n\n\n\nForecasting\n\ndf.tail()\n\n\n\n\n\n  \n    \n      \n      Value\n      Value First Difference\n      Value Second Difference\n      Seasonal Difference\n      Seasonal First Difference\n      forecast\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-08-01\n      3391.71\n      120.59\n      57.09\n      166.67\n      173.75\n      NaN\n    \n    \n      2020-08-31\n      3500.31\n      108.60\n      -11.99\n      223.00\n      56.33\n      NaN\n    \n    \n      2020-09-01\n      3526.65\n      26.34\n      -82.26\n      571.84\n      348.84\n      NaN\n    \n    \n      2020-09-30\n      3363.00\n      -163.65\n      -189.99\n      710.61\n      138.77\n      NaN\n    \n    \n      2020-10-01\n      3380.80\n      17.80\n      181.45\n      796.21\n      85.60\n      NaN\n    \n  \n\n\n\n\n\nfrom pandas.tseries.offsets import DateOffset\n\n\nfuture_dates = [df.index[-1] + DateOffset(months=x) for x in range(0,24) ]\n\n\nfuture_dates\n\n[Timestamp('2020-10-01 00:00:00'),\n Timestamp('2020-11-01 00:00:00'),\n Timestamp('2020-12-01 00:00:00'),\n Timestamp('2021-01-01 00:00:00'),\n Timestamp('2021-02-01 00:00:00'),\n Timestamp('2021-03-01 00:00:00'),\n Timestamp('2021-04-01 00:00:00'),\n Timestamp('2021-05-01 00:00:00'),\n Timestamp('2021-06-01 00:00:00'),\n Timestamp('2021-07-01 00:00:00'),\n Timestamp('2021-08-01 00:00:00'),\n Timestamp('2021-09-01 00:00:00'),\n Timestamp('2021-10-01 00:00:00'),\n Timestamp('2021-11-01 00:00:00'),\n Timestamp('2021-12-01 00:00:00'),\n Timestamp('2022-01-01 00:00:00'),\n Timestamp('2022-02-01 00:00:00'),\n Timestamp('2022-03-01 00:00:00'),\n Timestamp('2022-04-01 00:00:00'),\n Timestamp('2022-05-01 00:00:00'),\n Timestamp('2022-06-01 00:00:00'),\n Timestamp('2022-07-01 00:00:00'),\n Timestamp('2022-08-01 00:00:00'),\n Timestamp('2022-09-01 00:00:00')]\n\n\n\nfuture_dates_df = pd.DataFrame(index=future_dates[1:],columns=df.columns)\n\n\nfuture_df = pd.concat([df,future_dates_df])\n\n\nfuture_df.head()\n\n\n\n\n\n  \n    \n      \n      Value\n      Value First Difference\n      Value Second Difference\n      Seasonal Difference\n      Seasonal First Difference\n      forecast\n    \n  \n  \n    \n      1960-01-01\n      58.03\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1960-02-01\n      55.78\n      -2.25\n      NaN\n      NaN\n      NaN\n      58.03\n    \n    \n      1960-03-01\n      55.02\n      -0.76\n      1.49\n      NaN\n      NaN\n      58.03\n    \n    \n      1960-04-01\n      55.73\n      0.71\n      1.47\n      NaN\n      NaN\n      58.03\n    \n    \n      1960-05-01\n      55.22\n      -0.51\n      -1.22\n      NaN\n      NaN\n      58.03\n    \n  \n\n\n\n\n\nfuture_df.tail()\n\n\n\n\n\n  \n    \n      \n      Value\n      Value First Difference\n      Value Second Difference\n      Seasonal Difference\n      Seasonal First Difference\n      forecast\n    \n  \n  \n    \n      2022-05-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2022-06-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2022-07-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2022-08-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2022-09-01\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\n\nfuture_df['forecast'] = results.predict(start = 1, end = 720, dynamic= True)  \nfuture_df[['Value', 'forecast']].plot(figsize=(12, 8)) \n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f8872997fd0>"
  },
  {
    "objectID": "posts/2020-10-16-dask-nlp-gutenberg-books.html",
    "href": "posts/2020-10-16-dask-nlp-gutenberg-books.html",
    "title": "Using Dask with dask.bag and regex to parse The Brothers Karamazov from project gutenberg",
    "section": "",
    "text": "import dask.bag as db\nimport re\n\n\nbook_bag = db.from_url('https://www.gutenberg.org/files/28054/28054-0.txt')\n\n\nbook_bag.take(5)\n\n(b'\\xef\\xbb\\xbfThe Project Gutenberg EBook of The Brothers Karamazov by Fyodor\\r\\n',\n b'Dostoyevsky\\r\\n',\n b'\\r\\n',\n b'\\r\\n',\n b'\\r\\n')\n\n\n\nremove_spaces = book_bag.map(lambda x:x.strip())\n\n\nremove_spaces.take(10)\n\n(b'\\xef\\xbb\\xbfThe Project Gutenberg EBook of The Brothers Karamazov by Fyodor',\n b'Dostoyevsky',\n b'',\n b'',\n b'',\n b'This ebook is for the use of anyone anywhere in the United States and most',\n b'other parts of the world at no cost and with almost no restrictions',\n b'whatsoever. You may copy it, give it away or re\\xe2\\x80\\x90use it under the terms of',\n b'the Project Gutenberg License included with this eBook or online at',\n b'http://www.gutenberg.org/license. If you are not located in the United')\n\n\n\ndef decode_to_ascii(x):\n    return x.decode(\"ascii\",\"ignore\") \n\n\nascii_text = remove_spaces.map(decode_to_ascii)\n\n\nascii_text.take(10)\n\n('The Project Gutenberg EBook of The Brothers Karamazov by Fyodor',\n 'Dostoyevsky',\n '',\n '',\n '',\n 'This ebook is for the use of anyone anywhere in the United States and most',\n 'other parts of the world at no cost and with almost no restrictions',\n 'whatsoever. You may copy it, give it away or reuse it under the terms of',\n 'the Project Gutenberg License included with this eBook or online at',\n 'http://www.gutenberg.org/license. If you are not located in the United')\n\n\n\ndef remove_punctuation(x):\n    return re.sub(r'[^\\w\\s]','',x)\n\n\nremove_punctuation = ascii_text.map(remove_punctuation)\n\n\nremove_punctuation.take(10)\n\n('The Project Gutenberg EBook of The Brothers Karamazov by Fyodor',\n 'Dostoyevsky',\n '',\n '',\n '',\n 'This ebook is for the use of anyone anywhere in the United States and most',\n 'other parts of the world at no cost and with almost no restrictions',\n 'whatsoever You may copy it give it away or reuse it under the terms of',\n 'the Project Gutenberg License included with this eBook or online at',\n 'httpwwwgutenbergorglicense If you are not located in the United')\n\n\n\nlower_text = remove_punctuation.map(str.lower)\n\n\nlower_text.take(10)\n\n('the project gutenberg ebook of the brothers karamazov by fyodor',\n 'dostoyevsky',\n '',\n '',\n '',\n 'this ebook is for the use of anyone anywhere in the united states and most',\n 'other parts of the world at no cost and with almost no restrictions',\n 'whatsoever you may copy it give it away or reuse it under the terms of',\n 'the project gutenberg license included with this ebook or online at',\n 'httpwwwgutenbergorglicense if you are not located in the united')\n\n\n\nsplit_word_list = lower_text.map(lambda x: x.split(' '))\n\n\nsplit_word_list.take(10)\n\n(['the',\n  'project',\n  'gutenberg',\n  'ebook',\n  'of',\n  'the',\n  'brothers',\n  'karamazov',\n  'by',\n  'fyodor'],\n ['dostoyevsky'],\n [''],\n [''],\n [''],\n ['this',\n  'ebook',\n  'is',\n  'for',\n  'the',\n  'use',\n  'of',\n  'anyone',\n  'anywhere',\n  'in',\n  'the',\n  'united',\n  'states',\n  'and',\n  'most'],\n ['other',\n  'parts',\n  'of',\n  'the',\n  'world',\n  'at',\n  'no',\n  'cost',\n  'and',\n  'with',\n  'almost',\n  'no',\n  'restrictions'],\n ['whatsoever',\n  'you',\n  'may',\n  'copy',\n  'it',\n  'give',\n  'it',\n  'away',\n  'or',\n  'reuse',\n  'it',\n  'under',\n  'the',\n  'terms',\n  'of'],\n ['the',\n  'project',\n  'gutenberg',\n  'license',\n  'included',\n  'with',\n  'this',\n  'ebook',\n  'or',\n  'online',\n  'at'],\n ['httpwwwgutenbergorglicense',\n  'if',\n  'you',\n  'are',\n  'not',\n  'located',\n  'in',\n  'the',\n  'united'])\n\n\n\ndef remove_empty_words(word_list):\n    return list(filter(lambda a: a != '', word_list))\n\nnon_empty_words = split_word_list.filter(remove_empty_words)\n\n\nnon_empty_words.take(10)\n\n(['the',\n  'project',\n  'gutenberg',\n  'ebook',\n  'of',\n  'the',\n  'brothers',\n  'karamazov',\n  'by',\n  'fyodor'],\n ['dostoyevsky'],\n ['this',\n  'ebook',\n  'is',\n  'for',\n  'the',\n  'use',\n  'of',\n  'anyone',\n  'anywhere',\n  'in',\n  'the',\n  'united',\n  'states',\n  'and',\n  'most'],\n ['other',\n  'parts',\n  'of',\n  'the',\n  'world',\n  'at',\n  'no',\n  'cost',\n  'and',\n  'with',\n  'almost',\n  'no',\n  'restrictions'],\n ['whatsoever',\n  'you',\n  'may',\n  'copy',\n  'it',\n  'give',\n  'it',\n  'away',\n  'or',\n  'reuse',\n  'it',\n  'under',\n  'the',\n  'terms',\n  'of'],\n ['the',\n  'project',\n  'gutenberg',\n  'license',\n  'included',\n  'with',\n  'this',\n  'ebook',\n  'or',\n  'online',\n  'at'],\n ['httpwwwgutenbergorglicense',\n  'if',\n  'you',\n  'are',\n  'not',\n  'located',\n  'in',\n  'the',\n  'united'],\n ['states',\n  'youll',\n  'have',\n  'to',\n  'check',\n  'the',\n  'laws',\n  'of',\n  'the',\n  'country',\n  'where',\n  'you',\n  'are',\n  'located'],\n ['before', 'using', 'this', 'ebook'],\n ['title', 'the', 'brothers', 'karamazov'])\n\n\n\nall_words = non_empty_words.flatten()\n\n\ntype(all_words)\n\ndask.bag.core.Bag\n\n\n\nall_words.take(30)\n\n('the',\n 'project',\n 'gutenberg',\n 'ebook',\n 'of',\n 'the',\n 'brothers',\n 'karamazov',\n 'by',\n 'fyodor',\n 'dostoyevsky',\n 'this',\n 'ebook',\n 'is',\n 'for',\n 'the',\n 'use',\n 'of',\n 'anyone',\n 'anywhere',\n 'in',\n 'the',\n 'united',\n 'states',\n 'and',\n 'most',\n 'other',\n 'parts',\n 'of',\n 'the')\n\n\n\nchange_to_key_value = all_words.map(lambda x: (x, 1))\n\n\nchange_to_key_value.take(4)\n\n(('the', 1), ('project', 1), ('gutenberg', 1), ('ebook', 1))\n\n\n\ngrouped_words = all_words.groupby(lambda x:x)\n\n\ngrouped_words.take(1)\n\n(('the',\n  ['the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   ...]),)\n\n\n\nword_count = grouped_words.map(lambda x: (x[0], len(x[1])))\n\n\nword_count.take(10)\n\n(('the', 15379),\n ('project', 89),\n ('gutenberg', 88),\n ('ebook', 14),\n ('of', 7410),\n ('brothers', 82),\n ('karamazov', 170),\n ('by', 1165),\n ('fyodor', 303),\n ('dostoyevsky', 3))\n\n\n\nchange_to_key_value.take(10)\n\n(('the', 1),\n ('project', 1),\n ('gutenberg', 1),\n ('ebook', 1),\n ('of', 1),\n ('the', 1),\n ('brothers', 1),\n ('karamazov', 1),\n ('by', 1),\n ('fyodor', 1))\n\n\n\n# Take a running count of a word\n# In this case, the default value of \n# count needs to be provided\ndef add_bin_op(count, x):\n    return count + x[1]\n\n# Take the output from multiple bin_op(s)\n# and add them to get the total count of\n# a word\ndef add_combine_op(x, y):\n    return x + y\n\nword_count = change_to_key_value.foldby(lambda x: x[0],\n                                       add_bin_op, 0,\n                                       add_combine_op)\n\n\nword_count.take(10)\n\n(('the', 15379),\n ('project', 89),\n ('gutenberg', 88),\n ('ebook', 14),\n ('of', 7410),\n ('brothers', 82),\n ('karamazov', 170),\n ('by', 1165),\n ('fyodor', 303),\n ('dostoyevsky', 3))\n\n\n\nmuch_easier = all_words.frequencies()\n\n\nmuch_easier.take(10)\n\n(('the', 15379),\n ('project', 89),\n ('gutenberg', 88),\n ('ebook', 14),\n ('of', 7410),\n ('brothers', 82),\n ('karamazov', 170),\n ('by', 1165),\n ('fyodor', 303),\n ('dostoyevsky', 3))\n\n\n\nRemoving stop words in top word frequency counts\n\nfrom spacy.lang.en import STOP_WORDS\n\n\nwithout_stopwords = all_words.filter(lambda x: x not in STOP_WORDS)\n\n\nnew_freq = without_stopwords.frequencies()\n\n\nnew_freq.take(20)\n\n(('project', 89),\n ('gutenberg', 88),\n ('ebook', 14),\n ('brothers', 82),\n ('karamazov', 170),\n ('fyodor', 303),\n ('dostoyevsky', 3),\n ('use', 77),\n ('united', 24),\n ('states', 21),\n ('parts', 19),\n ('world', 182),\n ('cost', 12),\n ('restrictions', 2),\n ('whatsoever', 5),\n ('copy', 16),\n ('away', 445),\n ('reuse', 2),\n ('terms', 33),\n ('license', 14))\n\n\n\nnew_freq.topk(10)\n\ndask.bag<topk-aggregate, npartitions=1>\n\n\n\nnew_freq.topk(10, key=lambda x: x[1]).compute()\n\n[('alyosha', 1176),\n ('said', 993),\n ('know', 843),\n ('man', 842),\n ('mitya', 814),\n ('dont', 784),\n ('come', 772),\n ('father', 721),\n ('ivan', 677),\n ('time', 669)]"
  },
  {
    "objectID": "posts/2020-10-12-dask-xgboost-fiscal-data..html",
    "href": "posts/2020-10-12-dask-xgboost-fiscal-data..html",
    "title": "Visualizing Operations with Dask Dataframes on Fiscal Data",
    "section": "",
    "text": "import numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine(\"sqlite:///fiscal_data.db\")\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nengine.execute(\"SELECT * FROM fiscal_data LIMIT 1\").fetchall()\n\n[(1996, 'East China', 'Anhui', '2093.3', 50661, 631930, 147002)]\n\n\n\nsql = \"\"\"\nSELECT year\n, region\n, province\n, gdp\n, fdi\n, it\n, specific\nFROM fiscal_data\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.3\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.7\n      668128\n      3162299\n      656175.0\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      18753.73\n      1036576\n      2939778\n      844647.0\n    \n  \n\n360 rows × 7 columns\n\n\n\n\ndf.columns\n\nIndex(['year', 'region', 'province', 'gdp', 'fdi', 'it', 'specific'], dtype='object')\n\n\n\ndf.gdp.hist()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f0272396c50>\n\n\n\n\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n\n\n\n\nClient\n\n  Scheduler: inproc://192.168.1.71/5995/1\n  Dashboard: http://192.168.1.71:8787/status\n\n\n\nCluster\n\n  Workers: 3\n  Cores: 6\n  Memory: 12.00 GB\n\n\n\n\n\n\n\nfrom dask import dataframe as dd \n\n\nddf = dd.from_pandas(df, npartitions=5)\n\n\nprint(ddf)\n\nDask DataFrame Structure:\n                year  region province     gdp    fdi     it specific\nnpartitions=5                                                       \n0              int64  object   object  object  int64  int64  float64\n72               ...     ...      ...     ...    ...    ...      ...\n...              ...     ...      ...     ...    ...    ...      ...\n288              ...     ...      ...     ...    ...    ...      ...\n359              ...     ...      ...     ...    ...    ...      ...\nDask Name: from_pandas, 5 tasks\n\n\n\nddf.describe().visualize(filename='describe.png')\n\n\n\n\n\nddf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.3\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n  \n\n\n\n\n\nmax_gdp_per_region = ddf.groupby('region')['gdp'].max()\n\n\nmax_gdp_per_region.visualize()\n\n\n\n\n\nmax_gdp_per_region.compute()\n\nregion\nEast China             9705.02\nNorth China            9846.81\nNorthwest China         956.32\nSouth Central China     9439.6\nSouthwest China          937.5\nNortheast China        9304.52\nName: gdp, dtype: object\n\n\n\nddf\n\n\nDask DataFrame Structure:\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n    \n      npartitions=5\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      int64\n      object\n      object\n      object\n      int64\n      int64\n      float64\n    \n    \n      72\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      288\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      359\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n  \n\n\nDask Name: from_pandas, 5 tasks\n\n\n\nddf.npartitions\n\n5\n\n\n\nddf.npartitions\n\n5\n\n\n\nlen(ddf)\n\n360\n\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=\"4GB\")\nclient\n\n/home/gao/anaconda3/lib/python3.7/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use.\nPerhaps you already have a cluster running?\nHosting the HTTP server on port 39701 instead\n  http_address[\"port\"], self.http_server.port\n\n\n\n\n\n\nClient\n\n  Scheduler: inproc://192.168.1.71/5995/20\n  Dashboard: http://192.168.1.71:39701/status\n\n\n\nCluster\n\n  Workers: 3\n  Cores: 6\n  Memory: 12.00 GB\n\n\n\n\n\n\n\nclient.id\n\n'Client-9f9a71c2-0c90-11eb-976b-cff3b7a8059e'\n\n\n\nddf.describe().compute()\n\n\n\n\n\n  \n    \n      \n      year\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      count\n      360.000000\n      3.600000e+02\n      3.600000e+02\n      3.560000e+02\n    \n    \n      mean\n      2001.500000\n      1.961394e+05\n      2.165819e+06\n      5.834707e+05\n    \n    \n      std\n      3.456857\n      3.030440e+05\n      1.769294e+06\n      6.540553e+05\n    \n    \n      min\n      1996.000000\n      2.000000e+00\n      1.478970e+05\n      8.964000e+03\n    \n    \n      25%\n      1998.750000\n      3.309900e+04\n      1.077466e+06\n      2.237530e+05\n    \n    \n      50%\n      2001.500000\n      1.411025e+05\n      2.020634e+06\n      4.243700e+05\n    \n    \n      75%\n      2004.250000\n      4.065125e+05\n      3.375492e+06\n      1.011846e+06\n    \n    \n      max\n      2007.000000\n      1.743140e+06\n      1.053331e+07\n      3.937966e+06\n    \n  \n\n\n\n\n\nddf.columns\n\nIndex(['year', 'region', 'province', 'gdp', 'fdi', 'it', 'specific'], dtype='object')\n\n\n\nddf[\"year\"] = ddf[\"year\"].astype(int)\nddf[\"fdi\"] = ddf[\"fdi\"].astype(float)\n# ddf[\"province\"] = ddf[\"province\"].astype(float)\n# ddf[\"region\"] = ddf[\"region\"].astype(float)\nddf[\"gdp\"] = ddf[\"gdp\"].astype(float)\nddf[\"it\"] = ddf[\"it\"].astype(float)\n\n\nddf\n\n\nDask DataFrame Structure:\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n    \n      npartitions=5\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      int64\n      object\n      object\n      float64\n      float64\n      float64\n      float64\n    \n    \n      72\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      288\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      359\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n  \n\n\nDask Name: assign, 65 tasks\n\n\n\nddf.nlargest(20, 'gdp').compute()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      71\n      2007\n      South Central China\n      Guangdong\n      31777.01\n      1712603.0\n      4947824.0\n      859482.0\n    \n    \n      70\n      2006\n      South Central China\n      Guangdong\n      26587.76\n      1451065.0\n      4559252.0\n      1897575.0\n    \n    \n      263\n      2007\n      East China\n      Shandong\n      25776.91\n      1101159.0\n      6357869.0\n      2121243.0\n    \n    \n      69\n      2005\n      South Central China\n      Guangdong\n      22557.37\n      1236400.0\n      4327217.0\n      1491588.0\n    \n    \n      262\n      2006\n      East China\n      Shandong\n      21900.19\n      1000069.0\n      5304833.0\n      1204547.0\n    \n    \n      179\n      2007\n      East China\n      Jiangsu\n      21742.05\n      1743140.0\n      3557071.0\n      1188989.0\n    \n    \n      68\n      2004\n      South Central China\n      Guangdong\n      18864.62\n      1001158.0\n      5193902.0\n      1491588.0\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      18753.73\n      1036576.0\n      2939778.0\n      844647.0\n    \n    \n      178\n      2006\n      East China\n      Jiangsu\n      18598.69\n      1318339.0\n      2926542.0\n      1388043.0\n    \n    \n      261\n      2005\n      East China\n      Shandong\n      18366.87\n      897000.0\n      4142859.0\n      1011203.0\n    \n    \n      67\n      2003\n      South Central China\n      Guangdong\n      15844.64\n      782294.0\n      4073606.0\n      1550764.0\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935.0\n      2553268.0\n      1017303.0\n    \n    \n      260\n      2004\n      East China\n      Shandong\n      15021.84\n      870064.0\n      3732990.0\n      1011203.0\n    \n    \n      143\n      2007\n      South Central China\n      Henan\n      15012.46\n      306162.0\n      10533312.0\n      3860764.0\n    \n    \n      177\n      2005\n      East China\n      Jiangsu\n      15003.60\n      1213800.0\n      3479548.0\n      1483371.0\n    \n    \n      119\n      2007\n      North China\n      Hebei\n      13607.32\n      241621.0\n      7537692.0\n      2981235.0\n    \n    \n      66\n      2002\n      South Central China\n      Guangdong\n      13502.42\n      1133400.0\n      3545004.0\n      1235386.0\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000.0\n      2370200.0\n      656175.0\n    \n    \n      275\n      2007\n      East China\n      Shanghai\n      12494.01\n      792000.0\n      2386339.0\n      272744.0\n    \n    \n      176\n      2004\n      East China\n      Jiangsu\n      12442.87\n      1056365.0\n      2410257.0\n      1483371.0\n    \n  \n\n\n\n\n\nwithout_ec = ddf[ddf.region !='East China']\n\n\nwithout_ec.nlargest(20, 'gdp').compute()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      71\n      2007\n      South Central China\n      Guangdong\n      31777.01\n      1712603.0\n      4947824.0\n      859482.0\n    \n    \n      70\n      2006\n      South Central China\n      Guangdong\n      26587.76\n      1451065.0\n      4559252.0\n      1897575.0\n    \n    \n      69\n      2005\n      South Central China\n      Guangdong\n      22557.37\n      1236400.0\n      4327217.0\n      1491588.0\n    \n    \n      68\n      2004\n      South Central China\n      Guangdong\n      18864.62\n      1001158.0\n      5193902.0\n      1491588.0\n    \n    \n      67\n      2003\n      South Central China\n      Guangdong\n      15844.64\n      782294.0\n      4073606.0\n      1550764.0\n    \n    \n      143\n      2007\n      South Central China\n      Henan\n      15012.46\n      306162.0\n      10533312.0\n      3860764.0\n    \n    \n      119\n      2007\n      North China\n      Hebei\n      13607.32\n      241621.0\n      7537692.0\n      2981235.0\n    \n    \n      66\n      2002\n      South Central China\n      Guangdong\n      13502.42\n      1133400.0\n      3545004.0\n      1235386.0\n    \n    \n      142\n      2006\n      South Central China\n      Henan\n      12362.79\n      184526.0\n      7601825.0\n      2018158.0\n    \n    \n      65\n      2001\n      South Central China\n      Guangdong\n      12039.25\n      1193203.0\n      2152243.0\n      1257232.0\n    \n    \n      118\n      2006\n      North China\n      Hebei\n      11467.60\n      201434.0\n      5831974.0\n      1253141.0\n    \n    \n      64\n      2000\n      South Central China\n      Guangdong\n      10741.25\n      1128091.0\n      1927102.0\n      714572.0\n    \n    \n      141\n      2005\n      South Central China\n      Henan\n      10587.42\n      123000.0\n      5676863.0\n      1171796.0\n    \n    \n      299\n      2007\n      Southwest China\n      Sichuan\n      10562.39\n      149322.0\n      10384846.0\n      3937966.0\n    \n    \n      117\n      2005\n      North China\n      Hebei\n      10012.11\n      191000.0\n      4503640.0\n      859056.0\n    \n    \n      23\n      2007\n      North China\n      Beijing\n      9846.81\n      506572.0\n      1962192.0\n      752279.0\n    \n    \n      167\n      2007\n      South Central China\n      Hunan\n      9439.60\n      327051.0\n      8340692.0\n      3156087.0\n    \n    \n      155\n      2007\n      South Central China\n      Hubei\n      9333.40\n      276622.0\n      7666512.0\n      2922784.0\n    \n    \n      215\n      2007\n      Northeast China\n      Liaoning\n      9304.52\n      598554.0\n      5502192.0\n      3396397.0\n    \n    \n      63\n      1999\n      South Central China\n      Guangdong\n      9250.68\n      1165750.0\n      1789235.0\n      988521.0\n    \n  \n\n\n\n\n\nddf['province'].compute()\n\n0         Anhui\n1         Anhui\n2         Anhui\n3         Anhui\n4         Anhui\n         ...   \n355    Zhejiang\n356    Zhejiang\n357    Zhejiang\n358    Zhejiang\n359    Zhejiang\nName: province, Length: 360, dtype: object\n\n\n\nddf.where(ddf['province']=='Zhejiang').compute()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003.0\n      East China\n      Zhejiang\n      9705.02\n      498055.0\n      2261631.0\n      391292.0\n    \n    \n      356\n      2004.0\n      East China\n      Zhejiang\n      11648.70\n      668128.0\n      3162299.0\n      656175.0\n    \n    \n      357\n      2005.0\n      East China\n      Zhejiang\n      13417.68\n      772000.0\n      2370200.0\n      656175.0\n    \n    \n      358\n      2006.0\n      East China\n      Zhejiang\n      15718.47\n      888935.0\n      2553268.0\n      1017303.0\n    \n    \n      359\n      2007.0\n      East China\n      Zhejiang\n      18753.73\n      1036576.0\n      2939778.0\n      844647.0\n    \n  \n\n360 rows × 7 columns\n\n\n\n\nmask_after_2010 = ddf.where(ddf['year']>2000)\n\n\nmask_after_2010.compute()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003.0\n      East China\n      Zhejiang\n      9705.02\n      498055.0\n      2261631.0\n      391292.0\n    \n    \n      356\n      2004.0\n      East China\n      Zhejiang\n      11648.70\n      668128.0\n      3162299.0\n      656175.0\n    \n    \n      357\n      2005.0\n      East China\n      Zhejiang\n      13417.68\n      772000.0\n      2370200.0\n      656175.0\n    \n    \n      358\n      2006.0\n      East China\n      Zhejiang\n      15718.47\n      888935.0\n      2553268.0\n      1017303.0\n    \n    \n      359\n      2007.0\n      East China\n      Zhejiang\n      18753.73\n      1036576.0\n      2939778.0\n      844647.0\n    \n  \n\n360 rows × 7 columns\n\n\n\n\ndef add_some_text(cname, *args, **kwargs):\n    return \"Region name is \" + cname\n\ndummy_values = ddf['region'].apply(add_some_text, axis=1)\n\n/home/gao/anaconda3/lib/python3.7/site-packages/dask/dataframe/core.py:3208: UserWarning: \nYou did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\nTo provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n  Before: .apply(func)\n  After:  .apply(func, meta=('region', 'object'))\n\n  warnings.warn(meta_warning(meta))\n\n\n\ndummy_values\n\nDask Series Structure:\nnpartitions=5\n0      object\n72        ...\n        ...  \n288       ...\n359       ...\nName: region, dtype: object\nDask Name: apply, 75 tasks\n\n\n\ndummy_values.visualize()\n\n\n\n\n\ndummy_values.compute()\n\n0      Region name is East China\n1      Region name is East China\n2      Region name is East China\n3      Region name is East China\n4      Region name is East China\n                 ...            \n355    Region name is East China\n356    Region name is East China\n357    Region name is East China\n358    Region name is East China\n359    Region name is East China\nName: region, Length: 360, dtype: object\n\n\n\nmax_per_region_yr = ddf.groupby('region').apply(lambda x: x.loc[x['gdp'].idxmax(), 'year'])\n\n/home/gao/.local/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n  Before: .apply(func)\n  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n  or:     .apply(func, meta=('x', 'f8'))            for series result\n  \"\"\"Entry point for launching an IPython kernel.\n\n\n\nmax_per_region_yr.visualize()\n\n\n\n\n\nmax_per_region_yr.compute()\n\nregion\nNorth China            2007\nNortheast China        2007\nNorthwest China        2007\nSouth Central China    2007\nEast China             2007\nSouthwest China        2007\ndtype: int64"
  },
  {
    "objectID": "posts/2020-10-15-dask-xgboost-linear-regresion-copy1.html",
    "href": "posts/2020-10-15-dask-xgboost-linear-regresion-copy1.html",
    "title": "Linear Regression using Dask Data Frames",
    "section": "",
    "text": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples."
  },
  {
    "objectID": "posts/2020-10-15-dask-xgboost-linear-regresion-copy1.html#selecting-features-and-target",
    "href": "posts/2020-10-15-dask-xgboost-linear-regresion-copy1.html#selecting-features-and-target",
    "title": "Linear Regression using Dask Data Frames",
    "section": "Selecting Features and Target",
    "text": "Selecting Features and Target\n\nfeat_list = [\"year\", \"fdi\"]\ncat_feat_list = [\"region\", \"province\"]\ntarget = [\"gdp\"]\n\n\nddf[\"year\"] = ddf[\"year\"].astype(int)\nddf[\"fdi\"] = ddf[\"fdi\"].astype(float)\nddf[\"gdp\"] = ddf[\"gdp\"].astype(float)\nddf[\"it\"] = ddf[\"it\"].astype(float)\n\n\n#OHE\nfrom dask_ml.preprocessing import OneHotEncoder\n\n\nddf = ddf.categorize(cat_feat_list)\n\n\nohe = OneHotEncoder(sparse=False)\n\n\nohe_ddf = ohe.fit_transform(ddf[cat_feat_list])\n\n\nfeat_list = feat_list + ohe_ddf.columns.tolist()\nfeat_list = [f for f in feat_list if f not in cat_feat_list]\n\n\nddf_processed = (dd.concat([ddf,ohe_ddf], axis=1) [feat_list + target])\n\n\nddf_processed.compute()\n\n\n\n\n\n  \n    \n      \n      year\n      fdi\n      region_East China\n      region_North China\n      region_Southwest China\n      region_Northwest China\n      region_South Central China\n      region_Northeast China\n      province_Anhui\n      province_Beijing\n      ...\n      province_Shandong\n      province_Shanghai\n      province_Shanxi\n      province_Sichuan\n      province_Tianjin\n      province_Tibet\n      province_Xinjiang\n      province_Yunnan\n      province_Zhejiang\n      gdp\n    \n  \n  \n    \n      0\n      1996\n      50661.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2093.30\n    \n    \n      1\n      1997\n      43443.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2347.32\n    \n    \n      2\n      1998\n      27673.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2542.96\n    \n    \n      3\n      1999\n      26131.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2712.34\n    \n    \n      4\n      2000\n      31847.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2902.09\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      498055.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      9705.02\n    \n    \n      356\n      2004\n      668128.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      11648.70\n    \n    \n      357\n      2005\n      772000.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      13417.68\n    \n    \n      358\n      2006\n      888935.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      15718.47\n    \n    \n      359\n      2007\n      1036576.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      18753.73\n    \n  \n\n360 rows × 75 columns\n\n\n\n\nfeat_list\n\n['year',\n 'fdi',\n 'region_East China',\n 'region_North China',\n 'region_Southwest China',\n 'region_Northwest China',\n 'region_South Central China',\n 'region_Northeast China',\n 'province_Anhui',\n 'province_Beijing',\n 'province_Chongqing',\n 'province_Fujian',\n 'province_Gansu',\n 'province_Guangdong',\n 'province_Guangxi',\n 'province_Guizhou',\n 'province_Hainan',\n 'province_Hebei',\n 'province_Heilongjiang',\n 'province_Henan',\n 'province_Hubei',\n 'province_Hunan',\n 'province_Jiangsu',\n 'province_Jiangxi',\n 'province_Jilin',\n 'province_Liaoning',\n 'province_Ningxia',\n 'province_Qinghai',\n 'province_Shaanxi',\n 'province_Shandong',\n 'province_Shanghai',\n 'province_Shanxi',\n 'province_Sichuan',\n 'province_Tianjin',\n 'province_Tibet',\n 'province_Xinjiang',\n 'province_Yunnan',\n 'province_Zhejiang',\n 'region_East China',\n 'region_North China',\n 'region_Southwest China',\n 'region_Northwest China',\n 'region_South Central China',\n 'region_Northeast China',\n 'province_Anhui',\n 'province_Beijing',\n 'province_Chongqing',\n 'province_Fujian',\n 'province_Gansu',\n 'province_Guangdong',\n 'province_Guangxi',\n 'province_Guizhou',\n 'province_Hainan',\n 'province_Hebei',\n 'province_Heilongjiang',\n 'province_Henan',\n 'province_Hubei',\n 'province_Hunan',\n 'province_Jiangsu',\n 'province_Jiangxi',\n 'province_Jilin',\n 'province_Liaoning',\n 'province_Ningxia',\n 'province_Qinghai',\n 'province_Shaanxi',\n 'province_Shandong',\n 'province_Shanghai',\n 'province_Shanxi',\n 'province_Sichuan',\n 'province_Tianjin',\n 'province_Tibet',\n 'province_Xinjiang',\n 'province_Yunnan',\n 'province_Zhejiang']"
  },
  {
    "objectID": "posts/2020-10-15-dask-xgboost-linear-regresion-copy1.html#dask-linear-regression",
    "href": "posts/2020-10-15-dask-xgboost-linear-regresion-copy1.html#dask-linear-regression",
    "title": "Linear Regression using Dask Data Frames",
    "section": "Dask Linear Regression",
    "text": "Dask Linear Regression\n\nX=ddf_processed[feat_list].persist()\ny=ddf_processed[target].persist()\n\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression, Ridge\n\n\nX\n\n\nDask DataFrame Structure:\n\n\n\n  \n    \n      \n      year\n      fdi\n      region_East China\n      region_East China\n      region_North China\n      region_North China\n      region_Southwest China\n      region_Southwest China\n      region_Northwest China\n      region_Northwest China\n      region_South Central China\n      region_South Central China\n      region_Northeast China\n      region_Northeast China\n      province_Anhui\n      province_Anhui\n      province_Beijing\n      province_Beijing\n      province_Chongqing\n      province_Chongqing\n      province_Fujian\n      province_Fujian\n      province_Gansu\n      province_Gansu\n      province_Guangdong\n      province_Guangdong\n      province_Guangxi\n      province_Guangxi\n      province_Guizhou\n      province_Guizhou\n      province_Hainan\n      province_Hainan\n      province_Hebei\n      province_Hebei\n      province_Heilongjiang\n      province_Heilongjiang\n      province_Henan\n      province_Henan\n      province_Hubei\n      province_Hubei\n      province_Hunan\n      province_Hunan\n      province_Jiangsu\n      province_Jiangsu\n      province_Jiangxi\n      province_Jiangxi\n      province_Jilin\n      province_Jilin\n      province_Liaoning\n      province_Liaoning\n      province_Ningxia\n      province_Ningxia\n      province_Qinghai\n      province_Qinghai\n      province_Shaanxi\n      province_Shaanxi\n      province_Shandong\n      province_Shandong\n      province_Shanghai\n      province_Shanghai\n      province_Shanxi\n      province_Shanxi\n      province_Sichuan\n      province_Sichuan\n      province_Tianjin\n      province_Tianjin\n      province_Tibet\n      province_Tibet\n      province_Xinjiang\n      province_Xinjiang\n      province_Yunnan\n      province_Yunnan\n      province_Zhejiang\n      province_Zhejiang\n      region_East China\n      region_East China\n      region_North China\n      region_North China\n      region_Southwest China\n      region_Southwest China\n      region_Northwest China\n      region_Northwest China\n      region_South Central China\n      region_South Central China\n      region_Northeast China\n      region_Northeast China\n      province_Anhui\n      province_Anhui\n      province_Beijing\n      province_Beijing\n      province_Chongqing\n      province_Chongqing\n      province_Fujian\n      province_Fujian\n      province_Gansu\n      province_Gansu\n      province_Guangdong\n      province_Guangdong\n      province_Guangxi\n      province_Guangxi\n      province_Guizhou\n      province_Guizhou\n      province_Hainan\n      province_Hainan\n      province_Hebei\n      province_Hebei\n      province_Heilongjiang\n      province_Heilongjiang\n      province_Henan\n      province_Henan\n      province_Hubei\n      province_Hubei\n      province_Hunan\n      province_Hunan\n      province_Jiangsu\n      province_Jiangsu\n      province_Jiangxi\n      province_Jiangxi\n      province_Jilin\n      province_Jilin\n      province_Liaoning\n      province_Liaoning\n      province_Ningxia\n      province_Ningxia\n      province_Qinghai\n      province_Qinghai\n      province_Shaanxi\n      province_Shaanxi\n      province_Shandong\n      province_Shandong\n      province_Shanghai\n      province_Shanghai\n      province_Shanxi\n      province_Shanxi\n      province_Sichuan\n      province_Sichuan\n      province_Tianjin\n      province_Tianjin\n      province_Tibet\n      province_Tibet\n      province_Xinjiang\n      province_Xinjiang\n      province_Yunnan\n      province_Yunnan\n      province_Zhejiang\n      province_Zhejiang\n    \n    \n      npartitions=5\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      0\n      int64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n      float64\n    \n    \n      72\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      288\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      359\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n  \n\n\nDask Name: getitem, 5 tasks\n\n\n\nX.compute()\n\n\n\n\n\n  \n    \n      \n      year\n      fdi\n      region_East China\n      region_East China\n      region_North China\n      region_North China\n      region_Southwest China\n      region_Southwest China\n      region_Northwest China\n      region_Northwest China\n      ...\n      province_Tianjin\n      province_Tianjin\n      province_Tibet\n      province_Tibet\n      province_Xinjiang\n      province_Xinjiang\n      province_Yunnan\n      province_Yunnan\n      province_Zhejiang\n      province_Zhejiang\n    \n  \n  \n    \n      0\n      1996\n      50661.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      1\n      1997\n      43443.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      2\n      1998\n      27673.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      3\n      1999\n      26131.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      4\n      2000\n      31847.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      498055.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n    \n    \n      356\n      2004\n      668128.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n    \n    \n      357\n      2005\n      772000.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n    \n    \n      358\n      2006\n      888935.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n    \n    \n      359\n      2007\n      1036576.0\n      1.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      1.0\n    \n  \n\n360 rows × 146 columns\n\n\n\n\ny\n\n\nDask DataFrame Structure:\n\n\n\n  \n    \n      \n      gdp\n    \n    \n      npartitions=5\n      \n    \n  \n  \n    \n      0\n      float64\n    \n    \n      72\n      ...\n    \n    \n      ...\n      ...\n    \n    \n      288\n      ...\n    \n    \n      359\n      ...\n    \n  \n\n\nDask Name: getitem, 5 tasks\n\n\n\nLinReg = LinearRegression()\n\n\nLinReg.fit(X, y)\n\nLinearRegression()\n\n\n\nRidgeReg = Ridge()\nRidgeReg.fit(x, y)\n\nRidge()\n\n\n\nLinReg.predict(x)[:5]\n\narray([[1830.87851079],\n       [2076.99855135],\n       [2220.28956053],\n       [2534.65768132],\n       [2936.29581027]])\n\n\n\nRidgeReg.predict(x)[:5]\n\narray([[1804.41754025],\n       [2053.19939587],\n       [2200.05297844],\n       [2516.48507702],\n       [2919.42271884]])\n\n\n\nclient.restart()\n\n\n\n\n\nClient\n\n  Scheduler: inproc://192.168.1.71/9672/30\n  Dashboard: http://192.168.1.71:46451/status\n\n\n\nCluster\n\n  Workers: 3\n  Cores: 6\n  Memory: 12.00 GB\n\n\n\n\n\n\n\nclient.close()"
  },
  {
    "objectID": "posts/2020-10-22-stock-market-returns.html",
    "href": "posts/2020-10-22-stock-market-returns.html",
    "title": "Stock Market Analysis of Microsoft, Zoom, and Snowflake",
    "section": "",
    "text": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nimport pandas_datareader\nimport datetime\n\n\nimport pandas_datareader.data as web\n\n\nstart = datetime.datetime(2019, 1, 1)\nend = datetime.datetime(2021, 1, 1)\n\n#start = datetime.datetime(2012, 1, 1)\n#end = datetime.datetime(2017, 1, 1)\n#tesla = web.DataReader(\"TSLA\", 'yahoo', start, end)\n\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nTSLA_stock = web.DataReader('TSLA', 'yahoo', start, end)\nTSLA_stock.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2019-01-02\n      63.026001\n      59.759998\n      61.220001\n      62.023998\n      58293000.0\n      62.023998\n    \n    \n      2019-01-03\n      61.880001\n      59.476002\n      61.400002\n      60.071999\n      34826000.0\n      60.071999\n    \n    \n      2019-01-04\n      63.599998\n      60.546001\n      61.200001\n      63.537998\n      36970500.0\n      63.537998\n    \n    \n      2019-01-07\n      67.348000\n      63.549999\n      64.344002\n      66.991997\n      37756000.0\n      66.991997\n    \n    \n      2019-01-08\n      68.802002\n      65.403999\n      68.391998\n      67.070000\n      35042500.0\n      67.070000\n    \n  \n\n\n\n\n\nMSFT_stock['Open'].plot(label='MSFT_stock',figsize=(16,8),title='Open Price')\nZOOM_stock['Open'].plot(label='ZOOM_stock')\nTSLA_stock['Open'].plot(label='TSLA_stock')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7fabf2074c90>\n\n\n\n\n\n\n\nMSFT_stock['Volume'].plot(label='MSFT_stock',figsize=(16,8),title='Volume Traded')\nZOOM_stock['Volume'].plot(label='ZOOM_stock')\nTSLA_stock['Volume'].plot(label='TSLA_stock')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7fabf1feab90>\n\n\n\n\n\n\nMSFT_stock['Volume'].argmax()\n\n291\n\n\n\nMSFT_stock['Total Traded'] = MSFT_stock['Open']*MSFT_stock['Volume']\nTSLA_stock['Total Traded'] = TSLA_stock['Open']*TSLA_stock['Volume']\nZOOM_stock['Total Traded'] = ZOOM_stock['Open']*ZOOM_stock['Volume']\n\n\nMSFT_stock['Total Traded'].plot(label='MSFT_stock',figsize=(16,8))\nZOOM_stock['Total Traded'].plot(label='ZOOM_stock')\nTSLA_stock['Total Traded'].plot(label='TSLA_stock')\nplt.legend()\nplt.ylabel('Total Traded')\n\nText(0, 0.5, 'Total Traded')\n\n\n\n\n\n\nZOOM_stock['Total Traded'].argmax()\n\n346\n\n\nMA (Moving Averages)\n\nZOOM_stock['MA50'] = ZOOM_stock['Open'].rolling(50).mean()\nZOOM_stock['MA200'] = ZOOM_stock['Open'].rolling(200).mean()\nZOOM_stock[['Open','MA50','MA200']].plot(label='ZOOM_stock',figsize=(16,8))\n\n<AxesSubplot:xlabel='Date'>\n\n\n\n\n\n\nfrom pandas.plotting import scatter_matrix\n\ncar_comp = pd.concat([MSFT_stock['Open'],ZOOM_stock['Open'],SNOW_stock['Open']],axis=1)\n\ncar_comp.columns = ['MSFT_stock Open','ZOOM_stock Open','SNOW_stock Open']\n\n\n# You can use a semi-colon to remove the axes print outs\nscatter_matrix(car_comp,figsize=(8,8),alpha=0.2,hist_kwds={'bins':50});\n\n\n\n\n\nfrom mpl_finance import candlestick_ohlc\n\nfrom matplotlib.dates import DateFormatter, date2num, WeekdayLocator, DayLocator, MONDAY\n\n\n# Rest the index to get a column of January Dates\nMSFT_stock_reset = MSFT_stock.loc['2019-01':'2019-01'].reset_index()\n\n\n# Create a new column of numerical \"date\" values for matplotlib to use\nMSFT_stock_reset['date_ax'] = MSFT_stock_reset['Date'].apply(lambda date: date2num(date))\nMSFT_stock_values = [tuple(vals) for vals in MSFT_stock_reset[['date_ax', 'Open', 'High', 'Low', 'Close']].values]\n\n\nmondays = WeekdayLocator(MONDAY)        # major ticks on the mondays\nalldays = DayLocator()              # minor ticks on the days\nweekFormatter = DateFormatter('%b %d')  # e.g., Jan 12\ndayFormatter = DateFormatter('%d')      # e.g., 12\n\n\nfig, ax = plt.subplots()\nfig.subplots_adjust(bottom=0.2)\nax.xaxis.set_major_locator(mondays)\nax.xaxis.set_minor_locator(alldays)\nax.xaxis.set_major_formatter(weekFormatter)\n\ncandlestick_ohlc(ax, MSFT_stock_values, width=0.6, colorup='g',colordown='r');\n\n\n\n\n\n# Method 1: Using shift\nMSFT_stock['returns'] = (MSFT_stock['Close'] / MSFT_stock['Close'].shift(1) ) - 1\n\nMSFT_stock.head()\n\nMSFT_stock['returns'] = MSFT_stock['Close'].pct_change(1)\n\nMSFT_stock.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n      Total Traded\n      returns\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2019-01-02\n      101.750000\n      98.940002\n      99.550003\n      101.120003\n      35329300.0\n      98.860214\n      3.517032e+09\n      NaN\n    \n    \n      2019-01-03\n      100.190002\n      97.199997\n      100.099998\n      97.400002\n      42579100.0\n      95.223351\n      4.262168e+09\n      -0.036788\n    \n    \n      2019-01-04\n      102.510002\n      98.930000\n      99.720001\n      101.930000\n      44060600.0\n      99.652115\n      4.393723e+09\n      0.046509\n    \n    \n      2019-01-07\n      103.269997\n      100.980003\n      101.639999\n      102.059998\n      35656100.0\n      99.779205\n      3.624086e+09\n      0.001275\n    \n    \n      2019-01-08\n      103.970001\n      101.709999\n      103.040001\n      102.800003\n      31514400.0\n      100.502670\n      3.247244e+09\n      0.007251\n    \n  \n\n\n\n\n\nTSLA_stock['returns'] = TSLA_stock['Close'].pct_change(1)\nZOOM_stock['returns'] = ZOOM_stock['Close'].pct_change(1)\n\n\nTSLA_stock.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n      Total Traded\n      returns\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2019-01-02\n      63.026001\n      59.759998\n      61.220001\n      62.023998\n      58293000.0\n      62.023998\n      3.568698e+09\n      NaN\n    \n    \n      2019-01-03\n      61.880001\n      59.476002\n      61.400002\n      60.071999\n      34826000.0\n      60.071999\n      2.138316e+09\n      -0.031472\n    \n    \n      2019-01-04\n      63.599998\n      60.546001\n      61.200001\n      63.537998\n      36970500.0\n      63.537998\n      2.262595e+09\n      0.057697\n    \n    \n      2019-01-07\n      67.348000\n      63.549999\n      64.344002\n      66.991997\n      37756000.0\n      66.991997\n      2.429372e+09\n      0.054361\n    \n    \n      2019-01-08\n      68.802002\n      65.403999\n      68.391998\n      67.070000\n      35042500.0\n      67.070000\n      2.396627e+09\n      0.001164\n    \n  \n\n\n\n\n\nZOOM_stock.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n      Total Traded\n      MA50\n      MA200\n      returns\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2019-04-18\n      66.000000\n      60.320999\n      65.000000\n      62.000000\n      25764700\n      62.000000\n      1.674706e+09\n      NaN\n      NaN\n      NaN\n    \n    \n      2019-04-22\n      68.900002\n      59.939999\n      61.000000\n      65.699997\n      9949700\n      65.699997\n      6.069317e+08\n      NaN\n      NaN\n      0.059677\n    \n    \n      2019-04-23\n      74.168999\n      65.550003\n      66.870003\n      69.000000\n      6786500\n      69.000000\n      4.538133e+08\n      NaN\n      NaN\n      0.050228\n    \n    \n      2019-04-24\n      71.500000\n      63.160000\n      71.400002\n      63.200001\n      4973500\n      63.200001\n      3.551079e+08\n      NaN\n      NaN\n      -0.084058\n    \n    \n      2019-04-25\n      66.849998\n      62.599998\n      64.739998\n      65.000000\n      3863300\n      65.000000\n      2.501100e+08\n      NaN\n      NaN\n      0.028481\n    \n  \n\n\n\n\n\nTSLA_stock['returns'].hist(bins=50)\n\n<AxesSubplot:>\n\n\n\n\n\n\nMSFT_stock['returns'].hist(bins=50)\n\n<AxesSubplot:>\n\n\n\n\n\n\nZOOM_stock['returns'].hist(bins=50)\n\n<AxesSubplot:>\n\n\n\n\n\n\nMSFT_stock['returns'].hist(bins=100,label='MSFT_stock',figsize=(10,8),alpha=0.5)\nZOOM_stock['returns'].hist(bins=100,label='ZOOM_stock',alpha=0.5)\nTSLA_stock['returns'].hist(bins=100,label='TSLA_stock',alpha=0.5)\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7fabf1902d90>\n\n\n\n\n\n\nMSFT_stock['returns'].plot(kind='kde',label='MSFT_stock',figsize=(12,6))\nZOOM_stock['returns'].plot(kind='kde',label='ZOOM_stock')\nTSLA_stock['returns'].plot(kind='kde',label='TSLA_stock')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7fabf11a3e50>\n\n\n\n\n\n\nbox_df = pd.concat([MSFT_stock['returns'],ZOOM_stock['returns'],TSLA_stock['returns']],axis=1)\nbox_df.columns = ['MSFT_stock Returns',' ZOOM_stock Returns','TSLA_stock Returns']\nbox_df.plot(kind='box',figsize=(8,11),colormap='jet')\n\n<AxesSubplot:>\n\n\n\n\n\n\nscatter_matrix(box_df,figsize=(8,8),alpha=0.2,hist_kwds={'bins':50});\n\n\n\n\n\nscatter_matrix(box_df,figsize=(8,8),alpha=0.2,hist_kwds={'bins':50});\n\n\n\n\n\nbox_df.plot(kind='scatter',x=' ZOOM_stock Returns',y='MSFT_stock Returns',alpha=0.4,figsize=(10,8))\n\n<AxesSubplot:xlabel=' ZOOM_stock Returns', ylabel='MSFT_stock Returns'>"
  },
  {
    "objectID": "posts/2020-10-22-stock-market-returns.html#daily-return-and-cumulative-return",
    "href": "posts/2020-10-22-stock-market-returns.html#daily-return-and-cumulative-return",
    "title": "Stock Market Analysis of Microsoft, Zoom, and Snowflake",
    "section": "Daily Return and Cumulative Return",
    "text": "Daily Return and Cumulative Return\n\nMSFT_stock['Cumulative Return'] = (1 + MSFT_stock['returns']).cumprod()\n\nMSFT_stock.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n      Total Traded\n      returns\n      Cumulative Return\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2019-01-02\n      101.750000\n      98.940002\n      99.550003\n      101.120003\n      35329300.0\n      98.860214\n      3.517032e+09\n      NaN\n      NaN\n    \n    \n      2019-01-03\n      100.190002\n      97.199997\n      100.099998\n      97.400002\n      42579100.0\n      95.223351\n      4.262168e+09\n      -0.036788\n      0.963212\n    \n    \n      2019-01-04\n      102.510002\n      98.930000\n      99.720001\n      101.930000\n      44060600.0\n      99.652115\n      4.393723e+09\n      0.046509\n      1.008010\n    \n    \n      2019-01-07\n      103.269997\n      100.980003\n      101.639999\n      102.059998\n      35656100.0\n      99.779205\n      3.624086e+09\n      0.001275\n      1.009296\n    \n    \n      2019-01-08\n      103.970001\n      101.709999\n      103.040001\n      102.800003\n      31514400.0\n      100.502670\n      3.247244e+09\n      0.007251\n      1.016614\n    \n  \n\n\n\n\n\nTSLA_stock['Cumulative Return'] = (1 + TSLA_stock['returns']).cumprod()\nZOOM_stock['Cumulative Return'] = (1 + ZOOM_stock['returns']).cumprod()\n\n\nMSFT_stock['Cumulative Return'].plot(label='MSFT_stock',figsize=(16,8),title='Cumulative Return')\nTSLA_stock['Cumulative Return'].plot(label='TSLA_stock')\nZOOM_stock['Cumulative Return'].plot(label='ZOOM_stock')\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7fabf0c54650>\n\n\n\n\n\n\nfrom fbprophet import Prophet\n\n\nMSFT_stock['ds'] = MSFT_stock.index\nMSFT_stock['y'] = MSFT_stock.Open\n\nm = Prophet()\nm.fit(MSFT_stock)\n\nINFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n<fbprophet.forecaster.Prophet at 0x7fabf0b5e7d0>\n\n\n\nfuture = m.make_future_dataframe(periods=365)\nfuture.tail()\n\n\n\n\n\n  \n    \n      \n      ds\n    \n  \n  \n    \n      828\n      2021-11-02\n    \n    \n      829\n      2021-11-03\n    \n    \n      830\n      2021-11-04\n    \n    \n      831\n      2021-11-05\n    \n    \n      832\n      2021-11-06\n    \n  \n\n\n\n\n\nforecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n\n\n\n\n\n  \n    \n      \n      ds\n      yhat\n      yhat_lower\n      yhat_upper\n    \n  \n  \n    \n      828\n      2021-11-02\n      264.480553\n      180.297752\n      351.490936\n    \n    \n      829\n      2021-11-03\n      264.454392\n      181.887667\n      348.682850\n    \n    \n      830\n      2021-11-04\n      264.470521\n      183.308874\n      351.080864\n    \n    \n      831\n      2021-11-05\n      264.680107\n      182.084709\n      351.905134\n    \n    \n      832\n      2021-11-06\n      263.754254\n      180.125064\n      351.617704\n    \n  \n\n\n\n\n\nfig1 = m.plot(forecast)\n\n\n\n\n\nfig2 = m.plot_components(forecast)\n\n\n\n\n\nMSFT_stock['Open'].plot()\n\n<AxesSubplot:xlabel='Date'>\n\n\n\n\n\n\nimport statsmodels.api as sm\n\n\nMSFT_stock_cycle, MSFT_stock_trend = sm.tsa.filters.hpfilter(MSFT_stock['Open'])\n\n\nMSFT_stock_cycle\n\nDate\n2019-01-02   -1.311918\n2019-01-03   -1.097494\n2019-01-04   -1.812243\n2019-01-07   -0.224671\n2019-01-08    0.847869\n                ...   \n2020-11-02   -8.431408\n2020-11-03   -8.845880\n2020-11-04    1.239842\n2020-11-05    9.198111\n2020-11-06    9.350548\nName: Open_cycle, Length: 468, dtype: float64\n\n\n\nMSFT_stock['trend'] = MSFT_stock_trend\n\n\nMSFT_stock['trend']\n\nDate\n2019-01-02    100.861921\n2019-01-03    101.197493\n2019-01-04    101.532244\n2019-01-07    101.864670\n2019-01-08    102.192132\n                 ...    \n2020-11-02    212.721402\n2020-11-03    212.735880\n2020-11-04    212.780163\n2020-11-05    212.841883\n2020-11-06    212.909447\nName: trend, Length: 468, dtype: float64\n\n\n\nMSFT_stock[[\"trend\", \"Open\"]].plot(figsize=(16,6))\n\n<AxesSubplot:xlabel='Date'>\n\n\n\n\n\n\nTSLA_stock['ds'] = TSLA_stock.index\nTSLA_stock['y'] = TSLA_stock.Open\n\nm = Prophet()\nm.fit(TSLA_stock)\n\nINFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n<fbprophet.forecaster.Prophet at 0x7fabeaebca10>\n\n\n\n# Python\nTSLA_stock['cap'] = 8.5\nm = Prophet(growth='logistic')\nm.fit(TSLA_stock)\n\nINFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n<fbprophet.forecaster.Prophet at 0x7fabf081d190>\n\n\n\n# future = m.make_future_dataframe(periods=1826)\n# future['cap'] = 600\n# fcst = m.predict(future)\n# fig = m.plot(fcst)\n\n\nfuture = m.make_future_dataframe(periods=365)\nfuture.tail()\n\n\n\n\n\n  \n    \n      \n      ds\n    \n  \n  \n    \n      828\n      2021-11-02\n    \n    \n      829\n      2021-11-03\n    \n    \n      830\n      2021-11-04\n    \n    \n      831\n      2021-11-05\n    \n    \n      832\n      2021-11-06\n    \n  \n\n\n\n\n\nforecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n\n\n\n\n\n  \n    \n      \n      ds\n      yhat\n      yhat_lower\n      yhat_upper\n    \n  \n  \n    \n      828\n      2021-11-02\n      1136.958087\n      1029.660624\n      1241.408457\n    \n    \n      829\n      2021-11-03\n      1138.386277\n      1029.960322\n      1245.559028\n    \n    \n      830\n      2021-11-04\n      1139.054611\n      1027.658532\n      1245.938576\n    \n    \n      831\n      2021-11-05\n      1140.980461\n      1031.883548\n      1250.009090\n    \n    \n      832\n      2021-11-06\n      1142.201501\n      1037.157758\n      1245.925892\n    \n  \n\n\n\n\n\nfig1 = m.plot(forecast)\n\n\n\n\n\nfig2 = m.plot_components(forecast)\n\n\n\n\n\nZOOM_stock['ds'] = ZOOM_stock.index\nZOOM_stock['y'] = ZOOM_stock.Open\n\nm = Prophet()\nm.fit(ZOOM_stock)\n\nINFO:fbprophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n<fbprophet.forecaster.Prophet at 0x7fabf0b64790>\n\n\n\nfuture = m.make_future_dataframe(periods=365)\nfuture.tail()\n\n\n\n\n\n  \n    \n      \n      ds\n    \n  \n  \n    \n      754\n      2021-11-02\n    \n    \n      755\n      2021-11-03\n    \n    \n      756\n      2021-11-04\n    \n    \n      757\n      2021-11-05\n    \n    \n      758\n      2021-11-06\n    \n  \n\n\n\n\n\nforecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n\n\n\n\n\n  \n    \n      \n      ds\n      yhat\n      yhat_lower\n      yhat_upper\n    \n  \n  \n    \n      754\n      2021-11-02\n      1498.560150\n      1318.618163\n      1677.021462\n    \n    \n      755\n      2021-11-03\n      1500.430463\n      1324.946607\n      1679.133120\n    \n    \n      756\n      2021-11-04\n      1502.503460\n      1323.082402\n      1683.102389\n    \n    \n      757\n      2021-11-05\n      1505.218307\n      1323.789633\n      1683.789366\n    \n    \n      758\n      2021-11-06\n      1499.986193\n      1323.049103\n      1680.953619\n    \n  \n\n\n\n\n\nfig1 = m.plot(forecast)\n\n\n\n\n\nfig2 = m.plot_components(forecast)"
  },
  {
    "objectID": "posts/2020-10-21-quant_data_datareader_quandl.html",
    "href": "posts/2020-10-21-quant_data_datareader_quandl.html",
    "title": "Using the Quandl API and Pandas Datareader API to call Microsoft, Apple, Zoom, Snowflake stocks and other finance data",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport pandas_datareader.data as web\nimport datetime\n\n\nstart = datetime.datetime(2020, 1, 1)\nend = pd.to_datetime('today')\n\n\nAAPL_stock = web.DataReader('AAPL', 'yahoo', start, end)\nAAPL_stock.head()\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nSNOW_stock = web.DataReader('SNOW', 'yahoo', start, end)\nSNOW_stock.head()\n\n\n\n\n\n  \n    \n      \n      High\n      Low\n      Open\n      Close\n      Volume\n      Adj Close\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2020-09-16\n      319.0\n      231.110001\n      245.000000\n      253.929993\n      36099700\n      253.929993\n    \n    \n      2020-09-17\n      241.5\n      215.240005\n      230.759995\n      227.539993\n      11907500\n      227.539993\n    \n    \n      2020-09-18\n      249.0\n      218.589996\n      235.000000\n      240.000000\n      7475400\n      240.000000\n    \n    \n      2020-09-21\n      241.5\n      218.600006\n      230.000000\n      228.850006\n      5524900\n      228.850006\n    \n    \n      2020-09-22\n      239.0\n      225.149994\n      238.500000\n      235.160004\n      3889100\n      235.160004\n    \n  \n\n\n\n\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Open')\n\nMSFT_stock['Open'].plot(label='Microsoft')\nZOOM_stock['Open'].plot(label='Zoom')\nSNOW_stock['Open'].plot(label='Snowflake')\nAAPL_stock['Open'].plot(label='Apple')\nplt.legend()\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Volume')\n\nMSFT_stock['Volume'].plot(label='Microsoft')\nZOOM_stock['Volume'].plot(label='Zoom')\nSNOW_stock['Volume'].plot(label='Snowflake')\nAAPL_stock['Volume'].plot(label='Apple')\n\nplt.legend()\n\n<matplotlib.legend.Legend at 0x7f3e5dc577f0>\n\n\n\n\n\n\n\n\n\nimport pandas_datareader.data as web\n\nimport datetime\n\n\n\ngdp = web.DataReader(\"GDP\", \"fred\", start, end)\n\n\ngdp.head()\n\n\n\n\n\n  \n    \n      \n      GDP\n    \n    \n      DATE\n      \n    \n  \n  \n    \n      2020-01-01\n      21561.139\n    \n    \n      2020-04-01\n      19520.114\n    \n    \n      2020-07-01\n      21170.252\n    \n  \n\n\n\n\n\nimport quandl\n\n\n#quandl.ApiConfig.api_key = ''\n\n\nmydata = quandl.get(\"EIA/PET_RWTC_D\")\n\nmydata.head()\n\nLimitExceededError: (Status 429) (Quandl Error QELx01) You have exceeded the anonymous user limit of 50 calls per day. To make more calls today, please register for a free Quandl account and then include your API key with your requests.\n\n\n\nmydata.plot(figsize=(12,6))\n\n\n#mydata = quandl.get(\"EIA/PET_RWTC_D\", returns=\"numpy\",start_date=start,end_date=end)\n\n\nmydata = quandl.get(\"FRED/GDP\",start_date=start,end_date=end)\n\n\nmydata.head()\n\n\nmydata = quandl.get([\"NSE/OIL.1\", \"WIKI/AAPL.4\"],start_date=start,end_date=end)\n\n\nmydata.head()\n\n\n\n\n\n  \n    \n      \n      NSE/OIL - Open\n      WIKI/AAPL - Close\n    \n  \n  \n  \n\n\n\n\n\nmydata = quandl.get(\"FRED/GDP\")\n\n\nmydata = quandl.get('WIKI/FB',start_date=start,end_date=end)\n\n\nmydata.head()\n\n\n\n\n\n  \n    \n      \n      Open\n      High\n      Low\n      Close\n      Volume\n      Ex-Dividend\n      Split Ratio\n      Adj. Open\n      Adj. High\n      Adj. Low\n      Adj. Close\n      Adj. Volume\n    \n    \n      Date\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n  \n\n\n\n\n\nmydata = quandl.get('WIKI/FB.1',start_date=start,end_date=end)\n\nmydata.head()\n\n\n\n\n\n  \n    \n      \n      Open\n    \n    \n      Date\n      \n    \n  \n  \n  \n\n\n\n\n\nmydata = quandl.get('WIKI/FB.7',start_date=start,end_date=end)\nmydata.head()\n\n\n\n\n\n  \n    \n      \n      Split Ratio\n    \n    \n      Date\n      \n    \n  \n  \n  \n\n\n\n\n\n# Homes\n\n\nhouses = quandl.get('ZILLOW/M11_ZRIAH',start_date=start,end_date=end)\n\n\nhouses.head()\n\n\n\n\n\n  \n    \n      \n      Value\n    \n    \n      Date\n      \n    \n  \n  \n    \n      2020-01-31\n      3342.0\n    \n    \n      2020-02-29\n      3358.0\n    \n  \n\n\n\n\n\nhouses.plot()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f900d58fef0>"
  },
  {
    "objectID": "posts/2020-10-08-dask-day-1.html",
    "href": "posts/2020-10-08-dask-day-1.html",
    "title": "Using Dask for Arrays",
    "section": "",
    "text": "import numpy as np\nimport dask.array as da\n\n\nnp_arr = np.random.randint(20, size=20)\nnp_arr\n\narray([12, 18, 17,  7,  5,  9, 11,  3,  5, 15, 13, 13,  5, 12, 11, 16,  4,\n       10,  9,  7])\n\n\n\ndask_arr = da.random.randint(20, size=20, chunks=5)\n\n\ndask_arr\n\n\n\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  160 B   40 B \n     Shape  (20,)   (5,) \n     Count  4 Tasks  4 Chunks \n     Type  int64  numpy.ndarray \n  \n\n\n\n\n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  20\n  1\n\n\n\n\n\n\n\n## This is simply because Dask does lazy evaluaion.   \n### You need to call `compute()` to start the execution\n\n\ndask_arr.compute()\n\narray([ 3, 17,  5, 11, 19, 14, 14, 11,  9, 18,  9,  7, 10, 13, 10, 10, 11,\n       10,  9,  2])\n\n\n\ndask_arr.chunks\n\n((5, 5, 5, 5),)\n\n\n\ndask_arr_from_np = da.from_array(np_arr, chunks=5)\n\n\ndask_arr_from_np\n\n\n\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  160 B   40 B \n     Shape  (20,)   (5,) \n     Count  5 Tasks  4 Chunks \n     Type  int64  numpy.ndarray \n  \n\n\n\n\n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  20\n  1\n\n\n\n\n\n\n\ndask_arr_from_np.compute()\n\narray([12, 18, 17,  7,  5,  9, 11,  3,  5, 15, 13, 13,  5, 12, 11, 16,  4,\n       10,  9,  7])\n\n\n\n### array operations into a graph to tasks\n#### See : http://docs.dask.org/en/latest/graphviz.html\n\n\ndask_arr_from_np.sum().visualize()\n\n\n\n\n\ndask_arr_from_np.sum().visualize(rankdir=\"LR\")\n\n\n\n\n\n(dask_arr_from_np+1).visualize(rankdir=\"LR\")\n\n\n\n\n\ndask_arr_mean = da.mean(dask_arr_from_np)\ndask_arr_mean.compute()\n\n10.1\n\n\n\ndask_arr_mean.visualize(rankdir=\"LR\")\n\n\n\n\n\nx = da.random.random(10, chunks=2)\ny = da.random.random(10, chunks=2)\n\nsum_x_y = da.add(x, y) #similar to numpy.add\nmean_x_y = da.mean(sum_x_y)\n\n\nsum_x_y.compute()\n\narray([0.96028343, 0.55946179, 1.11161829, 1.28233368, 0.53130934,\n       0.86805782, 0.20173099, 0.77596276, 0.92576765, 1.04750609])\n\n\n\nsum_x_y.visualize()\n\n\n\n\n\nmean_x_y.visualize()\n\n\n\n\n\nda_arr_large = da.random.randint(10000, size=(50000, 50000),\n                                 chunks=(5000, 1000))   \nda_sum_large = da_arr_large.sum()   \n\n\n### Get no. bytes using `nbytes` : http://docs.dask.org/en/latest/array-api.html#dask.array.Array.nbytes\n\n\nda_arr_large.nbytes  \n\n20000000000\n\n\n\n### Convert bytes to GB, 1Gb = 1e+9 bytes\n\n\nda_arr_large.nbytes/1e+9\n\n20.0\n\n\n\nda_sum_large.compute()\n\n12498643590734\n\n\n\n# Dask 2\n\n\nsize_tuple = (500,500)\nchunks_tuple = (10,500)\n\n\nda_arr = da.random.randint(10, size=size_tuple,\n                           chunks=chunks_tuple)\nda_arr2 = da.random.randint(10, size=size_tuple,\n                            chunks=chunks_tuple)\n\n\ndef random_func(x):\n    return np.mean((((x * 2).T)**2),axis=0)\n\n\ngufoo = da.gufunc(random_func, signature=\"(i)->()\",\n                  output_dtypes=float,\n                  vectorize=True)\n\n\nrandom_op_arr = gufoo(da_arr)\nrandom_op_arr.compute()\n\narray([112.056, 107.44 , 111.024, 109.656, 118.832, 109.84 , 117.2  ,\n       111.952, 116.312, 117.368, 128.568, 111.144, 110.656, 112.648,\n       115.24 , 114.624, 113.912, 109.632, 112.864, 113.488, 119.248,\n       121.4  , 108.272, 118.784, 114.968, 115.216, 107.872, 113.6  ,\n       112.456, 112.48 , 114.864, 119.28 , 112.656, 110.208, 109.728,\n       120.576, 119.632, 118.12 , 112.888, 116.384, 113.192, 106.84 ,\n       111.72 , 115.928, 106.08 , 114.568, 121.512, 115.384, 113.864,\n       107.104, 114.32 , 116.176, 117.28 , 116.976, 117.784, 110.088,\n       121.696, 114.2  , 113.864, 116.072, 112.344, 113.808, 113.968,\n       110.472, 119.536, 113.84 , 109.328, 116.552, 119.056, 113.84 ,\n       117.872, 114.928, 116.336, 115.192, 115.808, 106.984, 116.984,\n       114.536, 116.496, 111.968, 115.216, 108.24 , 119.52 , 116.136,\n       111.144, 111.712, 119.224, 114.312, 110.464, 110.216, 111.288,\n       119.6  , 108.264, 114.456, 119.016, 107.032, 114.832, 108.056,\n       105.712, 110.64 , 103.4  , 106.768, 118.216, 112.44 , 113.728,\n       114.6  , 117.832, 108.288, 117.92 , 113.12 , 121.984, 112.776,\n       123.144, 115.968, 112.44 , 115.712, 112.144, 108.448, 114.752,\n       108.376, 101.296, 102.992, 117.872, 114.056, 115.736, 115.528,\n       122.072, 130.168, 106.992, 109.912, 117.872, 112.152, 112.184,\n       113.544, 116.496, 112.832, 108.712, 116.96 , 120.984, 117.808,\n       112.272, 111.816, 118.872, 116.376, 118.992, 112.344, 124.672,\n        97.576, 112.496, 117.92 , 102.392, 109.992, 112.016, 117.92 ,\n       108.352, 112.376, 121.008, 117.808, 113.504, 125.592, 114.936,\n       111.456, 116.488, 104.744, 114.136, 114.   , 107.256, 117.84 ,\n       111.872, 109.152, 118.752, 112.32 , 116.16 , 106.696, 109.472,\n       111.968, 118.264, 115.088, 112.864, 110.016, 111.888, 111.84 ,\n       118.488, 107.952, 121.52 , 126.52 , 112.12 , 110.952, 115.328,\n       110.064, 106.36 , 118.96 , 109.68 , 117.776, 107.112, 111.152,\n       113.888, 113.408, 114.992, 117.632, 116.648, 117.112, 118.2  ,\n       116.36 , 113.104, 113.6  , 112.208, 112.592, 117.192, 102.832,\n       112.08 , 113.744, 116.048, 117.368, 113.96 , 111.24 , 121.824,\n       112.56 , 110.192, 130.776, 111.656, 119.984, 113.592, 113.592,\n       106.664, 125.192, 113.6  , 117.12 , 106.24 , 112.856, 114.544,\n       117.16 , 108.344, 112.208, 109.112, 124.824, 109.824, 106.352,\n       115.568, 112.64 , 112.904, 112.736, 112.52 , 124.808, 120.32 ,\n       114.472, 119.528, 113.456, 112.448, 118.672, 110.016, 116.16 ,\n       122.048, 111.088, 114.56 , 107.448, 115.328, 111.656, 108.688,\n       116.904, 110.8  , 108.896, 112.136, 115.896, 111.848, 108.808,\n       114.504, 124.552, 116.248, 114.576, 110.56 , 112.152, 117.576,\n       125.44 , 110.72 , 108.072, 115.192, 116.048, 107.76 , 111.376,\n       121.608, 115.256, 113.84 , 105.672, 115.024, 115.864, 114.304,\n       123.344, 114.624, 115.696, 113.288, 116.688, 109.048, 125.264,\n       118.8  , 112.2  , 114.312, 109.728, 116.064, 113.808, 106.912,\n       109.288, 117.   , 114.632, 114.456, 110.168, 111.976, 117.816,\n       110.04 , 103.048, 113.656, 112.504, 113.8  , 120.04 , 120.224,\n       110.68 , 110.096, 116.12 , 113.424, 107.408, 111.296, 111.512,\n       117.432, 105.96 , 115.992, 118.44 , 110.024, 119.216, 111.664,\n       119.184, 109.824, 116.736, 116.76 , 107.544, 120.44 , 115.08 ,\n       110.136, 112.144, 113.888, 111.32 , 109.952, 117.096, 111.152,\n       115.728, 110.832, 113.312, 113.664, 112.016, 111.952, 114.896,\n       114.728, 107.848, 108.832, 122.384, 111.824, 107.384, 117.504,\n       117.344, 110.144, 109.568, 101.36 , 111.944, 105.512, 115.792,\n       112.08 , 104.568, 109.008, 108.992, 114.936, 113.008, 120.088,\n       117.328, 117.008, 107.584, 111.688, 115.664, 108.416, 119.48 ,\n       107.336, 120.184, 111.952, 115.824, 113.928, 117.064, 114.296,\n       111.56 , 120.04 , 112.256, 115.368, 109.112, 112.184, 112.128,\n       111.288, 117.856, 109.184, 113.128, 119.888, 110.656, 111.992,\n       116.704, 107.696, 111.608, 121.504, 110.296, 111.008, 112.072,\n       117.072, 115.68 , 108.888, 117.704, 113.112, 101.144, 112.36 ,\n       122.688, 112.016, 111.64 , 113.992, 117.08 , 109.976, 108.048,\n       110.504, 112.936, 111.776, 117.392, 116.568, 106.896, 105.224,\n       115.512, 117.   , 116.192, 113.344, 111.776, 114.312, 113.008,\n       114.768, 121.712, 112.528, 108.976, 106.648, 107.8  , 122.696,\n       104.064, 117.072, 119.064, 111.472, 112.752, 109.52 , 123.712,\n       114.032, 120.888, 109.84 , 123.36 , 111.576, 118.56 , 116.328,\n       113.048, 111.68 , 106.072, 109.752, 112.32 , 114.344, 114.976,\n       114.072, 121.792, 113.024, 109.864, 115.84 , 115.752, 118.648,\n       107.52 , 116.104, 112.464, 123.232, 112.32 , 116.952, 106.32 ,\n       110.992, 111.256, 113.616, 111.344, 115.216, 121.504, 117.504,\n       115.816, 116.6  , 111.08 , 108.776, 110.672, 109.464, 107.096,\n       112.928, 106.8  , 110.4  , 112.576, 114.648, 113.272, 112.504,\n       112.888, 112.84 , 111.496])\n\n\n\nrandom_op_arr.shape\n\n(500,)\n\n\n\n@da.as_gufunc(signature=\"(m,n),(n,j)->(m,j)\", output_dtypes=int, allow_rechunk=True)\ndef random_func(x, y):\n    return np.matmul(x, y)**2\n\n\nda_arr3 = da.random.randint(10, size=(200, 100), chunks=(10, 100))\nda_arr4 = da.random.randint(10, size=(100, 300), chunks=(5,5))\n\n\n# random_matmul = random_func(da_arr3, da_arr4)\n# random_matmul.compute()\n\n\nrandom_matmul.shape\n\n(200, 300)\n\n\n\n# Dask 3\n\n\nmy_arr = da.random.randint(10, size=20, chunks=3)\n\n\nmy_arr.compute()\n\narray([3, 8, 8, 7, 4, 1, 5, 7, 2, 7, 4, 4, 8, 0, 9, 3, 6, 7, 1, 5])\n\n\n\nmy_hundred_arr = my_arr + 100\nmy_hundred_arr.compute()\n\narray([103, 108, 108, 107, 104, 101, 105, 107, 102, 107, 104, 104, 108,\n       100, 109, 103, 106, 107, 101, 105])\n\n\n\n(my_arr * (-1)).compute()\n\narray([-3, -8, -8, -7, -4, -1, -5, -7, -2, -7, -4, -4, -8,  0, -9, -3, -6,\n       -7, -1, -5])\n\n\n\ndask_sum = my_arr.sum()\ndask_sum\n\n\n\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  8 B   8 B \n     Shape  ()   () \n     Count  17 Tasks  1 Chunks \n     Type  int64  numpy.ndarray \n  \n\n\n\n\n\n\n\n\n\n\nmy_arr.compute()\n\narray([3, 8, 8, 7, 4, 1, 5, 7, 2, 7, 4, 4, 8, 0, 9, 3, 6, 7, 1, 5])\n\n\n\ndask_sum.compute()\n\n99\n\n\n\nmy_ones_arr = da.ones((10,10), chunks=2, dtype=int)\n\n\nmy_ones_arr.compute()\n\narray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n\n\n\nmy_ones_arr.mean(axis=0).compute()\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\nmy_custom_array = da.random.randint(10, size=(4,4), chunks=(1,4))\n\n\nmy_custom_array.compute()\n\narray([[0, 1, 7, 6],\n       [0, 1, 2, 4],\n       [6, 3, 5, 3],\n       [3, 2, 2, 6]])\n\n\n\nmy_custom_array.mean(axis=0).compute()\n\narray([2.25, 1.75, 4.  , 4.75])\n\n\n\nmy_custom_array.mean(axis=1).compute()\n\narray([3.5 , 1.75, 4.25, 3.25])\n\n\n\n## Slicing\n\n\nmy_custom_array[1:3, 2:4]\n\n\n\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  32 B   16 B \n     Shape  (2, 2)   (1, 2) \n     Count  6 Tasks  2 Chunks \n     Type  int64  numpy.ndarray \n  \n\n\n\n\n\n  \n  \n  \n  \n\n  \n  \n  \n\n  \n  \n\n  \n  2\n  2\n\n\n\n\n\n\n\nmy_custom_array[1:3, 2:4].compute()\n\narray([[2, 4],\n       [5, 3]])\n\n\n\n## Broadcasting\n\n\nmy_custom_array.compute()\n\narray([[0, 1, 7, 6],\n       [0, 1, 2, 4],\n       [6, 3, 5, 3],\n       [3, 2, 2, 6]])\n\n\n\nmy_small_arr = da.ones(4, chunks=2)\nmy_small_arr.compute()\n\narray([1., 1., 1., 1.])\n\n\n\nbrd_example1 = da.add(my_custom_array, my_small_arr)\n\n\nbrd_example1.compute()\n\narray([[1., 2., 8., 7.],\n       [1., 2., 3., 5.],\n       [7., 4., 6., 4.],\n       [4., 3., 3., 7.]])\n\n\n\nten_arr = da.full_like(my_small_arr, 10)\n\n\nten_arr.compute()\n\narray([10., 10., 10., 10.])\n\n\n\nbrd_example2 = da.add(my_custom_array, ten_arr)\n\n\nbrd_example2.compute()\n\narray([[10., 11., 17., 16.],\n       [10., 11., 12., 14.],\n       [16., 13., 15., 13.],\n       [13., 12., 12., 16.]])\n\n\n\n## Reshaping\n\n\nmy_custom_array.shape\n\n(4, 4)\n\n\n\ncustom_arr_1d = my_custom_array.reshape(16)\n\n\ncustom_arr_1d\n\n\n\n\n\n\n  \n      Array  Chunk \n  \n  \n     Bytes  128 B   32 B \n     Shape  (16,)   (4,) \n     Count  8 Tasks  4 Chunks \n     Type  int64  numpy.ndarray \n  \n\n\n\n\n\n  \n  \n  \n\n  \n  \n  \n  \n  \n  \n\n  \n  \n\n  \n  16\n  1\n\n\n\n\n\n\n\ncustom_arr_1d.compute()\n\narray([0, 1, 7, 6, 0, 1, 2, 4, 6, 3, 5, 3, 3, 2, 2, 6])\n\n\n\n# Stacking\n\n\nstacked_arr = da.stack([brd_example1, brd_example2])\n\n\nstacked_arr.compute()\n\narray([[[ 1.,  2.,  8.,  7.],\n        [ 1.,  2.,  3.,  5.],\n        [ 7.,  4.,  6.,  4.],\n        [ 4.,  3.,  3.,  7.]],\n\n       [[10., 11., 17., 16.],\n        [10., 11., 12., 14.],\n        [16., 13., 15., 13.],\n        [13., 12., 12., 16.]]])\n\n\n\nanother_stacked = da.stack([brd_example1, brd_example2], axis=1)\n\n\nanother_stacked.compute()\n\narray([[[ 1.,  2.,  8.,  7.],\n        [10., 11., 17., 16.]],\n\n       [[ 1.,  2.,  3.,  5.],\n        [10., 11., 12., 14.]],\n\n       [[ 7.,  4.,  6.,  4.],\n        [16., 13., 15., 13.]],\n\n       [[ 4.,  3.,  3.,  7.],\n        [13., 12., 12., 16.]]])\n\n\n\n# Concatenate\n\n\nconcate_arr = da.concatenate([brd_example1, brd_example2])\n\n\nconcate_arr.compute()\n\narray([[ 1.,  2.,  8.,  7.],\n       [ 1.,  2.,  3.,  5.],\n       [ 7.,  4.,  6.,  4.],\n       [ 4.,  3.,  3.,  7.],\n       [10., 11., 17., 16.],\n       [10., 11., 12., 14.],\n       [16., 13., 15., 13.],\n       [13., 12., 12., 16.]])\n\n\n\nanother_concate_arr = da.concatenate([brd_example1, brd_example2],axis=1)\n\n\nanother_concate_arr.compute()\n\narray([[ 1.,  2.,  8.,  7., 10., 11., 17., 16.],\n       [ 1.,  2.,  3.,  5., 10., 11., 12., 14.],\n       [ 7.,  4.,  6.,  4., 16., 13., 15., 13.],\n       [ 4.,  3.,  3.,  7., 13., 12., 12., 16.]])\n\n\n\n# Dask 4\n\n\nimport numpy as np\nimport dask.array as da\n\n\nsize_tuple = (18000,18000)\nnp_arr = np.random.randint(10, size=size_tuple)\nnp_arr2 = np.random.randint(10, size=size_tuple)\n\n\n%time (((np_arr * 2).T)**2 + np_arr2 + 100).sum(axis=1).mean()\n\nMemoryError: \n\n\n\nchunks_tuple = (500, 500)\nda_arr = da.from_array(np_arr, chunks=chunks_tuple)\nda_arr2 = da.from_array(np_arr2, chunks=chunks_tuple)\n\n\n%time (((da_arr * 2).T)**2 + da_arr2 + 100).sum(axis=1).mean().compute()\n\nCPU times: user 10.1 s, sys: 362 ms, total: 10.5 s\nWall time: 2.47 s\n\n\n3933124.5174444444\n\n\n\nsize_tuple = (50000, 50000)\nnp_arr = np.random.randint(10, size=size_tuple)\nnp_arr2 = np.random.randint(10, size=size_tuple)\n\nMemoryError: \n\n\n\nchunks_tuple = (5000, 5000)\nda_arr = da.random.randint(10, size=size_tuple,\n                           chunks=chunks_tuple)\nda_arr2 = da.random.randint(10, size=size_tuple,\n                            chunks=chunks_tuple)\n\n\n%time (((da_arr * 2).T)**2 + da_arr2 + 100).sum(axis=1).mean().compute()\n\nCPU times: user 3min 10s, sys: 10.5 s, total: 3min 20s\nWall time: 28.2 s\n\n\n10925051.41748\n\n\n\nda_arr.nbytes/1e+9\n\n20.0"
  },
  {
    "objectID": "posts/2020-10-09-dask_fiscal-data-sqlite-db.html",
    "href": "posts/2020-10-09-dask_fiscal-data-sqlite-db.html",
    "title": "Moving fiscal data from a pandas dataframe to a sqlite local database",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\n\ndf = pd.read_csv('df_panel_fix.csv')\n\n\ndf.columns\n\nIndex(['Unnamed: 0', 'province', 'specific', 'general', 'year', 'gdp', 'fdi',\n       'rnr', 'rr', 'i', 'fr', 'reg', 'it'],\n      dtype='object')\n\n\n\ndf_subset = df[[\"year\", \"reg\", \"province\", \"gdp\", \"fdi\", 'it',\"specific\"]]\ndf_subset\n\n\n\n\n\n  \n    \n      \n      year\n      reg\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.30\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.70\n      668128\n      3162299\n      656175.0\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      18753.73\n      1036576\n      2939778\n      844647.0\n    \n  \n\n360 rows × 7 columns\n\n\n\n\ndf_subset.columns = [\"year\", \"region\", \"province\", \"gdp\", \"fdi\", 'it',\"specific\"]\n\n\ndf_subset\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.30\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.70\n      668128\n      3162299\n      656175.0\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      18753.73\n      1036576\n      2939778\n      844647.0\n    \n  \n\n360 rows × 7 columns\n\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///fiscal_data.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nfiscal_data = db.Table('fiscal_data', metadata, \n    db.Column('year',db.Integer, nullable=True, index=False),\n    db.Column('region',db.String, nullable=True),\n    db.Column('province',db.String, nullable=True),\n    db.Column('gdp',db.String, nullable=True),\n    db.Column('fdi',db.Integer, nullable=True),\n    db.Column('it',db.Integer, nullable=True),\n    db.Column('specific', db.Integer, nullable=True)\n)\n\n\nmetadata.create_all(engine) #Creates the table\n\n\nfiscal_data\n\nTable('fiscal_data', MetaData(bind=None), Column('year', Integer(), table=<fiscal_data>), Column('region', String(), table=<fiscal_data>), Column('province', String(), table=<fiscal_data>), Column('gdp', String(), table=<fiscal_data>), Column('fdi', Integer(), table=<fiscal_data>), Column('it', Integer(), table=<fiscal_data>), Column('specific', Integer(), table=<fiscal_data>), schema=None)\n\n\n\ndf_subset.to_sql('fiscal_data', con=engine, if_exists='append', index=False)\n\n\nengine.execute(\"SELECT year, region, province, gdp FROM fiscal_data LIMIT 10\").fetchall()\n\n[(1996, 'East China', 'Anhui', '2093.3'),\n (1997, 'East China', 'Anhui', '2347.32'),\n (1998, 'East China', 'Anhui', '2542.96'),\n (1999, 'East China', 'Anhui', '2712.34'),\n (2000, 'East China', 'Anhui', '2902.09'),\n (2001, 'East China', 'Anhui', '3246.71'),\n (2002, 'East China', 'Anhui', '3519.72'),\n (2003, 'East China', 'Anhui', '3923.11'),\n (2004, 'East China', 'Anhui', '4759.3'),\n (2005, 'East China', 'Anhui', '5350.17')]\n\n\n\nsql = \"\"\"\nSELECT year\n, region\n, province\n, gdp \nFROM fiscal_data\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf.tail(30)\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n    \n  \n  \n    \n      330\n      2002\n      Northwest China\n      Xinjiang\n      1612.65\n    \n    \n      331\n      2003\n      Northwest China\n      Xinjiang\n      1886.35\n    \n    \n      332\n      2004\n      Northwest China\n      Xinjiang\n      2209.09\n    \n    \n      333\n      2005\n      Northwest China\n      Xinjiang\n      2604.19\n    \n    \n      334\n      2006\n      Northwest China\n      Xinjiang\n      3045.26\n    \n    \n      335\n      2007\n      Northwest China\n      Xinjiang\n      3523.16\n    \n    \n      336\n      1996\n      Southwest China\n      Yunnan\n      1517.69\n    \n    \n      337\n      1997\n      Southwest China\n      Yunnan\n      1676.17\n    \n    \n      338\n      1998\n      Southwest China\n      Yunnan\n      1831.33\n    \n    \n      339\n      1999\n      Southwest China\n      Yunnan\n      1899.82\n    \n    \n      340\n      2000\n      Southwest China\n      Yunnan\n      2011.19\n    \n    \n      341\n      2001\n      Southwest China\n      Yunnan\n      2138.31\n    \n    \n      342\n      2002\n      Southwest China\n      Yunnan\n      2312.82\n    \n    \n      343\n      2003\n      Southwest China\n      Yunnan\n      2556.02\n    \n    \n      344\n      2004\n      Southwest China\n      Yunnan\n      3081.91\n    \n    \n      345\n      2005\n      Southwest China\n      Yunnan\n      3462.73\n    \n    \n      346\n      2006\n      Southwest China\n      Yunnan\n      3988.14\n    \n    \n      347\n      2007\n      Southwest China\n      Yunnan\n      4772.52\n    \n    \n      348\n      1996\n      East China\n      Zhejiang\n      4188.53\n    \n    \n      349\n      1997\n      East China\n      Zhejiang\n      4686.11\n    \n    \n      350\n      1998\n      East China\n      Zhejiang\n      5052.62\n    \n    \n      351\n      1999\n      East China\n      Zhejiang\n      5443.92\n    \n    \n      352\n      2000\n      East China\n      Zhejiang\n      6141.03\n    \n    \n      353\n      2001\n      East China\n      Zhejiang\n      6898.34\n    \n    \n      354\n      2002\n      East China\n      Zhejiang\n      8003.67\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.7\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      18753.73\n    \n  \n\n\n\n\n\n#http://manpages.ubuntu.com/manpages/precise/man1/sqlite3.1.html\n\n\n# sqlite3 fiscal_data.db\n\n# create table memos(text, priority INTEGER);\n# insert into memos values('example 1', 10);\n# insert into memos values('example 2', 100);\n# select * from memos;\n\n# sqlite3 -line fiscal_data.db 'select * from memos where priority > 20;'"
  },
  {
    "objectID": "posts/2020-10-14-dask-xgboost-ht-ohe.html",
    "href": "posts/2020-10-14-dask-xgboost-ht-ohe.html",
    "title": "Using dask_ml.preprocessing and OneHotEncoder for categorical encoding with Dask",
    "section": "",
    "text": "import numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine(\"sqlite:///fiscal.db\")\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\n#engine.execute(\"SELECT * FROM fiscal_data LIMIT 1\").fetchall()\n\n\nsql = \"\"\"\nSELECT year\n, region\n, province\n, gdp\n, fdi\n, it\n, specific\nFROM fiscal_table\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n\n\n\n\nClient\n\n  Scheduler: inproc://192.168.1.71/9390/1\n  Dashboard: http://192.168.1.71:8787/status\n\n\n\nCluster\n\n  Workers: 3\n  Cores: 6\n  Memory: 12.00 GB\n\n\n\n\n\n\n\nclient.restart()\n\n\n\n\n\nClient\n\n  Scheduler: inproc://192.168.1.71/9390/1\n  Dashboard: http://192.168.1.71:8787/status\n\n\n\nCluster\n\n  Workers: 3\n  Cores: 6\n  Memory: 12.00 GB\n\n\n\n\n\n\n\nfrom dask import dataframe as dd \n\n\nddf = dd.from_pandas(df, npartitions=5)\n\n\nprint(ddf)\n\nDask DataFrame Structure:\n                year  region province      gdp    fdi     it specific\nnpartitions=5                                                        \n0              int64  object   object  float64  int64  int64  float64\n72               ...     ...      ...      ...    ...    ...      ...\n...              ...     ...      ...      ...    ...    ...      ...\n288              ...     ...      ...      ...    ...    ...      ...\n359              ...     ...      ...      ...    ...    ...      ...\nDask Name: from_pandas, 5 tasks\n\n\n\nddf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.30\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n  \n\n\n\n\n\nclient.id\n\n'Client-0ac0cc94-0e22-11eb-a4ae-d71460f30774'\n\n\n\n# Selecting Features and Target\n\n\nfeat_list = [\"year\", \"fdi\"]\ncat_feat_list = [\"region\", \"province\"]\ntarget = [\"gdp\"]\n\n\nddf[\"year\"] = ddf[\"year\"].astype(int)\nddf[\"fdi\"] = ddf[\"fdi\"].astype(float)\nddf[\"gdp\"] = ddf[\"gdp\"].astype(float)\nddf[\"it\"] = ddf[\"it\"].astype(float)\n\n\n#OHE\nfrom dask_ml.preprocessing import OneHotEncoder\n\n\nddf = ddf.categorize(cat_feat_list)\n\n\nohe = OneHotEncoder(sparse=False)\n\n\nohe_ddf = ohe.fit_transform(ddf[cat_feat_list])\n\n\nfeat_list = feat_list + ohe_ddf.columns.tolist()\nfeat_list = [f for f in feat_list if f not in cat_feat_list]\n#client.close()\n\n\nddf_processed = (dd.concat([ddf,ohe_ddf], axis=1) [feat_list + target])\n\n\nddf_processed.compute()\n\n\n\n\n\n  \n    \n      \n      year\n      fdi\n      region_East China\n      region_North China\n      region_Southwest China\n      region_Northwest China\n      region_South Central China\n      region_Northeast China\n      province_Anhui\n      province_Beijing\n      ...\n      province_Shandong\n      province_Shanghai\n      province_Shanxi\n      province_Sichuan\n      province_Tianjin\n      province_Tibet\n      province_Xinjiang\n      province_Yunnan\n      province_Zhejiang\n      gdp\n    \n  \n  \n    \n      0\n      1996\n      50661.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2093.30\n    \n    \n      1\n      1997\n      43443.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2347.32\n    \n    \n      2\n      1998\n      27673.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2542.96\n    \n    \n      3\n      1999\n      26131.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2712.34\n    \n    \n      4\n      2000\n      31847.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      2902.09\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      498055.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      9705.02\n    \n    \n      356\n      2004\n      668128.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      11648.70\n    \n    \n      357\n      2005\n      772000.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      13417.68\n    \n    \n      358\n      2006\n      888935.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      15718.47\n    \n    \n      359\n      2007\n      1036576.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      1.0\n      18753.73\n    \n  \n\n360 rows × 39 columns\n\n\n\n\nclient.restart()\n\n\nclient.close()"
  },
  {
    "objectID": "posts/2020-10-13-dask-xgboost-fiscal-data.html",
    "href": "posts/2020-10-13-dask-xgboost-fiscal-data.html",
    "title": "Moving Dask XGboost with Fiscal Data, saving and loading Dask XGboost models",
    "section": "",
    "text": "import numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///fiscal.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nengine.execute(\"SELECT * FROM fiscal_table LIMIT 10\").fetchall()\n\n[(1996, 'East China', 'Anhui', 631930, 147002, 2093.3, 50661),\n (1997, 'East China', 'Anhui', 657860, 151981, 2347.32, 43443),\n (1998, 'East China', 'Anhui', 889463, 174930, 2542.96, 27673),\n (1999, 'East China', 'Anhui', 1227364, 285324, 2712.34, 26131),\n (2000, 'East China', 'Anhui', 1499110, 195580, 2902.09, 31847),\n (2001, 'East China', 'Anhui', 2165189, 250898, 3246.71, 33672),\n (2002, 'East China', 'Anhui', 2404936, 434149, 3519.72, 38375),\n (2003, 'East China', 'Anhui', 2815820, 619201, 3923.11, 36720),\n (2004, 'East China', 'Anhui', 3422176, 898441, 4759.3, 54669),\n (2005, 'East China', 'Anhui', 3874846, 898441, 5350.17, 69000)]\n\n\n\nsql = \"\"\"\nSELECT year\n, region\n, province\n, gdp\n, fdi\n, it\n, specific\nFROM fiscal_table\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.30\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      355\n      2003\n      East China\n      Zhejiang\n      9705.02\n      498055\n      2261631\n      391292.0\n    \n    \n      356\n      2004\n      East China\n      Zhejiang\n      11648.70\n      668128\n      3162299\n      656175.0\n    \n    \n      357\n      2005\n      East China\n      Zhejiang\n      13417.68\n      772000\n      2370200\n      656175.0\n    \n    \n      358\n      2006\n      East China\n      Zhejiang\n      15718.47\n      888935\n      2553268\n      1017303.0\n    \n    \n      359\n      2007\n      East China\n      Zhejiang\n      18753.73\n      1036576\n      2939778\n      844647.0\n    \n  \n\n360 rows × 7 columns\n\n\n\n\ndf.columns\n\nIndex(['year', 'region', 'province', 'gdp', 'fdi', 'it', 'specific'], dtype='object')\n\n\n\ndf.gdp.hist()\n\n<matplotlib.axes._subplots.AxesSubplot at 0x7f2677958128>\n\n\n\n\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n\n\n\n\nClient\n\n  Scheduler: inproc://192.168.1.71/13442/1\n  Dashboard: http://192.168.1.71:8787/status\n\n\n\nCluster\n\n  Workers: 3\n  Cores: 6\n  Memory: 12.00 GB\n\n\n\n\n\n\n\nclient.restart()\n\n\n\n\n\nClient\n\n  Scheduler: inproc://192.168.1.71/13442/1\n  Dashboard: http://192.168.1.71:8787/status\n\n\n\nCluster\n\n  Workers: 3\n  Cores: 6\n  Memory: 12.00 GB\n\n\n\n\n\n\n\nfrom dask import dataframe as dd \n\n\nddf = dd.from_pandas(df, npartitions=5)\n\n\nprint(ddf)\n\nDask DataFrame Structure:\n                year  region province      gdp    fdi     it specific\nnpartitions=5                                                        \n0              int64  object   object  float64  int64  int64  float64\n72               ...     ...      ...      ...    ...    ...      ...\n...              ...     ...      ...      ...    ...    ...      ...\n288              ...     ...      ...      ...    ...    ...      ...\n359              ...     ...      ...      ...    ...    ...      ...\nDask Name: from_pandas, 5 tasks\n\n\n\nddf.head()\n\n\n\n\n\n  \n    \n      \n      year\n      region\n      province\n      gdp\n      fdi\n      it\n      specific\n    \n  \n  \n    \n      0\n      1996\n      East China\n      Anhui\n      2093.30\n      50661\n      631930\n      147002.0\n    \n    \n      1\n      1997\n      East China\n      Anhui\n      2347.32\n      43443\n      657860\n      151981.0\n    \n    \n      2\n      1998\n      East China\n      Anhui\n      2542.96\n      27673\n      889463\n      174930.0\n    \n    \n      3\n      1999\n      East China\n      Anhui\n      2712.34\n      26131\n      1227364\n      285324.0\n    \n    \n      4\n      2000\n      East China\n      Anhui\n      2902.09\n      31847\n      1499110\n      195580.0\n    \n  \n\n\n\n\n\nclient.id\n\n'Client-e79fe0fe-0d59-11eb-b482-f9dc9eaa58ee'\n\n\n\nfeat_list = [\"year\", \"fdi\"]\ncat_feat_list = [\"region\", \"province\"]\ntarget = [\"gdp\"]\n\n\nddf[\"year\"] = ddf[\"year\"].astype(int)\nddf[\"fdi\"] = ddf[\"fdi\"].astype(float)\nddf[\"gdp\"] = ddf[\"gdp\"].astype(float)\nddf[\"it\"] = ddf[\"it\"].astype(float)\n# ddf[\"province\"] = ddf[\"province\"].astype(float)\n# ddf[\"region\"] = ddf[\"region\"].astype(float)\n\n\nx=ddf[feat_list].persist()\ny=ddf[target].persist()\n\n\nx\n\n\nDask DataFrame Structure:\n\n\n\n  \n    \n      \n      year\n      fdi\n    \n    \n      npartitions=5\n      \n      \n    \n  \n  \n    \n      0\n      int64\n      float64\n    \n    \n      72\n      ...\n      ...\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      288\n      ...\n      ...\n    \n    \n      359\n      ...\n      ...\n    \n  \n\n\nDask Name: getitem, 5 tasks\n\n\n\ny.compute()\n\n\n\n\n\n  \n    \n      \n      gdp\n    \n  \n  \n    \n      0\n      2093.30\n    \n    \n      1\n      2347.32\n    \n    \n      2\n      2542.96\n    \n    \n      3\n      2712.34\n    \n    \n      4\n      2902.09\n    \n    \n      ...\n      ...\n    \n    \n      355\n      9705.02\n    \n    \n      356\n      11648.70\n    \n    \n      357\n      13417.68\n    \n    \n      358\n      15718.47\n    \n    \n      359\n      18753.73\n    \n  \n\n360 rows × 1 columns\n\n\n\n\nprint(x.shape,y.shape)\n\n(Delayed('int-97d0cf00-db85-425b-a0d2-08297142db86'), 2) (Delayed('int-01cdae78-a995-48c1-9b93-277a008ad57a'), 1)\n\n\n\nx.count().compute()\n\nyear    360\nfdi     360\ndtype: int64\n\n\n\nfrom dask_ml.xgboost import XGBRegressor\n\n\nXGBR = XGBRegressor()\n\n\n%%time\nXGBR_model = XGBR.fit(x,y)\n\nCPU times: user 54.2 s, sys: 1.02 s, total: 55.2 s\nWall time: 18.4 s\n\n\n\nXGBR_model\n\nXGBRegressor()\n\n\n\nXGBR_model.save_model('fiscal_model')\n\n\nXGBR_model.load_model('fiscal_model')\n\n[08:43:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror."
  }
]